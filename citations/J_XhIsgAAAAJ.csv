Graph structure in the web,Andrei Broder; Ravi Kumar; Farzin Maghoul; Prabhakar Raghavan; Sridhar Rajagopalan; Raymie Stata; Andrew Tomkins; Janet Wiener,Abstract The study of the Web as a graph is not only fascinating in its own right; but alsoyields valuable insight into Web algorithms for crawling; searching and communitydiscovery; and the sociological phenomena which characterize its evolution. We report onexperiments on local and global properties of the Web graph using two AltaVista crawlseach with over 200 million pages and 1.5 billion links. Our study indicates that themacroscopic structure of the Web is considerably more intricate than suggested by earlierexperiments on a smaller scale.,Computer networks,2000,3786
Image indexing using color correlograms,Jing Huang; S Ravi Kumar; Mandar Mitra; Wei-Jing Zhu; Ramin Zabih,We define a new image feature called the color correlogram and use it for image indexingand comparison. This feature distills the spatial correlation of colors; and is both effectiveand inexpensive for content-based image retrieval. The correlogram robustly tolerates largechanges in appearance and shape caused by changes in viewing positions; camera zooms;etc. Experimental evidence suggests that this new feature outperforms not only thetraditional color histogram method but also the recently proposed histogram refinementmethods for image indexing/retrieval.,Computer Vision and Pattern Recognition; 1997. Proceedings.; 1997 IEEE Computer Society Conference on,1997,2155
Pig latin: a not-so-foreign language for data processing,Christopher Olston; Benjamin Reed; Utkarsh Srivastava; Ravi Kumar; Andrew Tomkins,Abstract There is a growing need for ad-hoc analysis of extremely large data sets; especiallyat internet companies where innovation critically depends on being able to analyzeterabytes of data collected every day. Parallel database products; eg; Teradata; offer asolution; but are usually prohibitively expensive at this scale. Besides; many of the peoplewho analyze this data are entrenched procedural programmers; who find the declarative;SQL style to be unnatural. The success of the more procedural map-reduce programmingmodel; and its associated scalable implementations on commodity hardware; is evidence ofthe above. However; the map-reduce paradigm is too low-level and rigid; and leads to agreat deal of custom user code that is hard to maintain; and reuse. We describe a newlanguage called Pig Latin that we have designed to fit in a sweet spot between the …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,2082
Rank aggregation methods for the web,Cynthia Dwork; Ravi Kumar; Moni Naor; D Sivakumar,ABSTRACT We consider the problem of combining ranking results from various sources. Inthe context of the Web; the main applications include building meta-search engines;combining ranking functions; selecting documents based on multiple criteria; and improvingsearch precision through word associations. We develop a set of techniques for the rankaggregation problem and compare their performance to that of well-known methods. Aprimary goal of our work is to design rank aggregation techniques that can effectivelycombat\spam;" a serious problem in Web searches. Experiments show that our methods aresimple; efficient; and effective.,Proceedings of the 10th international conference on World Wide Web,2001,1671
Structure and evolution of online social networks,Ravi Kumar; Jasmine Novak; Andrew Tomkins,Abstract In this work; we consider the evolution of structure within large online socialnetworks. We present a series of measurements of two large real networks; one from thefriend relation within the Flickr photo sharing application and the other from Yahoo! s 360social network. These networks together comprise in excess of 5 million people and 10million friendship links; and they are annotated with metadata capturing the time of everyevent in the life of the network. We show that these networks may be segmented into threeregions: singletons; who do not participate in the network; isolated communities; whichoverwhelmingly display star structure; and a giant component anchored by a well-connectedcore region that persists even in the absence of stars. We give a detailed characterization ofthe structure and evolution of these regions. We also present a simple model of network …,*,2010,1577
Propagation of trust and distrust,Ramanthan Guha; Ravi Kumar; Prabhakar Raghavan; Andrew Tomkins,Abstract A (directed) network of people connected by ratings or trust scores; and a model forpropagating those trust scores; is a fundamental building block in many of today's mostsuccessful e-commerce and recommendation systems. We develop a framework of trustpropagation schemes; each of which may be appropriate in certain circumstances; andevaluate the schemes on a large trust network consisting of 800K trust scores expressedamong 130K people. We show that a small number of expressed trusts/distrust perindividual allows us to predict trust between any two people in the system with highaccuracy. Our work appears to be the first to incorporate distrust in a computational trustpropagation setting.,Proceedings of the 13th international conference on World Wide Web,2004,1573
Trawling the Web for emerging cyber-communities,Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Andrew Tomkins,Abstract The Web harbors a large number of communities—groups of content-creatorssharing a common interest—each of which manifests itself as a set of interlinked Web pages.Newgroups and commercial Web directories together contain of the order of 20;000 suchcommunities; our particular interest here is on emerging communities—those that have littleor no representation in such fora. The subject of this paper is the systematic enumeration ofover 100;000 such emerging communities from a Web crawl: we call our process trawling.We motivate a graph-theoretic approach to locating such communities; and describe thealgorithms; and the algorithmic engineering necessary to find structures that subscribe tothis notion; the challenges in handling such a huge data set; and the results of ourexperiment.,Computer networks,1999,1387
The web as a graph: Measurements; models; and methods,Jon M Kleinberg; Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Andrew S Tomkins,Abstract The pages and hyperlinks of the World-Wide Web may be viewed as nodes andedges in a directed graph. This graph is a fascinating object of study: it has several hundredmillion nodes today; over a billion links; and appears to grow exponentially with time. Thereare many reasons—mathematical; sociological; and commercial—for studying the evolutionof this graph. In this paper we begin by describing two algorithms that operate on the Webgraph; addressing problems from Web search and automatic community discovery. We thenreport a number of measurements and properties of this graph that manifested themselvesas we ran these algorithms on the Web. Finally; we observe that traditional random graphmodels do not explain these observations; and we propose a new family of random graphmodels. These models point to a rich new sub-field of the study of random graphs; and …,International Computing and Combinatorics Conference,1999,1214
Comparing top k lists,Ronald Fagin; Ravi Kumar; Dakshinamurthi Sivakumar,Motivated by several applications; we introduce various distance measures between" top klists." Some of these distance measures are metrics; while others are not. For each of theselatter distance measures; we show that they are" almost" a metric in the following twoseemingly unrelated aspects:(i) they satisfy a relaxed version of the polygonal (hence;triangle) inequality; and (ii) there is a metric with positive constant multiples that bound ourmeasure above and below. This is not a coincidence---we show that these two notions ofalmost being a metric are the same. Based on the second notion; we define two distancemeasures to be equivalent if they are bounded above and below by constant multiples ofeach other. We thereby identify a large and robust equivalence class of distance measures.Besides the applications to the task of identifying good notions of (dis) similarity between …,SIAM Journal on discrete mathematics,2003,911
Mining the Web's link structure,Soumen Chakrabarti; Byron E Dom; S Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Andrew Tomkins; David Gibson; Jon Kleinberg,The Web is a hypertext body of approximately 300 million pages that continues to grow atroughly a million pages per day. Page variation is more prodigious than the data's raw scale:taken as a whole; the set of Web pages lacks a unifying structure and shows far moreauthoring style and content variation than that seen in traditional text document collections.This level of complexity makes an" off-the-shelf" database management and informationretrieval solution impossible. To date; index based search engines for the Web have beenthe primary tool by which users search for information. Such engines can build giant indicesthat let you quickly retrieve the set of all Web pages containing a given word or string.Experienced users can make effective use of such engines for tasks that can be solved bysearching for tightly constrained key words and phrases. These search engines are …,Computer,1999,902
Geographic routing in social networks,David Liben-Nowell; Jasmine Novak; Ravi Kumar; Prabhakar Raghavan; Andrew Tomkins,Abstract We live in a “small world;” where two arbitrary people are likely connected by ashort chain of intermediate friends. With scant information about a target individual; peoplecan successively forward a message along such a chain. Experimental studies have verifiedthis property in real social networks; and theoretical models have been advanced to explainit. However; existing theoretical models have not been shown to capture behavior in real-world social networks. Here; we introduce a richer model relating geography and social-network friendship; in which the probability of befriending a particular person is inverselyproportional to the number of closer people. In a large social network; we show that one-third of the friendships are independent of geography and the remainder exhibit theproposed relationship. Further; we prove analytically that short chains can be discovered …,Proceedings of the National Academy of Sciences of the United States of America,2005,824
On the bursty evolution of blogspace,Ravi Kumar; Jasmine Novak; Prabhakar Raghavan; Andrew Tomkins,Abstract We propose two new tools to address the evolution of hyperlinked corpora. First; wedefine time graphs to extend the traditional notion of an evolving directed graph; capturinglink creation as a point phenomenon in time. Second; we develop definitions and algorithmsfor time-dense community tracking; to crystallize the notion of community evolution. Wedevelop these tools in the context of Blogspace; the space of weblogs (or blogs). Our studyinvolves approximately 750 K links among 25 K blogs. We create a time graph on theseblogs by an automatic analysis of their internal time stamps. We then study the evolution ofconnected component structure and microscopic community structure in this time graph. Weshow that Blogspace underwent a transition behavior around the end of 2001; and has beenrapidly expanding; not just in metrics of scale but also in metrics of community structure …,World Wide Web,2005,795
Stochastic models for the web graph,Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; D Sivakumar; Andrew Tomkins; Eli Upfal,The Web may be viewed as a directed graph each of whose vertices is a static HTML Webpage; and each of whose edges corresponds to a hyperlink from one Web page to another.We propose and analyze random graph models inspired by a series of empiricalobservations on the Web. Our graph models differ from the traditional G/sub n; p/models intwo ways: 1. Independently chosen edges do not result in the statistics (degree distributions;clique multitudes) observed on the Web. Thus; edges in our model are statisticallydependent on each other. 2. Our model introduces new vertices in the graph as timeevolves. This captures the fact that the Web is changing with time. Our results are two fold:we show that graphs generated using our model exhibit the statistics observed on the Webgraph; and additionally; that natural graph models proposed earlier do not exhibit them …,Foundations of Computer Science; 2000. Proceedings. 41st Annual Symposium on,2000,789
Microscopic evolution of social networks,Jure Leskovec; Lars Backstrom; Ravi Kumar; Andrew Tomkins,Abstract We present a detailed study of network evolution by analyzing four large onlinesocial networks with full temporal information about node and edge arrivals. For the first timeat such a large scale; we study individual node arrival and edge creation processes thatcollectively lead to macroscopic properties of networks. Using a methodology based on themaximum-likelihood principle; we investigate a wide variety of network formation strategies;and show that edge locality plays a critical role in evolution of networks. Our findingssupplement earlier network models based on the inherently non-local preferentialattachment. Based on our observations; we develop a complete model of network evolution;where nodes arrive at a prespecified rate and select their lifetimes. Each node thenindependently initiates edges according to a" gap" process; selecting a destination for …,Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,2008,725
Influence and correlation in social networks,Aris Anagnostopoulos; Ravi Kumar; Mohammad Mahdian,Abstract In many online social systems; social ties between users play an important role indictating their behavior. One of the ways this can happen is through social influence; thephenomenon that the actions of a user can induce his/her friends to behave in a similar way.In systems where social influence exists; ideas; modes of behavior; or new technologies candiffuse through the network like an epidemic. Therefore; identifying and understandingsocial influence is of tremendous interest from both analysis and design points of view. Thisis a difficult task in general; since there are factors such as homophily or unobservedconfounding variables that can induce statistical correlation between the actions of friends ina social network. Distinguishing influence from these is essentially the problem ofdistinguishing correlation from causality; a notoriously hard statistical problem.,Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,2008,654
Evolutionary clustering,Deepayan Chakrabarti; Ravi Kumar; Andrew Tomkins,Abstract We consider the problem of clustering data over time. An evolutionary clusteringshould simultaneously optimize two potentially conflicting criteria: first; the clustering at anypoint in time should remain faithful to the current data as much as possible; and second; theclustering should not shift dramatically from one timestep to the next. We present a genericframework for this problem; and discuss evolutionary versions of two widely-used clusteringalgorithms within this framework: k-means and agglomerative hierarchical clustering. Weextensively evaluate these algorithms on real data sets and show that our algorithms cansimultaneously attain both high accuracy in capturing today's data; and high fidelity inreflecting yesterday's clustering.,Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,2006,570
Structure and evolution of blogspace,Ravi Kumar; Jasmine Novak; Prabhakar Raghavan; Andrew Tomkins,Blogs constitute a remarkable artifact of the Web. Most people think of them as Web pages withreverse chronological sequences of dated entries; usually with sidebars of profile informationand usually maintained and published with the help of a popular blog authoring tool. They tendto be quirky; highly personal; typically read by repeat visitors; and interwoven into a networkof tight-knit but active communities. We refer to the collection of blogs and all their links asblogspace. By analyzing the structure and content of more than one million blogs worldwide;we've now unearthed some fascinating insights into blogger behavior … An analysis of blogspacemust reflect at least two distinct perspectives: the temporal (how it evolves over time) and thespatial (how bloggers congregate in terms of interests and demographics). Studying them requiresdata sets with distinctive characteristics; in particular; the temporal needs a …,Communications of the ACM,2004,520
An information statistics approach to data stream and communication complexity,Ziv Bar-Yossef; Thathachar S Jayram; Ravi Kumar; D Sivakumar,Abstract We present a new method for proving strong lower bounds in communicationcomplexity. This method is based on the notion of the conditional information complexity of afunction which is the minimum amount of information about the inputs that has to berevealed by a communication protocol for the function. While conditional informationcomplexity is a lower bound on communication complexity; we show that it also admits adirect sum theorem. Direct sum decomposition reduces our task to that of proving conditionalinformation complexity lower bounds for simple problems (such as the AND of two bits). Forthe latter; we develop novel techniques based on Hellinger distance and its generalizations.Our paradigm leads to two main results:(1) An improved lower bound for the multi-party set-disjointness problem in the general communication complexity model; and a nearly …,Journal of Computer and System Sciences,2004,481
The predictive power of online chatter,Daniel Gruhl; Ramanathan Guha; Ravi Kumar; Jasmine Novak; Andrew Tomkins,Abstract An increasing fraction of the global discourse is migrating online in the form ofblogs; bulletin boards; web pages; wikis; editorials; and a dizzying array of new collaborativetechnologies. The migration has now proceeded to the point that topics reflecting certainindividual products are sufficiently popular to allow targeted online tracking of the ebb andflow of chatter around these topics. Based on an analysis of around half a million sales rankvalues for 2;340 books over a period of four months; and correlating postings in blogs;media; and web pages; we are able to draw several interesting conclusions. First; carefullyhand-crafted queries produce matching postings whose volume predicts sales ranks.Second; these queries can be automatically generated in many cases. And third; eventhough sales rank motion might be difficult to predict in general; algorithmic predictors can …,Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,2005,459
A sieve algorithm for the shortest lattice vector problem,Miklós Ajtai; Ravi Kumar; Dandapani Sivakumar,Abstract We present a randomized 2^{O (n)} time algorithm to compute a shortest non-zerovector in an n-dimensional rational lattice. The best known time upper bound for thisproblem was 2^{O (n\log n)} first given by Kannan [7] in 1983. We obtain severalconsequences of this algorithm for related problems on lattices and codes; including animprovement for polynomial time approximations to the shortest vector problem. In thisimprovement we gain a factor of log log n in the exponent of the approximating factor.,Proceedings of the thirty-third annual ACM symposium on Theory of computing,2001,427
Spatial color indexing and applications,Jing Huang; S Ravi Kumar; Mandar Mitra; Wei-Jing Zhu; Ramin Zabih,Abstract We define a new image feature called the color correlogram and use it for imageindexing and comparison. This feature distills the spatial correlation of colors and whencomputed efficiently; turns out to be both effective and inexpensive for content-based imageretrieval. The correlogram is robust in tolerating large changes in appearance and shapecaused by changes in viewing position; camera zoom; etc. Experimental evidence showsthat this new feature outperforms not only the traditional color histogram method but also therecently proposed histogram refinement methods for image indexing/retrieval. We alsoprovide a technique to cut down the storage requirement of the correlogram so that it is thesame as that of histograms; with only negligible performance penalty compared to theoriginal correlogram. We also suggest the use of color correlogram as a generic indexing …,International Journal of Computer Vision,1999,418
Efficient similarity search and classification via rank aggregation,Ronald Fagin; Ravi Kumar; Dandapani Sivakumar,Abstract We propose a novel approach to performing efficient similarity search andclassification in high dimensional data. In this framework; the database elements are vectorsin a Euclidean space. Given a query vector in the same space; the goal is to find elements ofthe database that are similar to the query. In our approach; a small number of independent"voters" rank the database elements based on similarity to the query. These rankings arethen combined by a highly efficient aggregation algorithm. Our methodology leads both totechniques for computing approximate nearest neighbors and to a conceptually richalternative to nearest neighbors. One instantiation of our methodology is as follows. Eachvoter projects all the vectors (database elements and the query) on a random line (differentfor each voter); and ranks the database elements based on the proximity of the …,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,416
Extracting large-scale knowledge bases from the web,Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Andrew Tomkins,Abstract The subject of this paper is the creation of knowledge bases by enumerating andorganizing all web occurrences of certain subgraphs. We focus on subgraphs that aresignatures of web phenomena such as tightly-focused topic communities; webrings;taxonomy trees; keiretsus; etc. For instance; the signature of a webring is a central page withbidirectional links to a number of other pages. We develop novel algorithms for suchenumeration problems. A key technical contribution is the development of a model for theevolution of the web graph; based on experimental observations derived from a snapshot ofthe web. We argue that our algorithms run efficiently in this model; and use the model toexplain some statistical phenomena on the web that emerged during our experiments.Finally; we describe the design and implementation of Campfire; a knowledge base of …,VLDB,1999,379
Counting distinct elements in a data stream,Ziv Bar-Yossef; TS Jayram; Ravi Kumar; D Sivakumar; Luca Trevisan,1 Computer Science Division; Univ. of California at Berkeley; Berkeley; CA 94720.zivi@cs.berkeley.edu 2 IBM Almaden Research Center; 650 Harry Road; San Jose; CA95120. {jayram; ravi; siva}@almaden.ibm.com 3 Computer Science Division; Univ. of Californiaat Berkeley; Berkeley; CA 94720. luca@cs.berkeley.edu … Abstract. We present three algorithmsto count the number of distinct elements in a data stream to within a factor of 1 ± ϵ. Our algorithmsimprove upon known algorithms for this problem; and offer a spectrum of time/spacetradeoffs … Let a = a1;... ;an be a sequence of n elements from the domain [m] = {1;... ;m}. Thezeroth-frequency moment of this sequence is the number of distinct elements that occur in thesequence and is denoted F0 = F0(a). In this paper we present three space- and time-efficientalgorithms for approximating F0 in the data stream model. In the data stream model; an …,International Workshop on Randomization and Approximation Techniques in Computer Science,2002,366
The Web as a graph,Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Dandapani Sivakumar; Andrew Tompkins; Eli Upfal,Abstract The pages and hyperlinks of the World-Wide Web may be viewed as nodes andedges in a directed graph. This graph has about a billion nodes today; several billion links;and appears to grow exponentially with time. There are many reasons—mathematical;sociological; and commercial—for studying the evolution of this graph. We first review a setof algorithms that operate on the Web graph; addressing problems from Web search;automatic community discovery; and classification. We then recall a number ofmeasurements and properties of the Web graph. Noting that traditional random graphmodels do not explain these observations; we propose a new family of random graphmodels.,Proceedings of the nineteenth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2000,366
Visualizing tags over time,Micah Dubinko; Ravi Kumar; Joseph Magnani; Jasmine Novak; Prabhakar Raghavan; Andrew Tomkins,Abstract We consider the problem of visualizing the evolution of tags within the Flickr (flickr.com) online image sharing community. Any user of the Flickr service may append a tag toany photo in the system. Over the past year; users have on average added over a milliontags each week. Understanding the evolution of these tags over time is therefore achallenging task. We present a new approach based on a characterization of the mostinteresting tags associated with a sliding interval of time. An animation provided via Flash ina Web browser allows the user to observe and interact with the interesting tags as theyevolve over time. New algorithms and data structures are required to support the efficientgeneration of this visualization. We combine a novel solution to an interval covering problemwith extensions to previous work on score aggregation in order to create an efficient …,ACM Transactions on the Web (TWEB),2007,364
Discovering large dense subgraphs in massive graphs,David Gibson; Ravi Kumar; Andrew Tomkins,Abstract We present a new algorithm for finding large; dense subgraphs in massive graphs.Our algorithm is based on a recursive application of fingerprinting via shingles; and isextremely efficient; capable of handling graphs with tens of billions of edges on a singlemachine with modest resources. We apply our algorithm to characterize the large; densesubgraphs of a graph showing connections between hosts on the World Wide Web; thisgraph contains over 50M hosts and 11B edges; gathered from 2.1 B web pages. Wemeasure the distribution of these dense subgraphs and their evolution over time. We showthat more than half of these hosts participate in some dense subgraph found by the analysis.There are several hundred giant dense subgraphs of at least ten thousand hosts; twothousand dense subgraphs at least a thousand hosts; and almost 64K dense subgraphs …,Proceedings of the 31st international conference on Very large data bases,2005,354
Self-similarity in the web,Stephen Dill; Ravi Kumar; Kevin S Mccurley; Sridhar Rajagopalan; D Sivakumar; Andrew Tomkins,Abstract Algorithmic tools for searching and mining the Web are becoming increasinglysophisticated and vital. In this context; algorithms that use and exploit structural informationabout the Web perform better than generic methods in both efficiency and reliability. Wepresent an extensive characterization of the graph structure of the Web; with a view toenabling high-performance applications that make use of this structure. In particular; weshow that the Web emerges as the outcome of a number of essentially independentstochastic processes that evolve at various scales. A striking consequence of this scaleinvariance is that the structure of the Web is" fractal"---cohesive subregions display the samecharacteristics as the Web at large. An understanding of this underlying fractal nature istherefore applicable to designing data services across multiple domains and scales. We …,ACM Transactions on Internet Technology (TOIT),2002,350
Scalable k-means++,Bahman Bahmani; Benjamin Moseley; Andrea Vattani; Ravi Kumar; Sergei Vassilvitskii,Abstract Over half a century old and showing no signs of aging; k-means remains one of themost popular data processing algorithms. As is well-known; a proper initialization of k-means is crucial for obtaining a good final solution. The recently proposed k-means++initialization algorithm achieves this; obtaining an initial set of centers that is provably closeto the optimum solution. A major downside of the k-means++ is its inherent sequentialnature; which limits its applicability to massive data: one must make k passes over the datato find a good initial set of centers. In this work we show how to drastically reduce thenumber of passes needed to obtain; in parallel; a good initialization. This is unlike prevailingefforts on parallelizing k-means that have mostly focused on the post-initialization phases ofk-means. We prove that our proposed initialization algorithm k-means|| obtains a nearly …,Proceedings of the VLDB Endowment,2012,323
Comparing partial rankings,Ronald Fagin; Ravi Kumar; Mohammad Mahdian; D Sivakumar; Erik Vee,Abstract. We provide a comprehensive picture of how to compare partial rankings; that is; rankingsthat allow ties. We propose several metrics to compare partial rankings and prove that they arewithin constant multiples of each other … Key words. partial ranking; bucket order;permutation; metric … 1. Introduction. The study of metrics on permutations (ie; full rankings)is classical and several well-studied metrics are known [10; 22]; including the Kendall tau distanceand the Spearman footrule distance. The rankings encountered in practice; however; often haveties (hence the name partial rankings); and metrics on such rankings are much less studied.Aside from its purely mathematical interest; the problem of defining metrics on partial rankingsis valuable in a number of applications. For example the rank aggre- gation problem for partialrankings arises naturally in multiple settings; including in online commerce; where users …,SIAM Journal on Discrete Mathematics,2006,304
Reductions in streaming algorithms; with an application to counting triangles in graphs,Ziv Bar-Yossef; Ravi Kumar; D Sivakumar,Abstract We introduce reductions in the streaming model as a tool in the design of streamingalgorithms. We develop the concept of list-efficient streaming algorithms that are essential tothe design of efficient streaming algorithms through reductions. Our results include a suite oflist-efficient streaming algorithms for basic statistical primitives. Using the reductionparadigm along with these tools; we design streaming algorithms for approximately countingthe number of triangles in a graph presented as a stream. A specific highlight of our work isthe first algorithm for the number of distinct elements in a data stream that achieves arbitraryapproximation factors.(Independently; Trevisan [Tre01] has solved this problem via adifferent approach; our algorithm has the advantage of being list-efficient.),Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms,2002,291
On compressing social networks,Flavio Chierichetti; Ravi Kumar; Silvio Lattanzi; Michael Mitzenmacher; Alessandro Panconesi; Prabhakar Raghavan,Abstract Motivated by structural properties of the Web graph that support efficient datastructures for in memory adjacency queries; we study the extent to which a large networkcan be compressed. Boldi and Vigna (WWW 2004); showed that Web graphs can becompressed down to three bits of storage per edge; we study the compressibility of socialnetworks where again adjacency queries are a fundamental primitive. To this end; wepropose simple combinatorial formulations that encapsulate efficient compressibility ofgraphs. We show that some of the problems are NP-hard yet admit effective heuristics; someof which can exploit properties of social networks such as link reciprocity. Our extensiveexperiments show that social networks and the Web graph exhibit vastly differentcompressibility characteristics.,Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,2009,251
Spot-checkers,Funda Ergün; Sampath Kannan; S Ravi Kumar; Ronitt Rubinfeld; Mahesh Viswanathan,Abstract On Labor Day weekend; the highway patrol sets up spot-checks at random pointson the freeways with the intention of deterring a large fraction of motorists from drivingincorrectly. We explore a very similar idea in the context of program checking to ascertainwith minimal overhead that a program output is reasonably correct. Our model of spot-checking requires that the spot-checker must run asymptotically much faster than thecombined length of the input and output. We then show that the spot-checking model can beapplied to problems in a wide range of areas; including problems regarding graphs; sets;and algebra. In particular; we present spot-checkers for sorting; convex hull; elementdistinctness; set containment; set equality; total orders; and correctness of group and fieldoperations. All of our spot-checkers are very simple to state and rely on testing that the …,Journal of Computer and System Sciences,2000,235
Searching the workplace web,Ronald Fagin; Ravi Kumar; Kevin S McCurley; Jasmine Novak; D Sivakumar; John A Tomlin; David P Williamson,Abstract The social impact from the World Wide Web cannot be underestimated; buttechnologies used to build the Web are also revolutionizing the sharing of business andgovernment information within intranets. In many ways the lessons learned from the Internetcarry over directly to intranets; but others do not apply. In particular; the social forces thatguide the development of intranets are quite different; and the determination of a" goodanswer" for intranet search is quite different than on the Internet. In this paper we study theproblem of intranet search. Our approach focuses on the use of rank aggregation; andallows us to examine the effects of different heuristics on ranking of search results.,Proceedings of the 12th international conference on World Wide Web,2003,223
Coding constructions for blacklisting problems without computational assumptions,Ravi Kumar; Sridhar Rajagopalan; Amit Sahai,Abstract We consider the broadcast exclusion problem: how to transmit a message over abroadcast channel shared by N= 2 n users so that all but some specified coalition of kexcluded users can understand the contents of the message. Using error-correcting codes;and avoiding any computational assumptions in our constructions; we construct naturalschemes that completely avoid any dependence on n in the transmission overhead.Specifically; we construct:(i)(for illustrative purposes); a randomized scheme where theserver's storage is exponential (in n); but the transmission overhead is O (k); and each user'sstorage is O (kn);(ii) a scheme based on polynomials where the transmission overhead is O(kn) and each user's storage is O (kn); and (iii) a scheme using algebraic-geometric codeswhere the transmission overhead is O (k 2) and each user is required to store O (kn) keys …,Annual International Cryptology Conference,1999,205
A characterization of online browsing behavior,Ravi Kumar; Andrew Tomkins,Abstract In this paper; we undertake a large-scale study of online user behavior based onsearch and toolbar logs. We propose a new CCS taxonomy of pageviews consisting ofContent (news; portals; games; verticals; multimedia); Communication (email; socialnetworking; forums; blogs; chat); and Search (Web search; item search; multimedia search).We show that roughly half of all pageviews online are content; one-third arecommunications; and the remaining one-sixth are search. We then give further breakdownsto characterize the pageviews within each high-level category. We then study the extent towhich pages of certain types are revisited by the same user over time; and the mechanismsby which users move from page to page; within and across hosts; and within and acrosspage types. We consider robust schemes for assigning responsibility for a pageview to …,Proceedings of the 19th international conference on World wide web,2010,201
On the hardness of approximating multicut and sparsest-cut,Shuchi Chawla; Robert Krauthgamer; Ravi Kumar; Yuval Rabani; D Sivakumar,Abstract. We show that the Multicut; Sparsest-Cut; and Min-2CNF≡ Deletion problems areNP-hard to approximate within every constant factor; assuming the Unique GamesConjecture of Khot (2002). A quantitatively stronger version of the conjecture implies aninapproximability factor of Ω (\log\log n).,computational complexity,2006,195
Spatial variation in search engine queries,Lars Backstrom; Jon Kleinberg; Ravi Kumar; Jasmine Novak,Abstract Local aspects of Web search-associating Web content and queries with geography-is a topic of growing interest. However; the underlying question of how spatial variation ismanifested in search queries is still not well understood. Here we develop a probabilisticframework for quantifying such spatial variation; on complete Yahoo! query logs; we find thatour model is able to localize large classes of queries to within a few miles of their naturalcenters based only on the distribution of activity for the query. Our model provides not onlyan estimate of a query's geographic center; but also a measure of its spatial dispersion;indicating whether it has highly local interest or broader regional or national appeal. We alsoshow how variations on our model can track geographically shifting topics over time;annotate a map with each location's" distinctive queries"; and delineate the" spheres of …,Proceedings of the 17th international conference on World Wide Web,2008,191
Combining supervised learning with color correlograms for content-based image retrieval,Jing Huang; S Ravi Kumar; Mandar Mitra,ABSTRACT The paper addresses how relevance feedback can be used to improve theperformance of content-based image retrieval. We present two supervised learningmethods: learning the query and learning the metric. We combine the leaming methods withthe recently proposed color correlograms for image indexing/retrieval. Our results on a largeimage database of over 20;000 images suggest that these learning methods are quiteeffective for content-based image retrieval.,Proceedings of the fifth ACM international conference on Multimedia,1997,184
Generalized distances between rankings,Ravi Kumar; Sergei Vassilvitskii,Abstract Spearman's footrule and Kendall's tau are two well established distances betweenrankings. They; however; fail to take into account concepts crucial to evaluating a result setin information retrieval: element relevance and positional information. That is; changing therank of a highly-relevant document should result in a higher penalty than changing the rankof an irrelevant document; a similar logic holds for the top versus the bottom of the resultordering. In this work; we extend both of these metrics to those with position and elementweights; and show that a variant of the Diaconis-Graham inequality still holds-thegeneralized two measures remain within a constant factor of each other for all permutations.We continue by extending the element weights into a distance metric between elements. Forexample; in search evaluation; swapping the order of two nearly duplicate results should …,Proceedings of the 19th international conference on World wide web,2010,181
Recommendation systems: A probabilistic analysis,Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Andrew Tomkins,Abstract A recommendation system tracks past actions of a group of users to makerecommendations to individual members of the group. The growth of computer-mediatedmarketing and commerce has led to increased interest in such systems. We introduce asimple analytical framework for recommendation systems; including a basis for defining theutility of such a system. We perform probabilistic analyses of algorithms within thisframework. These analyses yield insights into how much utility can be derived fromknowledge of past user actions.,Journal of Computer and System Sciences,2001,167
I know what you did last summer: query logs and user privacy,Rosie Jones; Ravi Kumar; Bo Pang; Andrew Tomkins,Abstract We investigate the subtle cues to user identity that may be exploited in attacks onthe privacy of users in web search query logs. We study the application of simple classifiersto map a sequence of queries into the gender; age; and location of the user issuing thequeries. We then show how these classifiers may be carefully combined at multiplegranularities to map a sequence of queries into a set of candidate users that is 300-600times smaller than random chance would allow. We show that this approach remainsaccurate even after removing personally identifiable information such as names/numbers orlimiting the size of the query log. We also present a new attack in which a real-worldacquaintance of a user attempts to identify that user in a large query log; using personalinformation. We show that combinations of small pieces of information about terms a user …,Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,2007,164
Hypersearching the web,Soumen Chakrabarti,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,http://www. sciam. com/1999/0699issue/0699raghavan. html,1999,155
Sic transit gloria telae: towards an understanding of the web's decay,Ziv Bar-Yossef; Andrei Z Broder; Ravi Kumar; Andrew Tomkins,Abstract The rapid growth of the web has been noted and tracked extensively. Recentstudies have however documented the dual phenomenon: web pages have small half lives;and thus the web exhibits rapid death as well. Consequently; page creators are faced withan increasingly burdensome task of keeping links up-to-date; and many are falling behind.In addition to just individual pages; collections of pages or even entire neighborhoods of theweb exhibit significant decay; rendering them less effective as information resources. Suchneighborhoods are identified only by frustrated searchers; seeking a way out of these staleneighborhoods; back to more up-to-date sections of the web; measuring the decay of a pagepurely on the basis of dead links on the page is too naive to reflect this frustration. In thispaper we formalize a strong notion of a decay measure and present algorithms for …,Proceedings of the 13th international conference on World Wide Web,2004,150
Searching with context,Reiner Kraft; Chi Chao Chang; Farzin Maghoul; Ravi Kumar,Abstract Contextual search refers to proactively capturing the information need of a user byautomatically augmenting the user query with information extracted from the search context;for example; by using terms from the web page the user is currently browsing or a file theuser is currently editing. We present three different algorithms to implement contextualsearch for the Web. The first; it query rewriting (QR); augments each query with appropriateterms from the search context and uses an off-the-shelf web search engine to answer thisaugmented query. The second; rank-biasing (RB); generates a representation of the contextand answers queries using a custom-built search engine that exploits this representation.The third; iterative filtering meta-search (IFM); generates multiple subqueries based on theuser query and appropriate terms from the search context; uses an off-the-shelf search …,Proceedings of the 15th international conference on World Wide Web,2006,147
Automatic hierarchical color image classification,Jing Huang; S Ravi Kumar; Ramin Zabih,Abstract Organizing images into semantic categories can be extremely useful for content-based image retrieval and image annotation. Grouping images into semantic classes is adifficult problem; however. Image classification attempts to solve this hard problem by usinglow-level image features. In this paper; we propose amethod for hierarchical classification ofimages via supervised learning. This scheme relies on using a good low-level feature andsubsequently performing feature-space reconfiguration using singular value decompositionto reduce noise and dimensionality. We use the training data to obtain a hierarchicalclassification tree that can be used to categorize new images. Our experimental resultssuggest that this scheme not only performs better than standard nearest-neighbortechniques; but also has both storage and computational advantages.,EURASIP Journal on Applied Signal Processing,2003,147
Experiments in topic distillation,Soumen Chakrabarti; Byron Dom; David Gibson; S Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Andrew Tomkins,We consider the problem of topic distillation: given a broad topic; distill a small number ofhigh-quality web pages that are most representative of the topic. While the web growsexponentially with time; the amount of information that the human end-user can digestremains roughly constant. Our goal is thus not to index; search or classify all the pages thatare possibly relevant to a topic; but only the most authoritative information on the requestedsubject. We describe recent experiments comparing topic resource lists compiled by ourCLEVER system to Yahoo! and Altavista. Our results show that for distillation; CLEVERperforms substantially better than Altavista; and surprisingly; typically performs better thanthe hand-built resources at Yahoo.,ACM SIGIR workshop on Hypertext Information Retrieval on the Web,1998,147
The complexity of approximating the entropy,Tugkan Batu; Sanjoy Dasgupta; Ravi Kumar; Ronitt Rubinfeld,We consider the problem of approximating the entropy of a discrete distribution underseveral different models of oracle access to the distribution. In the evaluation oracle model;the algorithm is given access to the explicit array of probabilities specifying the distribution.In this model; linear time in the size of the domain is both necessary and sufficient forapproximating the entropy. In the generation oracle model; the algorithm has access only toindependent samples from the distribution. In this case; we show that a γ-multiplicativeapproximation to the entropy can be obtained in O(n^(1+η)/γ^2\logn) time for distributionswith entropy Ω(γ/η); where n is the size of the domain of the distribution and η is an arbitrarilysmall positive constant. We show that this model does not permit a multiplicativeapproximation to the entropy in general. For the class of distributions to which our upper …,SIAM Journal on Computing,2005,141
A note on the limits of collusion-resistant watermarks,Funda Ergun; Joe Kilian; Ravi Kumar,Abstract In one proposed use of digital watermarks; the owner of a document D sells slightlydifferent documents; D 1; D 2;... to each buyer; if a buyer posts his/her document D i to theweb; the owner can identify the source of the leak. More general attacks are howeverpossible in which k buyers create some composite document D*; the goal of the owner is toidentify at least one of the conspirators. We show; for a reasonable model of digitalwatermarks; fundamental limits on their efficacy against collusive attacks. In particular; if theeffective document length is n; then at most O (n/ln n adversaries can defeat anywatermarking scheme. Our attack is; in the theoretical model; oblivious to the watermarkingscheme being used; in practice; it uses very little information about the watermarkingscheme. Thus; using a proprietary system seems to give only a very weak defense.,International Conference on the Theory and Applications of Cryptographic Techniques,1999,138
Testing random variables for independence and identity,Tugkan Batu; Eldar Fischer; Lance Fortnow; Ravi Kumar; Ronitt Rubinfeld; Patrick White,Given access to independent samples of a distribution A over [n]/spl times/[m]; we show howto test whether the distributions formed by projecting A to each coordinate are independent;ie; whether A is/spl epsi/-close in the L/sub 1/norm to the product distribution A/sub 1//spltimes/A/sub 2/for some distributions A/sub 1/over [n] and A/sub 2/over [m]. The samplecomplexity of our test is O/spl tilde/(n/sup 2/3/m/sup 1/3/poly (/spl epsi//sup-1/)); assumingwithout loss of generality that m/spl les/n. We also give a matching lower bound; up to poly(log n;/spl epsi//sup-1/) factors. Furthermore; given access to samples of a distribution X over[n]; we show how to test if X is/spl epsi/-close in L/sub 1/norm to an explicitly specifieddistribution Y. Our test uses O/spl tilde/(n/sup 1/2/poly (/spl epsi//sup-1/)) samples; whichnearly matches the known tight bounds for the case when Y is uniform.,Foundations of Computer Science; 2001. Proceedings. 42nd IEEE Symposium on,2001,135
Preferential behavior in online groups,Lars Backstrom; Ravi Kumar; Cameron Marlow; Jasmine Novak; Andrew Tomkins,Abstract Online communities in the form of message boards; listservs; and newsgroupscontinue to represent a considerable amount of the social activity on the Internet. Every yearthousands of groups ourish while others decline into relative obscurity; likewise; millions ofmembers join a new community every year; some of whom will come to manage ormoderate the conversation while others simply sit by the sidelines and observe. Theseprocesses of group formation; growth; and dissolution are central in social science; and inan online venue they have ramifications for the design and development of communitysoftware In this paper we explore a large corpus of thriving online communities. Thesegroups vary widely in size; moderation and privacy; and cover an equally diverse set ofsubject matter. We present a broad range of descriptive statistics of these groups. Using …,Proceedings of the 2008 International Conference on Web Search and Data Mining,2008,130
Link evolution: Analysis and algorithms,Steve Chien; Cynthia Dwork; Ravi Kumar; Daniel R Simon; D Sivakumar,We anticipate that future web search techniques will exploit changes in web structure andcontent. As a first step in this direction; we examine the problem of integrating observedchanges in link structure into static hyperlink-based ranking computations. We present avery efficient algorithm to incrementally compute good approximations to Google'sPageRank [Brin and Page 98]; as links evolve. Our experiments reveal that this algorithm isboth fast and yields excellent approximations to PageRank; even in light of large changes tothe link structure. Our algorithm derives intuition and partial justification from a rigoroussensitivity analysis of Markov chains. Consider a regular Markov chain with stationaryprobability π; and suppose the transition probability into a state j is increased. We prove thatthis can only cause• π j to increase–adding a link to a site can only cause the stationary …,Internet Mathematics,2004,123
Densest subgraph in streaming and mapreduce,Bahman Bahmani; Ravi Kumar; Sergei Vassilvitskii,Abstract The problem of finding locally dense components of a graph is an importantprimitive in data analysis; with wide-ranging applications from community mining to spamdetection and the discovery of biological network modules. In this paper we present newalgorithms for finding the densest subgraph in the streaming model. For any ε> 0; ouralgorithms make O (log 1+ ε n) passes over the input and find a subgraph whose density isguaranteed to be within a factor 2 (1+ ε) of the optimum. Our algorithms are also easilyparallelizable and we illustrate this by realizing them in the MapReduce model. In additionwe perform extensive experimental evaluation on massive real-world graphs showing theperformance and scalability of our algorithms in practice.,Proceedings of the VLDB Endowment,2012,121
A graph-theoretic approach to webpage segmentation,Deepayan Chakrabarti; Ravi Kumar; Kunal Punera,Abstract We consider the problem of segmenting a webpage into visually and semanticallycohesive pieces. Our approach is based on formulating an appropriate optimization problemon weighted graphs; where the weights capture if two nodes in the DOM tree should beplaced together or apart in the segmentation; we present a learning framework to learnthese weights from manually labeled data in a principled manner. Our work is a significantdeparture from previous heuristic and rule-based solutions to the segmentation problem.The results of our empirical analysis bring out interesting aspects of our framework;including variants of the optimization problem and the role of learning.,Proceedings of the 17th international conference on World Wide Web,2008,120
A sparse johnson: Lindenstrauss transform,Anirban Dasgupta; Ravi Kumar; Tamás Sarlós,Abstract Dimension reduction is a key algorithmic tool with many applications includingnearest-neighbor search; compressed sensing and linear algebra in the streaming model. Inthis work we obtain a sparse version of the fundamental tool in dimension reduction--theJohnson-Lindenstrauss transform. Using hashing and local densification; we construct asparse projection matrix with just~ O (1/ε) non-zero entries per column. We also show amatching lower bound on the sparsity for a large class of projection matrices. Our boundsare somewhat surprising; given the known lower bounds of Ω (1/ε 2) both on the number ofrows of any projection matrix and on the sparsity of projection matrices generated by naturalconstructions. Using this; we achieve an~ O (1/ε) update time per non-zero element for a (1ε)-approximate projection; thereby substantially outperforming the~ O (1/ε 2) update time …,Proceedings of the forty-second ACM symposium on Theory of computing,2010,118
Page-level template detection via isotonic smoothing,Deepayan Chakrabarti; Ravi Kumar; Kunal Punera,Abstract We develop a novel framework for the page-level template detection problem. Ourframework is built on two main ideas. The first is theautomatic generation of training data fora classifier that; given apage; assigns a templateness score to every DOM node of the page.The second is the global smoothing of these per-node classifier scores bysolving aregularized isotonic regression problem; the latter follows from a simple yet powerfulabstraction of templateness on a page. Our extensive experiments on human-labeled testdata show that our approachdetects templates effectively.,Proceedings of the 16th international conference on World Wide Web,2007,116
Selective private function evaluation with applications to private statistics,Ran Canetti; Yuval Ishai; Ravi Kumar; Michael K Reiter; Ronitt Rubinfeld; Rebecca N Wright,Abstract Motivated by the application of private statistical analysis of large databases; weconsider the problem of selective private function evaluation (SPFE). In this problem; a clientinteracts with one or more servers holding copies of a database x= x 1;…; xn in order tocompute f (xi 1;…; xim); for some function f and indices i= i 1;…; im chosen by the client.Ideally; the client must learn nothing more about the database than f (xi;…; xim); and theservers should learn nothing. Generic solutions for this problem; based on standardtechniques for secure function evaluation; incur communication complexity that is at leastlinear in n; making them prohibitive for large databases even when f in relatively simple andm is small. We present various approaches for constructing sublinear-communication SPFEprotocols; both for the general problem and for special cases of interest. Our solutions not …,Proceedings of the twentieth annual ACM symposium on Principles of distributed computing,2001,111
Automatic wrappers for large scale web extraction,Nilesh Dalvi; Ravi Kumar; Mohamed Soliman,Abstract We present a generic framework to make wrapper induction algorithms tolerant tonoise in the training data. This enables us to learn wrappers in a completely unsupervisedmanner from automatically and cheaply obtained noisy training data; eg; using dictionariesand regular expressions. By removing the site-level supervision that wrapper-basedtechniques require; we are able to perform information extraction at web-scale; withaccuracy unattained with existing unsupervised extraction techniques. Our system is used inproduction at Yahoo! and powers live applications.,Proceedings of the VLDB Endowment,2011,110
Dynamics of conversations,Ravi Kumar; Mohammad Mahdian; Mary McGlohon,Abstract How do online conversations build? Is there a common model that humancommunication follows? In this work we explore these questions in detail. We analyze thestructure of conversations in three different social datasets; namely; Usenet groups; Yahoo!Groups; and Twitter. We propose a simple mathematical model for the generation of basicconversation structures and then refine this model to take into account the identities of eachmember of the conversation.,Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,2010,109
The web and social networks,Ravi Kumar; P Ragbavan; Sridhar Rajagopalan; Andrew Tomkins,The sheer volume of Web data; together with its low signal-to-noise ratio; make it difficult fortext-based search engines to locate high-quality pages. Analyzing the links between Websites has dramatically improved the Web search experience and spawned research into theWeb's link structure. This research includes graph-theoretic studies of connectivity; whichhave shown the Web to have strong similarities with social networks. Self-similarity ispervasive in social networks. While researchers have observed Web self-similarity in othercontexts; finding a fractal structure in a graph theoretic setting adds further evidence to theWeb's small-world social nature. Thus; researchers seek to explain and exploit the humanbehavior implicit in the Web's evolving structure. How can we combine the power of Webnetworks with networks resulting from other human activity? Accomplishing this goal …,Computer,2002,109
Sampling algorithms: lower bounds and applications,Ziv Bar-Yossef; Ravi Kumar; D Sivakumar,Abstract We develop a framework to study probabilistic sampling algorithms thatapproximate general functions of the form\genfunc; where\domain and\range are arbitrarysets. Our goal is to obtain lower bounds on the query complexity of functions; namely thenumber of input variables x_i that any sampling algorithm needs to query to approximate f(x_1;\ldots; x_n). We define two quantitative properties of functions---the it block sensitivityand the minimum Hellinger distance---that give us techniques to prove lower bounds on thequery complexity. These techniques are quite general; easy to use; yet powerful enough toyield tight results. Our applications include the mean and higher statistical moments; themedian and other selection functions; and the frequency moments; where we obtain lowerbounds that are close to the corresponding upper bounds. We also point out some …,Proceedings of the thirty-third annual ACM symposium on Theory of computing,2001,104
Max-cover in map-reduce,Flavio Chierichetti; Ravi Kumar; Andrew Tomkins,Abstract The NP-hard Max-k-cover problem requires selecting k sets from a collection so asto maximize the size of the union. This classic problem occurs commonly in many settings inweb search and advertising. For moderately-sized instances; a greedy algorithm gives anapproximation of (1-1/e). However; the greedy algorithm requires updating scores ofarbitrary elements after each step; and hence becomes intractable for large datasets. Wegive the first max cover algorithm designed for today's large-scale commodity clusters. Ouralgorithm has provably almost the same approximation as greedy; but runs much faster.Furthermore; it can be easily expressed in the MapReduce programming paradigm; andrequires only polylogarithmically many passes over the data. Our experiments on five largeproblem instances show that our algorithm is practical and can achieve good speedups …,Proceedings of the 19th international conference on World wide web,2010,102
On scheduling in map-reduce and flow-shops,Benjamin Moseley; Anirban Dasgupta; Ravi Kumar; Tamás Sarlós,Abstract The map-reduce paradigm is now standard in industry and academia forprocessing large-scale data. In this work; we formalize job scheduling in map-reduce as anovel generalization of the two-stage classical flexible flow shop (FFS) problem: instead of asingle task at each stage; a job now consists of a set of tasks per stage. For thisgeneralization; we consider the problem of minimizing the total flowtime and give an efficient12-approximation in the offline setting and an online (1+ µ)-speed O (1/µ 2)-competitivealgorithm. Motivated by map-reduce; we revisit the two-stage flow shop problem; where wegive a dynamic program for minimizing the total flowtime when all jobs arrive at the sametime. If there are fixed number of job-types the dynamic program yields a PTAS; it is also aQPTAS when the processing times of jobs are polynomially bounded. This gives the first …,Proceedings of the twenty-third annual ACM symposium on Parallelism in algorithms and architectures,2011,97
A web of concepts,Nilesh Dalvi; Ravi Kumar; Bo Pang; Raghu Ramakrishnan; Andrew Tomkins; Philip Bohannon; Sathiya Keerthi; Srujana Merugu,Abstract We make the case for developing a web of concepts by starting with the currentview of web (comprised of hyperlinked pages; or documents; each seen as a bag of words);extracting concept-centric metadata; and stitching it together to create a semantically richaggregate view of all the information available on the web for each concept instance. Thegoal of building and maintaining such a web of concepts presents many challenges; but alsooffers the promise of enabling many powerful applications; including novel search andinformation discovery paradigms. We present the goal; motivate it with example usagescenarios and some analysis of Yahoo! logs; and discuss the challenges in building andleveraging such a web of concepts. We place this ambitious research agenda in the contextof the state of the art in the literature; and describe various ongoing efforts at Yahoo …,Proceedings of the twenty-eighth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2009,96
Mortal multi-armed bandits,Deepayan Chakrabarti; Ravi Kumar; Filip Radlinski; Eli Upfal,Abstract We formulate and study a new variant of the $ k $-armed bandit problem; motivatedby e-commerce applications. In our model; arms have (stochastic) lifetime after which theyexpire. In this setting an algorithm needs to continuously explore new arms; in contrast to thestandard $ k $-armed bandit model in which arms are available indefinitely and explorationis reduced once an optimal arm is identified with near-certainty. The main motivation for oursetting is online-advertising; where ads have limited lifetime due to; for example; the natureof their content and their campaign budget. An algorithm needs to choose among a largecollection of ads; more than can be fully explored within the ads' lifetime. We present anoptimal algorithm for the state-aware (deterministic reward function) case; and build on thistechnique to obtain an algorithm for the state-oblivious (stochastic reward function) case …,Advances in neural information processing systems,2009,96
On anonymizing query logs via token-based hashing,Ravi Kumar; Jasmine Novak; Bo Pang; Andrew Tomkins,Abstract In this paper we study the privacy preservation properties of aspecific technique forquery log anonymization: token-based hashing. In this approach; each query is tokenized;and then a secure hash function is applied to each token. We show that statisticaltechniques may be applied to partially compromise the anonymization. We then analyze thespecific risks that arise from these partial compromises; focused on revelation of identity fromunambiguous names; addresses; and so forth; and the revelation of facts associated with anidentity that are deemed to be highly sensitive. Our goal in this work is two fold: to show thattoken-based hashing is unsuitable for anonymization; and to present a concrete analysis ofspecific techniques that may be effective in breaching privacy; against which otheranonymization schemes should be measured.,Proceedings of the 16th international conference on World Wide Web,2007,95
Approximating edit distance efficiently,Ziv Bar-Yossef; TS Jayram; Robert Krauthgamer; Ravi Kumar,Edit distance has been extensively studied for the past several years. Nevertheless; nolinear-time algorithm is known to compute the edit distance between two strings; or even toapproximate it to within a modest factor. Furthermore; for various natural algorithmicproblems such as low-distortion embeddings into normed spaces; approximate nearest-neighbor schemes; and sketching algorithms; known results for the edit distance are ratherweak. We develop algorithms that solve gap versions of the edit distance problem: given twostrings of length n with the promise that their edit distance is either at most k or greaterthan/spl lscr/; decide which of the two holds. We present two sketching algorithms for gapversions of edit distance. Our first algorithm solves the k vs.(kn)/sup 2/3/gap problem; using aconstant size sketch. A more involved algorithm solves the stronger k vs./spl lscr/gap …,Foundations of Computer Science; 2004. Proceedings. 45th Annual IEEE Symposium on,2004,92
Fast greedy algorithms in mapreduce and streaming,Ravi Kumar; Benjamin Moseley; Sergei Vassilvitskii; Andrea Vattani,Abstract Greedy algorithms are practitioners' best friends—they are intuitive; are simple toimplement; and often lead to very good solutions. However; implementing greedy algorithmsin a distributed setting is challenging since the greedy choice is inherently sequential; and itis not clear how to take advantage of the extra processing power. Our main result is apowerful sampling technique that aids in parallelization of sequential algorithms. Armed withthis primitive; we then adapt a broad class of greedy algorithms to the MapReduceparadigm; this class includes maximum cover and submodular maximization subject to p-system constraint problems. Our method yields efficient algorithms that run in a logarithmicnumber of rounds while obtaining solutions that are arbitrarily close to those produced bythe standard sequential greedy algorithm. We begin with algorithms for modular …,ACM Transactions on Parallel Computing (TOPC),2015,90
Aggregating crowdsourced binary ratings,Nilesh Dalvi; Anirban Dasgupta; Ravi Kumar; Vibhor Rastogi,Abstract In this paper we analyze a crowdsourcing system consisting of a set of users and aset of binary choice questions. Each user has an unknown; fixed; reliability that determinesthe user's error rate in answering questions. The problem is to determine the truth values ofthe questions solely based on the user answers. Although this problem has been studiedextensively; theoretical error bounds have been shown only for restricted settings: when thegraph between users and questions is either random or complete. In this paper we considera general setting of the problem where the user--question graph can be arbitrary. We obtainbounds on the error rate of our algorithm and show it is governed by the expansion of thegraph. We demonstrate; using several synthetic and real datasets; that our algorithmoutperforms the state of the art.,Proceedings of the 22nd international conference on World Wide Web,2013,89
The discoverability of the web,Anirban Dasgupta; Arpita Ghosh; Ravi Kumar; Christopher Olston; Sandeep Pandey; Andrew Tomkins,Abstract Previous studies have highlighted the high arrival rate of new contenton the web.We study the extent to which this new content can beefficiently discovered by a crawler. Ourstudy has two parts. First; we study the inherent difficulty of the discovery problem usingamaximum cover formulation; under an assumption of perfect estimates oflikely sources oflinks to new content. Second; we relax thisassumption and study a more realistic setting inwhich algorithms mustuse historical statistics to estimate which pages are most likely toyieldlinks to new content. We recommend a simple algorithm thatperforms comparably to allapproaches we consider. We measure the emphoverhead of discovering new content;defined asthe average number of fetches required to discover one new page. Weshow firstthat with perfect foreknowledge of where to explore forlinks to new content; it is possible …,Proceedings of the 16th international conference on World Wide Web,2007,88
Are web users really markovian?,Flavio Chierichetti; Ravi Kumar; Prabhakar Raghavan; Tamas Sarlos,Abstract User modeling on the Web has rested on the fundamental assumption of Markovianbehavior---a user's next action depends only on her current state; and not the history leadingup to the current state. This forms the underpinning of PageRank web ranking; as well as anumber of techniques for targeting advertising to users. In this work we examine the validityof this assumption; using data from a number of Web settings. Our main result invokesstatistical order estimation tests for Markov chains to establish that Web users are not; in fact;Markovian. We study the extent to which the Markovian assumption is invalid; and derive anumber of avenues for further research.,Proceedings of the 21st international conference on World Wide Web,2012,83
An improved data stream algorithm for frequency moments,Don Coppersmith; Ravi Kumar,Abstract We present a simple; one-pass; Õ (√ n)-space data stream algorithm forapproximating the third frequency moment. This is the first improvement to the Õ (n 2/3)-space data stream algorithm of Alon; Matias; and Szegedy [AMS99]. the current known lowerbound for this problem is Ω (n 1/3)[BJKS02a]. Our algorithm can also be generalized to an Õ(n 1-1/(k-1))-space data stream algorithm for approximating the k-th frequency moment.Besides improving the Õ (n 1--1/k)-space upper bound [AMS99]; our algorithm beats the Ω(n 1--1/k)-sampling lower bound [BKS01] for this problem. Our method suggests a unifiedperspective of space-efficient data stream algorithms for all frequency moments.,Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms,2004,80
Sublinear algorithms for testing monotone and unimodal distributions,Tugkan Batu; Ravi Kumar; Ronitt Rubinfeld,Abstract The complexity of testing properties of monotone and unimodal distributions; whengiven access only to samples of the distribution; is investigated. Two kinds of sublinear-timealgorithms---those for testing monotonicity and those that take advantage of monotonicity---are provided. The first algorithm tests if a given distribution on [n] is monotone or far awayfrom any monotone distribution in L 1-norm; this algorithm uses O (√ n) samples and isshown to be nearly optimal. The next algorithm; given a joint distribution on [n] x [n]; tests if itis monotone or is far away from any monotone distribution in L 1-norm; this algorithm uses O(n 3/2) samples. The problems of testing if two monotone distributions are close in L 1-normand if two random variables with a monotone joint distribution are close to beingindependent in L 1-norm are also considered. Algorithms for these problems that use only …,Proceedings of the thirty-sixth annual ACM symposium on Theory of computing,2004,78
Two applications of information complexity,Thathachar S Jayram; Ravi Kumar; D Sivakumar,(1) In the two-party communication complexity model; we show that the tribes function on n inputs[6] has two-sided error randomized complexity Ω(n); while its nondetermin- stic complexity andco-nondeterministic complexity are both Θ( √ n). This separation between randomized andnondeter- ministic complexity is the best possible and it settles an open problem in Kushilevitzand Nisan [17]; which was also posed by Beame and Lawry [5]. (2) In the Boolean decision treemodel; we show that the recursive majority-of-three function on 3h inputs has ran- domized complexityΩ((7/3)h). The deterministic complex- ity of this function is Θ(3h); and the nondeterministiccom- plexity is Θ(2h). Our lower bound on the randomized com- plexity is a substantial improvementover any lower bound for this problem that can be obtained via the techniques of Saks and Wigderson[23]; Heiman and Wigderson [14] … Categories and Subject Descriptors F.1.3 …,Proceedings of the thirty-fifth annual ACM symposium on Theory of computing,2003,78
Sampling algorithms and coresets for \ell_p regression,Anirban Dasgupta; Petros Drineas; Boulos Harb; Ravi Kumar; Michael W Mahoney,The \ell_p regression problem takes as input a matrix A∈R^n*d; a vector b∈R^n; and anumber p∈1;∞); and it returns as output a number \calZ and a vector x_\scopt∈R^d suchthat \calZ=x∈R^d‖Ax-b‖_p=‖Ax_\scopt-b‖_p. In this paper; we construct coresets andobtain an efficient two-stage sampling-based approximation algorithm for the veryoverconstrained (n≫d) version of this classical problem; for all p∈1;∞). The first stage ofour algorithm nonuniformly samples ̂r_1=O(36^pd^\max{p/2+1;p\}+1) rows of A and thecorresponding elements of b; and then it solves the \ell_p regression problem on thesample; we prove this is an 8-approximation. The second stage of our algorithm uses theoutput of the first stage to resample ̂r_1/ϵ^2 constraints; and then it solves the \ell_pregression problem on the new sample; we prove this is a (1+ϵ)-approximation. Our …,SIAM Journal on Computing,2009,76
Sampling short lattice vectors and the closest lattice vector problem,Miklós Ajtai; Ravi Kumar; D Sivakumar,We present a 2/sup O (n)/time Turing reduction from the closest lattice vector problem to theshortest lattice vector problem. Our reduction assumes access to a subroutine that solvesSVP exactly and a subroutine to sample short vectors from a lattice; and computes a (1+/splepsi/)-approximation to CVP As a consequence; using the SVP algorithm from (Ajtai et al.;2001); we obtain a randomized 2 [O (1+/spl epsi//sup-1/) n] algorithm to obtain a (1+/splepsi/)-approximation for the closest lattice vector problem in n dimensions. This improves theexisting time bound of O (n!) for CVP achieved by a deterministic algorithm in (Blomer;2000).,Computational Complexity; 2002. Proceedings. 17th IEEE Annual Conference on,2002,70
Fast locality-sensitive hashing,Anirban Dasgupta; Ravi Kumar; Tamás Sarlós,Abstract Locality-sensitive hashing (LSH) is a basic primitive in several large-scale dataprocessing applications; including nearest-neighbor search; de-duplication; clustering; etc.In this paper we propose a new and simple method to speed up the widely-used Euclideanrealization of LSH. At the heart of our method is a fast way to estimate the Euclideandistance between two d-dimensional vectors; this is achieved by the use of randomizedHadamard transforms in a non-linear setting. This decreases the running time of a (k; L)-parameterized LSH from O (dkL) to O (dlog d+ kL). Our experiments show that using the newLSH in nearest-neighbor applications can improve their running times by significantamounts. To the best of our knowledge; this is the first running time improvement to LSH thatis both provable and practical.,Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,2011,68
Information theory methods in communication complexity,Ziv Bar-Yossef; Thathachar S Jayram; Ravi Kumar; D Sivakumar,We use tools and techniques from information theory to study communication complexityproblems in the one-way and simultaneous communication models. Our results include:(1) atight characterization of multi-party one-way communication complexity for productdistributions in terms of VC-dimension and shatter coefficients;(2) an equivalence of multi-party one-way and simultaneous communication models for product distributions;(3) a suiteof lower bounds for specific functions in the simultaneous communication model; mostnotably an optimal lower bound for the multi-party set disjointness problem of Alon etal.(1999) and for the generalized addressing function problem of Babai et al.(1996) forarbitrary groups. Methodologically; our main contribution is rendering communicationcomplexity problems in the framework of information theory. This allows us access to the …,Computational Complexity; 2002. Proceedings. 17th IEEE Annual Conference on,2002,66
Spectral filtering for resource discovery,Soumen Chakrabarti; Byron Dom; David Gibson; Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Andrew Tomkins,Abstract We develop a technique we call spectral filtering; for discovering high-qualitytopical resources in hyperlinked corpora. Through relevance and quality judgementscollected from 37 users; we show that; over 26 topics; spectral filtering usually finds webpages that are rated better than those returned by the hand-compiled Yahoo! resource list;and by the Altavista search engine.,ACM SIGIR workshop on Hypertext Information Retrieval on the Web,1998,66
Approximation algorithms for co-clustering,Aris Anagnostopoulos; Anirban Dasgupta; Ravi Kumar,Abstract Co-clustering is the simultaneous partitioning of the rows and columns of a matrixsuch that the blocks induced by the row/column partitions are good clusters. Motivated byseveral applications in text mining; market-basket analysis; and bioinformatics; this problemhas attracted severe attention in the past few years. Unfortunately; to date; most of thealgorithmic work on this problem has been heuristic in nature.,Proceedings of the twenty-seventh ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2008,65
Approximate counting of inversions in a data stream,Miklós Ajtai; TS Jayram; Ravi Kumar; D Sivakumar,Abstract (MATH) Inversions are used as a fundamental quantity to measure the sortednessof data; to evaluate different ranking methods for databases; and in the context of rankaggregation. Considering the volume of the data sets in these applications; the data streammodel {14; 2] is a natural setting to design efficient algorithms. We obtain a suite of space-efficient streaming algorithms for approximating the number of inversions in a permutation.The best space bound we achieve is $ O (\log n\log\log n) $ through a deterministicalgorithm. In contrast; we derive an $\Omega (n) $ lower bound for randomized exactcomputation for this problem; thus approximation is essential.(MATH) We also consider twogeneralizations of this problem:(1) approximating the number of inversions between twopermutations; for which we obtain a randomized $ O (\sqrt {n}\log n) $-space algorithm …,Proceedings of the thiry-fourth annual ACM symposium on Theory of Computing,2002,63
Object matching in tweets with spatial models,Nilesh Dalvi; Ravi Kumar; Bo Pang,Abstract Despite their 140-character limitation; tweets embody a lot of valuable information;especially temporal and spatial. In this paper we study the geographic aspects of tweets; fora given object domain. We propose a user-level model for spatial encoding in tweets thatgoes beyond the explicit geo-coding or place name mentions; this model can be used tomatch objects to tweets. We illustrate our model and methodology using restaurants as theobjects; and show a significant improvement in performance over using standard languagemodels. En route; we obtain a method to geolocate users who tweet about geolocatedobjects; this may be of independent interest.,Proceedings of the fifth ACM international conference on Web search and data mining,2012,61
De-duping URLs via rewrite rules,Anirban Dasgupta; Ravi Kumar; Amit Sasturkar,Abstract A large fraction of the URLs on the web contain duplicate (or near-duplicate)content. De-duping URLs is an extremely important problem for search engines; since all theprincipal functions of a search engine; including crawling; indexing; ranking; andpresentation; are adversely impacted by the presence of duplicate URLs. Traditionally; thede-duping problem has been addressed by fetching and examining the content of the URL;our approach here is different. Given a set of URLs partitioned into equivalence classesbased on the content (URLs in the same equivalence class have similar content); weaddress the problem of mining this set and learning URL rewrite rules that transform allURLs of an equivalence class to the same canonical form. These rewrite rules can then beapplied to eliminate duplicates among URLs that are encountered for the first time during …,Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,2008,61
A graph-theoretic approach to extract storylines from search results,Ravi Kumar; Uma Mahadevan; D Sivakumar,Abstract We present a graph-theoretic approach to discover storylines from search results.Storylines are windows that offer glimpses into interesting themes latent among the topsearch results for a query; they are different from; and complementary to; clusters obtainedthrough traditional approaches. Our framework is axiomatically developed andcombinatorial in nature; based on generalizations of the maximum induced matchingproblem on bipartite graphs. The core algorithmic task involved is to mine for signaturestructures in a robust graph representation of the search results. We present a very fastalgorithm for this task based on local search. Experiments show that the collection ofstorylines extracted through our algorithm offers a concise organization of the wealth ofinformation hidden beyond the first page of search results.,Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,2004,61
The one-way communication complexity of hamming distance,TS Jayram; Ravi Kumar; D Sivakumar,Abstract: Consider the following version of the Hamming distance problem for±1 vectors oflength n: the promise is that the distance is either at least n,Theory of Computing,2008,60
Estimating corpus size via queries,Andrei Broder; Marcus Fontura; Vanja Josifovski; Ravi Kumar; Rajeev Motwani; Shubha Nabar; Rina Panigrahy; Andrew Tomkins; Ying Xu,Abstract We consider the problem of estimating the size of a collection of documents usingonly a standard query interface. Our main idea is to construct an unbiased and low-varianceestimator that can closely approximate the size of any set of documents defined by certainconditions; including that each document in the set must match at least one query from auniformly sampleable query pool of known size; fixed in advance. Using this basic estimator;we propose two approaches to estimating corpus size. The first approach requires a uniformrandom sample of documents from the corpus. The second approach avoids this notoriouslydifficult sample generation problem; and instead uses two fairly uncorrelated sets of terms asquery pools; the accuracy of the second approach depends on the degree of correlationamong the two sets of terms. Experiments on a large TREC collection and on three major …,Proceedings of the 15th ACM international conference on Information and knowledge management,2006,59
Pagerank on an evolving graph,Bahman Bahmani; Ravi Kumar; Mohammad Mahdian; Eli Upfal,Abstract One of the most important features of the Web graph and social networks is thatthey are constantly evolving. The classical computational paradigm; which assumes a fixeddata set as an input to an algorithm that terminates; is inadequate for such settings. In thispaper we study the problem of computing PageRank on an evolving graph. We propose analgorithm that; at any moment in the time and by crawling a small portion of the graph;provides an estimate of the PageRank that is close to the true PageRank of the graph at thatmoment. We will also evaluate our algorithm experimentally on real data sets and onrandomly generated inputs. Under a stylized model of graph evolution; we show that ouralgorithm achieves a provable performance guarantee that is significantly better than thenaive algorithm that crawls the nodes in a round-robin fashion.,Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,2012,57
Estimating the sortedness of a data stream,Parikshit Gopalan; TS Jayram; Robert Krauthgamer; Ravi Kumar,Abstract The distance to monotonicity of a sequence is the minimum number of editoperations required to transform the sequence into an increasing order; this measure iscomplementary to the length of the longest increasing subsequence (LIS). We address thequestion of estimating these quantities in the one-pass data stream model and present thefirst sub-linear space algorithms for both problems. We first present O (√ n)-spacedeterministic algorithms that approximate the distance to monotonicity and the LIS to within afactor that is arbitrarily close to 1. We also show a lower bound of Ω (n) on the spacerequired by any randomized algorithm to compute the LIS (or alternatively the distance frommonotonicity) exactly; demonstrating that approximation is necessary for sub-linear spacecomputation; this bound improves upon the existing lower bound of Ω (√ n)[LNVZ06].,Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,2007,50
Random graph models for the web graph.,Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; D Sivakumar; Andrew Tomkins; Eli Upfal,*,FOCS,2000,50
Multi-structural databases,Ronald Fagin; R Guha; Ravi Kumar; Jasmine Novak; D Sivakumar; Andrew Tomkins,Abstract We introduce the Multi-Structural Database; a new data framework to supportefficient analysis of large; complex data sets. An instance of the model consists of a set ofdata objects; together with a schema that specifies segmentations of the set of data objectsaccording to multiple distinct criteria (eg; into a taxonomy based on a hierarchical attribute).Within this model; we develop a rich set of analytical operations and design highly efficientalgorithms for these operations. Our operations are formulated as optimization problems;and allow the user to analyze the underlying data in terms of the allowed segmentations.Our algorithms and results extend those of Fagin et al.[8] who studied composition ofmappings given by several kinds of constraints. In particular; they proved that full source-to-target tuple-generating dependencies (tgds) are closed under composition; but …,Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2005,48
Cell-probe lower bounds for the partial match problem,TS Jayram; Subhash Khot; Ravi Kumar; Yuval Rabani,Abstract Given a database of n points in {0; 1} d; the partial match problem is: In response toa query x in {0; 1;∗} d; is there a database point y such that for every i whenever xi≠∗; wehave xi= y i. In this paper we show randomized lower bounds in the cell–probe model for thiswell-studied problem (Analysis of associative retrieval algorithms; Ph. D. Thesis; StanfordUniversity; 1974; The Art of Computer Programming; Sorting and Searching; Addison-Wesley; Reading; MA; 1973; SIAM J. Comput. 5 (1)(1976) 19; J. Comput. System Sci. 57(1)(1998) 37; Proceedings of the 31st Annual ACM Symposium on Theory of Computing;1999; Proceedings of the 29th International Colloquium on Algorithms; Logic; andProgramming; 1999). Our lower bounds follow from a near-optimal asymmetriccommunication complexity lower bound for this problem. Specifically; we show that either …,Journal of Computer and System Sciences,2004,45
Nearest-neighbor caching for content-match applications,Sandeep Pandey; Andrei Broder; Flavio Chierichetti; Vanja Josifovski; Ravi Kumar; Sergei Vassilvitskii,Abstract Motivated by contextual advertising systems and other web applications involvingefficiency-accuracy tradeoffs; we study similarity caching. Here; a cache hit is said to occur ifthe requested item is similar but not necessarily equal to some cached item. We study twoobjectives that dictate the efficiency-accuracy tradeoff and provide our caching policies forthese objectives. By conducting extensive experiments on real data we show similaritycaching can significantly improve the efficiency of contextual advertising systems; withminimal impact on accuracy. Inspired by the above; we propose a simple generative modelthat embodies two fundamental characteristics of page requests arriving to advertisingsystems; namely; long-range dependences and similarities. We provide theoretical boundson the gains of similarity caching in this model and demonstrate these gains empirically …,Proceedings of the 18th international conference on World wide web,2009,44
Approximating latin square extensions,S Ravi Kumar; Alexander Russell; Ravi Sundaram,Abstract. In this paper we investigate the problem of computing the maximum number ofentries which can be added to a partially filled latin square. The decision version of thisquestion is known to be NP-complete. We present two approximation algorithms for theoptimization version of this question. We first prove that the greedy algorithm achieves afactor of 1/3. We then use insights derived from the linear relaxation of an integer program toobtain an algorithm based on matchings that achieves a better performance guarantee of1/2. These are the first known polynomial-time approximation algorithms for the latin squarecompletion problem that achieve nontrivial worst-case performance guarantees. Our study ismotivated by applications to lightpath assignment and switch configuration in wavelengthrouted multihop optical networks.,Algorithmica,1999,44
Mapreduce: Simplified data processing on large clusters; osdi’04: Sixth symposium on operating system design and implementation; san francisco; ca; december; 2...,Jeffrey Dean; Sanjay Ghemawat; S Dill; R Kumar; K McCurley; S Rajagopalan; D Sivakumar,*,S. Dill; R. Kumar; K. McCurley; S. Rajagopalan; D. Sivakumar; ad A. Tomkins; Self-similarity in the Web; Proc VLDB,2001,43
Event Detection via Communication Pattern Analysis.,Flavio Chierichetti; Jon M Kleinberg; Ravi Kumar; Mohammad Mahdian; Sandeep Pandey,Abstract Social media applications such as Twitter provide a powerful medium throughwhich users can communicate their observations with friends and with the world at large. Wehave witnessed live reporting of many events; from soccer games in Johannesburg torevolutions in Cairo and Tunis; and these reports have in many ways rivaled the contentprovided by the official media. Tapping into this valuable resource is a challenge; due to theheterogeneity and noise inherent in realtime text; diversity of languages; and fast-evolvinglinguistic norms. In this paper we seek to analyze a tweet stream to automatically discoverpoints in time when an important event happens; and to classify such events based on thetype of the sentiments they evoke; using only non-textual features of the tweeting pattern.This results not only in a robust way of analyzing tweet streams independent of the …,ICWSM,2014,42
Finding the jaccard median,Flavio Chierichetti; Ravi Kumar; Sandeep Pandey; Sergei Vassilvitskii,Abstract The median problem in the weighted Jaccard metric was analyzed by Späth in1981. Up until now; only an exponential-time exact algorithm was known. We (a) obtain aPTAS for the weighted Jaccard median problem and (b) show that the problem does notadmit a FPTAS (assuming P≠ NP); even when restricted to binary vectors. The PTAS is builton a number of different algorithmic ideas and the hardness result makes use of anespecially interesting gadget.,*,2010,42
Combinatorial feature selection problems,Moses Charikar; Venkatesan Guruswami; Ravi Kumar; Sridhar Rajagopalan; Amit Sahai,Motivated by frequently recurring themes in information retrieval and related disciplines; wedefine a genre of problems called combinatorial feature selection problems. Given a set S ofmultidimensional objects; the goal is to select a subset K of relevant dimensions (or features)such that some desired property/spl Pi/holds for the set S restricted to K. Depending on/splPi/; the goal could be to either maximize or minimize the size of the subset K. Several well-studied feature selection problems can be cast in this form. We study the problems in thisclass derived from several natural and interesting properties/spl Pi/; including variants of theclassical p-center problem as well as problems akin to determining the VC-dimension of aset system. Our main contribution is a theoretical framework for studying combinatorialfeature selection; providing (in most cases essentially tight) approximation algorithms and …,Foundations of Computer Science; 2000. Proceedings. 41st Annual Symposium on,2000,42
Fast approximate PCPs,Funda Ergün; Ravi Kumar; Ronitt Rubinfeld,Abstract We investigate the problem of when a prover can aid a verifier to reliably compute afunctionfaster than if the verifier were to compute the function on its own. We focus on thecase when it is enough for the verifier to know that the answer is close to correct. We use amodel of proof systems which is based on interactive proof systems; probabilisticallycheckable proof systems; program checkers; and CS proofs. We develop protocols forseveral optimization problems; in which the running time of the verifier is significantly lessthan the size of the input. For example; we give polylogarithmic time protocols for showingthe existence of a large Cut; a large matching and a small bin packing. In contrast; theprotocolsused to show that IP= PSPACE; MIP= NEXP and NP= PCP (lg n; 1)[Sha90; BFL91;ALM+ 98; BFLS90J require a verifier that runs in sl (n) time. In the process; we develop a …,Proceedings of the thirty-first annual ACM symposium on Theory of computing,1999,42
Vanity fair: privacy in querylog bundles,Rosie Jones; Ravi Kumar; Bo Pang; Andrew Tomkins,Abstract A recently proposed approach to address privacy concerns in storing web searchquerylogs is bundling logs of multiple users together. In this work we investigate privacyleaks that are possible even when querylogs from multiple users are bundled together;without any user or session identifiers. We begin by quantifying users' propensity to issueown-name vanity queries and geographically revealing queries. We show that thesepropensities interact badly with two forms of vulnerabilities in the bundling scheme. First;structural vulnerabilities arise due to properties of the heavy tail of the user search frequencydistribution; or the distribution of locations that appear within a user's queries. These heavytails may cause a user to appear visibly different from other users in the same bundle.Second; we demonstrate analytical vulnerabilities based on the ability to separate the …,Proceedings of the 17th ACM conference on Information and knowledge management,2008,41
Checking approximate computations of polynomials and functional equations,Funda Ergün; S Ravi Kumar; Ronitt Rubinfeld,A majority of the results on self-testing and correcting deal with programs which purport tocompute the correct results precisely. We relax this notion of correctness and show how tocheck programs that compute only a numerical approximation to the correct answer. Thetypes of programs that we deal with are those computing polynomials and functions definedby certain types of functional equations. We present results showing how to performapproximate checking; self-testing; and self-correcting of polynomials; settling in theaffirmative a question raised by [P. Gemmell et al.; Proceedings of the 23 rd ACMSymposium on Theory of Computing; 1991; pp. 32--42; R. Rubinfeld and M. Sudan;Proceedings of the Third Annual ACM-SIAM Symposium on Discrete Algorithms; Orlando;FL; 1992; pp. 23--43; R. Rubinfeld and M. Sudan; SIAM J. Comput.; 25 (1996); pp. 252 …,SIAM Journal on Computing,2001,39
On estimating the average degree,Anirban Dasgupta; Ravi Kumar; Tamas Sarlos,Abstract Networks are characterized by nodes and edges. While there has been a spate ofrecent work on estimating the number of nodes in a network; the edge-estimation questionappears to be largely unaddressed. In this work we consider the problem of estimating theaverage degree of a large network using efficient random sampling; where the number ofnodes is not known to the algorithm. We propose a new estimator for this problem that relieson access to node samples under a prescribed distribution. Next; we show how to efficientlyrealize this ideal estimator in a random walk setting. Our estimator has a natural and simpleimplementation using random walks; we bound its performance in terms of the mixing time ofthe underlying graph. We then show that our estimators are both provably and practicallybetter than many natural estimators for the problem. Our work contrasts with existing …,Proceedings of the 23rd international conference on World wide web,2014,36
The dynamics of repeat consumption,Ashton Anderson; Ravi Kumar; Andrew Tomkins; Sergei Vassilvitskii,Abstract We study the patterns by which a user consumes the same item repeatedly overtime; in a wide variety domains ranging from check-ins at the same business location to re-watches of the same video. We find that recency of consumption is the strongest predictor ofrepeat consumption. Based on this; we develop a model by which the item from $ t $timesteps ago is reconsumed with a probability proportional to a function of t. We studytheoretical properties of this model; develop algorithms to learn reconsumption likelihood asa function of t; and show a strong fit of the resulting inferred function via a power law withexponential cutoff. We then introduce a notion of item quality; show that it aloneunderperforms our recency-based model; and develop a hybrid model that predicts userchoice based on a combination of recency and quality. We show how the parameters of …,Proceedings of the 23rd international conference on World wide web,2014,36
Web structure mining: exploring hyperlinks and algorithms for information retrieval,Ravi Kumar; Ashutosh Kumar Singh,Problem statement: A study on hyperlink analysis and the algorithms used for link analysis inthe Web Information retrieval was done. Approach: This research was initiated because ofthe dependability of search engines for information retrieval in the web. Understand the webstructure mining and determine the importance of hyperlink in web information retrievalparticularly using the Google Search engine. Hyperlink analysis was important methodologyused by famous search engine Google to rank the pages. Results: The different algorithmsused for link analysis like PageRank (PR); Weighted PageRank (WPR) and Hyperlink-Induced Topic Search (HITS) algorithms are discussed andcompared. PageRank algorithmwas implemented using a Java program and the convergence of the PageRank values areshown in a chart form. Conclusion: This study was done basically to explore the link …,American Journal of applied sciences,2010,35
Evolution of two-sided markets,Ravi Kumar; Yury Lifshits; Andrew Tomkins,Abstract Two-sided markets arise when two different types of users may realize gains byinteracting with one another through one or more platforms or mediators. We initiate a studyof the evolution of such markets. We present an empirical analysis of the value accruing tomembers of each side of the market; based on the presence of the other side. We codify therange of value curves into a general theoretical model; characterize the equilibrium states oftwo-sided markets in our model; and prove that each platform will converge to one of theseequilibria. We give some early experimental results of the stability of two-sided markets; andclose with a theoretical treatment of the formation of different kinds of coalitions in suchmarkets.,Proceedings of the third ACM international conference on Web search and data mining,2010,34
A note on optical routing on trees,S Ravi Kumar; Rina Panigrahy; Alexander Russell; Ravi Sundaram,Abstract Bandwidth is a very valuable resource in wavelength division multiplexed opticalnetworks. The problem of finding an optimal assignment of wavelengths to requests is offundamental importance in bandwidth utilization. We present a polynomial-time algorithm forthis problem on fixed constant-size topologies. We combine this algorithm with ideas fromRaghavan and Upfal (1994) to obtain an optimal assignment of wavelengths on constantdegree undirected trees. Mihail; Kaklamanis; and Rao (1995) posed the following openquestion: what is the complexity of this problem on directed trees? We show that it is NP-complete both on binary and constant depth directed trees.,Information Processing Letters,1997,33
Correlation clustering in mapreduce,Flavio Chierichetti; Nilesh Dalvi; Ravi Kumar,Abstract Correlation clustering is a basic primitive in data miner's toolkit with applicationsranging from entity matching to social network analysis. The goal in correlation clustering is;given a graph with signed edges; partition the nodes into clusters to minimize the number ofdisagreements. In this paper we obtain a new algorithm for correlation clustering. Ouralgorithm is easily implementable in computational models such as MapReduce andstreaming; and runs in a small number of rounds. In addition; we show that our algorithmobtains an almost 3-approximation to the optimal correlation clustering. Experiments onhuge graphs demonstrate the scalability of our algorithm and its applicability to data miningproblems.,Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,2014,32
The hiring problem and Lake Wobegon strategies,Andrei Z Broder; Adam Kirsch; Ravi Kumar; Michael Mitzenmacher; Eli Upfal; Sergei Vassilvitskii,We introduce the hiring problem; in which a growing company continuously interviews anddecides whether to hire applicants. This problem is similar in spirit but quite different from thewell-studied secretary problem. Like the secretary problem; it captures fundamental aspectsof decision making under uncertainty and has many possible applications. We analyzenatural strategies of hiring above the current average; considering both the mean and themedian averages; we call these Lake Wobegon strategies. Like the hiring problem itself; ourstrategies are intuitive; simple to describe; and amenable to mathematically andeconomically significant modifications. We demonstrate several intriguing behaviors of thetwo strategies. Specifically; we show dramatic differences between hiring above the meanand above the median. We also show that both strategies are intrinsically connected to …,SIAM Journal on Computing,2009,32
Social sampling,Anirban Dasgupta; Ravi Kumar; D Sivakumar,Abstract We investigate a class of methods that we call" social sampling;" where participantsin a poll respond with a summary of their friends' putative responses to the poll. Socialsampling leads to a novel trade-off question: the savings in the number of samples (roughlythe average degree of the network of participants) vs. the systematic bias in the poll due tothe network structure. We provide precise analyses of estimators that result from this idea.With non-uniform sampling of nodes and non-uniform weighting of neighbors' responses; wedevise an ideal unbiased estimator. We show that the variance of this estimator is controlledby the second eigenvalue of the normalized Laplacian of the network (the network structurepenalty) and the correlation between node degrees and the property being measured (theeffective savings factor). In addition; we present a sequence of approximate estimators …,Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,2012,29
Summarization through submodularity and dispersion,Anirban Dasgupta; Ravi Kumar; Sujith Ravi,Abstract We propose a new optimization framework for summarization by generalizing thesubmodular framework of (Lin and Bilmes; 2011). In our framework the summarizationdesideratum is expressed as a sum of a submodular function and a nonsubmodularfunction; which we call dispersion; the latter uses inter-sentence dissimilarities in differentways in order to ensure non-redundancy of the summary. We consider three naturaldispersion functions and show that a greedy algorithm can obtain an approximately optimalsummary in all three cases. We conduct experiments on two corpora—DUC 2004 and usercomments on news articles—and show that the performance of our algorithm outperformsthose that rely only on submodularity.,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2013,28
On learning bounded-width branching programs,Funda Ergün; S Ravi Kumar; Ronitt Rubinfeld,Abstract In this paper; we study PAC-leaming algorithms for specialized classes ofdeterministic finite automata (DFA). Inpartictdar; we study branchingprogrsms; and weinvestigate the intluence of the width of the branching program on the difficulty of thelearning problem. We first present a distribution-free algorithm for learning width-2branching programs. We also give an algorithm for the proper learning of width-2 branchingprograms under uniform distribution on labeled samples. We then show that the existence ofan efficient algorithm for learning width-3 branching programs would imply the existence ofan efficient algorithm for learning DNF; which is not known to be the case. Fimlly; we showthat the existence of an algorithm for learning width-3 branching programs would also yieldan algorithm for learning a very restricted version of parity with noise.,Proceedings of the eighth annual conference on Computational learning theory,1995,28
Algorithms on evolving graphs,Aris Anagnostopoulos; Ravi Kumar; Mohammad Mahdian; Eli Upfal; Fabio Vandin,Abstract Motivated by applications that concern graphs that are evolving and massive innature; we define a new general framework for computing with such graphs. In ourframework; the graph changes over time and an algorithm can only track these changes byexplicitly probing the graph. This framework captures the inherent tradeoff between thecomplexity of maintaining an up-to-date view of the graph and the quality of resultscomputed with the available view. We apply this framework to two classical graphconnectivity problems; namely; path connectivity and minimum spanning trees; and obtainefficient algorithms.,Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,2012,27
Matching reviews to objects using a language model,Nilesh Dalvi; Ravi Kumar; Bo Pang; Andrew Tomkins,Abstract We develop a general method to match unstructured text reviews to a structured listof objects. For this; we propose a language model for generating reviews that incorporates adescription of objects and a generic review language model. This mixture model gives us aprincipled method to find; given a review; the object most likely to be the topic of the review.Extensive experiments and analysis on reviews from Yelp show that our language model-based method vastly outperforms traditional tf-idf-based methods.,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2,2009,26
The sketching complexity of pattern matching,Ziv Bar-Yossef; TS Jayram; Robert Krauthgamer; Ravi Kumar,Abstract We address the problems of pattern matching and approximate pattern matching inthe sketching model. We show that it is impossible to compress the text into a small sketchand use only the sketch to decide whether a given pattern occurs in the text. We also prove asketch size lower bound for approximate pattern matching; and show it is tight up to alogarithmic factor.,*,2004,26
On Semi-Automated Web Taxonomy Construction.,Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Andrew Tomkins,Abstract The subject of this paper is the semi-automatic construction of taxonomies over theWeb. We address the problem of discovering high-quality resources that belong in aparticular node of a taxonomy. We show that minimal additional effort is required to providerelevance feedback in a hyperlinked environment; resulting in significant and consistentimprovement in quality. Furthermore; this feedback is especially valuable for topics for whichit is more difficult to find high-quality pages. Enroute; we describe novel algorithms forhyperlink relevance feedback.,WebDB,2001,26
Great Question! Question Quality in Community Q&A.,Sujith Ravi; Bo Pang; Vibhor Rastogi; Ravi Kumar,Abstract Asking the right question in the right way is an art (and a science). In a communityquestion-answering setting; a good question is not just one that is found to be useful byother people: a question is good if it is also presented clearly and shows prior research.Using a community question-answering site that allows voting over the questions; we showthat there is a notion of question quality that goes beyond mere popularity. We presenttechniques using latent topic models to automatically predict the quality of questions basedon their content. Our best system achieves a prediction accuracy of 72%; beating out strongbaselines by a significant amount. We also examine the effect of question quality on thedynamics of user behavior and the longevity of questions.,ICWSM,2014,25
Top-k aggregation using intersections of ranked inputs,Ravi Kumar; Kunal Punera; Torsten Suel; Sergei Vassilvitskii,Abstract There has been considerable past work on efficiently computing top k objects byaggregating information from multiple ranked lists of these objects. An important instance ofthis problem is query processing in search engines: One has to combine information fromseveral different posting lists (rankings) of web pages (objects) to obtain the top k web pagesto answer user queries. Two particularly well-studied approaches to achieve efficiency in top-k aggregation include early-termination algorithms (eg; TA and NRA) and preaggregation ofsome of the input lists. However; there has been little work on a rigorous treatment ofcombining these approaches. We generalize the TA and NRA algorithms to the case whenpreaggregated intersection lists are available in addition to the original lists. We show thatour versions of TA and NRA continue to remain" instance optimal;" a very strong …,Proceedings of the Second ACM International Conference on Web Search and Data Mining,2009,25
Algorithms column: sublinear time algorithms,Ravi Kumar; Ronitt Rubinfeld,This issue's column is written by guest columnists; Ravi Kumar and Ronitt Rubinfeld. I amdelighted that they agreed to write the column at such short notice. Sublinear time algorithmshave received a lot of attention recently; and their timely column introduces the reader toseveral recent results and provides references for further readings. Samir Khuller,ACM SIGACT News,2003,25
Proofs; codes; and polynomial-time reducibilities,Ravi Kumar; D Sivakumar,We show how to construct proof systems for NP languages where a deterministic polynomial-time verifier can check membership; given any N/sup (2/3)+/spl epsi//bits of an N-bit witnessof membership. We also provide a slightly superpolynomial time proof system where theverifier can check membership; given only N/sup (1/2)+/spl epsi//bits of an N-bit witness.These pursuits are motivated by the work of Gal et. al.(1997). In addition; we construct proofsystems where a deterministic polynomial-time verifier can check membership; given an N-bit string that agrees with a legitimate witness on just (N/2)+ N/sup (4/5)+/spl epsi//bits. Ourresults and framework have applications for two related areas of research in complexitytheory: proof systems for NP; and the relative power of Cook reductions and Karp-Levin typereductions. Our proof techniques are based on algebraic coding theory and small sample …,Computational Complexity; 1999. Proceedings. Fourteenth Annual IEEE Conference on,1999,25
LSH-preserving functions and their applications,Flavio Chierichetti; Ravi Kumar,Abstract Locality sensitive hashing (LSH) is a key algorithmic tool that is widely used both intheory and practice. An important goal in the study of LSH is to understand which similarityfunctions admit an LSH; that is; are LSHable. In this article; we focus on the class oftransformations such that given any similarity that is LSHable; the transformed similarity willcontinue to be LSHable. We show a tight characterization of all such LSH-preservingtransformations: they are precisely the probability generating functions; up to scaling. As aconcrete application of this result; we study which set similarity measures are LSHable. Weobtain a complete characterization of similarity measures between two sets A and B that areratios of two linear functions of &mid; A∩ B&mid;; &mid; A&utri; B&mid;; &mid; A∪ B&mid;:such a measure is LSHable if and only if its corresponding distance is a metric. This result …,Journal of the ACM (JACM),2015,24
On targeting Markov segments,Moses Charikar; Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Andrew Tomkins,Abstract Consider two user populations; of which one is targered and the other is not. Usersin the targeted population follow a Markov chain on a space of n states. The untargetedpopulation follows another Markov chain; also defined on the same set of n states. Each timea user arrives at a state; he/she is presented with information appropriate for the targetedpopulation (an advertisement; or a recommendation) with some probability. Presenting theadvertisement incurs a cost. Notice that while the revenue grows in proportion to the flow oftargeted users through the state; the cost grows in proportion to the total flow (targeted anduntargeted) through the state. How can we compute the best advertisement policy? Theworld-wide web is a natural setting for such a problem. Internet service providers have trailinformation for building such Markovian user models where states correspond to pages …,Proceedings of the thirty-first annual ACM symposium on Theory of computing,1999,24
Models for the compressible web,Flavio Chierichetti; Ravi Kumar; Silvio Lattanzi; Alessandro Panconesi; Prabhakar Raghavan,Graphs resulting from human behavior (the web graph; friendship graphs; etc.) have hithertobeen viewed as a monolithic class of graphs with similar characteristics; for instance; theirdegree distributions are markedly heavy tailed. In this paper we take our understanding ofbehavioral graphs a step further by showing that an intriguing empirical property of webgraphs---their compressibility---cannot be exhibited by well-known graph models for the weband for social networks. We then develop a more nuanced model for web graphs and showthat it does exhibit compressibility; in addition to previously modeled web graph properties.,SIAM Journal on Computing,2013,22
Hierarchical topic segmentation of websites,Ravi Kumar; Kunal Punera; Andrew Tomkins,Abstract In this paper; we consider the problem of identifying and segmenting topicallycohesive regions in the URL tree of a large website. Each page of the website is assumed tohave a topic label or a distribution on topic labels generated using a standard classifier. Wedevelop a set of cost measures characterizing the benefit accrued by introducing asegmentation of the site based on the topic labels. We propose a general framework to usethese measures for describing the quality of a segmentation; we also provide an efficientalgorithm to find the best segmentation in this framework. Extensive experiments on human-labeled data confirm the soundness of our framework and suggest that a judicious choice ofcost measures allows the algorithm to perform surprisingly accurate topical segmentations.,Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,2006,22
Selecting diverse features via spectral regularization,Abhimanyu Das; Anirban Dasgupta; Ravi Kumar,Abstract We study the problem of diverse feature selection in linear regression: selecting asmall subset of diverse features that can predict a given objective. Diversity is useful forseveral reasons such as interpretability; robustness to noise; etc. We propose severalspectral regularizers that capture a notion of diversity of features and show that these are allsubmodular set functions. These regularizers; when added to the objective function for linearregression; result in approximately submodular functions; which can then be maximizedapproximately by efficient greedy and local search algorithms; with provable guarantees. Wecompare our algorithms to traditional greedy and $\ell_1 $-regularization schemes andshow that we obtain a more diverse set of features that result in the regression problembeing stable under perturbations.,Advances in neural information processing systems,2012,21
On the robustness of relevance measures with incomplete judgments,Tanuja Bompada; Chi-Chao Chang; John Chen; Ravi Kumar; Rajesh Shenoy,Abstract We investigate the robustness of three widely used IR relevance measures for largedata collections with incomplete judgments. The relevance measures we consider are thebpref measure introduced by Buckley and Voorhees [7]; the inferred average precision(infAP) introduced by Aslam and Yilmaz [4]; and the normalized discounted cumulative gain(NDCG) measure introduced by Järvelin and Kekäläinen [8]. Our main results show thatNDCG consistently performs better than both bpref and infAP. The experiments areperformed on standard TREC datasets; under different levels of incompleteness ofjudgments; and using two different evaluation methods; namely; the Kendall correlationmeasures order between system rankings and pairwise statistical significance testing; thelatter may be of independent interest.,Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,2007,21
Anchor-based proximity measures,Amruta Joshi; Ravi Kumar; Benjamin Reed; Andrew Tomkins,Abstract We present a family of measures of proximity of an arbitrary node in a directedgraph to a pre-specified subset of nodes; called the anchor. Our measures are based onthree different propagation schemesand two different uses of the connectivity structure of thegraph. We consider a web-specific application of the above measures with two disjointanchors-good and bad web pages-and study the accuracy of these measures in this context.,Proceedings of the 16th international conference on World Wide Web,2007,21
Core algorithms in the CLEVER system,Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Andrew Tomkins,1. INTRODUCTION The subject of this article is the CLEVER search system developed at theIBM Almaden Research Center. Our principal focus is a detailed and unified expo- sition of thevarious algorithmic components that make up the system. Many of these have hitherto appearedin a number of articles and reports; some have appeared in incomplete form; and others havenever been disclosed. In addition; we summarize the results of two user studies performed duringthe project. The Web has proven to be a fertile test bed for combining ideas from human behaviorand social network analysis together with traditional information re- trieval. The latter disciplinehas focused on relatively focused corpora that are small; with uniform and high-qualitydocuments. The networking revolution made it possible for hundreds of millions of individualsto create; share; and … This work was done while the authors were at the IBM Almaden …,ACM Transactions on Internet Technology (TOIT),2006,21
Compressed web indexes,Flavio Chierichetti; Ravi Kumar; Prabhakar Raghavan,Abstract Web search engines use indexes to efficiently retrieve pages containing specifiedquery terms; as well as pages linking to specified pages. The problem of compressedindexes that permit such fast retrieval has a long history. We consider the problem:assuming that the terms in (or links to) a page are generated from a probability distribution;how well compactly can we build such indexes that allow fast retrieval? Of particular interestis the case when the probability distribution is Zipfian (or a similar power law); since theseare the distributions that arise on the web. We obtain sharp bounds on the spacerequirement of Boolean indexes for text documents that follow Zipf's law. In the process wedevelop a general technique that applies to any probability distribution; not necessarily apower law; this is the first analysis of compression in indexes under arbitrary distributions …,Proceedings of the 18th international conference on World wide web,2009,20
Connectivity structure of bipartite graphs via the knc-plot,Ravi Kumar; Andrew Tomkins; Erik Vee,Abstract In this paper we introduce the k-neighbor connectivity plot; or KNC-plot; as a tool tostudy the macroscopic connectiv-ity structure of sparse bipartite graphs. Given a bipartitegraph G=(U; V; E); we say that two nodes in U are k-neighbors if there exist at least k distinctlength-two paths between them; this defines a k-neighborhood graph on U where the edgesare given by the k-neighbor relation. For example; in a bipartite graph of users and interests;two users are k-neighbors if they have at least k common interests. The KNC-plot shows thedegradation of connectivity of the graph as a function of k. We show that this tool provides aneffective and interpretable high-level characterization of the connectivity of a bipartite graphHowever; naive algorithms to compute the KNC-plot are inefficient for k> 1. We give anefficient and practical algorithm that runs in sub-quadratic time O (| E| 2-1/k) and is a non …,Proceedings of the 2008 International Conference on Web Search and Data Mining,2008,20
Navigating low-dimensional and hierarchical population networks,Ravi Kumar; David Liben-Nowell; Andrew Tomkins,Abstract Social networks are navigable small worlds; in which two arbitrary people are likelyconnected by a short path of intermediate friends that can be found by a “decentralized”routing algorithm using only local information. We develop a model of social networks basedon an arbitrary metric space of points; with population density varying across the points. Weconsider rank-based friendships; where the probability that person u befriends person v isinversely proportional to the number of people who are closer to u than v is. Our main resultis that greedy routing can find a short path (of expected polylogarithmic length) from anarbitrary source to a randomly chosen target; independent of the population densities; aslong as the doubling dimension of the metric space of locations is low. We also show thatgreedy routing finds short paths with good probability in tree-based metrics with varying …,European Symposium on Algorithms,2006,20
Theoretical analysis of geographic routing in social networks,Ravi Kumar; David Liben-Nowell; Jasmine Novak; Prabhakar Raghavan; Andrew Tomkins,We introduce a formal model for geographic social networks; and introduce the notion ofrank-based friendship; in which the probability that a person v is a friend of a person u isinversely proportional to the number of people w who live closer to u than v does. We thenprove our main theorem; showing that rank-based friendship is a sufficient explanation of thenavigability of any geographic social network that adheres to it.,*,2005,20
On threshold behavior in query incentive networks,Esteban Arcaute; Adam Kirsch; Ravi Kumar; David Liben-Nowell; Sergei Vassilvitskii,Abstract Motivated by the role of incentives in large-scale information systems; Kleinbergand Raghavan (FOCS 2005) studied strategic games in decentralized information networks.Given a branching process that specifies the network; the rarity of answers to a specificquestion; and a desired probability of success; how much reward does the root node need tooffer so that it receives an answer with this probability; when all of the nodes are playingstrategically? For a specific family of branching processes and a constant failure probability;they showed that the reward function exhibited a threshold behavior that depends on thebranching parameter b. In this paper we study two factors that can contribute to this transitionbehavior; namely; the branching process itself and the failure probability. On one hand weshow that the threshold behavior is robust with respect to the branching process: for all …,Proceedings of the 8th ACM conference on Electronic commerce,2007,19
On completing latin squares,Iman Hajirasouliha; Hossein Jowhari; Ravi Kumar; Ravi Sundaram,Abstract We present a (23-ϵ)-approximation algorithm for the partial latin square extension(PLSE) problem. This improves the current best bound of 1-1e due to Gomes; Regis; andShmoys 5. We also show that PLSE is APX-hard. We then consider two new and naturalvariants of PLSE. In the first; there is an added restriction that at most k colors are to be usedin the extension; for this problem; we prove a tight approximation threshold of 1-1e. In thesecond; the goal is to find the largest partial Latin square embedded in the given partialLatin square that can be extended to completion; we obtain a 14-approximation algorithm inthis case.,Annual Symposium on Theoretical Aspects of Computer Science,2007,19
On polynomial approximation to the shortest lattice vector length,Ravi Kumar; D Sivakumar,Abstract We obtain a 2^ n'*'time algorithm to approximate the length of the shortest vector inan n-dimensional lattice to within a factor of n*" 1"'. In this note we consider the complexity ofapproximating the shortest lattice vector length (called SVP-Length) when the approximationfactor is poly (n). An obvious candidate for producing polynomial approximations to SVP-Length—Schnorr's improvement of the Lovasz basis reduction algorithm [6]—turns out to beuninteresting: Schnorr's algorithm takes O (n2 (fc*/2+ 0 (fc>+ n2)) arithmetic steps (onpolynomial-sized operands) to produce a (\/6&) fc approximation. To obtain poly (n)approximation factors; k= fi (n); so the running time is 2n (nlosn); which is pointless in thelight of an 0 (nn} algorithm to solve it exactly [5]. We show:,Symposium on Discrete Algorithms: Proceedings of the twelfth annual ACM-SIAM symposium on Discrete algorithms,2001,19
Stochastic models for tabbed browsing,Flavio Chierichetti; Ravi Kumar; Andrew Tomkins,Abstract We present a model of tabbed browsing that represents a hybrid between a Markovprocess capturing the graph of hyperlinks; and a branching process capturing the birth anddeath of tabs. We present a mathematical criterion to characterize whether the process has asteady state independent of initial conditions; and we show how to characterize the limitingbehavior in both cases. We perform a series of experiments to compare our tabbed browsingmodel with pagerank; and show that tabbed browsing is able to explain 15-25% of thedeviation between actual measured browsing behavior and the behavior predicted by thesimple pagerank model. We find this to be a surprising result; as the tabbed browsing modeldoes not make use of any notion of site popularity; but simply captures deviations in userlikelihood to open and close tabs from a particular node in the graph.,Proceedings of the 19th international conference on World wide web,2010,18
An analysis framework for search sequences,Qiaozhu Mei; Kristina Klinkner; Ravi Kumar; Andrew Tomkins,Abstract In this paper we present a general framework to study sequences of searchactivities performed by a user. Our framework provides (i) a vocabulary to discuss types offeatures; models; and tasks;(ii) straightforward feature re-use across problems;(iii) realisticbaselines for many sequence analysis tasks we study; and (iv) a simple mechanism todevelop baselines for sequence analysis tasks beyond those studied in this paper. Usingthis framework we study a set of fourteen sequence analysis tasks with a range of featuresand models. While we show that most tasks benefit from features based on recent history;we also identify two categories of" sequence-resistant" tasks for which simple classes oflocal features perform as well as richer features and models.,Proceedings of the 18th ACM conference on Information and knowledge management,2009,18
Relaxation in text search using taxonomies,Marcus Fontoura; Vanja Josifovski; Ravi Kumar; Christopher Olston; Andrew Tomkins; Sergei Vassilvitskii,Abstract In this paper we propose a novel document retrieval model in which text queries areaugmented with multi-dimensional taxonomy restrictions. These restrictions may be relaxedat a cost to result quality. This new model may be applicable in many arenas; includingmultifaceted; product; and local search; where documents are augmented with hierarchicalmetadata such as topic or location. We present efficient algorithms for indexing and queryprocessing in this new retrieval model. We decompose query processing into two sub-problems: first; an online search problem to determine the correct overall level of relaxationcost that must be incurred to generate the top k results; and second; a budgeted relaxationsearch problem in which all results at a particular relaxation cost must be produced atminimal cost. We show the latter problem is solvable exactly in two hierarchical …,Proceedings of the VLDB Endowment,2008,18
Sparse and lopsided set disjointness via information theory,Anirban Dasgupta; Ravi Kumar; D Sivakumar,Abstract We study two natural variations of the set disjointness problem; arguably the mostcentral problem in communication complexity. For the k-sparse set disjointness problem;where the parties each hold ak-element subset of an n-element universe; we show a tight Θ(k log k) bound on the randomized one-way communication complexity. In addition; wepresent a slightly simpler proof of an O (k) upper bound on the general randomizedcommunication complexity of this problem; due originally to Håstad and Wigderson. For thelopsided set disjointness problem; we obtain a simpler proof of Pătraşcu's breakthroughresult; based on the information cost method of Bar-Yossef et al. The information-theoreticproof is both significantly simpler and intuitive; this is the first time the direct summethodology based on information cost has been successfully adapted to the asymmetric …,*,2012,17
Search in the lost sense of query: Question formulation in web search queries and its temporal changes,Bo Pang; Ravi Kumar,Abstract Web search is an information-seeking activity. Often times; this amounts to a userseeking answers to a question. However; queries; which encode user's information need;are typically not expressed as full-length natural language sentences---in particular; asquestions. Rather; they consist of one or more text fragments. As humans become moresearch-engine-savvy; do natural-language questions still have a role to play in web search?Through a systematic; large-scale study; we find to our surprise that as time goes by; webusers are more likely to use questions to express their search intent.,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2,2011,17
Hiring a secretary from a poset,Ravi Kumar; Silvio Lattanzi; Sergei Vassilvitskii; Andrea Vattani,Abstract The secretary problem lies at the core of mechanism design for online auctions. Inthis work we study the generalization of the classical secretary problem in a setting wherethere is only a partial order between the elements and the goal of the algorithm is to returnone of the maximal elements of the poset. This is equivalent to the auction setting where theseller has a multidimensional objective function with only a partial order among theoutcomes. We obtain an algorithm that succeeds with probability at least kk/(k-1)((1+ log k1/(k-1)) k-1); where k is the number of maximal elements in the poset and is the onlyinformation about the poset that is known to the algorithm; the success probabilityapproaches the classical bound of 1/e as k-> 1. On the other hand; we prove an almostmatching upper bound of k-1/(k-1) on the success probability of any algorithm for this …,Proceedings of the 12th ACM conference on Electronic commerce,2011,17
Sorting and selection on dynamic data,Aris Anagnostopoulos; Ravi Kumar; Mohammad Mahdian; Eli Upfal,Abstract We formulate and study a new computational model for dynamic data. In this model;the data changes gradually and the goal of an algorithm is to compute the solution to someproblem on the data at each time step; under the constraint that it only has limited access tothe data each time. As the data is constantly changing and the algorithm might be unawareof these changes; it cannot be expected to always output the exact right solution; we areinterested in algorithms that guarantee to output an approximate solution. In particular; wefocus on the fundamental problems of sorting and selection; where the true ordering of theelements changes slowly. We provide algorithms with performance close to the optimal inexpectation and with high probability.,Theoretical Computer Science,2011,17
Efficient implementation of large-scale multi-structural databases,Ronald Fagin; Ph Kolaitis; Ravi Kumar; Jasmine Novak; D Sivakumar; Andrew Tomkins,Abstract In earlier work; we defined" multi-structural databases;" a data model to supportefficient analysis of large; complex data sets over multiple numerical and hierarchicaldimensions. We defined three types of queries over this data model; each of which requiredsolving an optimization problem. An example is to find the ten most significant non-overlapping regions of geography crossed with time in which coverage of the Olympics wasmuch stronger in newspapers than online sources. In this paper; we present a general queryframework capturing the original three queries as part of a much broader family. We thengive efficient algorithms for particular subclasses of this family. Finally; we describe animplementation of these algorithms that operates on a collection of several billion webdocuments. Using our algorithms in conjunction with random sampling techniques; our …,Proceedings of the 31st international conference on Very large data bases,2005,17
Variable latent semantic indexing,Anirban Dasgupta; Ravi Kumar; Prabhakar Raghavan; Andrew Tomkins,Abstract Latent Semantic Indexing is a classical method to produce optimal low-rankapproximations of a term-document matrix. However; in the context of a particular querydistribution; the approximation thus produced need not be optimal. We propose VLSI; a newquery-dependent (or" variable") low-rank approximation that minimizes approximation errorfor any specified query distribution. With this tool; it is possible to tailor the LSI technique toparticular settings; often resulting in vastly improved approximations at much lowerdimensionality. We validate this method via a series of experiments on classical corpora;showing that VLSI typically performs similarly to LSI with an order of magnitude fewerdimensions.,Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,2005,17
Fast approximate pcps,Funda Ergun; Ravi Kumar; Ronitt Rubinfeld,*,CONF PROC ANNU ACM SYMP THEORY COMPUT. pp. 41-50. 1999,1999,17
Optimizing two-dimensional search results presentation,Flavio Chierichetti; Ravi Kumar; Prabhakar Raghavan,Abstract Classic search engine results are presented as an ordered list of documents andthe problem of presentation trivially reduces to ordering documents by their scores. This isbecause users scan a list presentation from top to bottom. This leads to natural listoptimization measures such as the discounted cumulative gain (DCG) and the rank-biasedprecision (RBP). Increasingly; search engines are using two-dimensional resultspresentations; image and shopping search results are long-standing examples. Thesimplistic heuristic used in practice is to place images by row-major order in the matrixpresentation. However; a variety of evidence suggests that users' scan of pages is not in thismatrix order. In this paper we (1) view users' scan of a results page as a Markov chain; whichyields DCG and RBP as special cases for linear lists;(2) formulate; study; and develop …,Proceedings of the fourth ACM international conference on Web search and data mining,2011,16
Optimizing web traffic via the media scheduling problem,Lars Backstrom; Jon Kleinberg; Ravi Kumar,Abstract Website traffic varies through time in consistent and predictable ways; with highesttraffic in the middle of the day. When providing media content to visitors; it is important topresent repeat visitors with new content so that they keep coming back. In this paper wepresent an algorithm to balance the need to keep a website fresh with new content with thedesire to present the best content to the most visitors at times of peak traffic. We formulatethis as the media scheduling problem; where we attempt to maximize total clicks; given theoverall traffic pattern and the time varying clickthrough rates of available media content. Wepresent an efficient algorithm to perform this scheduling under certain conditions and applythis algorithm to real data obtained from server logs; showing evidence of significantimprovements in traffic from our algorithmic schedules. Finally; we analyze the click data …,Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,2009,15
Quicklink selection for navigational query results,Deepayan Chakrabarti; Ravi Kumar; Kunal Punera,Abstract Quicklinks for a website are navigational shortcuts displayed below the websitehomepage on a search results page; and that let the users directly jump to selected pointsinside the website. Since the real-estate on a search results page is constrained andvaluable; picking the best set of quicklinks to maximize the benefits for a majority of the usersbecomes an important problem for search engines. Using user browsing trails obtained frombrowser toolbars; and a simple probabilistic model; we formulate the quicklink selectionproblem as a combinatorial optimizaton problem. We first demonstrate the hardness of theobjective; and then propose an algorithm that is provably within a factor of 1-1/e of theoptimal. We also propose a different algorithm that works on trees and that can find theoptimal solution; unlike the previous algorithm; this algorithm can incorporate natural …,Proceedings of the 18th international conference on World wide web,2009,15
Modeling user consumption sequences,Austin R Benson; Ravi Kumar; Andrew Tomkins,Abstract We study sequences of consumption in which the same item may be consumedmultiple times. We identify two macroscopic behavior patterns of repeated consumptions.First; in a given user's lifetime; very few items live for a long time. Second; the lastconsumptions of an item exhibit growing inter-arrival gaps consistent with the notion ofincreasing boredom leading up to eventual abandonment. We then present what is to ourknowledge the first holistic model of sequential repeated consumption; covering allobserved aspects of this behavior. Our simple and purely combinatorial model includes noplanted notion of lifetime distributions or user boredom; nonetheless; the model correctlypredicts both of these phenomena. Further; we provide theoretical analysis of the behaviorof the model confirming these phenomena. Additionally; the model quantitatively matches …,Proceedings of the 25th International Conference on World Wide Web,2016,14
Efficient algorithms for public-private social networks,Flavio Chierichetti; Alessandro Epasto; Ravi Kumar; Silvio Lattanzi; Vahab Mirrokni,Abstract We introduce the public-private model of graphs. In this model; we have a publicgraph and each node in the public graph has an associated private graph. The motivationfor studying this model stems from social networks; where the nodes are the users; thepublic graph is visible to everyone; and the private graph at each node is visible only to theuser at the node. From each node's viewpoint; the graph is just a union of its private graphand the public graph. We consider the problem of efficiently computing various properties ofthe graphs from each node's point of view; with minimal amount of recomputation on thepublic graph. To illustrate the richness of our model; we explore two powerful computationalparadigms for studying large graphs; namely; sketching and sampling; and focus on somekey problems in social networks and show efficient algorithms in the public-private graph …,Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2015,14
Attention and selection in online choice tasks,Vidhya Navalpakkam; Ravi Kumar; Lihong Li; D Sivakumar,Abstract The task of selecting one among several items in a visual display is extremelycommon in daily life and is executed billions of times every day on the Web. Attention is vitalfor selection; but the end-to-end process of what draws and sustains attention; and how thatinfluences selection; remains poorly understood. We study this in a complex multi-itemselection setting; where participants selected one among eight news articles presented in agrid layout on a screen. By varying the position; saliency; and topic of the news items; weidentify the relative importance of these visual and semantic factors in attention andselection. We present a simple model of attention that predicts many key features such asattention shifts and dwell time per item. Potential applications of our findings includeoptimizing visual displays to drive user attention.,International Conference on User Modeling; Adaptation; and Personalization,2012,14
Similarity caching,Flavio Chierichetti; Ravi Kumar; Sergei Vassilvitskii,Abstract We introduce the similarity caching problem; a variant of classical caching in whichan algorithm can return an element from the cache that is similar; but not necessarilyidentical; to the query element. We are motivated by buffer management questions inapproximate nearest-neighbor applications; especially in the context of caching targetedadvertisements on the web. Formally; we assume the queries lie in a metric space; withdistance function d (.;.). A query p is considered a cache hit if there is a point q in the cachethat is sufficiently close to p; ie; for a threshold radius r; we have d (p; q)≤ r. The goal is thento minimize the number of cache misses; vis-à-vis the optimal algorithm. As with classicalcaching; we use the competitive ratio to measure the performance of different algorithms.While similarity caching is a strict generalization of classical caching; we show that unless …,Proceedings of the twenty-eighth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2009,14
Minimizing wirelength in zero and bounded skew clock trees,Moses Charikar; Jon Kleinberg; Ravi Kumar; Sridhar Rajagopalan; Amit Sahai; Andrew Tomkins,An important problem in VLSI design is distributing a clock signal to synchronous elementsin a VLSI circuit so that the signal arrives at all elements simultaneously. The signal isdistributed by means of a clock routing tree rooted at a global clock source. The difference inlength between the longest and shortest root-leaf path is called the skew of the tree. Theproblem is to construct a clock tree with zero skew (to achieve synchronicity) and minimalsum of edge lengths (so that circuit area and clock tree capacitance are minimized). We givethe first constant-factor approximation algorithms for this problem and its variants that arisein the VLSI context. For the zero skew problem in general metric spaces; we give anapproximation algorithm with a performance guarantee of 2 e. For the L 1 version on theplane; we give an (8/ln 2)-approximation algorithm.,SIAM Journal on Discrete Mathematics,2004,14
Topic distillation and spectral filtering,Soumen Chakrabarti; Byron E Dom; David Gibson; Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Andrew Tomkins,Abstract This paper discuss topic distillation; an information retrieval problemthat isemerging as a critical task for the www. Algorithms for this problemmust distill a smallnumber of high-quality documents addressing a broadtopic from a large set of candidates.We give a review of the literature; and compare the problem with relatedtasks such asclassification; clustering; and indexing. We then describe ageneral approach to topicdistillation with applications to searching andpartitioning; based on the algebraic propertiesof matrices derived fromparticular documents within the corpus. Our method–which we callspecial filtering–combines the use of terms; hyperlinks and anchor-textto improve retrievalperformance. We give results for broad-topic querieson the www; and also give someanecdotal results applying the sametechniques to US Supreme Court law cases; US …,Artificial Intelligence Review,1999,14
Driven by food: Modeling geographic choice,Ravi Kumar; Mohammad Mahdian; Bo Pang; Andrew Tomkins; Sergei Vassilvitskii,Abstract In this work we study the dynamics of geographic choice; ie; how users choose onefrom a set of objects in a geographic region. We postulate a model in which an object isselected from a slate of candidates with probability that depends on how far it is (distance)and how many closer alternatives exist (rank). Under a discrete choice formulation; weargue that there exists a factored form in which unknown functions of rank and distance maybe combined to produce an accurate estimate of the likelihood that a user will select eachalternative. We then learn these hidden functions and show that each can be closelyapproximated by an appropriately parameterized lognormal; even though the respectivemarginals look quite different. We give a theoretical justification to support the presence oflognormal distributions. We then apply this framework to study restaurant choices in map …,Proceedings of the Eighth ACM International Conference on Web Search and Data Mining,2015,13
Inverting a steady-state,Ravi Kumar; Andrew Tomkins; Sergei Vassilvitskii; Erik Vee,Abstract We consider the problem of inferring choices made by users based only onaggregate data containing the relative popularity of each item. We propose a framework thatmodels the problem as that of inferring a Markov chain given a stationary distribution.Formally; we are given a graph and a target steady-state distribution on its nodes. We arealso give a mapping from per-node scores to a transition matrix; from a broad family of suchmappings. The goal is to set the scores of each node such that the resulting transition matrixinduces the desired steady state. We prove sufficient conditions under which this problem isfeasible and; for the feasible instances; obtain a simple algorithm for a generic version of theproblem. This iterative algorithm provably finds the unique solution to this problem and has apolynomial rate of convergence; in practice we find that the algorithm converges after …,Proceedings of the Eighth ACM International Conference on Web Search and Data Mining,2015,13
For a few dollars less: Identifying review pages sans human labels,Luciano Barbosa; Ravi Kumar; Bo Pang; Andrew Tomkins,Abstract We address the problem of large-scale automatic detection of online reviewswithout using any human labels. We propose an efficient method that combines two basicideas: Building a classifier from a large number of noisy examples and using the structure ofthe website to enhance the performance of this classifier. Experiments suggest that ourmethod is competitive against supervised learning methods that mandate expensive humaneffort.,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,2009,13
Unweaving a web of documents,R Guha; Ravi Kumar; D Sivakumar; Ravi Sundaram,Abstract We develop an algorithmic framework to decompose a collection of time-stampedtext documents into semantically coherent threads. Our formulation leads to a graphdecomposition problem on directed acyclic graphs; for which we obtain three algorithms---an exact algorithm that is based on minimum cost flow and two more efficient algorithmsbased on maximum matching and dynamic programming that solve specific versions of thegraph decomposition problem. Applications of our algorithms include superiorsummarization of news search results; improved browsing paradigms for large collections oftext-intensive corpora; and integration of time-stamped documents from a variety of sources.Experimental results based on over 250;000 news articles from a major newspaper over aperiod of four years demonstrate that our algorithms efficiently identify robust threads of …,Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,2005,13
Conversational flow in Oxford-style debates,Justine Zhang; Ravi Kumar; Sujith Ravi; Cristian Danescu-Niculescu-Mizil,Abstract: Public debates are a common platform for presenting and juxtaposing divergingviews on important issues. In this work we propose a methodology for tracking how ideasflow between participants throughout a debate. We use this approach in a case study ofOxford-style debates---a competitive format where the winner is determined by audiencevotes---and show how the outcome of a debate depends on aspects of conversational flow.In particular; we find that winners tend to make better use of a debate's interactivecomponent than losers; by actively pursuing their opponents' points rather than promotingtheir own ideas over the course of the conversation.,arXiv preprint arXiv:1604.03114,2016,12
On sampling nodes in a network,Flavio Chiericetti; Anirban Dasgupta; Ravi Kumar; Silvio Lattanzi; Tamás Sarlós,Abstract Random walk is an important tool in many graph mining applications includingestimating graph parameters; sampling portions of the graph; and extracting densecommunities. In this paper we consider the problem of sampling nodes from a large graphaccording to a prescribed distribution by using random walk as the basic primitive. Our goalis to obtain algorithms that make a small number of queries to the graph but output a nodethat is sampled according to the prescribed distribution. Focusing on the uniform distributioncase; we study the query complexity of three algorithms and show a near-tight boundexpressed in terms of the parameters of the graph such as average degree and the mixingtime. Both theoretically and empirically; we show that some algorithms are preferable inpractice than the others. We also extend our study to the problem of sampling nodes …,Proceedings of the 25th International Conference on World Wide Web,2016,12
Cross-validation and mean-square stability,Satyen Kale; Ravi Kumar; Sergei Vassilvitskii,Abstract: A popular practical method of obtaining a good estimate of the error rate of alearning algorithm is k-fold cross-validation. Here; the set of examples is first partitioned intok equal-sized folds. Each fold acts as a test set for evaluating the hypothesis learned on theother k− 1 folds. The average error across the k hypotheses is used as an estimate of theerror rate. Although widely used; especially with small values of k (such as 10); the cross-validation method has heretofore resisted theoretical analysis due to the fact that the kdistinct estimates have inherent correlations between them. With only sanity-check boundsknown; there is no compelling reason to use the k-fold cross-validation estimate over asimpler holdout estimate. Conventional wisdom is that the averaging in cross-validationleads to a tighter concentration of the estimate of the error around its mean. In this paper …,In Proceedings of the Second Symposium on Innovations in Computer Science (ICS2011,2011,12
Generating succinct titles for web urls,Deepayan Chakrabarti; Ravi Kumar; Kunal Punera,Abstract How can a search engine automatically provide the best and most appropriate titlefor a result URL (link-title) so that users will be persuaded to click on the URL? We considerthe problem of automatically generating link-titles for URLs and propose a general statisticalframework for solving this problem. The framework is based on using information from adiverse collection of sources; each of which can be thought of as contributing one or morecandidate link-titles for the URL. It can also incorporate the context in which the link-title willbe used; along with constraints on its length. Our framework is applicable to severalscenarios: obtaining succinct titles for displaying quicklinks; obtaining titles for URLs thatlack a good title; constructing succinct sitemaps; etc. Extensive experiments show that ourmethod is very effective; producing results that are at least 20% better than non-trivial …,Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,2008,12
Optimizing query rewrites for keyword-based advertising,Azarakhsh Malekian; Chi-Chao Chang; Ravi Kumar; Grant Wang,Abstract We consider the problem of query rewrites in the context of pay-per-click searchadvertising. Given a three-layer graph consisting of queries; query rewrites; and thecorresponding ads that can be served for the rewrites; we formulate a family of graphcovering problems whose goals are to suggest a subset of ads with the maximum benefit bysuggesting rewrites for a given query. We obtain constant-factor approximation algorithmsfor these covering problems; under two versions of constraints and a realistic notion of adbenefit. We perform experiments on real data and show that our algorithms are capable ofoutperforming a competitive baseline algorithm in terms of the benefit of the rewrites.,Proceedings of the 9th ACM conference on Electronic commerce,2008,12
Self-testing without the generator bottleneck,Funda Ergün; S Ravi Kumar; D Sivakumar,Suppose P is a program designed to compute a function f defined on a group G. The task ofself-testing P; that is; testing if P computes f correctly on most inputs; usually involves testingexplicitly if P computes f correctly on every generator of G. In the case of multivariatefunctions; the number of generators; and hence the number of such tests; becomesprohibitively large. We refer to this problem as the generator bottleneck. We develop atechnique that can be used to overcome the generator bottleneck for functions that have acertain nice structure; specifically if the relationship between the values of the function on theset of generators is easily checkable. Using our technique; we build the first efficient self-testers for many linear; multilinear; and some nonlinear functions. This includes the FFT; andvarious polynomial functions. All of the self-testers we present make only O (1) calls to the …,SIAM Journal on Computing,2000,12
A translation model for matching reviews to objects,Nilesh Dalvi; Ravi Kumar; Bo Pang; Andrew Tomkins,Abstract We develop a generic method for the review matching problem; which is to matchunstructured text reviews to a list of objects; where each object has a set of attributes. To thisend; we propose a translation model for generating reviews from a structured description ofobjects. We develop an EM-based method to estimate the model parameters and use thismodel to find; given a review; the object most likely to be the topic of the review. We conductextensive experiments on two large-scale datasets: a collection of restaurant reviews fromYelp and a collection of movie reviews from IMDb. The experiments show that our translationmodel-based method is superior to traditional tf-idf based methods as well as a recentmixture model-based method for the review matching problem.,Proceedings of the 18th ACM conference on Information and knowledge management,2009,11
Para'Normal'Activity: On the Distribution of Average Ratings.,Nilesh N Dalvi; Ravi Kumar; Bo Pang,Abstract In this paper we study the distribution of average user rating of entities in threedifferent domains: restaurants; movies; and products. We find that the distribution is heavilyskewed; closely resembling a log-normal in all the cases. In contrast; the distribution ofaverage critic rating is much closer to a normal distribution. We propose user selection biasas the underlying behavioral phenomenon causing this disparity in the two distributions. Weshow that selection bias can indeed lead to a skew in the distribution of user ratings evenwhen we assume the quality of entities are normally distributed. Finally; we apply theseinsights to the problem of predicting the overall rating of an entity given its few initial ratings;and obtain a simple method that outperforms strong baselines.,ICWSM,2013,10
Corrigendum to efficient similarity search and classification via rank aggregation by Ronald Fagin; Ravi Kumar and D. Sivakumar (proc. SIGMOD'03),Alexandr Andoni; Ronald Fagin; Ravi Kumar; Mihai Patrascu; D Sivakumar,In this corrigendum; we correct an error in the paper [1]. The error was discovered byAlexandr Andoni; and the corrected theorem is due to the three authors of [1]; along withAlexandr Andoni and Mihai Patrascu. Theorem 4 of [1] states: Let D be a collection of npoints in Rd. Let r1;... rm be random unit vectors in Rd; where m= αϵ− 2 log n with α suitablychosen. Let q∈ Rd be an arbitrary point; and define; for each i with 1≤ i≤ m; the ranked listLi of the n points in D by sorting them in increasing order of their distances to the projectionof q along ri. For each element x of D; let medrank (x)= median (L1 (x);...; Lm (x)). Let z be amember of D such that medrank (z) is minimized. Then with probability at least 1− 1/n; wehave z− q2≤(1+ ϵ) x− q2 for all x∈ D. As stated; the above theorem does not hold; but aversion of it holds if one replaces the median over ranks by a median over suitably …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,10
On the unique shortest lattice vector problem,S Ravi Kumar; D Sivakumar,Page 1. Theoretical Computer Science 255 (2001) 641–648 www.elsevier.com/locate/tcs NoteOn the unique shortest lattice vector problem S. Ravi Kumar; D. Sivakumar∗ IBM AlmadenResearch Center; Department K-53=B-1; 650 Harry Road; San Jose; CA 95120-6099; USAReceived 15 August 1998; revised 10 August 2000; accepted 31 August 2000 Communicatedby O. Watanabe Abstract We show that the problem of deciding whether a given rational latticeL has a vector of length less than some given value r is NP-hard; even under the promise thatL has exactly zero or one vector of length less than r. cO 2001 Published by Elsevier ScienceBV Keywords: Integer lattices; Shortest vector problem; Unique solutions 1. Introduction Is it easierto decide instances of NP-hard problems when they are given with the additional promise thatthe associated search problem has exactly zero or one solution …,Theoretical Computer Science,2001,10
Sampling hidden objects using nearest-neighbor oracles,Nilesh Dalvi; Ravi Kumar; Ashwin Machanavajjhala; Vibhor Rastogi,Abstract Given an unknown set of objects embedded in the Euclidean plane and a nearest-neighbor oracle; how to estimate the set size and other properties of the objects? In thispaper we address this problem. We propose an efficient method that uses the Voronoipartitioning of the space by the objects and a nearest-neighbor oracle. Our method can beused in the hidden web/databases context where the goal is to estimate the number ofcertain objects of interest. Here; we assume that each object has a geographic location andthe nearest-neighbor oracle can be realized by applications such as maps; local; or store-locator APIs. We illustrate the performance of our method on several real-world datasets.,Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,2011,9
Shatterplots: Fast tools for mining large graphs,Ana Paula Appel; Ravi Kumar; Deepayan Chakrabarti; Jure Leskovec; Christos Faloutsos; Andrew Tomkins,Abstract Graphs appear in several settings; like social networks; recommendation systems;computer communication networks; gene/protein biological networks; among others. Adeep; recurring question is “What do real graphs look like?” That is; how can we separatereal ones from synthetic or real graphs with masked portions? The main contribution of thispaper is ShatterPlots; a simple and powerful algorithm to extract patterns from real graphsthat help us spot fake/masked graphs. The idea is to shatter a graph; by deleting edges;force it to reach a critical (“Shattering”) point; and study the properties at that point. One ofthe most striking patterns is the “30-per-cent “: at the Shattering point; all real and syntheticgraphs have about 30% more nodes than edges. One of our most discriminative patterns isthe “NodeShatteringRatio “; which can almost perfectly separate the real graphs from the …,*,2009,9
Programmable clustering,Sreenivas Gollapudi; Ravi Kumar; D Sivakumar,Abstract We initiate a novel study of clustering problems. Rather than specifying an explicitobjective function to optimize; our framework allows the user of clustering algorithm tospecify; via a first-order formula; what constitutes an acceptable clustering to them. While theresulting genre of problems includes; in general; NP-complete problems; we highlight threespecific first-order formulae; and provide efficient algorithms for the resulting clusteringproblems.,Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2006,9
A note on the shortest lattice vector problem,Ravi Kumar; D Sivakumar,Is it easier to decide instances of NP-hard problems when they are given with the additional promisethat the associated search problem has exactly zero or one solu- tion? Over a decade ago; Valiantand Vazirani [VV86] proved a beautiful result that shows that this is not the case. Moreformally; they gave a probabilistic many-one reduc- tion from the NP-complete Boolean formulasatisfiability problem to the problem of deciding whether a Boolean formula is satisfiable underthe promise that it has either zero or one satisfying assignment. Virtually all known NP- completedecision problems are known to be NP-complete under polynomial-time many-one reductionsthat preserve the number of solutions (often called parsimonious reduc- tions; see [Pap94; pages441–442]). Therefore; it follows that the zero-or-one promise version of every NP-complete decisionproblem is also NP-hard. In a recent breakthrough; Ajtai [Ajt98] showed that the problem …,Computational Complexity; 1999. Proceedings. Fourteenth Annual IEEE Conference on,1999,9
Counting graphlets: Space vs time,Marco Bressan; Flavio Chierichetti; Ravi Kumar; Stefano Leucci; Alessandro Panconesi,Abstract Counting graphlets is a well-studied problem in graph mining and social networkanalysis. Recently; several papers explored very simple and natural approaches based onMonte Carlo sampling of Markov Chains (MC); and reported encouraging results. We show;perhaps surprisingly; that this approach is outperformed by a carefully engineered version ofcolor coding (CC)[1]; a sophisticated algorithmic technique that we extend to the case ofgraphlet sampling and for which we prove strong statistical guarantees. Our computationalexperiments on graphs with millions of nodes show CC to be more accurate than MC.Furthermore; we formally show that the mixing time of the MC approach is too high ingeneral; even when the input graph has high conductance. All this comes at a pricehowever. While MC is very efficient in terms of space; CC's memory requirements become …,Proceedings of the Tenth ACM International Conference on Web Search and Data Mining,2017,8
Mining web logs: applications and challenges,Ravi Kumar,Abstract Web logs record the primary interaction of users with web pages in general andsearch engines in particular. There are two sources for such logs: user trails obtained fromtoolbars and query/click information obtained from search engines. In this talk we willaddress the task of mining this rich data to improve user experience on the web. We willillustrate a few applications; together with the modeling and algorithmic challenges that stemfrom these applications. We will also discuss the privacy issues that arise in this context.,Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,2009,8
A note on the set systems used for broadcast encryption,Ravi Kumar; Alexander Russell,Abstract An exclusive set system is a family of subsets of a universe with the property thatevery large subset may be written as the union of subsets from the family. We obtain newupper bounds on the size of such families; showing that in a universe of n elements; there isa system of 48k 3 (nk) r/k In subsets with the property that every subset of the universe ofsize n--r can be written as the union of k subsets in the system. Such sets systems form thecombinatorial foundation of many broadcast encryption schemes.,Proceedings of the fourteenth annual ACM-SIAM symposium on Discrete algorithms,2003,8
Efficient self-testing/self-correction of linear recurrences,S Ravi Kumar; D Sivakumar,The authors consider the problem of designing self-testers/self-correctors for functionsdefined by linear recurrences. They present the first complete package of efficient andsimple self-testers; self-correctors; and result-checkers for such functions. The results areproved by demonstrating an efficient reduction from this problem to the problem of testinglinear functions over certain matrix groups. The tools include spectral analysis of matricesover finite fields; and various counting arguments that extend known techniques. The matrixtwist yields simple and efficient self-testers for all linear recurrences. They also show atechnique of using convolution identities to obtain very simple self-testers and selfcorrectors. Their techniques promise new and efficient ways of testing VLSI chips forapplications in control engineering; signal processing; etc. An interesting consequence of …,Foundations of Computer Science; 1996. Proceedings.; 37th Annual Symposium on,1996,8
Local search methods for k-means with outliers,Shalmoli Gupta; Ravi Kumar; Kefu Lu; Benjamin Moseley; Sergei Vassilvitskii,Abstract We study the problem of k-means clustering in the presence of outliers. The goal isto cluster a set of data points to minimize the variance of the points assigned to the samecluster; with the freedom of ignoring a small set of data points that can be labeled as outliers.Clustering with outliers has received a lot of attention in the data processing community; butpractical; efficient; and provably good algorithms remain unknown for the most popular k-means objective. Our work proposes a simple local search-based algorithm for k-meansclustering with outliers. We prove that this algorithm achieves constant-factor approximatesolutions and can be combined with known sketching techniques to scale to large data sets.Using empirical evaluation on both synthetic and large-scale real-world data; wedemonstrate that the algorithm dominates recently proposed heuristic approaches for the …,Proceedings of the VLDB Endowment,2017,7
On the relevance of irrelevant alternatives,Austin R Benson; Ravi Kumar; Andrew Tomkins,Abstract Multinomial logistic regression is a powerful tool to model choice from a finite set ofalternatives; but it comes with an underlying model assumption called the independence ofirrelevant alternatives; stating that any item added to the set of choices will decrease allother items' likelihood by an equal fraction. We perform statistical tests of this assumptionacross a variety of datasets and give results showing how often it is violated. When thisaxiom is violated; choice theorists will often invoke a richer model known as nested logisticregression; in which information about competition among items is encoded in a treestructure known as a nest. However; to our knowledge there are no known algorithms toinduce the correct nest structure. We present the first such algorithm; which runs in quadratictime under an oracle model; and we pair it with a matching lower bound.,Proceedings of the 25th International Conference on World Wide Web,2016,7
On learning mixture models for permutations,Flavio Chierichetti; Anirban Dasgupta; Ravi Kumar; Silvio Lattanzi,Abstract In this paper we consider the problem of learning a mixture of permutations; whereeach component of the mixture is generated by a stochastic process. Learning permutationmixtures arises in practical settings when a set of items is ranked by different sub-populations and the rankings of users in a sub-population tend to agree with each other.While there is some applied work on learning such mixtures; they have been mostly heuristicin nature. We study the problem where the permutations in a mixture component aregenerated by the classical Mallows process in which each component is associated with acenter and a scalar parameter. We show that even when the centers are arbitrarilyseparated; with exponentially many samples one can learn the mixture; provided theparameters are all the same and known; we also show that the latter two assumptions are …,Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science,2015,7
Refer-to-as Relations as Semantic Knowledge.,Song Feng; Sujith Ravi; Ravi Kumar; Polina Kuznetsova; Wei Liu; Alexander C Berg; Tamara L Berg; Yejin Choi,Abstract We study Refer-to-as relations as a new type of semantic knowledge. Compared tothe much studied Is-a relation; which concerns factual taxonomic knowledge; Refer-to-asrelations aim to address pragmatic semantic knowledge. For example; a “penguin” is a “bird”from a taxonomic point of view; but people rarely refer to a “penguin” as a “bird” invernacular use. This observation closely relates to the entry-level categorization studied inPsychology. We posit that Refer-toas relations can be learned from data; and that bothtextual and visual information would be helpful in inferring the relations. By integratingexisting lexical structure knowledge with language statistics and visual similarities; weformulate a collective inference approach to map all object names in an encyclopedia tocommonly used names for each object. Our contributions include a new labeled data set …,AAAI,2015,6
A Characterization of Online Search Behaviour,R Kumar; A Tomkins,*,Data Engineering Bullettin,2009,6
Efficient discovery of authoritative resources,Ravi Kumar; Kevin Lang; Cameron Marlow; Andrew Tomkins,Given a dynamic corpus whose content and attention are changing on a daily basis; is itpossible to collect and maintain the high-quality resources with a minimal investment? Weaddress two problems that arise from this question for hyperlinked corpora such as webpages or blogs: how to efficiently discover the correct set of authoritative resources given afixed network; and how to track these resources over time as new entrants arrive; oldstandbys depart; and existing participants change roles.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,6
Social networks: From the Web to knowledge management,Ravi Kumar; Prabhakar Raghavan; Sridhar Rajagopalan; Andrew Tomkins,Abstract The study of the Web as a network has resulted in a better understanding of thesociology of Web content creation. This has paid off in higher-precision search engines andmore effective algorithms for data mining the Web. The bulk of this chapter is devoted toreviewing the research in this area in the broader context of social networks. We point outthat the broader setting holds abundant promise for addressing a variety of problems inknowledge management.,*,2003,6
On mixtures of Markov chains,Rishi Gupta; Ravi Kumar; Sergei Vassilvitskii,Abstract We study the problem of reconstructing a mixture of Markov chains from thetrajectories generated by random walks through the state space. Under mild non-degeneracy conditions; we show that we can uniquely reconstruct the underlying chains byonly considering trajectories of length three; which represent triples of states. Our algorithmis spectral in nature; and is easy to implement.,Advances in neural information processing systems,2016,5
On reconstructing a hidden permutation,Flavio Chierichetti; Anirban Dasgupta; Ravi Kumar; Silvio Lattanzi,Abstract The Mallows model is a classical model for generating noisy perturbations of ahidden permutation; where the magnitude of the perturbations is determined by a singleparameter. In this work we consider the following reconstruction problem: given severalperturbations of a hidden permutation that are generated according to the Mallows model;each with its own parameter; how to recover the hidden permutation? When the parametersare approximately known and satisfy certain conditions; we obtain a simple algorithm forreconstructing the hidden permutation; we also show that these conditions are nearlyinevitable for reconstruction. We then provide an algorithm to estimate the parametersthemselves. En route we obtain a precise characterization of the swapping probability in theMallows model.,LIPIcs-Leibniz International Proceedings in Informatics,2014,5
Influence and Correlation in Social Network,Mohammad Mahdian; A Anagnostopoulos; R Kumar,Page 1. Influence and Correlation in Social Networks Mohammad Mahdian Yahoo! ResearchJoint work with Aris Anagnostopoulos and Ravi Kumar to appear in KDD'08. Page 2. Socialsystems ∎ Social network: graph that represents relationships between independent agents. ∎Social networks are everywhere and are shaping our lives: ❑ Network of professional contacts(eg; for finding jobs) ❑ Network of colleagues (eg; for learning new techniques) ❑ Web 2.0systems: ∎ Online social networks: facebook; myspace; orkut; IM; linkedIn; twitter; … ∎ Contentsharing: flickr; del.icio.us; youtube; weblogs; … ∎ Content creation: wikipedia; … Page 3.Research on Social Networks ∎ The Online Revolution: ❑ People switch more and more oftheir interactions from offline to online ❑ Pushing the # of contacts we can keep track of ❑Redefining privacy ❑ Ideal for experiments in social sciences …,Proceeding of the 14-th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2008,5
Fast approximate probabilistically checkable proofs,Funda Ergun; Ravi Kumar; Ronitt Rubinfeld,We investigate the question of when a verifier; with the aid of a proof; can reliably compute afunction faster than it can without the proof. The proof system model that we use is based ona variant of the Probabilistically Checkable Proofs (PCP) model; in which a verifier canascertain the correctness of the proof by looking at very few locations in the proof. However;known results in the PCP model require that the verifier spend time linear in the size of theinput in order to determine where to query the proof. In this work; we focus on the case whenit is enough for the verifier to know that the answer is close to correct; and develop anapproximate PCP model. We construct approximate PCPs for several optimization problems;in which the total running time of the verifier is significantly less than the size of the input. Forexample; we give polylogarithmic time approximate PCPs for showing the existence of a …,Information and Computation,2004,5
Roundness estimation via random sampling,Ravi Kumar; D Sivakumar,Abstract We present an efficient probabilistic algorithm to estimate the roundness of aconvex object on the plane. The probing model we use; that is; the type of access thealgorithm has to the object; was defined by Cole and Yap [CY87]; and is related to physicaldevices employed in computational metrology. This algorithm is not only simple but alsovery different from and more efficient than previous algorithms for this problem [Swa93;LL91; EFNN89; MSY97]. Our analysis involves proving sharp versions of the planarisoperimetric inequality and using them in conjunction with results from geometricprobability.,Symposium on Discrete Algorithms: Proceedings of the tenth annual ACM-SIAM symposium on Discrete algorithms,1999,5
A Comparative Study and Analysis of Web Service Testing Tools,Ravi Kumar; AJ Singh,Abstract Software testing in present era is the process of validating and verifying thecorrectness of software. Automated testing tool enables the developer and tester toautomate the whole process of testing in software development life cycle (SDLC). Testing isvery important phase of SDLC where the software is examined properly and modificationsare proposed. Thus testing is necessary for quality of service provided by software. Webservice is widely used concept now a days and less literature is available regarding webservice performance and SOAP messaging. The objective of this paper is to conduct thecomparative study of automated tool for web services as a leading tool in black box testautomation. This study will help in the promotion and usage of various open source webservice tool toward performance of real time network using quality of service (QOS) …,International Journal of Computer Science and Mobile ComputingIJCSMC,2015,4
Deterministic decentralized search in random graphs,Esteban Arcaute; Ning Chen; Ravi Kumar; David Liben-Nowell; Mohammad Mahdian; Hamid Nazerzadeh; Ying Xu,Abstract We study a general framework for decentralized search in random graphs. Ourmain focus is on deterministic memoryless search algorithms that use only local informationto reach their destination in a bounded number of steps in expectation. This class includes(with small modifications) the search algorithms used in Kleinberg's pioneering work on long-range percolation graphs and hierarchical network models. We give a characterization ofsearchable graphs in this model; and use this characterization to prove a monotonicityproperty for searchability.,Internet Mathematics,2008,4
Sketching; Embedding and Dimensionality Reduction in Information Theoretic Spaces,Amirali Abdullah; Ravi Kumar; Andrew McGregor; Sergei Vassilvitskii; Suresh Venkatasubramanian,Abstract Sketching and dimensionality reduction are powerful techniques for speeding upalgorithms for massive data. However unlike the rich toolbox available for the l2 2 distance;there are no robust results of this nature known for most popular informationtheoreticmeasures. In this paper we show how to embed information distances like the χ2 andJensen-Shannon divergences efficiently in low dimensional spaces while preserving allpairwise distances. We then prove a dimensionality reduction result for the Hellinger;Jensen–Shannon; and χ2 divergences that preserves the information geometry of thedistributions; specifically; by retaining the simplex structure of the space. While our first resultalready implies these divergences can be explicitly embedded in the Euclidean space;retaining the simplex structure is important because it allows us to do inferences in the …,Artificial Intelligence and Statistics,2016,3
Approximate modularity,Flavio Chierichetti; Abhimanyu Das; Anirban Dasgupta; Ravi Kumar,A set function on a ground set of size n is approximately modular if it satisfies everymodularity requirement to within an additive error; approximate modularity is the set analogof approximate linearity. In this paper we study how close; in additive error; canapproximately modular functions be to truly modular functions. We first obtain a polynomialtime algorithm that makes O (n 2 log n) queries to any approximately modular function toreconstruct a modular function that is O (√ n)-close. We also show an almost matchinglower bound: any algorithm world need super polynomially many queries to construct amodular function that is o (√(n/log n))-close. In a striking contrast to these near-tightcomputational reconstruction bounds; we then show that for any approximately modularfunction; there exists a modular function that is O (log n)-close.,Foundations of Computer Science (FOCS); 2015 IEEE 56th Annual Symposium on,2015,3
Dynamic Gaits and Control in Flexible Body Quadruped Robot,P Murali Krishna; R Prasanth Kumar; S Srivastava,Legged robots are highly attractive for military purposes such as carrying heavy loads onuneven terrain for long durations because of the higher mobility they give on rough terraincompared to wheeled vehicles/robots. Existing state-of-the-art quadruped robots developedby Boston Dynamics such as LittleDog and BigDog do not have flexible bodies. It can beeasily seen that the agility of quadruped animals such as dogs; cats; and deer etc. depend toa large extent on their ability to flex their bodies. However; simulation study on step climbingin 3D terrain quadruped robot locomotion with flexible body has not been reported inliterature. This paper aims to study the effect of body flexibility on stability and energyefficiency in walking mode; trot mode and running (bounding) mode on step climbing.,Proc. Of the 1st and 16th Natl. Conf. on Machines and Mechanisms (iNaCoMM2013),2013,3
Markov layout,Flavio Chierichetti; Ravi Kumar; Prabhakar Raghavan,Consider the problem of laying out a set of n images that match a query onto the nodes ofa√ n×√ n grid. We are given a score for each image; as well as the distribution of patternsby which a user's eye scans the nodes of the grid and we wish to maximize the expectedtotal score of images selected by the user. This is a special case of the Markov layoutproblem; in which we are given a Markov chain M together with a set of objects to be placedat the states of the Markov chain. Each object has a utility to the user if viewed; as well as astopping probability with which the user ceases to look further at objects. This layoutproblem is prototypical in a number of applications in web search and advertising;particularly in an emerging genre of search results pages from major engines. In a differentclass of applications; the states of the Markov chain are web pages at a publishers …,Foundations of Computer Science (FOCS); 2011 IEEE 52nd Annual Symposium on,2011,3
Balanced allocation with succinct representation,Saeed Alaei; Ravi Kumar; Azarakhsh Malekian; Erik Vee,Abstract Motivated by applications in guaranteed delivery in computational advertising; weconsider the general problem of balanced allocation in a bipartite supply-demand setting.Our formulation captures the notion of deviation from being balanced by a convex penaltyfunction. While this formulation admits a convex programming solution; we strive for morerobust and scalable algorithms. For the case of L 1 penalty functions we obtain a simplecombinatorial algorithm based on min-cost flow in graphs and show how to precompute alinear amount of information such that the allocation along any edge can be approximated inconstant time. We then extend our combinatorial solution to any convex function by solving aconvex cost flow. These scalable methods may have applications in other contextsstipulating balanced allocation.,Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,2010,3
Dense subgraph extraction,David Gibson; Ravi Kumar; Kevin S McCurley; Andrew Tomkins,A clique in a graph is a collection of vertices such that each pair of vertices in the collectionhas an edge between them. A clique is the extreme form of a dense subgraph: Everypossible edge within the subgraph exists. In this chapter; we will consider weakening thisnotion by searching for collections of vertices such that many; but not necessarily all; pairswithin the collection have an edge between them. We will call such a collection of verticesand the edges between them a dense subgraph. The particular definition we use willdepend on the situation; we consider several variants.,Mining graph data,2007,3
An overview of the sieve algorithm for the shortest lattice vector problem,Miklós Ajtai; Ravi Kumar; Dandapani Sivakumar,Page 1. An Overview of the Sieve Algorithm for the Shortest Lattice Vector Problem Miklós Ajtai;Ravi Kumar; and Dandapani Sivakumar IBM Almaden Research Center 650 Harry Road; SanJose; CA 95120 {ajtai;ravi;siva}@almaden.ibm.com We present an overview of a randomized2 O(n) time algorithm to compute a shortest non-zero vector in an n-dimensional rational lattice.The complete details of this algorithm can be found in [2]. A lattice is a discrete additive subgroupof Rn . One way to specify a lattice is through a basis. A basis B = {b1;...;bn} is a set of linearlyindependent vectors in Rn . The lattice generated by a basis B is L = L(B) = { ∑n i=1 cibi | ci ∈Z}. The shortest lattice vector problem (SVP) is the problem of finding a shortest …,*,2001,3
Property testing of Abelian group operations,S Ravi Kumar; Ronitt Rubinfeld,Abstract Given an nn table of a cancellative operation on a domain of size n; we investigatethe complexity of determining whether is close (equal on most pairs of inputs) to anassociative; commutative; and cancellative group operation 0. We show that one canperform such a test in O (n) time. In contrast; quadratic time is necessary and sufficient to testthat a given operation is cancellative; associative; and commutative. We give a sub-quadratic algorithm for the case when is not known to be cancellative. Our techniques for thecase when is not known to be cancellative were later used by [EKK+97] to test that a functionis associative in the same case. Furthermore; we show how to compute 0 in constant time;given access to. We show that our simple test can be used to quickly check the validity oftables of abelian groups and fields. Another application of our results is to testing …,*,1998,3
Faster algorithms for optical switch configuration,S Ravi Kumar; Alexander Russell; Ravi Sundaram,All-optical networks using wavelength division multiplexing are increasingly coming to beregarded as the technology of choice for the next generation of wide-area backbonenetworks. These networks incorporate optical switches that employ the concept of LatinRouters for assigning wavelengths to routes. The issue of maximizing wavelength utilizationat these switching devices is of great importance since it lends to significant improvements inoverall network performance. In this paper we present two fast approximation algorithms-GREEDY and MATCH for the problem of maximizing wavelength utilization at Latin Routers.These are the first known polynomial-time approximation algorithms for the problem ofmaximizing the number of entries that can be added to a partially filled Latin Square thatachieve non-trivial worst-case performance guarantees. These algorithms are easily …,Communications; 1997. ICC'97 Montreal; Towards the Knowledge Millennium. 1997 IEEE International Conference on,1997,3
Algorithms for $\ell_p $ Low Rank Approximation,Flavio Chierichetti; Sreenivas Gollapudi; Ravi Kumar; Silvio Lattanzi; Rina Panigrahy; David P Woodruff,Abstract: We consider the problem of approximating a given matrix by a low-rank matrix soas to minimize the entrywise $\ell_p $-approximation error; for any $ p\geq 1$; the case $ p=2$ is the classical SVD problem. We obtain the first provably good approximation algorithmsfor this version of low-rank approximation that work for every value of $ p\geq 1$; including $p=\infty $. Our algorithms are simple; easy to implement; work well in practice; and illustrateinteresting tradeoffs between the approximation quality; the running time; and the rank of theapproximating matrix.,arXiv preprint arXiv:1705.06730,2017,2
Linear additive markov processes,Ravi Kumar; Maithra Raghu; Tamás Sarlós; Andrew Tomkins,Abstract We introduce LAMP: the Linear Additive Markov Process. Transitions in LAMP maybe influenced by states visited in the distant history of the process; but unlike higher-orderMarkov processes; LAMP retains an efficient parameterization. LAMP also allows thespecific dependence on history to be learned efficiently from data.,Proceedings of the 26th International Conference on World Wide Web,2017,2
The distortion of locality sensitive hashing,Flavio Chierichetti; Ravi Kumar; Alessandro Panconesi; Erisa Terolli,Abstract Given a pairwise similarity notion between objects; locality sensitive hashing (LSH)aims to construct a hash function family over the universe of objects such that the probabilitytwo objects hash to the same value is their similarity. LSH is a powerful algorithmic tool forlarge-scale applications and much work has been done to understand LSHable similarities;ie; similarities that admit an LSH. In this paper we focus on similarities that are provably non-LSHable and propose a notion of distortion to capture the approximation of such a similarityby a similarity that is LSHable. We consider several well-known non-LSHable similaritiesand show tight upper and lower bounds on their distortion. We also experimentally show thatour upper bounds translate to e,LIPIcs-Leibniz International Proceedings in Informatics,2017,2
Near-optimal bounds for cross-validation via loss stability,Ravi Kumar; Daniel Lokshtanov; Sergei Vassilvitskii; Andrea Vattani,Abstract Multi-fold cross-validation is an established practice to estimate the error rate of alearning algorithm. Quantifying the variance reduction gains due to cross-validation hasbeen challenging due to the inherent correlations introduced by the folds. In this work weintroduce a new and weak measure called loss stability and relate the cross-validationperformance to this measure; we also establish that this relationship is near-optimal. Ourwork thus quantitatively improves the current best bounds on cross-validation.,International Conference on Machine Learning,2013,2
Mechanism Design for Complexity-Constrained Bidders,Ravi Kumar; Mohammad Mahdian; Amin Sayedi,Abstract A well-known result due to Vickery gives a mechanism for selling a number ofgoods to interested buyers in a way that achieves the maximum social welfare. In practice; aproblem with this mechanism is that it requires the buyers to specify a large number ofvalues. In this paper we study the problem of designing optimal mechanisms subject toconstraints on the complexity of the bidding language in a setting where buyers haveadditive valuations for a large set of goods. This setting is motivated by sponsored searchauctions; where the valuations of the advertisers are more or less additive; and the numberof keywords that are up for sale is huge. We give a complete solution for this problem whenthe valuations of the buyers are drawn from simple classes of prior distributions. For a morerealistic class of priors; we show that a mechanism akin to the broad match mechanism …,International Workshop on Internet and Network Economics,2009,2
Online Social Networks: Modeling and Mining,Ravi Kumar,ABSTRACT Online social networks have become major and driving phenomena on theWeb. In this talk; we will address key modeling and algorithmic questions related to largeonline social networks. From the modeling perspective; we raise the question of whetherthere is a generative model for network evolution. The availability of time-stamped datamakes it possible to study this question at an extremely fine granularity. We exhibit a simple;natural model that leads to synthetic networks with properties similar to the online ones.From an algorithmic viewpoint; we focus on data mining challenges posed by the magnitudeof data in these networks. In particular; we examine topics related to influence andcorrelation in user activities and compressibility of such networks.,Conf. on Web Search and Data Mining,2009,2
On finding frequent elements in a data stream,Ravi Kumar; Rina Panigrahy,Abstract We consider the problem of finding the most frequent elements in the data streammodel; this problem has a linear lower bound in terms of the input length. In this paper weobtain sharper space lower bounds for this problem; not in terms of the length of the input asis traditionally done; but in terms of the quantitative properties (in this case; distribution of theelement frequencies) of the input per se; this lower bound matches the best known upperbound for this problem. These bounds suggest the study of data stream algorithms throughan instance-specific lens.,*,2007,2
Story of distinct elements,Ravi Kumar,Page 1. Dec 20; 2006 Workshop on Data Streams; IITK 1 A STORY OF DISTINCT ELEMENTSRavi Kumar Yahoo! Research Sunnyvale; CA ravikumar@yahoo-inc.com Page 2. Dec 20; 2006Workshop on Data Streams; IITK 2 ε results about F 0 (This represents joint works with Bar-Yossef;Jayram; Sivakumar; Trevisan) Page 3. Dec 20; 2006 Workshop on Data Streams; IITK 3 Data streammodel Modeling efficient computation on massive data Compute a function of inputs X = x 1 ; …;x n Approximate; randomize; and be space-efficient! Page 4. Dec 20; 2006 Workshop on DataStreams; IITK 4 Finding distinct elements ○ Given X = x 1 ; …; x n compute F 0 (X); the numberof distinct elements in X; in the data stream model Assume x i ε [m] ○ (ε;δ)-approximation: OutputF' 0 (X) such that with probability at least 1 - δ; F' 0 (X) = (1 ± ε) F 0 (X) ○ Zeroth frequency moment ○Assume log m = O(log n); otherwise hash input …,IITK Workshop on Algorithms for Data Streams,2006,2
Department of Chemical Engineering,Subramanian Aiyer; Ben Anthony; Abinaya Arunachalam; Akshai Ashok Kumar; Chinmay Bhandarkar; Jubayed gani Chowdhury; Aparajita Dasgupta; Rahul Dhariyal; Aliasgar fazal Dholfad; Daliya Geoge; Rajib Ghosh Chaudhuri; Jithin Gopakumar; Shanelle Govekar; Bharat B Gulyani; Rashmi Gulyani; Monica Isukapalli; Pankti Joshi; Reshma Kalvade; Ananat Kannan; Apurva Kumar; Ravi Kumar; Ankita Lalwani; Devi Malayil; Nikhil Mundhra; B Muralidharan; Renuka Avanthi Nandam; Nishant Pandya; Shreya Patne; Shreyas Patwardhan; Rokith Rajan; Aparna Rajendrakumar; Shreya Rekhapalli; Parvathy Sathyaprakasan; Maurya Shah; Parth Shah; Amardeep Singh; Tarun Sitaraman; Rajesh S Suresh; Thomas Varghese; Taqdees Imtiaz Wadekar; Abdul Wasee; Nikita Wilson,Find researchers and browse publications; full-texts; contact details and general informationrelated to the Department of Chemical Engineering at BITS Pilani; Dubai.,*,1987,2
An Algorithmic View of Voting,Ronald Fagin; Ravi Kumar; Mohammad Mahdian; D Sivakumar; Erik Vee,We offer a novel classification of voting methods popular in social choice theory. Ourclassification is based on the more general problem of rank aggregation in which; beyondelecting a winner; we also seek to compute an aggregate ranking of all the candidates;moreover; our classification is offered from a computational perspective---based on whetheror not the voting method generalizes to an aggregation algorithm guaranteed to producesolutions that are near optimal in minimizing the distance of the aggregate ranking to thevoters' rankings with respect to one of three well-known distance measures: the Kendall tau;the Spearman footrule; and the Spearman rho measures. We show that methods based onthe average rank of the candidates (Borda counting); on the median rank of the candidates;and on the number of pairwise-majority wins (Copeland) all satisfy the near-optimality …,SIAM Journal on Discrete Mathematics,2016,1
The complexity of LSH feasibility,Flavio Chierichetti; Ravi Kumar; Mohammad Mahdian,Abstract In this paper we study the complexity of the following feasibility problem: given ann× n similarity matrix S as input; is there a locality sensitive hash (LSH) for S? We show thatthe LSH feasibility problem is NP-hard even in the following strong promise version: either Sadmits an LSH or S is at ℓ 1-distance at least n 2− ϵ from every similarity that admits an LSH.We complement this hardness result by providing an O˜(3 n) algorithm for the LSH feasibilityproblem; which improves upon the naïve n Θ (n) time algorithm; we prove that this runningtime is tight; modulo constants; under the Exponential Time Hypothesis.,Theoretical Computer Science,2014,1
Online social networks: modeling and mining: invited talk,Ravi Kumar,Abstract Online social networks have become major and driving phenomena on the Web. Inthis talk; we will address key modeling and algorithmic questions related to large onlinesocial networks. From the modeling perspective; we raise the question of whether there is agenerative model for network evolution. The availability of time-stamped data makes itpossible to study this question at an extremely fine granularity. We exhibit a simple; naturalmodel that leads to synthetic networks with properties similar to the online ones. From analgorithmic viewpoint; we focus on data mining challenges posed by the magnitude of datain these networks. In particular; we examine topics related to influence and correlation inuser activities and compressibility of such networks.,Proceedings of the Second ACM International Conference on Web Search and Data Mining,2009,1
Social networks: looking ahead,Ravi Kumar; Alexander Tuzhilin; Christos Faloutsos; David Jensen; Gueorgi Kossinets; Jure Leskovec; Andrew Tomkins,Abstract By now; online social networks have become an indispensable part of both onlineand offline lives of human beings. A large fraction of time spent online by a user is directlyinfluence by the social networks to which he/she belongs. This calls for a deeperexamination of social networks as large-scale dynamic objects that foster efficient person-person interaction. The goal of our panel is to discuss social networks from various researchangles. In particular; we plan to focus on the following broad research-related topics: largescale data mining; algorithmic questions; sociological aspects; privacy; web search; etc. Wewill also discuss the business and societal impacts of social networks. Each of these topicshas generated a lot of research in recent years and while taking stock of what has beendone; we will also be discussing the directions in which these topics are headed; from …,Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,2008,1
Checking properties of polynomials,Bruno Codenotti; Funda Ergün; Peter Gemmell; S Ravi Kumar,Abstract In this paper we show how to construct efficient checkers for programs thatsupposedly compute properties of polynomials. The properties we consider are roots;norms; and other analytic/algebraic functions of polynomials. In our model; both the programII and the polynomial p are available to the checker each as a black box. We show how tocheck programs that compute a specific root (eg; the largest) or a subset of roots of the givenpolynomial. The checkers; in addition to never computing the root (s) themselves; strive tominimize both the running time (preferably o (deg 2 p)) and the number of black boxevaluations of p (preferably o (deg p)). We obtain deterministic checkers when a separationbound between the roots is known and probabilistic checkers when the roots can bearbitrarily close. We then extend the checkers to handle the situations when the program …,International Colloquium on Automata; Languages; and Programming,1997,1
Learning distributions from random walks,Funda Ergün; S Ravi Kumar; Ronitt Rubinfeld,Abstract We introduce a new model of distributions generated by random walks on graphs.This model suggests a variety of learning problems; using the definitions and models ofdistribution learning defined in [6]. Our framework is general enough to model previouslystudied distribution learning problems; as well as to suggest new applications. We describespecial cases of the general problem; and investigate their relative difficulty. We presentalgorithms to solve the learning problem under various conditions.,Proceedings of the tenth annual conference on Computational learning theory,1997,1
Discrete Choice; Permutations; and Reconstruction,Flavio Chierichetti; Ravi Kumar; Andrew Tomkins,Abstract In this paper we study the well-known family of Random Utility Models; developedover 50 years ago to codify rational user behavior in choosing one item from a finite set ofoptions. In this setting each user draws iid from some distribution a utility function mappingeach item in the universe to a real-valued utility. The user is then offered a subset of theitems; and selects the one of maximum utility. A Max-Dist oracle for this choice model takesany subset of items and returns the probability (over the distribution of utility functions) thateach will be selected. A discrete choice algorithm; given access to a Max-Dist oracle; mustreturn a function that approximates the oracle. We show three primary results. First; we showthat any algorithm exactly reproducing the oracle must make exponentially many queries.Second; we show an equivalent representation of the distribution over utility functions …,*,2018,*
A Discrete Choice Model for Subset Selection,Austin R Benson; Ravi Kumar; Andrew Tomkins,ABSTRACT Multinomial logistic regression is a classical technique for modeling howindividuals choose an item from a finite set of alternatives. This methodology is a workhorsein both discrete choice theory and machine learning. However; it is unclear how togeneralize multinomial logistic regression to subset selection; allowing the choice of morethan one item at a time. We present a new model for subset selection derived from theperspective of random utility maximization in discrete choice theory. In our model; the qualityof a subset is determined by the quality of its elements; plus an optional correction. Given abudget on the number of subsets that may receive correction; we develop a framework forlearning the quality scores for each item; the choice of subsets; and the correction for eachsubset. We show that; given the subsets to receive correction; we can efficiently and …,*,2018,*
Partitioning Orders in Online Shopping Services,Sreenivas Gollapudi; Ravi Kumar; Debmalya Panigrahi; Rina Panigrahy,In an online shopping home delivery service (such as Amazon Fresh or Google Express);customers place online orders for items to be delivered at home; each order comprising oneor more items from a certain store. ese orders are sent in batches to the store where pickerspick the items in the orders physically from the store so as to be dispatched for shipping backto the customer. e job of picking the items in the collection of orders needs to be dividedamong a certain number of pickers. Since an order is directly sent for delivery a er its itemshave been picked; it is important that an entire order is assigned to exactly one picker and isnot split across multiple,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,2017,*
Online trajectory generation for wide ditch crossing of biped robots using control constraints,V Janardhan; R Prasanth Kumar,Abstract This work proposes a multibody dynamics approach to generate joint trajectories fora five degrees of freedom biped robot crossing a wide ditch whose width is greater than orequal to the robot's leg length. Trajectories are generated using control constraints thatdepend on the horizontal distance traveled by the center of mass and are not explicitlydependent on time. Behavior of the biped robot for various initial postures is studiedconsidering dynamic balance; friction; and impact in order to find the preferred initialpostures considering the net energy consumption and peak power requirements at variousjoints. Several cases of friction; zero moment point location; and the center of mass heightvariation are considered in the study. Using the proposed approach; feasible trajectories foran adult sized biped robot could be generated for a wide ditch of 1.05 m width at …,Robotics and Autonomous Systems,2017,*
Caching with Dual Costs,Anirban Dasgupta; Ravi Kumar; Tamás Sarlós,Abstract Caching mechanisms in distributed and social settings face the issue that the itemscan frequently change; requiring the cached versions to be updated to maintain coherence.There is thus a trade-off between incurring cache misses on read requests and cache hits onupdate requests. Motivated by this we consider the following dual cost variant of theclassical caching problem: each request for an item can be either a read or a write. If therequest is read and the item is not in the cache; then a read-miss cost is incurred and if therequest is write and the item is in the cache; then a write-hit cost is incurred. The goal is todesign a caching algorithm that minimizes the sum of read-miss and write-hit costs. Westudy online and offline algorithms for this problem. For the online version of the problem; weobtain an efficient algorithm whose cost is provably close to near-optimal cost. This …,Proceedings of the 26th International Conference on World Wide Web Companion,2017,*
Big Data.,Naren Ramakrishnan; Ravi Kumar,If we apply the Gartner Hype Cycle for Emerging Technologies to big data; the technology haspassed through the peak of inflated expectations and the trough of disillusionment; and is nowmoving steadily along the slope of enlightenment (www.gartner.com/technology/research/methodologies/hype-cycle.jsp). The datafication of our world means that big data permeates not only scienceand engineering; but also such diverse and creative disciplines as the arts and humanities …For this special theme issue; we have assembled a group of five articles that represent moderntrends in big data by examining specific technologies and developments spanningdatabases; algorithms; and applications … One of the early technologies associated with thebig data revolution is NoSQL. As this part of the hype cycle passed; there was a lack of consensusabout what NoSQL denotes; as the term has been used to conflate a large set of features …,IEEE Computer,2016,*
Sequences; Choices; and their Dynamics.,Ravi Kumar,ABSTRACT Sequences arise in many online and offline settings: urls to visit; songs to listento; videos to watch; restaurants to dine at; and so on. User-generated sequences are tightlyrelated to mechanisms of choice; where a user must select one from a finite set ofalternatives. In this talk; we will discuss a class of problems arising from studying suchsequences and the role discrete choice theory plays in these problems. We will presentmodeling and algorithmic approaches to some of these problems and illustrate them in thecontext of large-scale data analysis.,COMAD,2016,*
Algorithms and Models for the Web Graph,David F Gleich; Júlia Komjáthy; Nelly Litvak,This volume contains the papers presented at WAW2015; the 12th Workshop on Algorithmsand Models for the Web-Graph held during December 10–11; 2015; in Eindhoven. Therewere 24 submissions. Each submission was reviewed by at least one; and on average two;Program Committee members. The committee decided to accept 15 papers. The programalso included three invited talks; by Mariana Olvera-Cravioto (Columbia University); Remcovan der Hofstad (Eindhoven University of Technology); and Paul Van Dooren (CatholicUniversity of Louvain). This year the workshop was accompanied by a school aimed at PhDstudents; postdocs; and young researchers. The speakers of the school were Dean Eckles(Facebook); David F. Gleich; Kyle Kloster (Purdue University); and Tobias Müller (UtrechtUniversity). Analyzing data as graphs has transitioned from a minor subfield into a major …,Lecture Notes in Computer Science,2015,*
Learning entangled single-sample Gaussians,Flavio Chierichetti; Anirban Dasgupta; Ravi Kumar; Silvio Lattanzi,Abstract We introduce a new model of Gaussian mixtures; motivated by the setting where thedata points correspond to ratings on a set of items provided by users who have widelyvarying expertise; and each user can rate an item at most once. In this mixture model; eachitem i has a true quality μ i; each user has a variance (lack of expertise) σ j 2; and the ratingof a user j on an item i consists of a single sample independently drawn from the Normaldistribution N (μ i; σ j 2). The aim is to learn the unknown item qualities μ i's as precisely aspossible. We study the single item case and obtain efficient algorithms for the problem;complemented by near-matching lower bounds; we also obtain preliminary results for themultiple items case.,Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms,2014,*
Rank quantization,Ravi Kumar; Ronny Lempel; Roy Schwartz; Sergei Vassilvitskii,Abstract We study the problem of aggregating and summarizing partial orders; on a largescale. Our motivation is two-fold: to discover elements at similar preference levels and toreduce the number of bits needed to store an element's position in a full ranking. Weproceed in two steps: first; we find a total order by linearizing the rankings induced by themultiple partial orders and removing potentially inconsistent pairwise preferences. Next;given a total order; we introduce and formalize the rank quantization problem; whichintuitively aims to bucketize the total order in a manner that mostly preserves the relationsappearing in the partial orders. We show an exact quadratic-time quantization algorithm; aswell as a greedy 2/3-approximation algorithm whose running is substantially faster onsparse instances. As an application; we aggregate rankings of top-10 search results over …,Proceedings of the sixth ACM international conference on Web search and data mining,2013,*
Introduction to the Special Issue on Algorithms and Models for the Web Graph,Anthony Bonato; Ravi Kumar; D Sivakumar,This issue of Internet Mathematics includes a selection of papers that were presented at theSeventh Workshop on Algorithms and Models for the Web-Graph; WAW 2010; held atStanford University in December 2010. The papers in this issue; unlike the conferenceproceedings of the workshop; do not have page limits and contain full versions of proofs andalgorithms. All the articles have been thoroughly reviewed in accordance with the usual highstandards of Internet Mathematics. The papers address a number of topics related tocomplex networks such as network-creation games; applications of PageRank; efficienttriangle-counting algorithms; and models for online social networks. The last decade hasseen an explosive growth in research on complex networks; ranging from the web graph; toonline social networks; to protein–protein interaction networks. Such research has been …,Internet Mathematics,2012,*
An algorithmic treatment of strong queries,Ravi Kumar; Silvio Lattanzi; Prabhakar Raghavan,Abstract A strong query for a target document with respect to an index is the smallest queryfor which the target document is returned by the index as the top result for the query. Thestrong query problem was first studied more than a decade ago in the context of measuringsearch engine overlap. Despite its simple-to-state nature and its longevity in the field; thisproblem has not been sufficiently addressed in a formal manner. In this paper we providethe first rigorous treatment of the strong query problem. We show an interesting connectionbetween this problem and the set cover problem; and use it to obtain basic hardness andalgorithmic results. Experiments on more than 10K documents show that our proposedalgorithm performs much better than the widely-used word frequency-based heuristic. Enroute; our study suggests that less than four words on average can be sufficient to …,Proceedings of the fourth ACM international conference on Web search and data mining,2011,*
An Experimental Study on Spam Detection Algorithms,Ashutosh Kumar Singh; Ravi Kumar; Alex Goh Kwang Leng,Abstract—This paper explores different Web spam detection algorithms like TrustRank andderivatives of TrustRank. TrustRank is implemented and compared with state of the artPageRank on large dataset and experimentally proved that the TrustRank algorithm hadfilter out spam effectively.,Proceeding of IEEE Conference TENCON 2011,2011,*
Web Page Quality Metrics,Ravi Kumar,W3C was founded in 1994 by the inventor of the World Wide Web Tim Berners-Lee as avendor-neutral forum for building consensus around Web technologies. The consortiumconsists of member organization and dedicated staff of technical experts. Membership isopen to any organization or individual whose application is reviewed and approved by theW3C. Usually W3C members invest significant resources into the Web technologies. W3Cfulfils its mission by creation of recommendations enjoying status of international standards.In the first 10 years of existence; it produced over eighty W3C recommendations. W3C isresponsible for such technologies as HTML; XHTML; XML; XML Schema; CSS; SOAP;WSDL and others. W3C members play a leading role in the development of therecommendations. W3C initiatives involve international; national; and regional …,*,2009,*
Communication lower bounds via the chromatic number,Ravi Kumar; D Sivakumar,Abstract We present a new method for obtaining lower bounds on communicationcomplexity. Our method is based on associating with a binary function fa graph G f such thatlog χ (G f) captures N 0 (f)+ N 1 (f). Here χ (G) denotes the chromatic number of G; and N 0 (f)and N 1 (f) denote; respectively; the nondeterministic communication complexity of f and f.Thus log χ (G f) is a lower bound on the deterministic as well as zero-error randomizedcommunication complexity of f. Our characterization opens the possibility of using variousrelaxations of the chromatic number as lower bound techniques for communicationcomplexity. In particular; we show how various (known) lower bounds can be derived byemploying the clique number; the Lovász ϑ-function; and graph entropy lower bounds onthe chromatic number.,International Conference on Foundations of Software Technology and Theoretical Computer Science,2007,*
Matrix and random walk methods in web analysis,Ravi Kumar,Page 1. Matrix and random walk methods in web analysis Ravi Kumar Yahoo! Researchravikumar@yahoo-inc.com Page 2. Yahoo! Research 2 Outline • Background • Matrix methodsin web information retrieval • HITS/Clever • PageRank • Some applications • Random walkmethods in some web analysis • Measurements • Communities • Proximity • Trust Page 3. Yahoo!Research 3 Caveat • Not meant to an exhaustive survey of known results/applications Page 4.Yahoo! Research 4 Web information retrieval Page 5. Yahoo! Research 5 Web informationretrieval How to find information on the web? The Web ~ 1010 web pages ~ 1011 hyperlinksConstantly changing Page 6. Yahoo! Research 6 Classic information retrieval (IR) Input: Set ofdocuments Goal: Given a query; find documents that are most relevant to the query Method: •Preprocess the documents to build an index • Search at run-time Page 7. Yahoo …,MAM 2006: An International Conference to Celebrate the 150th Anniversary of the Birth of AA Markov: June 12-14,2006,*
Finding (Short) Paths in Social Networks,André Allavena; Anirban Dasgupta; John Hopcroft; Ravi Kumar,While several analytic models aim to explain the existence of short paths in social networkssuch as the web; relatively few address the problem of efficiently finding them; especially ina decentralized manner. Since developing purely decentralized search algorithms ingeneral social-network models appears hard; we relax the notion of decentralized search byallowing the option of storing a small amount of preprocessed information about the network.We show that one can identify a small set of vertices in an undirected social network so thatconnectivity information of the vertices in this set can be used in conjunction with the localconnectivity properties to perform decentralized search and find short paths betweenvertices. Our results are for random graphs with power law degree distribution generated bya variant of the expected degree model.,Internet Mathematics,2006,*
