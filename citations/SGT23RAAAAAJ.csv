Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS),Anshumali Shrivastava; Ping Li,Abstract We present the first provably sublinear time hashing algorithm forapproximate\emph {Maximum Inner Product Search}(MIPS). Searching with (un-normalized)inner product as the underlying similarity measure is a known difficult problem and findinghashing schemes for MIPS was considered hard. While the existing Locality SensitiveHashing (LSH) framework is insufficient for solving MIPS; in this paper we extend the LSHframework to allow asymmetric hashing schemes. Our proposal is based on a keyobservation that the problem of finding maximum inner products; after independentasymmetric transformations; can be converted into the problem of approximate nearneighbor search in classical settings. This key observation makes efficient sublinear hashingscheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework; this …,Advances in Neural Information Processing Systems,2014,133
Hashing algorithms for large-scale learning,Ping Li; Anshumali Shrivastava; Joshua L Moore; Arnd C König,Abstract Minwise hashing is a standard technique in the context of search for efficientlycomputing set similarities. The recent development of b-bit minwise hashing provides asubstantial improvement by storing only the lowest b bits of each hashed value. In thispaper; we demonstrate that b-bit minwise hashing can be naturally integrated with linearlearning algorithms such as linear SVM and logistic regression; to solve large-scale andhigh-dimensional statistical learning tasks; especially when the data do not fit in memory.We compare $ b $-bit minwise hashing with the Count-Min (CM) and Vowpal Wabbit (VW)algorithms; which have essentially the same variances as random projections. Ourtheoretical and empirical comparisons illustrate that b-bit minwise hashing is significantlymore accurate (at the same storage cost) than VW (and random projections) for binary …,Advances in neural information processing systems,2011,104
Densifying one permutation hashing via rotation for fast near neighbor search,Anshumali Shrivastava; Ping Li,Abstract The query complexity of\em locality sensitive hashing (LSH) based similarity searchis dominated by the number of hash evaluations; and this number grows with the datasize\citeProc: Indyk_STOC98. In industrial applications such as search where the data areoften high-dimensional and binary (eg; text n-grams);\em minwise hashing is widelyadopted; which requires applying a large number of permutations on the data. This is costlyin computation and energy-consumption. In this paper; we propose a hashing techniquewhich generates all the necessary hash evaluations needed for similarity search; using onesingle permutation. The heart of the proposed hash function is a “rotation” scheme whichdensifies the sparse sketches of\em one permutation hashing\citeProc:Li_Owen_Zhang_NIPS12 in an unbiased fashion thereby maintaining the LSH property …,International Conference on Machine Learning,2014,52
Improved asymmetric locality sensitive hashing (ALSH) for maximum inner product search (MIPS),Anshumali Shrivastava; Ping Li,Abstract: Recently it was shown that the problem of Maximum Inner Product Search (MIPS)is efficient and it admits provably sub-linear hashing algorithms. Asymmetric transformationsbefore hashing were the key in solving MIPS which was otherwise hard. In the prior work;the authors use asymmetric transformations which convert the problem of approximate MIPSinto the problem of approximate near neighbor search which can be efficiently solved usinghashing. In this work; we provide a different transformation which converts the problem ofapproximate MIPS into the problem of approximate cosine similarity search which can beefficiently solved using signed random projections. Theoretical analysis show that the newscheme is significantly better than the original scheme for MIPS. Experimental evaluationsstrongly support the theoretical findings.,arXiv preprint arXiv:1410.5410,2014,36
Asymmetric minwise hashing for indexing binary inner products and set containment,Anshumali Shrivastava; Ping Li,Abstract Minwise hashing (Minhash) is a widely popular indexing scheme in practice.Minhash is designed for estimating set resemblance and is known to be suboptimal in manyapplications where the desired measure is set overlap (ie; inner product between binaryvectors) or set containment. Minhash has inherent bias towards smaller sets; whichadversely affects its performance in applications where such a penalization is not desirable.In this paper; we propose asymmetric minwise hashing ({\em MH-ALSH}); to provide asolution to this well-known problem. The new scheme utilizes asymmetric transformations tocancel the bias of traditional minhash towards smaller sets; making the final``collisionprobability''monotonic in the inner product. Our theoretical comparisons show that; for thetask of retrieving with binary inner products; asymmetric minhash is provably better than …,Proceedings of the 24th International Conference on World Wide Web,2015,33
In defense of minhash over simhash,Anshumali Shrivastava; Ping Li,Abstract MinHash and SimHash are the two widely adopted Locality Sensitive Hashing(LSH) algorithms for large-scale data processing applications. Deciding which LSH to usefor a particular problem at hand is an important question; which has no clear answer in theexisting literature. In this study; we provide a theoretical answer (validated by experiments)that MinHash virtually always outperforms SimHash when the data are binary; as common inpractice such as search. The collision probability of MinHash is a function of\emresemblance similarity (\mathcalR); while the collision probability of SimHash is a functionof\em cosine similarity (\mathcalS). To provide a common basis for comparison; we evaluateretrieval results in terms of\mathcalS for both MinHash and SimHash. This evaluation is validas we can prove that MinHash is a valid LSH with respect to\mathcalS; by using a general …,Artificial Intelligence and Statistics,2014,32
Fast near neighbor search in high-dimensional binary data,Anshumali Shrivastava; Ping Li,Abstract Numerous applications in search; databases; machine learning; and computervision; can benefit from efficient algorithms for near neighbor search. This paper proposes asimple framework for fast near neighbor search in high-dimensional binary data; which arecommon in practice (eg; text). We develop a very simple and effective strategy for sub-lineartime near neighbor search; by creating hash tables directly using the bits generated by b-bitminwise hashing. The advantages of our method are demonstrated through thoroughcomparisons with two strong baselines: spectral hashing and sign (1-bit) randomprojections.,Joint European Conference on Machine Learning and Knowledge Discovery in Databases,2012,24
Improved Densification of One Permutation Hashing,Anshumali Shrivastava; Ping Li,Abstract: The existing work on densification of one permutation hashing reduces the queryprocessing cost of the $(K; L) $-parameterized Locality Sensitive Hashing (LSH) algorithmwith minwise hashing; from $ O (dKL) $ to merely $ O (d+ KL) $; where $ d $ is the number ofnonzeros of the data vector; $ K $ is the number of hashes in each hash table; and $ L $ isthe number of hash tables. While that is a substantial improvement; our analysis reveals thatthe existing densification scheme is sub-optimal. In particular; there is no enoughrandomness in that procedure; which affects its accuracy on very sparse datasets. In thispaper; we provide a new densification procedure which is provably better than the existingscheme. This improvement is more significant for very sparse datasets which are commonover the web. The improved technique has the same cost of $ O (d+ KL) $ for query …,Uncertainty In Artificial Intelligence 2014,2014,23
Scalable and sustainable deep learning via randomized hashing,Ryan Spring; Anshumali Shrivastava,Abstract Current deep learning architectures are growing larger in order to learn fromcomplex datasets. These architectures require giant matrix multiplication operations to trainmillions of parameters. Conversely; there is another growing trend to bring deep learning tolow-power; embedded devices. The matrix operations; associated with the training andtesting of deep networks; are very expensive from a computational and energy standpoint.We present a novel hashing-based technique to drastically reduce the amount ofcomputation needed to train and test neural networks. Our approach combines two recentideas; Adaptive Dropout and Randomized Hashing for Maximum Inner Product Search(MIPS); to select the nodes with the highest activations efficiently. Our new algorithm fordeep learning reduces the overall computational cost of the forward and backward …,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2017,19
Coding for random projections,Ping Li; Michael Mitzenmacher; Anshumali Shrivastava,Abstract The method of random projections has become popular for large-scale applicationsin statistical learning; information retrieval; bio-informatics and other applications. Using awell-designed\textbfcoding scheme for the projected data; which determines the number ofbits needed for each projected value and how to allocate these bits; can significantlyimprove the effectiveness of the algorithm; in storage cost as well as computational speed. Inthis paper; we study a number of simple coding schemes; focusing on the task of similarityestimation and on an application to training linear classifiers. We demonstratethat\textbfuniform quantization outperforms the standard and influential method\citeProc:Datar_SCG04; which used a\em window-and-random offset scheme. Indeed; we argue thatin many cases coding with just a small number of bits suffices. Furthermore; we also …,International Conference on Machine Learning,2014,19
Simple and efficient weighted minwise hashing,Anshumali Shrivastava,Abstract Weighted minwise hashing (WMH) is one of the fundamental subroutine; requiredby many celebrated approximation algorithms; commonly adopted in industrial practice forlarge-scale search and learning. The resource bottleneck with WMH is the computation ofmultiple (typically a few hundreds to thousands) independent hashes of the data. Wepropose a simple rejection type sampling scheme based on a carefully designed red-greenmap; where we show that the number of rejected sample has exactly the same distributionas weighted minwise sampling. The running time of our method; for many practical datasets;is an order of magnitude smaller than existing methods. Experimental evaluations; on realdatasets; show that for computing 500 WMH; our proposal can be 60000x faster than theIoffe's method without losing any accuracy. Our method is also around 100x faster than …,Advances in Neural Information Processing Systems,2016,13
A new space for comparing graphs,Anshumali Shrivastava; Ping Li,Finding a new mathematical representation for graphs; which allows direct comparisonbetween different graph structures; is an open-ended research direction. Having such arepresentation is the first prerequisite for a variety of machine learning algorithms likeclassification; clustering; etc.; over graph datasets. In this paper; we propose a symmetricpositive semidefinite matrix with the (i; j)-th entry equal to the covariance betweennormalized vectors A ie and A je (e being vector of all ones) as a representation for a graphwith adjacency matrix A. We show that the proposed matrix representation encodes thespectrum of the underlying adjacency matrix and it also contains information about thecounts of small sub-structures present in the graph such as triangles and small paths. Inaddition; we show that this matrix is a “graph invariant”. All these properties make the …,Advances in Social Networks Analysis and Mining (ASONAM); 2014 IEEE/ACM International Conference on,2014,11
Gpu-based minwise hashing: Gpu-based minwise hashing,Ping Li; Anshumali Shrivastava; Christian A Konig,Abstract Minwise hashing is a standard technique for efficient set similarity estimation in thecontext of search. The recent work of b-bit minwise hashing provided a substantialimprovement by storing only the lowest b bits of each hashed value. Both minwise hashingand b-bit minwise hashing require an expensive preprocessing step for applying k (eg; k=500) permutations on the entire data in order to compute k minimal values as the hasheddata. In this paper; we developed a parallelization scheme using GPUs; which reduced theprocessing time by a factor of 20-80. Reducing the preprocessing time is highly beneficial inpractice; for example; for duplicate web page detection (where minwise hashing is a majorstep in the crawling pipeline) or for increasing the testing speed of online classifiers (whenthe test data are not preprocessed).,Proceedings of the 21st International Conference on World Wide Web,2012,11
Beyond Pairwise: Provably Fast Algorithms for Approximate $ k $-Way Similarity Search,Anshumali Shrivastava; Ping Li,Abstract We go beyond the notion of pairwise similarity and look into search problems with $k $-way similarity functions. In this paper; we focus on problems related to\emph {3-wayJaccard} similarity: $\mathcal {R}^{3way}=\frac {| S_1\cap S_2\cap S_3|}{| S_1\cup S_2\cupS_3|} $; $ S_1; S_2; S_3\in\mathcal {C} $; where $\mathcal {C} $ is a size $ n $ collection ofsets (or binary vectors). We show that approximate $\mathcal {R}^{3way} $ similarity searchproblems admit fast algorithms with provable guarantees; analogous to the pairwise case.Our analysis and speedup guarantees naturally extend to $ k $-way resemblance. In theprocess; we extend traditional framework of\emph {locality sensitive hashing (LSH)} tohandle higher order similarities; which could be of independent theoretical interest. Theapplicability of $\mathcal {R}^{3way} $ search is shown on the Google sets" application …,Advances in Neural Information Processing Systems,2013,10
Optimal densification for fast and accurate minwise hashing,Anshumali Shrivastava,Abstract: Minwise hashing is a fundamental and one of the most successful hashingalgorithm in the literature. Recent advances based on the idea of densification~\cite {Proc:OneHashLSH_ICML14; Proc: Shrivastava_UAI14} have shown that it is possible to compute$ k $ minwise hashes; of a vector with $ d $ nonzeros; in mere $(d+ k) $ computations; asignificant improvement over the classical $ O (dk) $. These advances have led to analgorithmic improvement in the query complexity of traditional indexing algorithms based onminwise hashing. Unfortunately; the variance of the current densification techniques isunnecessarily high; which leads to significantly poor accuracy compared to vanilla minwisehashing; especially when the data is sparse. In this paper; we provide a novel densificationscheme which relies on carefully tailored 2-universal hashes. We show that the proposed …,arXiv preprint arXiv:1703.04664,2017,9
Time Adaptive Sketches (Ada-Sketches) for Summarizing Data Streams,Anshumali Shrivastava; Arnd Christian Konig; Mikhail Bilenko,Abstract Obtaining frequency information of data streams; in limited space; is a well-recognized problem in literature. A number of recent practical applications (such as those incomputational advertising) require temporally-aware solutions: obtaining historical countstatistics for both time-points as well as time-ranges. In these scenarios; accuracy ofestimates is typically more important for recent instances than for older ones; we call thisdesirable property Time Adaptiveness. With this observation;[20] introduced the Hokusaitechnique based on count-min sketches for estimating the frequency of any given item at anygiven time. The proposed approach is problematic in practice; as its memory requirementsgrow linearly with time; and it produces discontinuities in the estimation accuracy. In thiswork; we describe a new method; Time-adaptive Sketches;(Ada-sketch); that overcomes …,Proceedings of the 2016 International Conference on Management of Data,2016,8
b-bit minwise hashing in practice: Large-scale batch and online learning and using GPUs for fast preprocessing with simple hash functions,Ping Li; Anshumali Shrivastava; Arnd Christian Konig,Abstract: In this paper; we study several critical issues which must be tackled before one canapply b-bit minwise hashing to the volumes of data often used industrial applications;especially in the context of search. 1.(b-bit) Minwise hashing requires an expensivepreprocessing step that computes k (eg; 500) minimal values after applying thecorresponding permutations for each data vector. We developed a parallelization schemeusing GPUs and observed that the preprocessing time can be reduced by a factor of 20-80and becomes substantially smaller than the data loading time.,arXiv preprint arXiv:1205.2958,2012,7
A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators for Partition Function Computation in Log-Linear Models,Ryan Spring; Anshumali Shrivastava,Abstract: Log-linear models are arguably the most successful class of graphical models forlarge-scale applications because of their simplicity and tractability. Learning and inferencewith these models require calculating the partition function; which is a major bottleneck andintractable for large state spaces. Importance Sampling (IS) and MCMC-based approachesare lucrative. However; the condition of having a" good" proposal distribution is often notsatisfied in practice. In this paper; we add a new dimension to efficient estimation viasampling. We propose a new sampling scheme and an unbiased estimator that estimatesthe partition function accurately in sub-linear time. Our samples are generated in near-constant time using locality sensitive hashing (LSH); and so are correlated andunnormalized. We demonstrate the effectiveness of our proposed approach by comparing …,arXiv preprint arXiv:1703.05160,2017,6
Capsule: A camera-based positioning system using learning,Yongshik Moon; Soonhyun Noh; Daedong Park; Chen Luo; Anshumali Shrivastava; Seongsoo Hong; Krishna Palem,We show the first camera based (privacy-preserving) indoor mobile positioning system;CaPSuLe; which does not involve any communication (or data transfer) with any otherdevice or the cloud. The algorithm only needs 78.9 MB of memory and can localize a mobiledevice with 92.11% accuracy. Furthermore this is done in 1.92 seconds of on-devicecomputation consuming 3.77 Joules of energy; as evaluated on Samsung Galaxy S4platform. At the core; our solutions uses a hashing-based image matching algorithm which ismore than 500× cheaper; both in energy and computation cost; over existing state-of-the-artmatching techniques. This significant reduction allows us to perform end-to-end computationlocally on the mobile device. In contrast traditional approaches would consume 2100 Joulesand takes more than 1000 seconds with a small accuracy increase of 0.89%. The ability …,System-on-Chip Conference (SOCC); 2016 29th IEEE International,2016,6
Coding for random projections and approximate near neighbor search,Ping Li; Michael Mitzenmacher; Anshumali Shrivastava,Abstract: This technical note compares two coding (quantization) schemes for randomprojections in the context of sub-linear time approximate near neighbor search. The firstscheme is based on uniform quantization while the second scheme utilizes a uniformquantization plus a uniformly random offset (which has been popular in practice). The priorwork compared the two schemes in the context of similarity estimation and training linearclassifiers; with the conclusion that the step of random offset is not necessary and may hurtthe performance (depending on the similarity level). The task of near neighbor search isrelated to similarity estimation with importance distinctions and requires own study. In thispaper; we demonstrate that in the context of near neighbor search; the step of random offsetis not needed either and may hurt the performance (sometimes significantly so …,arXiv preprint arXiv:1403.8144,2014,6
Blocking Methods Applied to Casualty Records from the Syrian Conflict,Peter Sadosky; Anshumali Shrivastava; Megan Price; Rebecca C Steorts,Abstract: Estimation of death counts and associated standard errors is of great importance inarmed conflict such as the ongoing violence in Syria; as well as historical conflicts inGuatemala; Per\'u; Colombia; Timor Leste; and Kosovo. For example; statistical estimates ofdeath counts were cited as important evidence in the trial of General Efra\'in R\'ios Montt foracts of genocide in Guatemala. Estimation relies on both record linkage and multiplesystems estimation. A key first step in this process is identifying ways to partition the recordssuch that they are computationally manageable. This step is referred to as blocking and is amajor challenge for the Syrian database since it is sparse in the number of duplicate recordsand feature poor in its attributes. As a consequence; we propose locality sensitive hashing(LSH) methods to overcome these challenges. We demonstrate the computational …,arXiv preprint arXiv:1510.07714,2015,5
Probabilistic hashing techniques for big data,Anshumali Shrivastava,Abstract We investigate probabilistic hashing techniques for addressing computational andmemory challenges in large scale machine learning and data mining systems. In this thesis;we show that the traditional idea of hashing goes far beyond near-neighbor search andthere are some striking new possibilities. We show that hashing can improve state of the artlarge scale learning algorithms; and it goes beyond the conventional notions of pairwisesimilarities. Despite being a very well studied topic in literature; we found severalopportunities for fundamentally improving some of the well know textbook hashingalgorithms. In particular; we show that the traditional way of computing minwise hashes isunnecessarily expensive and without loosing anything we can achieve an order ofmagnitude speedup. We also found that for cosine similarity search there is a better …,*,2015,5
Training logistic regression and svm on 200gb data using b-bit minwise hashing and comparisons with vowpal wabbit (VW),Ping Li; Anshumali Shrivastava; Christian Konig,Abstract: We generated a dataset of 200 GB with 10^ 9 features; to test our recent b-bitminwise hashing algorithms for training very large-scale logistic regression and SVM. Theresults confirm our prior work that; compared with the VW hashing algorithm (which has thesame variance as random projections); b-bit minwise hashing is substantially more accurateat the same storage. For example; with merely 30 hashed values per data point; b-bitminwise hashing can achieve similar accuracies as VW with 2^ 14 hashed values per datapoint.,arXiv preprint arXiv:1108.3072,2011,5
Fast multi-task learning for query spelling correction,Xu Sun; Anshumali Shrivastava; Ping Li,Abstract In this paper; we explore the use of a novel online multi-task learning framework forthe task of search query spelling correction. In our procedure; correction candidates areinitially generated by a ranker-based system and then re-ranked by our multi-task learningalgorithm. With the proposed multi-task learning method; we are able to effectively transferinformation from different and highly biased training datasets; for improving spellingcorrection on all datasets. Our experiments are conducted on three query spelling correctiondatasets including the well-known TREC benchmark dataset. The experimental resultsdemonstrate that our proposed method considerably outperforms the existing baselinesystems in terms of accuracy. Importantly; the proposed method is about one order ofmagnitude faster than baseline systems in terms of training speed. Compared to the …,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,4
SSH (Sketch; Shingle; & Hash) for Indexing Massive-Scale Time Series,Chen Luo; Anshumali Shrivastava,Abstract Similarity search on time series is a frequent operation in large-scale data-drivenapplications. Sophisticated similarity measures are standard for time series matching; asthey are usually misaligned. Dynamic Time Warping or DTW is the most widely usedsimilarity measure for time series because it combines alignment and matching at the sametime. However; the alignment makes DTW slow. To speed up the expensive similarity searchwith DTW; branch and bound based pruning strategies are adopted. However; branch andbound based pruning are only useful for very short queries (low dimensional time series);and the bounds are quite weak for longer queries. Due to the loose bounds branch andbound pruning strategy boils down to a brute-force search. To circumvent this issue; wedesign SSH (Sketch; Shingle; & Hashing); an efficient and approximate hashing scheme …,NIPS 2016 Time Series Workshop,2017,3
Unique Entity Estimation with Application to the Syrian Conflict,Beidi Chen; Anshumali Shrivastava; Rebecca C Steorts,Abstract: Entity resolution identifies and removes duplicate entities in large; noisy databasesand has grown in both usage and new developments as a result of increased dataavailability. Nevertheless; entity resolution has tradeoffs regarding assumptions of the datageneration process; error rates; and computational scalability that make it a difficult task forreal applications. In this paper; we focus on a related problem of unique entity estimation;which is the task of estimating the unique number of entities and associated standard errorsin a data set with duplicate entities. Unique entity estimation shares many fundamentalchallenges of entity resolution; namely; that the computational cost of all-to-all entitycomparisons is intractable for large databases. To circumvent this computational barrier; wepropose an efficient (near-linear time) estimation algorithm based on locality sensitive …,arXiv preprint arXiv:1710.02690,2017,2
FLASH: Randomized Algorithms Accelerated over CPU-GPU for Ultra-High Dimensional Similarity Search,Yiqiu Wang; Anshumali Shrivastava; Junghee Ryu,Abstract: We present FLASH ({\bf F} ast {\bf L} SH {\bf A} lgorithm for {\bf S} imilarity searchaccelerated with {\bf H} PC (High-Performance Computing)); a similarity search system forultra-high dimensional datasets on a single machine; which does not require similaritycomputation. Our system is an auspicious illustration of the power of randomized algorithmscarefully tailored for high-performance computing platforms. We leverage LSH stylerandomized indexing procedure and combine it with several principled techniques; such asreservoir sampling; recent advances in one-pass minwise hashing; and count basedestimations. The combination; while retaining sound theoretical guarantees; reduces thecomputational as well as parallelization overhead of our proposal. We provide CPU andhybrid CPU-GPU implementations of FLASH for replicability of our results this https URL …,arXiv preprint arXiv:1709.01190,2017,2
Arrays of (locality-sensitive) Count Estimators (ACE): High-Speed Anomaly Detection via Cache Lookups,Chen Luo; Anshumali Shrivastava,Abstract: Anomaly detection is one of the frequent and important subroutines deployed inlarge-scale data processing systems. Even being a well-studied topic; existing techniquesfor unsupervised anomaly detection require storing significant amounts of data; which isprohibitive from memory and latency perspective. In the big-data world existing methods failto address the new set of memory and latency constraints. In this paper; we propose ACE(Arrays of (locality-sensitive) Count Estimators) algorithm that can be 60x faster than theELKI package~\cite {DBLP: conf/ssd/AchtertBKSZ09}; which has the fastest implementationof the unsupervised anomaly detection algorithms. ACE algorithm requires less than $4 MB$ memory; to dynamically compress the full data information into a set of count arrays. Thesetiny $4 MB $ arrays of counts are sufficient for unsupervised anomaly detection. At the …,arXiv preprint arXiv:1706.06664,2017,2
2-Bit Random Projections; Nonlinear Estimators; and Approximate Near Neighbor Search,Ping Li; Michael Mitzenmacher; Anshumali Shrivastava,Abstract: The method of random projections has become a standard tool for machinelearning; data mining; and search with massive data at Web scale. The effective use ofrandom projections requires efficient coding schemes for quantizing (real-valued) projecteddata into integers. In this paper; we focus on a simple 2-bit coding scheme. In particular; wedevelop accurate nonlinear estimators of data similarity based on the 2-bit strategy. Thiswork will have important practical applications. For example; in the task of near neighborsearch; a crucial step (often called re-ranking) is to compute or estimate data similaritiesonce a set of candidate data points have been identified by hash table techniques. This re-ranking step can take advantage of the proposed coding scheme and estimator. As a relatedtask; in this paper; we also study a simple uniform quantization scheme for the purpose of …,arXiv preprint arXiv:1602.06577,2016,2
Graph kernels via functional embedding,Anshumali Shrivastava; Ping Li,Abstract: We propose a representation of graph as a functional object derived from thepower iteration of the underlying adjacency matrix. The proposed functional representationis a graph invariant; ie; the functional remains unchanged under any reordering of thevertices. This property eliminates the difficulty of handling exponentially many isomorphicforms. Bhattacharyya kernel constructed between these functionals significantly outperformsthe state-of-the-art graph kernels on 3 out of the 4 standard benchmark graph classificationdatasets; demonstrating the superiority of our approach. The proposed methodology issimple and runs in time linear in the number of edges; which makes our kernel more efficientand scalable compared to many widely adopted graph kernels with running time cubic in thenumber of vertices. Subjects: Learning (cs. LG); Artificial Intelligence (cs. AI); Machine …,arXiv preprint arXiv:1404.5214,2014,2
A new mathematical space for social networks,Anshumali Shrivastava; Ping Li,Abstract Finding a new mathematical representation for graphs; which allows directcomparison between different graph structures; is still an open-ended research direction.Having such a representation in a common; well-understood metric space is the firstprerequisite for a variety of machine learning algorithms like classification; clustering; etc;over graph datasets. In this study; we propose a symmetric positive semidefinite matrix withthe (i; j)-th entry equal to the covariance between normalized vectors Aie and Aje as arepresentation for graph with adjacency matrix A. We argue that the covariance betweenvectors of the form Aie and Aje; given some i and j; is an informative feature. We presenttheoretical results supporting this argument. Our representation; being a covariance matrixin a fixed dimensional metric space; can be directly compared across different graph …,Frontiers of Network Analysis: Methods; Models; and Applications; NIPS Workshop,2013,2
Query spelling correction using multi-task learning,Xu Sun; Anshumali Shrivastava; Ping Li,Abstract This paper explores the use of online multi-task learning for search query spellingcorrection; by effectively transferring information from different and biased training datasetsfor improving spelling correction across datasets. Experiments were conducted on threequery spelling correction datasets; including the well-known TREC benchmark data. Ourexperimental results demonstrate that the proposed method considerably outperformsexisting baseline systems in terms of accuracy. Importantly; the proposed method is aboutone-order of magnitude faster than baseline systems in terms of training speed. In contrast toexisting methods which typically require more than (eg;) 50 training passes; our algorithmcan very closely approach the empirical optimum in around five passes.,Proceedings of the 21st International Conference on World Wide Web,2012,2
Sub-linear Privacy-preserving Search with Untrusted Server and Semi-honest Parties,M Sadegh Riazi; Beidi Chen; Anshumali Shrivastava; Dan Wallach; Farinaz Koushanfar,Abstract: Privacy-preserving Near-neighbor search (PP-NNS) is a well-studied problem inthe literature. The overwhelming growth in the size of current datasets and the lack of anytruly secure server in the online world render the existing solutions impractical either due totheir high computational requirements or the non-realistic assumptions which potentiallycompromise privacy. PP-NNS with multiple (semi-honest) data owners having query timesub-linear in the number of users has been proposed as an open research direction. In thispaper; we provide the first such algorithm which has a sub-linear query time and the abilityto handle semi-honest (honest but curious) parties. Our algorithm can further manage thesituation where a large chunk of the server information is being compromised. Probabilisticembedding based on Locality Sensitive Hashing (LSH) is the algorithm of choice for sub …,arXiv preprint arXiv:1612.01835,2016,1
b-bit minwise hashing in practice,Ping Li; Anshumali Shrivastava; Arnd Christian König,Abstract Minwise hashing is a standard technique in the context of search for approximatingset similarities. The recent work [26; 32] demonstrated a potential use of b-bit minwisehashing [23; 24] for efficient search and learning on massive; high-dimensional; binary data(which are typical for many applications in Web search and text mining). In this paper; wefocus on a number of critical issues which must be addressed before one can apply b-bitminwise hashing to the volumes of data often used industrial applications. Minwise hashingrequires an expensive preprocessing step that computes k (eg; 500) minimal values afterapplying the corresponding permutations for each data vector. We developed aparallelization scheme using GPUs and observed that the preprocessing time can bereduced by a factor of 20~ 80 and becomes substantially smaller than the data loading …,Proceedings of the 5th Asia-Pacific Symposium on Internetware,2013,1
b-Bit Minwise Hashing for Large-Scale Learning,Ping Li; Anshumali Shrivastava; Joshua Moore; Arnd Christian König,Abstract Minwise hashing is a standard technique in the context of search for efficientlycomputing set similarities. The recent development of b-bit minwise hashing provides asubstantial improvement by storing only the lowest b bits of each hashed value. In thispaper; we demonstrate that b-bit minwise hashing can be naturally integrated with linearlearning algorithms such as linear SVM and logistic regression; to solve large-scale andhigh-dimensional statistical learning tasks; especially when the data do not fit in memory.We compare b-bit minwise hashing with the Count-Min (CM) and Vowpal Wabbit (VW)algorithms; which have essentially the same variances as random projections. Ourtheoretical and empirical comparisons illustrate that b-bit minwise hashing is significantlymore accurate (at the same storage cost) than VW (and random projections) for binary …,Big Learning,2011,1
Scaling-up Split-Merge MCMC with Locality Sensitive Sampling (LSS),Chen Luo; Anshumali Shrivastava,Abstract: Split-Merge MCMC (Monte Carlo Markov Chain) is one of the essential andpopular variants of MCMC for problems when an MCMC state consists of an unknownnumber of components or clusters. It is well known that state-of-the-art methods for split-merge MCMC do not scale well. Strategies for rapid mixing requires smart and informativeproposals to reduce the rejection rate. However; all known smart proposals involve cost atleast linear in the size of the data $\ge O (N) $; to suggest informative transitions. Thus; thecost of each iteration is prohibitive for massive scale datasets. It is further known thatuninformative but computationally efficient proposals; such as random split-merge; leads toextremely slow convergence. This tradeoff between mixing time and per update cost seemshard to get around. In this paper; we get around this tradeoff by utilizing simple similarity …,arXiv preprint arXiv:1802.07444,2018,*
LSH-SAMPLING BREAKS THE COMPUTATIONAL CHICKEN-AND-EGG LOOP IN ADAPTIVE STOCHASTIC GRADIENT ESTIMATION,Beidi Chen; Yingchen Xu; Anshumali Shrivastava,Abstract: Stochastic Gradient Descent or SGD is the most popular optimization algorithm forlarge-scale problems. SGD estimates the gradient by uniform sampling with sample sizeone. There have been several other works that suggest faster epoch wise convergence byusing weighted non-uniform sampling for better gradient estimates. Unfortunately; the per-iteration cost of maintaining this adaptive distribution for gradient estimation is more thancalculating the full gradient. As a result; the false impression of faster convergence initerations leads to slower convergence in time; which we call a chicken-and-egg loop. In thispaper; we break this barrier by providing the first demonstration of a sampling scheme;which leads to superior gradient estimation; while keeping the sampling cost per iterationsimilar to that of the uniform sampling. Such an algorithm is possible due to the sampling …,*,2018,*
Scalable Estimation via LSH Samplers (LSS),Ryan Spring; Anshumali Shrivastava,Abstract: The softmax function has multiple applications in large-scale machine learning.However; calculating the partition function is a major bottleneck for large state spaces. In thispaper; we propose a new sampling scheme using locality-sensitive hashing (LSH) and anunbiased estimator that approximates the partition function accurately in sub-linear time. Thesamples are correlated and unnormalized; but the derived estimator is unbiased. Wedemonstrate the significant advantages of our proposal by comparing the speed andaccuracy of LSH-Based Samplers (LSS) against other state-of-the-art estimation techniques.,*,2018,*
A4C: Anticipatory Asynchronous Advantage Actor-Critic,Tharun Medini; Xun Luan; Anshumali Shrivastava,Abstract: We propose to extend existing deep reinforcement learning (Deep RL) algorithmsby allowing them to additionally choose sequences of actions as a part of their policy. Thismodification forces the network to anticipate the reward of action sequences; which; as weshow; improves the exploration leading to better convergence. We propose a method thatsqueezes more gradients from the same number of episodes and thereby achieves higherscores and converges faster. Our proposal is simple; flexible; and can be easily incorporatedinto any Deep RL framework. We show the power of our scheme by consistentlyoutperforming the state-of-the-art GA3C algorithm on popular Atari Games.,*,2018,*
Anticipatory Asynchronous Advantage Actor-Critic (A4C): The power of Anticipation in Deep Reinforcement Learning,Xun Luan; Tharun Medini; Anshumali Shrivastava,Abstract: We propose to extend existing deep reinforcement learning (Deep RL) algorithmsby allowing them to additionally choose sequences of actions as a part of their policy. Thismodification forces the network to anticipate the reward of action sequences; which; as weshow; improves the exploration leading to better convergence. Our proposal is simple;flexible; and can be easily incorporated into any Deep RL framework. We show the power ofour scheme by consistently outperforming the state-of-the-art GA3C algorithm on severalpopular Atari Games.,*,2018,*
Accelerating Dependency Graph Learning from Heterogeneous Categorical Event Streams via Knowledge Transfer,Chen Luo; Zhengzhang Chen; Lu-An Tang; Anshumali Shrivastava; Zhichun Li,Abstract: Dependency graph; as a heterogeneous graph representing the intrinsicrelationships between different pairs of system entities; is essential to many data analysisapplications; such as root cause diagnosis; intrusion detection; etc. Given a well-traineddependency graph from a source domain and an immature dependency graph from a targetdomain; how can we extract the entity and dependency knowledge from the source toenhance the target? One way is to directly apply a mature dependency graph learned from asource domain to the target domain. But due to the domain variety problem; directly usingthe source dependency graph often can not achieve good performance. Traditional transferlearning methods mainly focus on numerical data and are not applicable. In this paper; wepropose ACRET; a knowledge transfer based model for accelerating dependency graph …,arXiv preprint arXiv:1708.07867,2017,*
RHash: robust hashing via l∞-norm distortion,Amirali Aghazadeh; Andrew Lan; Anshumali Shrivastava; Richard Baraniuk,Abstract Hashing is an important tool in large-scale machine learning. Unfortunately; currentdata-dependent hashing algorithms are not robust to small perturbations of the data points;which degrades the performance of nearest neighbor (NN) search. The culprit is theminimization of the l2-norm; average distortion among pairs of points to find the hashfunction. Inspired by recent progress in robust optimization; we develop a novel hashingalgorithm; dubbed RHash; that instead minimizes the l∞-norm; worst-case distortion amongpairs of points. We develop practical and efficient implementations of RHash that couple thealternating direction method of multipliers (ADMM) framework with column generation toscale well to large datasets. A range of experimental evaluations demonstrate the superiorityof RHash over ten state-of-the-art binary hashing schemes. In particular; we show that …,Proceedings of the 26th International Joint Conference on Artificial Intelligence,2017,*
Location detection for navigation using IMUs with a map through coarse-grained machine learning,E J Jose Gonzalez; Chen Luo; Anshumali Shrivastava; Krishna Palem; Yongshik Moon; Soonhyun Noh; Daedong Park; Seongsoo Hong,Abstract Location detection or localization supporting navigation has assumed significantimportance in the recent past. In particular; techniques that exploit cheap inertialmeasurement units (IMU); the gyroscope and the accelerometer; have garnered attention;especially in an embedded computing context. However; these sensors measurements arequite unreliable; and it is widely believed that these sensors by themselves are too noisy forlocalization with acceptable accuracy. Consequently; several lines of work embody othercostly alternatives to lower the impact of accumulated errors associated with IMU basedapproaches; invariably leading to very high energy costs resulting in lowered battery life. Inthis paper; we show that IMUs are sufficient by themselves if we augment them with knownstructural or geographical information about the physical area being explored by the user …,Proceedings of the Conference on Design; Automation & Test in Europe,2017,*
Revisiting Winner Take All (WTA) Hashing for Sparse Datasets,Beidi Chen; Anshumali Shrivastava,Abstract: WTA (Winner Take All) hashing has been successfully applied in many large scalevision applications. This hashing scheme was tailored to take advantage of the comparativereasoning (or order based information); which showed significant accuracy improvements. Inthis paper; we identify a subtle issue with WTA; which grows with the sparsity of the datasets.This issue limits the discriminative power of WTA. We then propose a solution for thisproblem based on the idea of Densification which provably fixes the issue. Our experimentsshow that Densified WTA Hashing outperforms Vanilla WTA both in image classification andretrieval tasks consistently and significantly. Subjects: Computer Vision and PatternRecognition (cs. CV); Information Retrieval (cs. IR) Cite as: arXiv: 1612.01834 [cs. CV](orarXiv: 1612.01834 v1 [cs. CV] for this version) Submission history From: Beidi Chen [view …,arXiv preprint arXiv:1612.01834,2016,*
Sub-Linear Privacy-Preserving Near-Neighbor Search with Untrusted Server on Large-Scale Datasets,M Sadegh Riazi; Beidi Chen; Anshumali Shrivastava; Dan Wallach; Farinaz Koushanfar,Abstract In Near-Neighbor Search (NNS); a new client queries a database (held by a server)for the most similar data (near-neighbors) given a certain similarity metric. The Privacy-Preserving variant (PP-NNS) requires that neither server nor the client shall learninformation about the other party's data except what can be inferred from the outcome ofNNS. The overwhelming growth in the size of current datasets and the lack of a truly secureserver in the online world render the existing solutions impractical; either due to their highcomputational requirements or non-realistic assumptions which potentially compromiseprivacy. PP-NNS having query time {\it sub-linear} in the size of the database has beensuggested as an open research direction by Li et al.(CCSW'15). In this paper; we provide thefirst such algorithm; called Secure Locality Sensitive Indexing (SLSI) which has a sub …,arXiv preprint arXiv:1612.01835,2016,*
Special session: SoC architectures for machine learning through inexactness,Seongsoo Hong; Anshumali Shrivastava,A not-for-profit organization; IEEE is the world's largest technical professional organization dedicatedto advancing technology for the benefit of humanity. © Copyright 2017 IEEE - All rightsreserved. Use of this web site signifies your agreement to the terms and conditions.,System-on-Chip Conference (SOCC); 2016 29th IEEE International,2016,*
Near-Isometric Binary Hashing for Large-scale Datasets,Amirali Aghazadeh; Andrew Lan; Anshumali Shrivastava; Richard Baraniuk,Abstract: We develop a scalable algorithm to learn binary hash codes for indexing large-scale datasets. Near-isometric binary hashing (NIBH) is a data-dependent hashing schemethat quantizes the output of a learned low-dimensional embedding to obtain a binary hashcode. In contrast to conventional hashing schemes; which typically rely on an $\ell_2 $-norm(ie; average distortion) minimization; NIBH is based on a $\ell_ {\infty} $-norm (ie; worst-casedistortion) minimization that provides several benefits; including superior distance; ranking;and near-neighbor preservation performance. We develop a practical and efficient algorithmfor NIBH based on column generation that scales well to large datasets. A range ofexperimental evaluations demonstrate the superiority of NIBH over ten state-of-the-art binaryhashing schemes. Subjects: Data Structures and Algorithms (cs. DS) Cite as: arXiv …,arXiv preprint arXiv:1603.03836,2016,*
Sub-Linear Privacy-Preserving Near-Neighbor Search with Untrusted Server on Large-Scale Datasets,M Sadegh Riazi; Beidi Chen; Anshumali Shrivastava; Dan Wallach; Farinaz Koushanfar,ABSTRACT In Near-Neighbor Search (NNS); a new client queries a database (held by aserver) for the most similar data (near-neighbors) given a certain similarity metric. ThePrivacy-Preserving variant (PP-NNS) requires that neither server nor the client shall learninformation about the other party's data except what can be inferred from the outcome ofNNS. The overwhelming growth in the size of current datasets and the lack of a truly secureserver in the online world render the existing solutions impractical; either due to their highcomputational requirements or non-realistic assumptions which potentially compromiseprivacy. PP-NNS having query time sub-linear in the size of the database has beensuggested as an open research direction by Li et al.(CCSW'15). In this paper; we provide thefirst such algorithm; called Secure Locality Sensitive Indexing (SLSI) which has a sub …,*,*,*
Sub-linear Privacy-preserving Search with Unsecured Server and Semi-honest Parties,M Sadegh Riazi; Beidi Chen; Anshumali Shrivastava; Dan Wallach; Farinaz Koushanfar,ABSTRACT Privacy-preserving Near-neighbor search (PP-NNS) is a wellstudied problem inthe literature. The overwhelming growth in the size of current datasets and the lack of anytruly secure server in the online world render the existing solutions impractical either due totheir high computational requirements or the non-realistic assumptions which potentiallycompromise privacy. PP-NNS with multiple (semi-honest) data owners having query timesub-linear in the number of users has been proposed as an open research direction in [30].In this paper; we provide the first such algorithm which has a sub-linear query time and theability to handle semi-honest (honest but curious) parties. Our algorithm can further managethe situation where a large chunk of the server information is being compromised.Probabilistic embedding based on Locality Sensitive Hashing (LSH) is the algorithm of …,*,*,*
Location Detection for Navigation Using IMUs with a Map Through Coarse-Grained Machine Learning,Anshumali Shrivastava; Krishna Palem; Yongshik Moon; Soonhyun Noh; Daedong Park; Seongsoo Hong,Abstract—Location detection or localization supporting navigation has assumed significantimportance in the recent past. In particular; techniques that exploit cheap inertialmeasurement units (IMU); the gyroscope and the accelerometer; have garnered attention;especially in an embedded computing context. However; these sensors measurements arequite unreliable; and it is widely believed that these sensors by themselves are too noisy forlocalization with acceptable accuracy. Consequently; several lines of work embody othercostly alternatives to lower the impact of accumulated errors associated with IMU basedapproaches; invariably leading to very high energy costs resulting in lowered battery life. Inthis paper; we show that IMUs are sufficient by themselves if we augment them with knownstructural or geographical information about the physical area being explored by the user …,*,*,*
Supplementary Material: Simple and Efficient Weighted Minwise Hashing,Anshumali Shrivastava,1 Proof of Correctness First note that; every number between [0; M] is random and equally likelyin a random sampling. Therefore; for a given point x; at the time we stop we sample uniformlyfrom the green region xgreen = ∪D i=1 [Mi; Mi + xi]. Consider the index j defined as; j =min{h(x); h(y)} … For any pair of points x and y; consider the following three events: 1) h(x) =h(y) = j; 2) h(x) > h(y) = j and 3) j = h(x) < h(y). Observe that; h(x) = h(y) = j if and only if rj ∈ xgreen∩ ygreen … Since rj is uniformly chosen; we have … Expectation follows immediately fromthe fact that the number of sampling step taken before the process stops;which is also h(x) isa geometric random variable with p = sx. The collision probability follows from observing thatPr(h(x) > k) = (1 − sx)k ≤ δ which implies k ≤ log δ log (1−sx) yielding the required bound. 1.2Proof of Corollary 1 … Proof: The proof follows from Jensens Inequality; E(log x) ≤ log …,*,*,*
