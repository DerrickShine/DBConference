Top-k query evaluation with probabilistic guarantees,Martin Theobald; Gerhard Weikum; Ralf Schenkel,Abstract Top-k queries based on ranking elements of multidimensional datasets are afundamental building block for many kinds of information discovery. The best known general-purpose algorithm for evaluating top-k queries is Fagin's threshold algorithm (TA). Since theuser's goal behind top-k queries is to identify one or a few relevant and novel data items; it isintriguing to use approximate variants of TA to reduce run-time costs. This paper introducesa family of approximate top-k algorithms based on probabilistic arguments. When scanningindex lists of the underlying multidimensional data space in descending order of localscores; various forms of convolution and derived bounds are employed to predict when it issafe; with high probability; to drop candidate items and to prune the index scans. Theprecision and the efficiency of the developed methods are experimentally evaluated …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,283
Entity resolution with iterative blocking,Steven Euijong Whang; David Menestrina; Georgia Koutrika; Martin Theobald; Hector Garcia-Molina,Abstract Entity Resolution (ER) is the problem of identifying which records in a databaserefer to the same real-world entity. An exhaustive ER process involves computing thesimilarities between pairs of records; which can be very expensive for large datasets.Various blocking techniques can be used to enhance the performance of ER by dividing therecords into blocks in multiple ways and only comparing records within the same block.However; most blocking techniques process blocks separately and do not exploit the resultsof other blocks. In this paper; we propose an iterative blocking framework where the ERresults of blocks are reflected to subsequently processed blocks. Blocks are now iterativelyprocessed until no block contains any more matching records. Compared to simple blocking;iterative blocking may achieve higher accuracy because reflecting the ER results of …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,207
Databases with uncertainty and lineage,Omar Benjelloun; Anish Das Sarma; Alon Halevy; Martin Theobald; Jennifer Widom,Abstract This paper introduces uldb s; an extension of relational databases with simple yetexpressive constructs for representing and manipulating both lineage and uncertainty.Uncertain data and data lineage are two important areas of data management that havebeen considered extensively in isolation; however many applications require the features intandem. Fundamentally; lineage enables simple and consistent representation of uncertaindata; it correlates uncertainty in query results with uncertainty in the input data; and queryprocessing with lineage and uncertainty together presents computational benefits overtreating them separately. We show that the uldb representation is complete; and that itpermits straightforward implementation of many relational operations. We define two notionsof uldb minimality--data-minimal and lineage-minimal--and study minimization of uldb …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,204
Spotsigs: robust and efficient near duplicate detection in large web collections,Martin Theobald; Jonathan Siddharth; Andreas Paepcke,Abstract Motivated by our work with political scientists who need to manually analyze largeWeb archives of news sites; we present SpotSigs; a new algorithm for extracting andmatching signatures for near duplicate detection in large Web crawls. Our spot signaturesare designed to favor natural-language portions of Web pages over advertisements andnavigational bars. The contributions of SpotSigs are twofold: 1) by combining stopwordantecedents with short chains of adjacent content terms; we create robust documentsignatures with a natural ability to filter out noisy components of Web pages that wouldotherwise distract pure n-gram-based approaches such as Shingling; 2) we provide an exactand efficient; self-tuning matching algorithm that exploits a novel combination of collectionpartitioning and inverted index pruning for high-dimensional similarity search …,Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,2008,180
An efficient and versatile query engine for TopX search,Martin Theobald; Ralf Schenkel; Gerhard Weikum,Abstract This paper presents a novel engine; coined TopX; for efficient ranked retrieval ofXML documents over semistructured but nonschematic data collections. The algorithmfollows the paradigm of threshold algorithms for top-k query processing with a focus oninexpensive sequential accesses to index lists and only a few judiciously scheduled randomaccesses. The difficulties in applying the existing top-k algorithms to XML data lie in 1) theneed to consider scores for XML elements while aggregating them at the document level; 2)the combination of vague content conditions with XML path conditions; 3) the need to relaxquery conditions if too few results satisfy all conditions; and 4) the selectivity estimation forboth content and structure conditions and their impact on evaluation strategies. TopXaddresses these issues by precomputing score and path information in an appropriately …,Proceedings of the 31st international conference on Very large data bases,2005,174
Scalable knowledge harvesting with high precision and high recall,Ndapandula Nakashole; Martin Theobald; Gerhard Weikum,Abstract Harvesting relational facts from Web sources has received great attention forautomatically constructing large knowledge bases. Stateof-the-art approaches combinepattern-based gathering of fact candidates with constraint-based reasoning. However; theystill face major challenges regarding the trade-offs between precision; recall; and scalability.Techniques that scale well are susceptible to noisy patterns that degrade precision; whiletechniques that employ deep reasoning for high precision cannot cope with Web-scale data.This paper presents a scalable system; called PROSPERA; for high-quality knowledgeharvesting. We propose a new notion of ngram-itemsets for richer patterns; and use MaxSat-based constraint reasoning on both the quality of patterns and the validity of fact candidates.We compute pattern-occurrence statistics for two benefits: they serve to prune the …,Proceedings of the fourth ACM international conference on Web search and data mining,2011,168
Io-top-k: Index-access optimized top-k query processing,Holger Bast; Debapriyo Majumdar; Ralf Schenkel; Martin Theobald; Gerhard Weikum,Abstract Top-k query processing is an important building block for ranked retrieval; withapplications ranging from text and data integration to distributed aggregation of network logsand sensor data. Top-k queries operate on index lists for a query's elementary conditionsand aggregate scores for result candidates. One of the best implementation methods in thissetting is the family of threshold algorithms; which aim to terminate the index scans as earlyas possible based on lower and upper bounds for the final scores of result candidates. Thisprocedure performs sequential disk accesses for sorted index scans; but also has the optionof performing random accesses to resolve score uncertainty. This entails scheduling for thetwo kinds of accesses: 1) the prioritization of different index lists in the sequential accesses;and 2) the decision on when to perform random accesses and for which candidates. The …,Proceedings of the 32nd international conference on Very large data bases,2006,167
From information to knowledge: harvesting entities and relationships from web sources,Gerhard Weikum; Martin Theobald,Abstract There are major trends to advance the functionality of search engines to a moreexpressive semantic level. This is enabled by the advent of knowledge-sharing communitiessuch as Wikipedia and the progress in automatically extracting entities and relationshipsfrom semistructured as well as natural-language Web sources. Recent endeavors of thiskind include DBpedia; EntityCube; KnowItAll; ReadTheWeb; and our own YAGO-NAGAproject (and others). The goal is to automatically construct and maintain a comprehensiveknowledge base of facts about named entities; their semantic classes; and their mutualrelations as well as temporal contexts; with high precision and high recall. This tutorialdiscusses state-of-the-art methods; research opportunities; and open challenges along thisavenue of knowledge harvesting.,Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2010,146
Exploiting lineage for confidence computation in uncertain and probabilistic databases,Anish Das Sarma; Martin Theobald; Jennifer Widom,We study the problem of computing query results with confidence values in ULDBs:relational databases with uncertainty and lineage. ULDBs; which subsume probabilisticdatabases; offer an alternative decoupled method of computing confidence values: Insteadof computing confidences during query processing; compute them afterwards based onlineage. This approach enables a wider space of query plans; and it permits selectivecomputations when not all confidence values are needed. This paper develops a suite ofalgorithms and optimizations for a broad class of relational queries on ULDBs. We provideconfidence computation algorithms for single data items; as well as efficient batch algorithmsto compute confidences for an entire relation or database. All algorithms incorporatememoization to avoid redundant computations; and they have been implemented in the …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,131
KORE: keyphrase overlap relatedness for entity disambiguation,Johannes Hoffart; Stephan Seufert; Dat Ba Nguyen; Martin Theobald; Gerhard Weikum,Abstract Measuring the semantic relatedness between two entities is the basis for numeroustasks in IR; NLP; and Web-based knowledge extraction. This paper focuses ondisambiguating names in a Web or text document by jointly mapping all names ontosemantically related entities registered in a knowledge base. To this end; we havedeveloped a novel notion of semantic relatedness between two entities represented as setsof weighted (multi-word) keyphrases; with consideration of partially overlapping phrases.This measure improves the quality of prior link-based models; and also eliminates the needfor (usually Wikipedia-centric) explicit interlinkage between entities. Thus; our method ismore versatile and can cope with long-tail and newly emerging entities that have few or nolinks associated with them. For efficiency; we have developed approximation techniques …,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,111
TopX: efficient and versatile top-k query processing for semistructured data,Martin Theobald; Holger Bast; Debapriyo Majumdar; Ralf Schenkel; Gerhard Weikum,Abstract Recent IR extensions to XML query languages such as Xpath 1.0 Full-Text or theNEXI query language of the INEX benchmark series reflect the emerging interest in IR-styleranked retrieval over semistructured data. TopX is a top-k retrieval engine for text andsemistructured data. It terminates query execution as soon as it can safely determine the ktop-ranked result elements according to a monotonic score aggregation function withrespect to a multidimensional query. It efficiently supports vague search on both content-andstructure-oriented query conditions for dynamic query relaxation with controllable influenceon the result ranking. The main contributions of this paper unfold into four main points:(1)fully implemented models and algorithms for ranked XML retrieval with XPath Full-Textfunctionality;(2) efficient and effective top-k query processing for semistructured data;(3) …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,109
TriAD: a distributed shared-nothing RDF engine based on asynchronous message passing,Sairam Gurajada; Stephan Seufert; Iris Miliaraki; Martin Theobald,Abstract We investigate a new approach to the design of distributed; shared-nothing RDFengines. Our engine; coined" TriAD"; combines join-ahead pruning via a novel form of RDFgraph summarization with a locality-based; horizontal partitioning of RDF triples into a grid-like; distributed index structure. The multi-threaded and distributed execution of joins inTriAD is facilitated by an asynchronous Message Passing protocol which allows us to runmultiple join operators along a query plan in a fully parallel; asynchronous fashion. Webelieve that our architecture provides a so far unique approach to join-ahead pruning in adistributed environment; as the more classical form of sideways information passing wouldnot permit for executing distributed joins in an asynchronous way. Our experiments over theLUBM; BTC and WSDTS benchmarks demonstrate that TriAD consistently outperforms …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,100
Trio-One: Layering uncertainty and lineage on a conventional DBMS,Michi Mutsuzaki; Martin Theobald; Ander De Keijzer; Jennifer Widom; Parag Agrawal; Omar Benjelloun; Anish Das Sarma; Raghotham Murthy; Tomoe Sugihara,ABSTRACT Trio is a new kind of database system that supports data; uncertainty; andlineage in a fully integrated manner. The first Trio prototype; dubbed Trio-One; is built on topof a conventional DBMS using data and query translation techniques together with a smallnumber of stored procedures. This paper describes Trio-One's translation scheme andsystem architecture; showing how it efficiently and easily supports the Trio data model andquery language.,Third Biennial Conference on Innovative Data Systems Research,2007,94
Exploiting Structure; Annotation; and Ontological Knowledge for Automatic Classification of XML Data.,Martin Theobald; Ralf Schenkel; Gerhard Weikum,ABSTRACT This paper investigates how to automatically classify schemaless XML data intoa user-defined topic directory. The main focus is on constructing appropriate feature spaceson which a classifier operates. In addition to the usual text-based term frequency vectors; westudy XML twigs and tag paths as extended features that can be combined with text termoccurrences in XML elements. Moreover; we show how to leverage ontological backgroundinformation; more specifically; the WordNet thesaurus; for the construction of moreexpressive feature spaces. For efficiency our implementation computes featuresincrementally and caches ontology entries. Our experiments demonstrate the improvedaccuracy of automatic classification based on the enhanced feature spaces.,WebDB,2003,86
Efficient and self-tuning incremental query expansion for top-k query processing,Martin Theobald; Ralf Schenkel; Gerhard Weikum,Abstract We present a novel approach for efficient and self-tuning query expansion that isembedded into a top-k query processor with candidate pruning. Traditional query expansionmethods select expansion terms whose thematic similarity to the original query terms isabove some specified threshold; thus generating a disjunctive query with much higherdimensionality. This poses three major problems: 1) the need for hand-tuning the expansionthreshold; 2) the potential topic dilution with overly aggressive expansion; and 3) thedrastically increased execution cost of a high-dimensional query. The method developed inthis paper addresses all three problems by dynamically and incrementally merging theinverted lists for the potential expansion terms with the lists for the original query terms. Apriority queue is used for maintaining result candidates; the pruning of candidates is …,Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,2005,82
Efficient text proximity search,Ralf Schenkel; Andreas Broschart; Seungwon Hwang; Martin Theobald; Gerhard Weikum,Abstract In addition to purely occurrence-based relevance models; term proximity has beenfrequently used to enhance retrieval quality of keyword-oriented retrieval systems. Whilethere have been approaches on effective scoring functions that incorporate proximity; therehas not been much work on algorithms or access methods for their efficient evaluation. Thispaper presents an efficient evaluation framework including a proximity scoring functionintegrated within a top-k query engine for text retrieval. We propose precomputed andmaterialized index structures that boost performance. The increased retrieval effectivenessand efficiency of our framework are demonstrated through extensive experiments on a verylarge text benchmark collection. In combination with static index pruning for the proximitylists; our algorithm achieves an improvement of two orders of magnitude compared to a …,International Symposium on String Processing and Information Retrieval,2007,74
The BINGO! System for Information Portal Generation and Expert Web Search.,Sergej Sizov; Martin Theobald; Stefan Siersdorfer; Gerhard Weikum; Jens Graupmann; Michael Biwer; Patrick Zimmer,Abstract This paper presents the BINGO! focused crawler; an advanced tool for informationportal generation and expert Web search. In contrast to standard search engines such asGoogle which are solely based on precomputed index structures; a focused crawlerinterleaves crawling; automatic classification; link analysis and assessment; and textfiltering. A crawl is started from a user-provided set of training data and aims to collectcomprehensive results for the given topics. The focused crawling paradigm has beenaround for a few years and many of our techniques are adopted from the informationretrieval and machine learning literature. BINGO! is a system-oriented effort to integrate asuite of techniques into a comprehensive and versatile tool. The paper discusses its overallarchitecture and main components; important lessons from early experimentation and the …,CIDR,2003,68
Word sense disambiguation for exploiting hierarchical thesauri in text classification,Dimitrios Mavroeidis; George Tsatsaronis; Michalis Vazirgiannis; Martin Theobald; Gerhard Weikum,Abstract The introduction of hierarchical thesauri (HT) that contain significant semanticinformation; has led researchers to investigate their potential for improving performance ofthe text classification task; extending the traditional “bag of words” representation;incorporating syntactic and semantic relationships among words. In this paper we addressthis problem by proposing a Word Sense Disambiguation (WSD) approach based on theintuition that word proximity in the document implies proximity also in the HT graph. Weargue that the high precision exhibited by our WSD algorithm in various humanly-disambiguated benchmark datasets; is appropriate for the classification task. Moreover; wedefine a semantic kernel; based on the general concept of GVSM kernels; that captures thesemantic relations contained in the hierarchical thesaurus. Finally; we conduct …,European Conference on Principles of Data Mining and Knowledge Discovery,2005,59
Structural feedback for keyword-based XML retrieval,Ralf Schenkel; Martin Theobald,Abstract Keyword-based queries are an important means to retrieve information from XMLcollections with unknown or complex schemas. Relevance Feedback integrates relevanceinformation provided by a user to enhance retrieval quality. For keyword-based XMLqueries; feedback engines usually generate an expanded keyword query from the content ofelements marked as relevant or nonrelevant. This approach that is inspired by text-based IRcompletely ignores the semistructured nature of XML. This paper makes the important stepfrom pure content-based to structural feedback. It presents a framework that expands akeyword query into a full-fledged content-and-structure query. Extensive experiments withthe established INEX benchmark and our TopX search engine show the feasibility of ourapproach.,European Conference on Information Retrieval,2006,48
Feedback-driven structural query expansion for ranked retrieval of XML data,Ralf Schenkel; Martin Theobald,Abstract Relevance Feedback is an important way to enhance retrieval quality by integratingrelevance information provided by a user. In XML retrieval; feedback engines usuallygenerate an expanded query from the content of elements marked as relevant ornonrelevant. This approach that is inspired by text-based IR completely ignores thesemistructured nature of XML. This paper makes the important step from content-based tostructural feedback. It presents an integrated solution for expanding keyword queries withnew content; path; and document constraints. An extensible framework evaluates suchquery conditions with existing keyword-based XML search engines while allowing to easilyintegrate new dimensions of feedback. Extensive experiments with the established INEXbenchmark show the feasibility of our approach.,International Conference on Extending Database Technology,2006,43
TopX and XXL at INEX 2005,Martin Theobald; Ralf Schenkel; Gerhard Weikum,Abstract We participated with two different and independent search engines in this year'sINEX round: The XXL Search Engine and the TopX engine. As this is the first participation forTopX; this paper focuses on the design principles; scoring; query evaluation and results ofTopX. We shortly discuss the results with XXL afterwards.,International Workshop of the Initiative for the Evaluation of XML Retrieval,2005,42
Top-k query processing in probabilistic databases with non-materialized views,Maximilian Dylla; Iris Miliaraki; Martin Theobald,We investigate a novel approach of computing confidence bounds for top-k ranking queriesin probabilistic databases with non-materialized views. Unlike related approaches; wepresent an exact pruning algorithm for finding the top-ranked query answers according totheir marginal probabilities without the need to first materialize all answer candidates via theviews. Specifically; we consider conjunctive queries over multiple levels of select-project-join views; the latter of which are cast into Datalog rules which we ground in a top-downfashion directly at query processing time. To our knowledge; this work is the first to addressintegrated data and confidence computations for intensional query evaluations in the contextof probabilistic databases by considering confidence bounds over first-order lineageformulas. We extend our query processing techniques by a tool-suite of scheduling …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,37
Database foundations for scalable RDF processing,Katja Hose; Ralf Schenkel; Martin Theobald; Gerhard Weikum,Abstract As more and more data is provided in RDF format; storing huge amounts of RDFdata and efficiently processing queries on such data is becoming increasingly important.The first part of the lecture will introduce state-of-the-art techniques for scalably storing andquerying RDF with relational systems; including alternatives for storing RDF; efficient indexstructures; and query optimization techniques. As centralized RDF repositories havelimitations in scalability and failure tolerance; decentralized architectures have beenproposed. The second part of the lecture will highlight system architectures and strategiesfor distributed RDF processing. We cover search engines as well as federated queryprocessing; highlight differences to classic federated database systems; and discussefficient techniques for distributed query processing in general and for RDF data in …,Proceedings of the 7th international conference on Reasoning web: semantic technologies for the web of data,2011,33
COMPASS: A concept-based Web search engine for HTML; XML; and deep Web data,Jens Graupmann; Michael Biwer; Christian Zimmer; Patrick Zimmer; Matthias Bender; Martin Theobald; Gerhard Weikum,Today's web search engines are still following the paradigm of keyword-based search.Although this is the best choice for large scale search engines in terms of throughput andscalability; it inherently limits the ability to accomplish more meaningful query tasks. XMLquery engines (eg; based on XQuery or XPath); on the other hand; have powerful querycapabilities; but at the same time their dedication to XML data with a global schema is theirweakness; because most web information is still stored in diverse formats and does notconform to common schemas. Typical web formats include static HTML pages or pages thatare generated dynamically from underlying database systems; accessible only throughportal interfaces. We have developed an expressive style of conceptbased and context-aware querying with relevance ranking that encompasses different; non-schematic data …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,33
Relevance feedback for structural query expansion,Ralf Schenkel; Martin Theobald,Abstract Keyword-based queries are an important means to retrieve information from XMLcollections with unknown or complex schemas. Relevance Feedback integrates relevanceinformation provided by a user to enhance retrieval quality. For keyword-based XMLqueries; feedback engines usually generate an expanded keyword query from the content ofelements marked as relevant or nonrelevant. This approach that is inspired by text-based IRcompletely ignores the semistructured nature of XML. This paper makes the important stepfrom pure content-based to structural feedback. It presents two independent approaches thatinclude structural dimensions in a feedback-driven query evaluation: The first approachreranks the result list of a keyword-based search engine; using structural features derivedfrom results with known relevance. The second approach expands a keyword query into a …,International Workshop of the Initiative for the Evaluation of XML Retrieval,2005,31
Photospread: a spreadsheet for managing photos,Sean Kandel; Andreas Paepcke; Martin Theobald; Hector Garcia-Molina; Eric Abelson,Abstract PhotoSpread is a spreadsheet system for organizing and analyzing photocollections. It extends the current spreadsheet paradigm in two ways:(a) PhotoSpreadaccommodates sets of objects (eg; photos) annotated with tags (attribute-value pairs).Formulas can manipulate object sets and refer to tags.(b) Photos can be reorganized (tagsand location changed) by drag-and-drop operations on the spreadsheet. The PhotoSpreaddesign was driven by the needs of field biologists who have large collections of annotatedphotos. The paper describes the PhotoSpread functionality and the design choices made.,Proceedings of the SIGCHI conference on human factors in computing systems,2008,30
Learning word-to-concept mappings for automatic text classification,Georgiana Ifrim; Martin Theobald; Gerhard Weikum,Abstract For both classification and retrieval of natural language text documents; thestandard document representation is a term vector where a term is simply a morphologicalnormal form of the corresponding word. A potentially better approach would be to map everyword onto a concept; the proper word sense and use this additional information in thelearning process. In this paper we address the problem of automatically classifying naturallanguage text documents. We investigate the effect of word to concept mappings and wordsense disambiguation techniques on improving classification accuracy. We use the WordNetthesaurus as a background knowledge base and propose a generative language modelapproach to document classification. We show experimental results comparing theperformance of our model with Naive Bayes and SVM classifiers.,Learning in Web Search Workshop; ICML,2005,26
AIDA-light: High-Throughput Named-Entity Disambiguation.,Dat Ba Nguyen; Johannes Hoffart; Martin Theobald; Gerhard Weikum,ABSTRACT To advance the Web of Linked Data; mapping ambiguous names in structuredand unstructured contents onto knowledge bases would be a vital asset. State-of-the-artmethods for Named Entity Disambiguation (NED) face major tradeoffs regardingefficiency/scalability vs. accuracy. Fast methods use relatively simple context features andavoid computationally expensive algorithms for joint inference. While doing very well onprominent entities in clear input texts; these methods achieve only moderate accuracy whenfed with difficult inputs. On the other hand; methods that rely on rich context features andjoint inference for mapping names onto entities pay the price of being much slower. Thispaper presents AIDA-light which achieves high accuracy on difficult inputs while also beingfast and scalable. AIDA-light uses a novel kind of two-stage mapping algorithm. It first …,LDOW,2014,25
A temporal-probabilistic database model for information extraction,Maximilian Dylla; Iris Miliaraki; Martin Theobald,Abstract Temporal annotations of facts are a key component both for building a high-accuracy knowledge base and for answering queries over the resulting temporal knowledgebase with high precision and recall. In this paper; we present a temporal-probabilisticdatabase model for cleaning uncertain temporal facts obtained from information extractionmethods. Specifically; we consider a combination of temporal deduction rules; temporalconsistency constraints and probabilistic inference based on the common possible-worldssemantics with data lineage; and we study the theoretical properties of this data model. Wefurther develop a query engine which is capable of scaling to very large temporal knowledgebases; with nearly interactive query response times over millions of uncertain facts andhundreds of thousands of grounded rules. Our experiments over two real-world datasets …,Proceedings of the VLDB Endowment,2013,25
LIVE: a lineage-supported versioned DBMS,Anish Das Sarma; Martin Theobald; Jennifer Widom,Abstract This paper presents LIVE; a complete DBMS designed for applications with manystored derived relations; and with a need for simple versioning capabilities when base datais modified. Target applications include; for example; scientific data management and dataintegration. A key feature of LIVE is the use of lineage (provenance) to support modificationsand versioning in this environment. In our system; lineage significantly facilitates both:(1)efficient propagation of modifications from base to derived data; and (2) efficient execution ofa wide class of queries over versioned; derived data. LIVE is fully implemented; detailedexperimental results are presented that validate our techniques.,International Conference on Scientific and Statistical Database Management,2010,23
Yago-qa: Answering questions by structured knowledge queries,Peter Adolphs; Martin Theobald; Ulrich Schafer; Hans Uszkoreit; Gerhard Weikum,We present a natural-language question-answering system that gives access to theaccumulated knowledge of one of the largest community projects on the Web-Wikipedia-viaan automatically acquired structured knowledge base. Key to building such a system is toestablish mappings from natural language expressions to semantic representations. Wepropose to acquire these mappings by data-driven methods-corpus harvesting andparaphrasing and present a preliminary empirical study that demonstrates the viability of ourmethod.,Semantic Computing (ICSC); 2011 Fifth IEEE International Conference on,2011,22
-From Focused Crawling to Expert Information: An Application Framework for Web Exploration and Portal Generation,Sergej Sizov; Jens Graupmann; Martin Theobald,Focused crawling is a relatively new; promising approach to improving the recall of expertsearch on the Web. It typically starts from a user-or community specific tree of topics alongwith a few training documents for each tree node; and then crawls the Web with focus onthese topics of interest. This process can efficiently build a theme-specific; hierarchicaldirectory whose nodes are populated with relevant high-quality documents for expert Websearch. The BINGO! focused crawler implements an approach that aims to overcome thelimitations of the initial training data. BINGO! identifies; among the crawled and positivelyclassified documents of a topic; characteristic archetypes and uses them for periodicallyretraining the classifier. This way the crawler is dynamically adapted based on the mostsignificant documents seen so far.From Focused Crawling to Expert Information: an …,*,2003,20
Time-aware Reasoning in Uncertain Knowledge Bases.,Yafang Wang; Mohamed Yahya; Martin Theobald,Abstract. Time information is ubiquitous on the Web; and considering temporal constraintsamong facts extracted from the Web is key for high-precision query answering over time-variant factual data. In this paper; we present a simple and efficient representation model fortimedependent uncertainty in combination with first-order inference rules and recursivequeries over RDF-like knowledge bases. In the spirit of data lineage; the intensional (ie; rule-based) structure of query answers is reflected by Boolean formulas that capture the logicaldependencies of each derived answer fact back to its extensional roots (ie; base facts). Ourapproach incorporates simple weight aggregations for begin; end and during evidences forbase facts; but also generalizes the common possibleworlds semantics known fromprobabilistic databases to histogram-like confidence distributions for derived facts. In …,MUD,2010,19
Towards a statistically semantic web,Gerhard Weikum; Jens Graupmann; Ralf Schenkel; Martin Theobald,Abstract The envisioned Semantic Web aims to provide richly annotated and explicitlystructured Web pages in XML; RDF; or description logics; based upon underlying ontologiesand thesauri. Ideally; this should enable a wealth of query processing and semanticreasoning capabilities using XQuery and logical inference engines. However; we believethat the diversity and uncertainty of terminologies and schema-like annotations will makeprecise querying on a Web scale extremely elusive if not hopeless; and the same argumentholds for large-scale dynamic federations of Deep Web sources. Therefore; ontology-basedreasoning and querying needs to be enhanced by statistical means; leading to relevance-ranked lists as query results. This paper presents steps towards such a “statisticallysemantic” Web and outlines technical challenges. We discuss how statistically quantified …,International Conference on Conceptual Modeling,2004,19
Resolving Temporal Conflicts in Inconsistent RDF Knowledge Bases.,Maximilian Dylla; Mauro Sozio; Martin Theobald,Abstract: Recent trends in information extraction have allowed us to not only extract largesemantic knowledge bases from structured or loosely structured Web sources; but to alsoextract additional annotations along with the RDF facts these knowledge bases contain.Among the most important types of annotations are spatial and temporal annotations. Inparticular the latter temporal annotations help us to reflect that a majority of facts is not staticbut highly ephemeral in the real world; ie; facts are valid for only a limited amount of time; ormultiple facts stand in temporal dependencies with each other. In this paper; we present adeclarative reasoning framework to express and process temporal consistency constraintsand queries via first-order logical predicates. We define a subclass of first-order constraintswith temporal predicates for which the knowledge base is guaranteed to be satisfiable …,BTW,2011,18
Overview of the INEX 2012 Linked Data Track.,Qiuyue Wang; Jaap Kamps; Georgina Ramirez Camps; Maarten Marx; Anne Schuth; Martin Theobald; Sairam Gurajada; Arunav Mishra,Overview of the INEX 2012 Linked Data Track Qiuyue Wang1; Jaap Kamps2; Georgina RamırezCamps3; Maarten Marx2; Anne Schuth2; Martin Theobald4; Sairam Gurajada4; and Arunav Mishra41Renmin University of China; Beijing; China 2University of Amsterdam; Amsterdam; The Netherlands3Universitat Pompeu Fabra; Barcelona; Spain 4Max Planck Institute for Informatics;Saarbrücken; Germany Abstract. This paper provides an overview of the Linked Data Track thatwas newly introduced to the set of INEX tracks in 2012. 1 Introduction The goal of the new LinkedData Track was to investigate retrieval techniques over a combination of textual and highly structureddata; where rich textual contents from Wikipedia articles serve as the basis for retrieval andranking; while addtional RDF properties carry key information about semantic relations amongentities that cannot be captured by keywords alone. Our intension in organizing this new …,CLEF (Online Working Notes/Labs/Workshop),2012,17
Overview of the inex 2011 data-centric track,Qiuyue Wang; Georgina Ramírez; Maarten Marx; Martin Theobald; Jaap Kamps,Abstract This paper presents an overview of the INEX 2011 Data-Centric Track. Having thead hoc search task running its second year; we introduced a new task; faceted search task;whose goal is to provide the infrastructure to investigate and evaluate different techniquesand strategies of recommending facet-values to aid the user to navigate through a large setof query results and quickly identify the results of interest. The same IMDB collection as lastyear was used for both tasks. A total of 9 active participants contributed a total of 60 topics forboth tasks and submitted 35 ad hoc search runs and 13 faceted search runs. A total of 38 adhoc search topics were assessed; which included 18 subtopics for 13 faceted search topics.We discuss the setup for both tasks and the results obtained by their participants.,International Workshop of the Initiative for the Evaluation of XML Retrieval,2011,17
Crowdsourcing assessments for XML ranked retrieval,Omar Alonso; Ralf Schenkel; Martin Theobald,Abstract Crowdsourcing has gained a lot of attention as a viable approach for conducting IRevaluations. This paper shows through a series of experiments on INEX data thatcrowdsourcing can be a good alternative for relevance assessment in the context of XMLretrieval.,European Conference on Information Retrieval,2010,17
J-NERD: joint named entity recognition and disambiguation with rich linguistic features,Dat Ba Nguyen; Martin Theobald; Gerhard Weikum,Abstract Methods for Named Entity Recognition and Disambiguation (NERD) perform NERand NED in two separate stages. Therefore; NED may be penalized with respect to precisionby NER false positives; and suffers in recall from NER false negatives. Conversely; NEDdoes not fully exploit information computed by NER such as types of mentions. This paperpresents J-NERD; a new approach to perform NER and NED jointly; by means of aprobabilistic graphical model that captures mention spans; mention types; and the mappingof mentions to entities in a knowledge base. We present experiments with different kinds oftexts from the CoNLL'03; ACE'05; and ClueWeb'09-FACC1 corpora. J-NERD consistentlyoutperforms state-of-the-art competitors in end-to-end NERD precision; recall; and F1.,Transactions of the Association for Computational Linguistics,2016,16
Spreadsheet system and method for managing photos,*,A spreadsheet system stores objects and formulas in the cells of the spreadsheet; along withmetadata tags. Each cell may contain multiple objects (such as a group of photos). Drag-and-drop user actions automatically change tags associated with the moved objects based on aformula stored in the target cell if a forcing semantics mode is active; or automaticallychange a formula stored in the target cell if a non-forcing semantics mode is active. Aformula in a cell selects a set of objects and may include set operators such as union;intersection; and minus. The formula may also filter sets to select a subset based onspecified tag attributes of objects. The display of the spreadsheet adaptively selects a subsetof objects to render in a cell and appropriately scales photos for optimal viewing.,*,2012,16
Find your advisor: robust knowledge gathering from the web,Ndapandula Nakashole; Martin Theobald; Gerhard Weikum,Abstract We present a robust method for gathering relational facts from the Web; based onmatching generalized patterns which are automatically learned from seed facts for relationsof interest. Our approach combines these generalized patterns for high recall informationextraction with a rule-based; declarative reasoning approach to also ensure high precision.Newly extracted candidate facts are assigned statistical weights which reflect the strengthsof the patterns used to extract them. For checking the plausibility of candidate facts withrespect to existing knowledge and competing hypotheses; we use an efficient algorithm forweighted Max-Sat over propositional-logic clauses. In contrast to prior work on reasoning-based information extraction; we employ richer statistics and smart pruning to bound thenumber of grounded rules passed on to the Max-Sat solver.,Procceedings of the 13th International Workshop on the Web and Databases,2010,15
URDF: Efficient reasoning in uncertain RDF knowledge bases with soft and hard rules,Martin Theobald; Mauro Sozio; Fabian Suchanek; Ndapandula Nakashole,Zusammenfassung We present URDF; an efficient reasoning framework for graph-based;nonschematic RDF knowledge bases and SPARQL-like queries. URDF augments first-orderreasoning by a combination of soft rules; with Datalog-style recursive implications; and hardrules; in the shape of mutually exclusive sets of facts. It incorporates the common possibleworlds semantics with independent base facts as it is prevalent in most probabilisticdatabase approaches; but also supports semantically more expressive; probabilistic first-order representations such as Markov Logic Networks. As knowledge extraction on theWeboften is an iterative (and inherently noisy) process; URDF explicitly targets the resolution ofinconsistencies between the underlying RDF base facts and the inference rules. Core of ourapproach is a novel and efficient approximation algorithm for a generalized version of the …,*,2010,15
Extraction of conditional probabilities of the relationships between drugs; diseases; and genes from PubMed guided by relationships in PharmGKB,Martin Theobald; Nigam Shah; Jeff Shrager,Abstract Guided by curated associations between genes; treatments (ie; drugs); anddiseases in pharmGKB; we constructed n-way Bayesian networks based on conditionalprobability tables (cpt's) extracted from co-occurrence statistics over the entire Pubmedcorpus; producing a broad-coverage analysis of the relationships between these biologicalentities. The networks suggest hypotheses regarding drug mechanisms; treatmentbiomarkers; and/or potential markers of genetic disease. The cpt's enable Trio; an inferentialdatabase; to query indirect (inferred) relationships via an SQL-like query language.,Summit on translational bioinformatics,2009,15
The BINGO! focused crawler: From bookmarks to archetypes,Sergej Sizov; Stefan Siersdorfer; Martin Theobald; Gerhard Weikum,The BINGO! system implements an approach to focused crawling that aims to overcome thelimitations of the initial training data. To this end; BINGO! identifies; among the crawled andpositively classified documents of a topic; characteristic" archetypes" and uses them forperiodically re-training the classifier; this way the crawler is dynamically adapted based onthe most significant documents seen so far. Two kinds of archetypes are considered: goodauthorities as determined by employing Kleinberg's link analysis algorithm; and documentsthat have been automatically classified with high confidence using a linear SVM classifier.,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,15
BINGO! and DAFFODIL: Personalized exploration of digital libraries and web sources,Martin Theobald; Claus-Peter Klas,Abstract D affodil is a digital library system targeted at strategic support of advanced usersduring the information search process. It provides user-customizable" stratagems" forexploring and managing digital library objects with meta data annotations over a federationof heterogeneous digital libraries. B ingo! is a focused crawler that learns how to gatherthematically relevant documents from the Web and Deep-Web sources. This paper presentsa coupling architecture for D affodil and B ingo! that allows advanced users to explore digitallibraries and Web sources in a comprehensive and coherent way. Starting from a user'sinterest profile in D affodil; B ingo! is instructed to find thematically similar documents on theWeb; leading to high-quality recommendations that reach beyond the information that canbe directly found in digital libraries. Our experimental studies demonstrate that this …,Coupling approaches; coupling media and coupling languages for information retrieval,2004,13
Query-Time Reasoning in Uncertain RDF Knowledge Bases with Soft and Hard Rules.,Ndapandula Nakashole; Mauro Sozio; Fabian M Suchanek; Martin Theobald,ABSTRACT Recent advances in information extraction have paved the way for the automaticconstruction and growth of large; semantic knowledge bases from Web sources. However;the very nature of these extraction techniques entails that the resulting RDF knowledgebases may face a significant amount of incorrect; incomplete; or even inconsistent (ie;uncertain) factual knowledge; which makes efficient query answering over this kind ofuncertain RDF data a challenge. Our engine; coined URDF; augments first-order reasoningby a combination of soft rules (Datalog-style implications); which are grounded in adeductive fashion in order to derive new facts from existing ones; and hard rules (mutual-exclusiveness constraints); which enforce additional consistency constraints among bothbase and derived facts. At the core of our approach is an efficient approximation algorithm …,VLDS,2012,12
Interactive reasoning in uncertain RDF knowledge bases,Timm Meiser; Maximilian Dylla; Martin Theobald,Abstract Recent advances in Web-based information extraction have allowed for theautomatic construction of large; semantic knowledge bases; which are typically captured inRDF format. The very nature of the applied extraction techniques however entails that theresulting RDF knowledge bases may face a significant amount of incorrect; incomplete; oreven inconsistent (ie; uncertain) factual knowledge; which makes query answering over thiskind of data a challenge. Our reasoner; coined URDF; supports SPARQL queries along withrule-based; first-order predicate logic to infer new facts and to resolve data uncertainty overmillions of RDF triplets directly at query time. We demonstrate a fully interactive reasoningengine; combining a Java-based reasoning backend and a Flash-based visualizationfrontend in a dynamic client-server architecture. Our visualization frontend provides …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,12
BINGO!: Bookmark-Induced Gathering of Information.,Sergej Sizov; Martin Theobald; Stefan Siersdorfer; Gerhard Weikum,Abstract Focused (thematic) crawling is a relatively new; promising approach to improvingthe recall of expert search on the Web. It involves the automatic classiﬁcation of visiteddocuments into a user-or community-speciﬁc topic hierarchy (ontology). The quality of thetraining data for the classiﬁer is the most critical issue and potential bottleneck for theeffectivity and scale of a focused crawler This paper presents the BINGO! approach tofocused crawling that aims to overcome the limitations of the initial training data. To this end;BINGO! identiﬁes; among the crawled and positively classiﬁed documents of a topic;characteristic" archetypes" and uses them for periodically re-training the classiﬁer; this waythe crawler is dynamically adapted based on the most signiﬁcant documents seen so far.Two kinds of archetypes are considered: good authorities as determined by employing …,WISE,2002,12
Correlation-Based Refinement of Rules with Numerical Attributes.,André Melo; Martin Theobald; Johanna Völker,Abstract Learning rules is a common way of extracting useful information from knowledge ordata bases. Many of such data sets contain numerical attributes. However; approaches likeInductive Logic Programming (ILP) or association rule mining are optimized for data withcategorical values; and considering numerical attributes is expensive. In this paper; wepresent an extension to the top-down ILP algorithm; which enables an efficient discovery ofdatalog rules from data with both numerical and categorical attributes. Our approachcomprises a preprocessing phase for computing the correlations between numerical andcategorical attributes; as well as an extension to the ILP refinement step; which enables usto detect interesting candidate rules and to suggest refinements with relevant attributecombinations. We report on experiments with US Census data; Freebase and DBpedia …,FLAIRS Conference,2014,10
Match graph construction for large image databases,Kwang In Kim; James Tompkin; Martin Theobald; Jan Kautz; Christian Theobalt,Abstract How best to efficiently establish correspondence among a large set of images orvideo frames is an interesting unanswered question. For large databases; the highcomputational cost of performing pair-wise image matching is a major problem. However; formany applications; images are inherently sparsely connected; and so current techniques tryto correctly estimate small potentially matching subsets of databases upon which to performexpensive pair-wise matching. Our contribution is to pose the identification of potentialmatches as a link prediction problem in an image correspondence graph; and to propose aneffective algorithm to solve this problem. Our algorithm facilitates incremental imagematching: initially; the match graph is very sparse; but it becomes dense as we alternatebetween link prediction and verification. We demonstrate the effectiveness of our …,European Conference on Computer Vision,2012,9
Using graph summarization for join-ahead pruning in a distributed RDF engine,Sairam Gurajada; Stephan Seufert; Iris Miliaraki; Martin Theobald,Abstract The need for scalable and efficient RDF stores has seen a high demand recently.Many efficient systems; both centralized and distributed; have been proposed. Since a row-oriented output is required by SPARQL; most of the current systems rely on relational joins.One of the problems with relational joins; though; is a performance bottleneck imposed bythe generation of large intermediate relations which could be avoided by using moreaccurate data and pruning statistics. To address this problem; recently several systems havebeen proposed that employ bisimulation-based graph summaries--adopted from XMLindexing--over large RDF graphs in order to facilitate join-ahead pruning. In this paper; wediscuss a different; locality-based; graph summarization approach for RDF data andhighlight its utilization for join-ahead pruning in a distributed SPARQL engine. Based on …,Proceedings of Semantic Web Information Management on Semantic Web Information Management,2014,8
Overview of INEX 2013,Patrice Bellot; Antoine Doucet; Shlomo Geva; Sairam Gurajada; Jaap Kamps; Gabriella Kazai; Marijn Koolen; Arunav Mishra; Véronique Moriceau; Josiane Mothe; Michael Preminger; Eric SanJuan; Ralf Schenkel; Xavier Tannier; Martin Theobald; Matthew Trappett; Qiuyue Wang,Abstract INEX investigates focused retrieval from structured documents by providing largetest collections of structured documents; uniform evaluation measures; and a forum fororganizations to compare their results. This paper reports on the INEX 2013 evaluationcampaign; which consisted of four activities addressing three themes: searchingprofessional and user generated data (Social Book Search track); searching structured orsemantic data (Linked Data track); and focused retrieval (Snippet Retrieval and TweetContextualization tracks). INEX 2013 was an exciting year for INEX in which weconsolidated the collaboration with (other activities in) CLEF and for the second time ran ourworkshop as part of the CLEF labs in order to facilitate knowledge transfer between theevaluation forums. This paper gives an overview of all the INEX 2013 tracks; their aims …,International Conference of the Cross-Language Evaluation Forum for European Languages,2013,8
Report on INEX 2012,Patrice Bellot; Timothy Chappell; Antoine Doucet; Shlomo Geva; Sairam Gurajada; Jaap Kamps; Gabriella Kazai; Marijn Koolen; Monica Landoni; Maarten Marx; Arunav Mishra; Véronique Moriceau; Josiane Mothe; Michael Preminger; Georgina Ramirez; Mark Sanderson; Eric Sanjuan; Falk Scholer; A Schuh; Xavier Tannier; Martin Theobald; Matthew Trappett; Andrew Trotman; Qiuyue Wang,Abstract INEX investigates focused retrieval from structured documents by providing largetest collections of structured documents; uniform evaluation measures; and a forum fororganizations to compare their results. This paper reports on the INEX'12 evaluationcampaign; which consisted of a five tracks: Linked Data; Relevance Feedback; SnippetRetrieval; Social Book Search; and Tweet Contextualization. INEX'12 was an exciting yearfor INEX in which we joined forces with CLEF and for the first time ran our workshop as partof the CLEF labs in order to facilitate knowledge transfer between the evaluation forums.,ACM SIGIR Forum,2012,8
Report on INEX 2008,Gianluca Demartin; Ludovic Denoye; Antoine Douce; Khairun Nisa Fachry; Patrick Gallinar; Shlomo Gev; Wei-Che Huang; Tereza Iofciu; Jaap Kamps; Gabriella Kazai; Marijn Koolen; Monica Landoni; Ragnar Nordlie; Nils Pharo; Ralf Schenkel; Martin Theobald; Andrew Trotman; Arjen P De Vries; Alan Woodley; Jianhan Zhu,Abstract INEX investigates focused retrieval from structured documents by providing largetest collections of structured documents; uniform evaluation measures; and a forum fororganizations to compare their results. This paper reports on the INEX 2008 evaluationcampaign; which consisted of a wide range of tracks: Ad hoc; Book; Efficiency; EntityRanking; Interactive; QA; Link the Wiki; and XML Mining.,ACM SIGIR Forum,2009,8
Data modifications and versioning in Trio,Anish Das Sarma; Martin Theobald; Jennifer Widom,The field of uncertain databases has recently attracted considerable interest. Manymotivating applications for uncertainty rely fundamentally on improving the quality of dataover time; through modifications; as additional information becomes available; eg; afteranalysis (as in scientific data management systems); with user feedback (as in {\em pay-as-you-go} data integration). Incorporating data modifications; while still serving applications'needs to access and query older data (as in hypothetical databases); necessitates light\-weight versioning for such applications. This paper presents the first DBMS for uncertaindata that supports data modifications and versioning. Our work is in the context of {\em Trio};a project at Stanford for managing data {\em uncertainty} and {\em lineage}. We introduceSQL-based language constructs for data modifications and lightweight versioning in Trio's …,*,2008,8
The topx db&ir engine,Martin Theobald; Ralf Schenkel; Gerhard Weikum,Abstract This paper proposes a demo of the TopX search engine; an extensive frameworkfor unified indexing; querying; and ranking of large collections of unstructured;semistructured; and structured data. TopX integrates efficient algorithms for top-k-styleranked retrieval with powerful scoring models for text and XML documents; as well asdynamic and self-tuning query expansion based on background ontologies.,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,8
Overview of the INEX 2013 Linked Data Track.,Sairam Gurajada; Jaap Kamps; Arunav Mishra; Ralf Schenkel; Martin Theobald; Qiuyue Wang,Abstract. This paper provides an overview of the INEX Linked Data Track; which went into itssecond iteration in 2013 … As in the previous year [7]; the goal of the INEX Linked Data Track1was to investigate retrieval techniques over a combination of textual and highly struc- tureddata; where rich textual contents from Wikipedia articles serve as the basis for retrieval and rankingtechniques; while additional RDF properties carry key information about semantic relationshipsamong entities that cannot be cap- tured by keywords alone. As opposed to the previousyear; the Linked Data Track employed a new form of a reference collection; which was purelybased on openly available dumps of English Wikipedia articles (using a snapshot from June1st; 2012; in MediaWiki XML format) plus two canonical subsets of the DBpedia 3.8 [3] and YAGO2[4] collections (in RDF NT format). In addition to this reference collection; we provided two …,CLEF (Working Notes),2013,7
Experiments with proximity-aware scoring for XML retrieval at INEX 2008,Andreas Broschart; Ralf Schenkel; Martin Theobald,Abstract Proximity enhanced scoring models significantly improve retrieval quality in textretrieval. For XML IR; we can sometimes enhance the retrieval efficacy by exploitingknowledge about the document structure combined with established text IR methods. Thispaper elaborates on our approach used for INEX 2008 which modifies a proximity scoringmodel from text retrieval for usage in XML IR and extends it by taking the document structureinformation into account.,International Workshop of the Initiative for the Evaluation of XML Retrieval,2008,7
The photospread query language,Sean Kandel; Andreas Paepcke; Martin Theobald; Hector Garcia-Molina,This document defines the data model as well as the syntax and semantics of the formulalanguage employed by PhotoSpread. It is inspired by Excel with specialized and enrichedfunctionality for managing and tagging large photo collections in a spreadsheet.PhotoSpread allows for capturing; storing; arranging; manipulating; and querying arbitrarytagged photo objects with an intuitive and easy-to-learn; yet expressive formula language.,*,2007,7
Querying and learning in probabilistic databases,Maximilian Dylla; Martin Theobald; Iris Miliaraki,Abstract Probabilistic Databases (PDBs) lie at the expressive intersection of databases; first-order logic; and probability theory. PDBs employ logical deduction rules to process Select-Project-Join (SPJ) queries; which form the basis for a variety of declarative query languagessuch as Datalog; Relational Algebra; and SQL. They employ logical consistency constraintsto resolve data inconsistencies; and they represent query answers via logical lineageformulas (aka.“data provenance”) to trace the dependencies between these answers andthe input tuples that led to their derivation. While the literature on PDBs dates back to morethan 25 years of research; only fairly recently the key role of lineage for establishing a closedand complete representation model of relational operations over this kind of probabilisticdata was discovered. Although PDBs benefit from their efficient and scalable database …,Reasoning Web International Summer School,2014,6
Dictionary-based named entity recognition,Artem Boldyrev; Gerhard Weikum; Christian Theobalt,Author: Boldyrev; Artem et al.; Genre: Thesis; Published in Print: 2013;Title: Dictionary-based Named Entity Recognition.,*,2013,6
TopX 2.0 at the inex 2009 ad-hoc and efficiency tracks,Martin Theobald; Ablimit Aji; Ralf Schenkel,Abstract This paper presents the results of our INEX 2009 Ad-hoc and Efficiency trackexperiments. While our scoring model remained almost unchanged in comparison toprevious years; we focused on a complete redesign of our XML indexing component withrespect to the increased need for scalability that came with the new 2009 INEX Wikipediacollection; which is about 10 times larger than the previous INEX collection. TopX nowsupports a CAS-specific distributed index structure; with a completely parallel execution ofall indexing steps; including parsing; sampling of term statistics for our element-specificBM25 ranking model; as well as sorting and compressing the index lists into our finalinverted block-index structure. Overall; TopX ranked among the top 3 systems in both the Ad-hoc and Efficiency tracks; with a maximum value of 0.61 for iP [0.01] and 0.29 for MAiP in …,International Workshop of the Initiative for the Evaluation of XML Retrieval,2009,6
TopX@ INEX 2007,Andreas Broschart; Ralf Schenkel; Martin Theobald; Gerhard Weikum,Abstract This paper describes the setup and results of the Max-Planck-Institut für Informatik'scontributions for the INEX 2007 AdHoc Track task. The runs were produced with TopX; asearch engine for ranked retrieval of XML data that supports a probabilistic scoring model forfull-text content conditions and tag-term combinations; path conditions as exact or relaxableconstraints; and ontology-based relaxation of terms and tag names.,International Workshop of the Initiative for the Evaluation of XML Retrieval,2007,6
Topx–adhoc track and feedback task,Martin Theobald; Andreas Broschart; Ralf Schenkel; Silvana Solomon; Gerhard Weikum,Abstract This paper describes the setup and results of the Max-Planck-Institut für Informatik'scontributions for the INEX 2006 AdHoc Track and Feedback task. The runs were producedwith the TopX system; which is a top-k retrieval engine for text and XML data that uses acombination of BM25-based content and structural scores.,International Workshop of the Initiative for the Evaluation of XML Retrieval,2006,6
TopX: efficient and versatile top-k query processing for text; structured; and semistructured data,Martin Theobald,TopX is a top-k retrieval engine for text and XML data. Unlike Boolean engines; it stopsquery processing as soon as it can safely determine the k top-ranked result objectsaccording to a monotonous score aggregation function with respect to a multidimensionalquery. The main contributions of the thesis unfold into four main points; confirmed byprevious publications at international conferences or workshops:• Top-k query processingwith probabilistic guarantees.• Index-access optimized top-k query processing.• Dynamicand self-tuning; incremental query expansion for top-k query processing.• Efficient supportfor ranked XML retrieval and full-text search. Our experiments demonstrate the viability andimproved efficiency of our approach compared to existing related work for a broad variety ofretrieval scenarios.,*,2006,6
Classification and focused crawling for semistructured data,Martin Theobald; Ralf Schenkel; Gerhard Weikum,Abstract Despite the great advances in XML data management and querying; the currentlyprevalent XPath-or XQuery-centric approaches face severe limitations when applied to XMLdocuments in large intranets; digital libraries; federations of scientific data repositories; andultimately the Web. In such environments; data has much more diverse structure andannotations than in a business-data setting and there is virtually no hope for a commonschema or DTD that all the data complies with. Without a schema; however; databasestylequerying would often produce either empty result sets; namely; when queries are overlyspecific; or way too many results; namely; when search predicates are overly broad; thelatter being the result of the user not knowing enough about the structure and annotations ofthe data.,*,2003,6
Learning tuple probabilities in probabilistic databases,Maximilian Dylla; Martin Theobald,Abstract Learning the parameters of complex probabilistic-relational models from labeledtraining data is a standard technique in machine learning; which has been intensivelystudied in the subfield of Statistical Relational Learning (SRL); but---so far---this is still anunder-investigated topic in the context of Probabilistic Databases (PDBs). In this paper; wefocus on learning the probability values of base tuples in a PDB from query answers; thelatter of which are represented as labeled lineage formulas. Specifically; we consider labelsin the form of pairs; each consisting of a Boolean lineage formula and a marginal probabilitythat comes attached to the corresponding query answer. The resulting learning problem canbe viewed as the inverse problem to confidence computations in PDBs: given a set oflabeled query answers; learn the probability values of the base tuples; such that the …,*,2014,5
Report on INEX 2013,Patrice Bellot; Antoine Doucet; Shlomo Geva; Sairam Gurajada; Jaap Kamps; Gabriella Kazai; Marijn Koolen; Arunav Mishra; Véronique Moriceau; Josiane Mothe; Michael Preminger; Eric Sanjuan; Ralf Schenkel; Xavier Tannier; Martin Theobald; Matthew Trappett; Andrew Trotman; Mark Sanderson; Falk Scholer; Qiuyue Wang,Abstract INEX investigates focused retrieval from structured documents by providing largetest collections of structured documents; uniform evaluation measures; and a forum fororganizations to compare their results. This paper reports on the INEX 2013 evaluationcampaign; which consisted of four activities addressing three themes: searchingprofessional and user generated data (Social Book Search track); searching structured orsemantic data (Linked Data track); and focused retrieval (Snippet Retrieval and TweetContextualization tracks). INEX 2013 was an exciting year for INEX in which weconsolidated the collaboration with (other activities in) CLEF and for the second time ran ourworkshop as part of the CLEF labs in order to facilitate knowledge transfer between theevaluation forums. This paper gives an overview of all the INEX 2013 tracks; their aims …,ACM SIGIR Forum,2013,5
Report on INEX 2009,Thomas Beckers; Patrice Bellot; Gianluca Demartini; Ludovic Denoyer; Christopher M De Vries; Antoine Doucet; Khairun Nisa Fachry; Norbert Fuhr; Patrick Gallinari; Shlomo Geva; W-C Huang; Tereza Iofciu; Jaap Kamps; Gabriella Kazai; Marijn Koolen; Sangeetha Kutty; Monica Landoni; Miro Lehtonen; Véronique Moriceau; Richi Nayak; Ragnar Nordlie; Nils Pharo; Eric Sanjuan; Ralf Schenkel; Xavier Tannier; Martin Theobald; James A Thom; Andrew Trotman; Arjen P De Vries,Abstract INEX investigates focused retrieval from structured documents by providing largetest collections of structured documents; uniform evaluation measures; and a forum fororganizations to compare their results. This paper reports on the INEX 2009 evaluationcampaign; which consisted of a wide range of tracks: Ad hoc; Book; Efficiency; EntityRanking; Interactive; QA; Link the Wiki; and XML Mining. INEX in running entirely onvolunteer effort by the IR research community: anyone with an idea and some time to spend;can have a major impact.,ACM SIGIR Forum,2010,5
Overview of the INEX 2009 efficiency track,Ralf Schenkel; Martin Theobald,Abstract This paper presents an overview of the Efficiency Track that was run for the secondtime in 2009. This track is intended to provide a common forum for the evaluation of both theeffectiveness and efficiency of XML ranked retrieval approaches on real data and realqueries. The Efficiency Track significantly extends the Ad-Hoc Track by systematicallyinvestigating different types of queries and retrieval scenarios; such as classic ad-hocsearch; high-dimensional query expansion settings; and queries with a deeply nestedstructure (with all topics being available in both the NEXI-style CO and CAS formulations; aswell as in their XPath 2.0 Full-Text counterparts). The track received 68 runs submitted by 4participating groups using 5 different systems. The best systems achieved interactiveretrieval times for ad-hoc search; with a result quality comparable to the best runs in the …,International Workshop of the Initiative for the Evaluation of XML Retrieval,2009,5
Focused access to XML documents [electronic resource]: 6th International Workshop of the Initiative for the Evaluation of XML Retrieval; INEX 2007; Dagstuhl Castle;...,Norbert Fuhr,*,*,2008,5
Distributed set reachability,Sairam Gurajada; Martin Theobald,Abstract In this paper; we focus on the efficient and scalable processing of set-reachabilityqueries over a distributed; directed data graph. A" set-reachability query" is a generalizedform of a reachability query; in which we consider two sets S and T of source and targetvertices; respectively; to be given as the query. The result of a set-reachability query are allpairs of source and target vertices (s; t); with s--S and t# 8712; T; where s is reachable to t(denoted as S↝ T). In case the data graph is partitioned into multiple; edge-and vertex-disjoint subgraphs (eg; when distributed across multiple compute nodes in a cluster); werefer to the resulting set-reachability problem as" distributed set reachability". The key goal inprocessing a distributed set-reachability query over a partitioned data graph both efficientlyand in a scalable manner is (1) to avoid redundant computations within the local compute …,Proceedings of the 2016 International Conference on Management of Data,2016,4
Design and evaluation of an ir-benchmark for sparql queries with fulltext conditions,Arunav Mishra; Sairam Gurajada; Martin Theobald,Abstract In this paper; we describe our goals in introducing a new; annotated benchmarkcollection; with which we aim to bridge the gap between the fundamentally different aspectsthat are involved in querying both structured and unstructured data. This semantically richcollection; captured in a unified XML format; combines components (unstructured text;semistructured infoboxes; and category structure) from 3.1 Million Wikipedia articles withhighly structured RDF properties from both DBpedia and YAGO2. The new collection servesas the basis of the INEX 2012 Ad-hoc; Faceted Search; and Jeopardy retrieval tasks. With afocus on the new Jeopardy task; we particularly motivate the usage of the collection forquestion-answering (QA) style retrieval settings; which we also exemplify by introducing aset of 90 QA-style benchmark queries which come shipped in a SPARQL-based query …,Proceedings of the fifth workshop on Exploiting semantic annotations in information retrieval,2012,4
Overview of the INEX 2008 efficiency track,Martin Theobald; Ralf Schenkel,Abstract This paper presents an overview of the Efficiency Track that was newly introducedto INEX in 2008. The new INEX Efficiency Track is intended to provide a common forum forthe evaluation of both the effectiveness and efficiency of XML ranked retrieval approacheson real data and real queries. As opposed to the purely synthetic XMark or XBenchbenchmark settings that are still prevalent in efficiency-oriented XML retrieval tasks; theEfficiency Track continues the INEX tradition using a rich pool of manually assessedrelevance judgments for measuring retrieval effectiveness. Thus; one of the main goals is toattract more groups from the DB community to INEX; being able to study effectiveness/efficiency trade-offs in XML ranked retrieval for a broad audience from both the DB and IRcommunities. The Efficiency Track significantly extends the Ad-Hoc Track by …,International Workshop of the Initiative for the Evaluation of XML Retrieval,2008,4
Efficient querying and learning in probabilistic and temporal databases,Maximilian Dylla,Probabilistische Datenbanken können große Mengen an ungewissen Informationenspeichern; anfragen und verwalten. Diese Doktorarbeit treibt den Stand der Technik indiesem Gebiet auf drei Arten vorran: 1. Ein abgeschlossenes und vollständigesDatenmodell für temporale; probabilistische Datenbanken wird präsentiert. Anfragenwerden mittels Deduktionsregeln gestellt; welche logische Formeln induzieren; die sowohlZeit als auch Ungewissheit erfassen. 2. Ein Methode zur Berechnung der k Anwortenhöchster Wahrscheinlichkeit wird entwickelt. Sie basiert auf logischen Formeln erster Stufe;die Mengen an Antwortkandidaten repräsentieren. Beschränkungen der Wahrscheinlichkeitdieser Formeln ermöglichen das Kürzen von Antworten mit niedriger Wahrscheinlichkeit. 3.Das Problem des Lernens von Tupelwahrscheinlichkeiten für das Aktualisieren und …,*,2014,3
10 Years of Probabilistic Querying–What Next?,Martin Theobald; Luc De Raedt; Maximilian Dylla; Angelika Kimmig; Iris Miliaraki,Abstract Over the past decade; the two research areas of probabilistic databases andprobabilistic programming have intensively studied the problem of making structuredprobabilistic inference scalable; but—so far—both areas developed almost independently ofone another. While probabilistic databases have focused on describing tractable queryclasses based on the structure of query plans and data lineage; probabilistic programminghas contributed sophisticated inference techniques based on knowledge compilation andlifted (first-order) inference. Both fields have developed their own variants of—both exactand approximate—top-k algorithms for query evaluation; and both investigate queryoptimization techniques known from SQL; Datalog; and Prolog; which all calls for a moreintensive study of the commonalities and integration of the two fields. Moreover; we …,East European Conference on Advances in Databases and Information Systems,2013,3
D2R2: Disk-oriented deductive reasoning in a RISC-style RDF engine,Mohamed Yahya; Martin Theobald,Abstract Deductive reasoning lies in the expressive intersection of Datalog and DescriptionLogics. In this paper; we present the D2R2 engine; which implements deductive reasoningcapabilities based on the Query-Sub-Query (QSQR) algorithm on top of the disk-orientedRDF-3X engine. D2R2 aims to bridge the gap between rule-oriented (intensional) reasoningwith deduction rules and data-oriented (extensional) processing of large joins; over a set ofhighly tuned; disk-based index structures for large RDF collections. We present ageneralization of QSQR; which allows for dynamic sub-query scheduling and chaining ofextensional predicates into atomic join patterns—two key extensions for coupling QSQR witha disk-oriented storage backend. Experiments over a set of recursive queries and a verylarge knowledge base; consisting of 20 million RDF facts; as well as comparisons to disk …,*,2011,3
Explanations in Dialogue Systems through Uncertain RDF Knowledge Bases.,Daniel Sonntag; Martin Theobald,Abstract. We implemented a generic dialogue shell that can be configured for and applied todomain-specific dialogue applications. The dialogue system works robustly for a newdomain when the application backend can automatically infer previously unknownknowledge (facts) and provide explanations for the inference steps involved. For thispurpose; we employ URDF; a query engine for uncertain and potentially inconsistent RDFknowledge bases. URDF supports rule-based; first-order predicate logic as used in OWL-Lite and OWL-DL; with simple and effective top-down reasoning capabilities. Thismechanism also generates explanation graphs. These graphs can then be displayed in theGUI of the dialogue shell and help the user understand the underlying reasoning processes.We believe that proper explanations are a main factor for increasing the level of user trust …,ExaCt,2010,3
of Proceedings: Advances in Database Technology-EDBT 2006: 10th International Conference on Extending Database Technology,Ralf Schenkel; Martin Theobald,Abstract/Description: Relevance Feedback is an important way to enhance retrieval qualityby integrating relevance information provided by a user. In XML retrieval; feedback enginesusually generate an expanded query from the content of elements marked as relevant ornonrelevant. This approach that is inspired by text-based IR completely ignores thesemistructured nature of XML. This paper makes the important step from content-based tostructural feedback. It presents an integrated solution for expanding keyword queries withnew content; path; and document constraints. An extensible framework evaluates suchquery conditions with existing keyword-based XML search engines while allowing to easilyintegrate new dimensions of feedback. Extensive experiments with the established INEXbenchmark show the feasibility of our approach.,*,2006,3
Summary generation for temporal extractions,Yafang Wang; Zhaochun Ren; Martin Theobald; Maximilian Dylla; Gerard de Melo,Abstract Recent advances in knowledge harvesting have enabled us to collect largeamounts of facts about entities from Web sources. A good portion of these facts have atemporal scope that; for example; allows us to concisely capture a person's biography.However; raw sets of facts are not well suited for presentation to human end users. Thispaper develops a novel abstraction-based method to summarize a set of facts into natural-language sentences. Our method distills temporal knowledge from Web documents andgenerates a concise summary according to a particular user's interest; such as; for example;a soccer player's career. Our experiments are conducted on biography-style Wikipediapages; and the results demonstrate the good performance of our system in comparison toexisting text-summarization methods.,International Conference on Database and Expert Systems Applications,2016,2
TopX 2.0 at the INEX 2008 Efficiency Track,Martin Theobald; Mohammed AbuJarour; Ralf Schenkel,Abstract For the INEX Efficiency Track 2008; we were just on time to finish and evaluate ourbrand-new TopX 2.0 prototype. Complementing our long-running effort on efficient top-kquery processing on top of a relational back-end; we now switched to a compressed object-oriented storage for text-centric XML data with direct access to customized inverted files;along with a complete reimplementation of the engine in C++. Our INEX 2008 experimentsdemonstrate efficiency gains of up to a factor of 30 compared to the previous Java/JDBC-based TopX 1.0 implementation over a relational back-end. TopX 2.0 achieves overallruntimes of less than 51 seconds for the entire batch of 568 Efficiency Track topics in theircontent-and-structure (CAS) version and less than 29 seconds for the content-only (CO)version; respectively; using a top-15; focused (ie; non-overlapping) retrieval mode—an …,International Workshop of the Initiative for the Evaluation of XML Retrieval,2008,2
TopX–AdHoc and Feedback Tasks,Martin Theobald; Andreas Broschart; Ralf Schenkel; Silvana Solomon; Gerhard Weikum,TopX–AdHoc and Feedback Tasks Martin Theobald; Andreas Broschart; Ralf Schenkel; SilvanaSolomon; and Gerhard Weikum Max-Planck-Institut für Informatik Saarbrücken; Germanyhttp://www. mpi-inf. mpg. de/departments/d5/ {mtb; abrosch; schenkel; solomon; weikum}@mpi-inf. mpg. de Abstract. This paper describes the setup and results of our contribu- tions tothe INEX 2006 AdHoc and Feedback tasks. 1 System Overview TopX [10; 11] aims to bridgethe fields of database systems (DB) and informa- tion retrieval (IR). From a DB viewpoint; it providesan efficient algorithmic basis for top-k query processing over multidimensional datasets; rangingfrom structured data such as product catalogs (eg; bookstores; real estate; movies; etc.) to unstructuredtext documents (with keywords or stemmed terms defin- ing the feature space) and semistructuredXML data in between. From an IR viewpoint; TopX provides ranked retrieval based on a …,Center for Computer Science http://inex. is. informatik. uni-duisburg. de/2006,2006,2
IO-Top-k at TREC 2006: Terabyte Track,Holger Bast; Debapriyo Majumdar; Ralf Schenkel; Martin Theobald; Gerhard Weikum,Abstract This paper describes the setup and results of our contribution to the TREC 2006Terabyte Track. Our implementation was based on the algorithms proposed in [1]“IO-Top-k:Index-Access Optimized Top-K Query Processing; VLDB'06”; with a main focus on theefficiency track. 1.,Proceedings of the 15th Text REtrieval Conference (TREC 2006,2006,2
Query-driven on-the-fly knowledge base construction,Dat Ba Nguyen; Abdalghani Abujabal; Nam Khanh Tran; Martin Theobald; Gerhard Weikum,Abstract Today's openly available knowledge bases; such as DBpedia; Yago; Wikidata orFreebase; capture billions of facts about the world's entities. However; even the largestamong these (i) are still limited in up-to-date coverage of what happens in the real world;and (ii) miss out on many relevant predicates that precisely capture the wide variety ofrelationships among entities. To overcome both of these limitations; we propose a novelapproach to build on-the-fly knowledge bases in a query-driven manner. Our system; calledQKBfly; supports analysts and journalists as well as question answering on emerging topics;by dynamically acquiring relevant facts as timely and comprehensively as possible. QKBflyis based on a semantic-graph representation of sentences; by which we perform three key IEtasks; namely named-entity disambiguation; co-reference resolution and relation …,Proceedings of the VLDB Endowment,2017,1
J-REED: Joint Relation Extraction and Entity Disambiguation,Dat Ba Nguyen; Martin Theobald; Gerhard Weikum,ABSTRACT Information extraction (IE) from text sources can either be performed as Model-based IE (ie; by using a pre-specified domain of target entities and relations) or as Open IE(ie; with no particular assumptions about the target domain). While Model-based IE haslimited coverage; Open IE merely yields triples of surface phrases which are usually notdisambiguated into a canonical set of entities and relations. This paper presents J-REED: ajoint approach for entity disambiguation and relation extraction that is based on probabilisticgraphical models. J-REED merges ideas from both Model-based and Open IE by mappingsurface names to a background knowledge base; and by making surface relations as crispas possible.,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management; CIKM 2017; Singapore; November 06-10; 2017,2017,1
Distributed processing of generalized graph-pattern queries in SPARQL 1.1,Sairam Gurajada; Martin Theobald,Abstract: We propose an efficient and scalable architecture for processing generalizedgraph-pattern queries as they are specified by the current W3C recommendation of theSPARQL 1.1" Query Language" component. Specifically; the class of queries we considerconsists of sets of SPARQL triple patterns with labeled property paths. From a relationalperspective; this class resolves to conjunctive queries of relational joins with additionalgraph-reachability predicates. For the scalable; ie; distributed; processing of this kind ofqueries over very large RDF collections; we develop a suitable partitioning and indexingscheme; which allows us to shard the RDF triples over an entire cluster of compute nodesand to process an incoming SPARQL query over all of the relevant graph partitions (andthus compute nodes) in parallel. Unlike most prior works in this field; we specifically aim …,arXiv preprint arXiv:1609.05293,2016,1
SPAR-Key: Processing SPARQL-Fulltext Queries to Solve Jeopardy! Clues.,Arunav Mishra; Sairam Gurajada; Martin Theobald,The LOD track of INEX 2012 introduced the new Wikipedia-LODv1.1 collection that combinedhighly structured semantic data and unstructured (or semi-structured) textual data with the goalof improving IR-tasks (adhoch search task and faceted search task) and Question-Answeringtasks (Jeopardy task). The entity-centric collection comprised XML-ified documents; coinedWiki-XML documents; as the basic unit of data; where each documents corresponded to a Wikipediaentity; and combined semantic (RDF) data from DBpedia and YAGO2 Knowledge Bases; andtextual data from the Wikipedia article that describe the entity. The Jeopardy task; defined onthe Wikipedia-LODv1.1 collection as a part of the LOD track; evaluated retrieval techniques overa unique query benchmark of 90 queries; given in the new SPARQL-FT format. TheSPARQL-FT queries of the benchmark represented a translation of Jeopardy-style …,CLEF (Working Notes),2013,1
Running SPARQL-Fulltext Queries Inside a Relational DBMS,Arunav Mishra; Sairam Gurajada; Martin Theobald,Running SPARQL-Fulltext Queries Inside a Relational DBMS Arunav Mishra1; SairamGurajada1; and Martin Theobald1 Max Planck Institute for Informatics; Saarbrücken; GermanyAbstract. We describe the indexing; ranking; and query processing techniques we implementedin order to process a new kind of SPARQL-fulltext queries that were provided in the context ofthe INEX 2012 Jeopardy task. 1 Introduction The INEX 2012 Linked Data track provides a newdata collection that aims to com- bine the benefits of both text-oriented and structured retrievalssettings in one unified data collection. For the rapid development of a new query engine thatcould handle this particular combination of XML markup and RDF-styleresource/property-pairs; we de- cided to opt for a relational database system as storageback-end; which allows us to index the collection and to retrieve both the SPARQL-and …,Copyright cG2012 remains with the author/owner (s). The unreviewed pre-proceedings are collections of work submitted before the December workshops. They are not peer reviewed; are not quality controlled; and contain known errors in content and editing. The proceedings; published after the Workshop; is the authoritative reference for the work done at INEX.,2012,1
Efficient entity disambiguation via similarity hashing,Dat Ba Nguyen; Martin Theobald; Gerhard Weikum,Abstract The task of Named Entity Disambiguation (NED); which maps mentions ofambiguous names in natural language onto a set of known entities; has been an importantissue in many areas including machine translation and information extraction. Working witha huge amount of data (eg more than three million entities in Yago); some parts in an NEDsystem which estimate the probability of a mention matching an entity; the similarity betweena mention and an entity and the coherence among entity candidates for all mentionstogether might become bottlenecks. Thus; it is challenging for an interactive NED system toreach not only high accuracy but also efficiency. This thesis presents an efficient way ofdisambiguating named entities by similarity hashing. Our framework is integrated with AIDAwhich is an on-line tool for entity detection and disambiguation developed at Max-Planck …,*,2012,1
A Distributed In-Memory SPARQL Query Processor based on Message Passing,Arnab Kumar Dutta; Martin Theobald; Ralf Schenkel,Abstract Often it has been observed that querying over a large corpus of RDF datapossesses severe bottlenecks in terms of query response time or throughput. Most of theexisting systems which are currently available are localized in a single node. As analternative; query processing can achieve better performance results if we can introducesome parallelism into the process of evaluating the queries. This gains an edge oversequential query evaluation and has a significant enhancement over the response times. Inthis thesis work; we try to develop a baseline version of a distributed query processingsystem. This particular system architecture is distinctly different from other classic distributedRDF engines. Firstly; our system has an in-memory data management scheme unlike otherdisk based engines. Secondly; instead of employing an off-the-shelf distributed …,Universitat des Saarlandes Max-Planck-Institut fur Informatik,2012,1
Deriving a web-scale common sense fact knowledge base,Niket Tandon; Gerhard Weikum; Gerard de Melo; Martin Theobald,Abstract The fact that birds have feathers and ice is cold seems trivially true. Yet; mostmachine-readable sources of knowledge either lack such common sense facts entirely orhave only limited coverage. Prior work on automated knowledge base construction haslargely focused on relations between named entities and on taxonomic knowledge; whiledisregarding common sense properties. Extracting such structured data from text ischallenging; especially due to the scarcity of explicitly expressed knowledge. Even whenrelying on large document collections; patternbased information extraction approachestypically discover insufficient amounts of information. This thesis investigates harvestingmassive amounts of common sense knowledge using the textual knowledge of the entireWeb; yet staying away from the massive engineering efforts in procuring such a large …,*,2011,1
Extraction of Temporal Facts and Events from Wikipedia,Erdal Kuzey; Gerhard Weikum; Martin Theobald,Abstract In recent years; the great success of Wikipedia and the progress in informationextraction techniques led to automatic construction of large scale knowledge bases whichhave Subject-Predicate-Object style facts extracted from both semi-structured and naturallanguage text of Wikipedia articles. Those knowledge bases consist of millions of entities;relations about them and their semantic types. Unfortunately; most of the current knowledgebases focus on static facts and ignore their temporal dimension; although; the vast majorityof facts are evolving with time and are valid during a particular time period. In this thesis; weintroduce a complete information extraction framework which harvests temporal facts andevents from semi-structured and free text of Wikipedia articles to enrich a temporal ontology(T-YAGO). Furthermore; this thesis discusses methods for introducing a temporal …,Master's thesis; Saarland University,2011,1
Generating personalized destination suggestions for automotive navigation systems under uncertainty,Michael Feld; Martin Theobald; Christoph Stahl; Timm Meiser; Christian Müller,Abstract. Programming a car's navigation system manually takes time and is error-prone.When the address is not handy; a cumbersome search may start. Changing the destinationwhile driving is even more problematic. Given a modern car's role as an information hub; weargue that an intelligent system could in many cases infer the right destination or have itamong the top N suggestions. In this work; we propose a personalized navigation systemthat is built from three main ingredients: strong user models; knowledge source fusion; andreasoning under uncertainty. We focus on emails as one particular knowledge source;exploring the uncertainties involved when extracting empirical data of email appointments.,User Modeling; Adaptation and Personalization (UMAP),2011,1
Learning Soft Inference Rules in Large and Uncertain Knowledge Bases,Christina Teflioudi; Martin Theobald; Gerhard Weikum,Abstract Recent progress in information extraction has enabled us to create large semanticknowledge bases with millions of RDF facts extracted from the Web. Nevertheless; theresulting knowledge bases are still incomplete or might contain inconsistencies; eitherbecause of the heuristic nature of the extraction process; or due to the varying reliability ofthe Web sources from which they were collected. One possible way of resolving both issuesis to reinforce the knowledge base with deductive power by appending first-order logicalinference rules; which help to describe and to further constrain the domain with which theontology deals. In our work; we investigate learning these rules directly from the data usingInductive Logic Programming (ILP); a well known technique; which lies in the intersection ofmachine learning and logic. Although powerful; ILP is inherently expensive as there is a …,*,2011,1
Harvesting Knowledge from Web Data and Text,Hady Lauw; Ralf Schenkel; Fabian Suchanek; Martin Theobald; Gerhard Weikum,The Web bears the potential of being the world's greatest encyclopedic source; but we arefar from fully ex-ploiting this potential. Valuable scientific and cultural content is interspersedwith a huge amount of noisy; low-quality; unstructured text and media. The proliferation ofknowledge-sharing communities like Wikipedia and the advances in automated informationextraction from Web pages give rise to an unprecedented opportunity: Can wesystematically harvest facts from the Web and compile them into a comprehensive machine-readable knowledge base? Such a knowledge base would contain not only the world'sentities; but also their semantic properties; and their relationships with each other. Imagine a“Structured Wikipedia” that has the same scale and richness as Wikipedia itself; but offers aprecise and concise representation of knowledge; eg; in the RDF format. This would …,CIKM,2010,1
TopX 2.0 at the INEX 2008 Efﬁciency Track,Martin Theobald; Mohammed AbuJarour; Ralf Schenkel,Abstract. For the INEX Efﬁciency Track 2008; we were just on time to ﬁnish and evaluate ourbrand-new TopX 2.0 prototype. Complementing our longrunning effort on efﬁcient top-kquery processing on top of a relational back-end; we now switched to a compressed object-oriented storage for text-centric XML data with direct access to customized inverted ﬁles;along with a complete reimplementation of the engine in C++. Our INEX 2008 experimentsdemonstrate efﬁciency gains of up to a factor of 30 compared to the previous Java/JDBC-based TopX 1.0 implementation over a relational back-end. TopX 2.0 achieves overallruntimes of less than 51 seconds for the entire batch of 568 Efﬁciency Track topics in theircontent-and-structure (CAS) version and less than 29 seconds for the content-only (CO)version; respectively; using a top-15; focused (ie; nonoverlapping) retrieval mode—an …,Advances in Focused Retrieval: 7th International Workshop of the Initiative for the Evaluation of XML Retrieval; INEX 2008; Dagstuhl Castle; Germany; December 15-18; 2009. Revised and Selected Papers,2009,1
Ranked XML Processing,Amélie Marian; Ralf Schenkel; Martin Theobald,Many organizations; eg; government statistical offices and search engine companies; collectpotentially sensitive information regarding individuals either to publish this data for research;or in return for useful services. While some data collection organizations; like the census; arelegally required not to breach the privacy of the individuals; other data collectionorganizations may not be trusted to uphold privacy. Hence; if U denotes the original datacontaining sensitive information about a set of individuals; then an untrusted data collectoror researcher should only have access to an anonymized version of the data; U*; that doesnot disclose the sensitive information about the individuals. A randomized anonymizationalgorithm R is said to be a privacy preserving randomization method if for every table T; andfor every output T*= R (T); the privacy of all the sensitive information of each individual in …,*,2009,1
TopX-Efficient and Versatile Top-k Query Process-ing for Text; Semistructured; and Structured Data.,Martin Theobald; Ralf Schenkel; Gerhard Weikum,Abstract: This paper presents a comprehensive overview of the TopX search engine; anextensive framework for unified indexing and querying large collections of unstructured;semistructured; and structured data. Residing at the very synapse of database (DB)engineering and information retrieval (IR); it integrates efficient scheduling algorithms for top-k-style ranked retrieval with powerful scoring models; as well as dynamic and self-throttlingquery expansion facilities.,BTW,2007,1
Efficient Top-k Query Processing for Text; Semistructured; and Structured Data,Martin Theobald,TopX is a top-k retrieval engine for text and XML data. Unlike Boolean engines; it stopsquery processing as soon as it can safely determine the k top-ranked result objectsaccording to a monotonous score aggregation function with respect to a multidimensionalquery. The main contributions of the thesis unfold into four main points; confirmed byprevious publications at international conferences or workshops:\beginitemize\item Top-kquery processing with probabilistic guarantees.\item Index-access optimized top-k queryprocessing.\item Dynamic and self-tuning; incremental query expansion for top-k queryprocessing.\item Efficient support for ranked XML retrieval and full-text search.\enditemizeOur experiments demonstrate the viability and improved efficiency of our approachcompared to existing related work for a broad variety of retrieval scenarios.,*,2006,1
Conceptual Modeling-ER 2004: 23rd International Conference on Conceptual Modeling; Shanghai; China; November 8-12; 2004. Proceedings,Paolo Atzeni; Wesley Chu; Hongjun Lu; Shuigeng Zhou; Tok Wang Ling,The 23rd International Conference on Conceptual Modeling (ER 2004) was held inShanghai; China; November 8–12; 2004. Conceptual modeling is a fundamental techniqueused in analysis and design as a real-world abstraction and as the basis for communicationbetween technology experts and their clients and users. It has become a fundamentalmechanism for understanding and representing organizations; including new e-worlds; andthe information systems that support them. The International Conference on ConceptualModeling provides a major forum for presenting and discussing current research andapplications in which conceptual modeling is the major emphasis. Since the first edition in1979; the ER conference has evolved into the most prestigious one in the areas ofconceptual modeling research and applications. Its purpose is to identify challenging …,*,2005,1
From focused crawling to expert information: An application framework for web exploration and portal generation,Jens Graupmann; Sergej Sizov; Martin Theobald,Focused crawling is a relatively new; promising approach to improving the recall of expertsearch on the Web. It typically starts from a user-or communityspecific tree of topics alongwith a few training documents for each tree node; and then crawls the Web with focus onthese topics of interest. This process can efficiently build a theme-specific; hierarchicaldirectory whose nodes are populated with relevant high-quality documents for expert Websearch. The BINGO! focused crawler implements an approach that aims to overcome thelimitations of the initial training data. To this end; BINGO! identifies; among the crawled andpositively classified documents of a topic; characteristic archetypes (good authorities asdetermined by Kleinberg's HITS algorithm; and documents classified with high confidenceusing a linear SVM) and uses them for periodically retraining the classifier; this way the …,VLDB 2003,2003,1
BINGO! Ein thematisch fokussierender Crawler zur Generierung personalisierter Ontologien,Martin Theobald; Stefan Siersdorfer; Sergej Sizov,Zusammenfassung Fokussierendes Crawling ist ein viel versprechender Ansatz zurVerbesserung der Ausbeute einer Expertensuche über einem spezifischen Themenbereichdes Webs. Dieses Verfahren beinhaltet die automatische Klassifikation von Dokumenten ineine benutzerspezifische Hierarchie von Themen; die wir auch als Ontologie bezeichnen.Die Qualität der Trainingsdaten des Klassifikators ist der kritischste Punkt für die Effektivitäteines fokussierenden Crawlers. Der BINGO!-Ansatz versucht die Grenzen einerTrainingsbasis mit nur wenigen intellektuell kategorisierten Dokumenten zu überwinden undin einer automatisierten Wachstumsphase selbständig eine breite Trainingsbasis durch dieIdentifikation themenspezifischer” Archetypen” zu generieren. Die anschließendeErntephase vervollständigt dann die Ontologie nach iterativem Neutrainieren des …,Datenbanken und Informationssysteme Universität des Saarlandes WS,2001,1
Proceedings-2017 ILILAS Distinguished Lectures,Pascal Bouvry; Raymond Bisdorff; Christoph Schommer; Ulrich Sorger; Martin Theobald; Leon van der Torre,The ILIAS Research Lab (see ilias.uni.lu) is part of the Department of Computer Science of theUniver- sity of Luxembourg. ILIAS research and teaching activities are concerned with Artificialintelligence in general; the theoretical foundations and the algorithmic realisation of informationprocessing; eg; Ma- chine Learning; Decision Support; Big Data Management; DataScience; and Reasoning in complex and dynamic environments given limited resources andincomplete or uncertain information. ILIAS fosters on fundamental and applied research andlives a cross-disciplinary thinking by common research projects with other faculties and departmentsof the University as well as with all interdisciplinary Centres (LCSB; SnT; and C2DH).Moreover; the High-Performance-Centre (HPC) is headed by Prof Bouvry … The ILIAS DistinguishedLectures are an initiative by the ILIAS Research Lab. The lectures foster on stimulating …,*,2018,*
GCAI 2017. 3rd Global Conference on Artificial Intelligence,Christoph Benzmüller; Christine Lisetti; Martin Theobald,*,*,2017,*
GCAI 2017: 3rd Global Conference on Artificial Intelligence; Miami; FL; USA; 18-22 October 2017,Christoph Benzmüller; Christine Lisetti; Martin Theobald,*,*,2017,*
Concept Recognition in European and National Law,Rohan Nanda; Giovanni Siragusa; Martin Theobald; Guido Boella; Livio Robaldo; Francesco Costamagna,Abstract. This paper presents a concept recognition system for European and nationallegislation. Current named entity recognition (NER) systems do not focus on identifyingconcepts which are essential for interpretation and harmonization of European and nationallaw. We utilized the IATE (Inter-Active Terminology for Europe) vocabulary; a state-of-the-artnamed entity recognition system and Wikipedia to generate an annotated corpus for conceptrecognition. We applied conditional random fields (CRF) to identify concepts on a corpus ofEuropean directives and Statutory Instruments (SIs) of the United Kingdom. The CRF-basedconcept recognition system achieved an F1 score of 0.71 over the combined corpus ofdirectives and SIs. Our results indicate the usability of a CRF-based learning system overdictionary tagging and state-of-the-art methods.,Frontiers in Artificial Intelligence and Applications; Volume 302: Legal Knowledge and Information Systems,2017,*
Learning Tuple Probabilities,Maximilian Dylla; Martin Theobald,Abstract: Learning the parameters of complex probabilistic-relational models from labeledtraining data is a standard technique in machine learning; which has been intensivelystudied in the subfield of Statistical Relational Learning (SRL); but---so far---this is still anunder-investigated topic in the context of Probabilistic Databases (PDBs). In this paper; wefocus on learning the probability values of base tuples in a PDB from labeled lineageformulas. The resulting learning problem can be viewed as the inverse problem toconfidence computations in PDBs: given a set of labeled query answers; learn theprobability values of the base tuples; such that the marginal probabilities of the queryanswers again yield in the assigned probability labels. We analyze the learning problemfrom a theoretical perspective; cast it into an optimization problem; and provide an …,arXiv preprint arXiv:1609.05103,2016,*
Authors’ Addresses,Maximilian Dylla; E Campus; Martin Theobald,Abstract Learning the parameters of complex probabilistic-relational models from labeledtraining data is a standard technique in machine learning; which has been intensivelystudied in the subfield of Statistical Relational Learning (SRL); but—so far—this is still anunder-investigated topic in the context of Probabilistic Databases (PDBs). In this paper; wefocus on learning the probability values of base tuples in a PDB from query answers; thelatter of which are represented as labeled lineage formulas. Specifically; we consider labelsin the form of pairs; each consisting of a Boolean lineage formula and a marginal probabilitythat comes attached to the corresponding query answer. The resulting learning problem canbe viewed as the inverse problem to confidence computations in PDBs: given a set oflabeled query answers; learn the probability values of the base tuples; such that the …,*,2014,*
Learning Rules With Categorical Attributes from Linked Data Sources,André de Oliveira Melo,Author: de Oliveira Melo; André; Genre: Thesis; Published in Print: 2013; Title: LearningRules With Categorical Attributes from Linked Data Sources.,*,2013,*
Design and Evaluation of an Incremental Ranking Model for Large-Scale Data Stored in HBase,Patrice Bellot; Antoine Doucet; Shlomo Geva; Sairam Gurajada; Jaap Kamps; Gabriella Kazai; Marijn Koolen; Arunav Mishra; Veronique Moriceau; Josiane Mothe; Michael Preminger; Eric SanJuan; Ralf Schenkel; Xavier Tannier; Martin Theobald; Matthew Trappett; Qiuyue Wang,*,*,2013,*
Design and Evaluation of an IR-Benchmark for SPARQL Fulltext Queries,Arunav Mishra; Martin Theobald; Gerhard Weikum,Abstract In this thesis; we design a new IR-benchmark that aims to bridge the prevailing gapbetween traditional keyword-based retrieval techniques and semantic web-based retrievaltechniques. We present a unique; entity-centric data collection; coined Wikipedia-LOD; thataims to combine the benefits of both text-oriented and structured retrieval settings. Thiscollection combines RDF data from DBpedia and YAGO2 structured Knowledge Bases(KBs); and textual data from the contents Wikipedia articles into XML-ified documents; calledthe Wiki-XML documents; corresponding to every Wikipedia entity. To evaluate such acollection; we introduce a new query format; called SPARQL-fulltext (SPARQL-FT) queries.We design the SPARQL-FT query format by extending the W3C standard SPARQL withadditional FTContains operator that constraints an entity by a set of keywords …,*,2013,*
Proceedings of the first International Workshop On Open Data; WOD-2012,Guillaume Raschia; Martin Theobald; Ioana Manolescu,Abstract WOD-2012 aims at facilitating new trends and ideas from a broad range of topicsconcerned within the widely-spread Open Data movement; from the viewpoint of computerscience research. While being most commonly known from the recent Linked Open Datamovement; the concept of publishing data explicitly as Open Data has meanwhiledeveloped many variants and facets that go beyond publishing large and highly structuredRDF/S repositories. Open Data comprises text and semi-structured data; but also open multi-modal contents; including music; images; and videos. With the increasing amount of datathat is published by governments (see; eg; data. gov; data. gov. uk or data. gouv. fr); byinternational organizations (data. worldbank. org or data. undp. org) and by scientificcommunities (tdar. org; cds. u-strasbg. fr; GenBank; IRIS or KNB) explicitly under an Open …,arXiv preprint arXiv:1204.3726,2012,*
Towards an Architecture for Open-Domain Information Extraction: Integrated Extraction; Clustering; and Reasoning with Patterns,Artem Boldyrev; Martin Theobald; Gerhard Weikum,Abstract In this thesis; we present an integrated approach for automatic knowledgeextraction from unstructured sources. We combine occurrences extraction; pattern clustering;and probabilistic reasoning in a single pipeline. The occurrence extraction step subsumesstatistical pattern mining; which is widely used in information extraction. Moreover; in thisstep; a pattern lifting and an entity disambiguation are performed. The pattern lifting replacessome predefining parts of speech by their word classes. For pattern clustering we use matrixfactorization and facts from a knowledge base such as YAGO. The distributed stochasticgradient descent algorithm for matrix factorization enables large scale information extraction.We use the output of the matrix factorization to find a high quality mapping from patterns torelations. In the probabilistic reasoning step; we introduce new rules as follows. A pattern …,*,2012,*
Proceedings of the First International Workshop On Open Data,Guillaume Raschia; Martin Theobald; Ioana Manolescu,Herausgeber: Raschia; Guillaume et al.; Genre: Konferenzband; Online veröffentlicht: 2012;Titel: Proceedings of the First International Workshop On Open Data.,WOD 2012,2012,*
Lineage Enabled Query Answering in Uncertain Knowledge Bases,Javeria Iqbal; Martin Theobald; Ing Sebastian Michel,Abstract We present a unified framework for query answering over uncertain RDFknowledge bases. Specifically; our proposed design combines correlated base facts with aquery driven; top down deductive grounding phase of first-order logic formulas (ie; Hornrules) followed by a probabilistic inference phase. In addition to static input correlationsamong base facts; we employ the lineage structure obtained from processing the rulesduring grounding phase; in order to trace the logical dependencies of query answers (ie;derived facts) back to the base facts. Thus; correlations (or more precisely: dependencies)among facts in a knowledge base may arise from two sources: 1) static input dependenciesobtained from real-world observations; and 2) dynamic dependencies induced at query timeby the rule-based lineage structure of the query answer. Our implementation employs …,*,2011,*
Context-Aware Timeline for Entity Exploration,Anh Tuan Tran; Gerhard Weikum; Nicoleta Preda; Shady Elbassuoni; Martin Theobald,Abstract With millions of articles in multiple languages; Wikipedia has become the de-factosource of reference on the Internet today. Each article on Wikipedia contains encyclopedicinformation about various topics (people; events; inventions; etc.) and implicitly representsan entity. Extracting the most important facts about such entity will help users to find desiredinformation more quickly and effectively. However; this task is challenging due to theincomplete and noisy nature of Wikipedia articles. This calls for a mechanism to detect andsummarize the most important information about an entity on Wikipedia. This thesisproposes and implements CATE (Context-Aware Timeline for Entity Exploration); aframework that utilizes Wikipedia to summarize and visualize the important aspects ofentities in a timeline fashion. Such a system will help users to draw quickly an informative …,*,2011,*
of Proceedings: Rule-Based Modeling and Computing on the Semantic Web: 5th International Symposium; RuleML 2011-America,Mohamed Yahya; Martin Theobald,*,*,2011,*
Semantic Knowledge Bases from Web Sources,Fabian Suchanek; Martin Theobald; Gerhard Weikum; Hady Lauw; Ralf Schenkel,The Web bears the potential of being the world's greatest encyclopedic source; but we arefar from fully ex-ploiting this potential. Valuable scientific and cultural content is interspersedwith a huge amount of noisy; low-quality; unstructured text and media. The proliferation ofknowledge-sharing communities like Wikipedia and the advances in automated informationextraction from Web pages give rise to an unprecedented opportunity: Can wesystematically harvest facts from the Web and compile them into a comprehensive machine-readable knowledge base? Such a knowledge base would contain not only the world'sentities; but also their semantic properties; and their relationships with each other. Imagine a“Structured Wikipedia” that has the same scale and richness as Wikipedia itself; but offers aprecise and concise representation of knowledge; eg; in the RDF format. This would …,IJCAI,2011,*
of Proceedings: Reasoning Web: Semantic Technologies for the Web of Data; 7th International Summer School 2011,Katja Hose; Ralf Schenkel; Martin Theobald; Gerhard Weikum,*,*,2011,*
Accelerating Rule-Based Reasoning in Disk-Resident RDF Knowledge Bases,Mohamed Yahya; Martin Theobald; Gerhard Weikum,Abstract Collections of tens of millions of automatically extracted facts represented using thesubject-predicate-object RDF model are available for several domains. As big as thesecollections are; they are unable to capture all information about a domain; simply becausethe sources from which they were extracted are incomplete. This can be tackled by creatingknowledge bases where facts are enforced with rules showing how new facts can begenerated from existing ones and constraints which must hold in the relevant domain.Querying such knowledge bases is expensive for two main reasons. First; data is diskresident; which makes access to it slow. Secondly; rule definitions can be recursive; whichrequires special query evaluation techniques and renders traditional cost-based queryoptimization and join-ordering techniques less effective. This thesis presents the …,*,2010,*
of Proceedings: Focused Retrieval and Evaluation: 8th International Workshop of the Initiative for the Evaluation of XML Retrieval; INEX 2009,Andreas Broschart; Ralf Schenkel,Document title: Index Tuning for Efficient Proximity-Enhanced Query Processing Authors: Broschart;Andreas; Schenkel; Ralf Document type: Conference-Paper Language: English Audience: ExpertsOnly Title of Series: Lecture Notes in Computer Science External Publication Status: publishedTitle of Proceedings: Focused Retrieval and Evaluation : 8th International Workshop of the Initiativefor the Evaluation of XML Retrieval; INEX 2009 Place of Conference/Meeting: Brisbane; AustraliaFull Name(s) of Editor(s) of Proceedings: Geva; Shlomo; Kamps; Jaap; Trotman; Andrew Placeof Publication: Berlin Volume (in Journal): 6203 Last Change of the Resource (YYYY-MM-DD):2011-02-18 (Start) Date of Conference/Meeting (YYYY-MM-DD): 2009-12-07 Date of Publication(YYYY-MM-DD): 2010 End Date of Conference/Meeting (YYYY-MM-DD): 2009-12-09 IntendedEducational Use: No Publisher: Springer …,*,2010,*
of (Report) Series: Technical Report/Stanford University,Anish Das Sarma; Martin Theobald; Jennifer Widom,*,*,2010,*
Working Group: Classification; Representation and Modeling.,S Das; C Koch; B König-Ries; Ander de Keijzer; V Markl; A Deshpande; M van Keulen; PJ Haas; IF Ilyas; T Neumann; D Olteanu; M Theobald; V Vassalos,KNAW Narcis. Back to search results. Publication Working Group: Classification;Representation and Modeling. (2009). Pagina-navigatie: Main …,*,2009,*
of Proceedings: INEX 2009 Workshop Preproceedings,Martin Theobald; Ablimit Aji; Ralf Schenkel,Abstract/Description: This paper presents the results of our INEX 2009 Ad-hoc and Efficiencytrack experiments. While our scoring model remained almost unchanged in comparison toprevious years; we focused on a complete redesign of our XML indexing component withrespect to the increased need for scalability that came with the new 2009 INEX Wikipediacollection; which is about 10 times larger than the previous INEX collection. TopX nowsupports a CAS-specific distributed index structure; with a completely {\em parallel}execution of all indexing steps; including parsing; sampling of term statistics for our element-specific BM25 ranking model; as well as sorting and compressing the index lists for our finalinverted block-index. Overall; TopX ranked among the top 3 systems in both the Ad-hoc andEfficiency tracks; with a maximum value of 0.61 for iP [0.01] and 0.29 for MAiP in focused …,*,2009,*
TopX 2.0 at the INEX 2008 Efficiency Track: A (Very) Fast Object-Store for Top-k-Style XML Full-Text Search,M Theobald; M AbuJarour; R Schenkel,*,LECTURE NOTES IN COMPUTER SCIENCE,2009,*
08421 Working Group: Classification; Representation and Modeling,Anish Das Sarma; Ander de Keijzer; Amol Deshpande; Peter J Haas; Ihab F Ilyas; Christoph Koch; Thomas Neumann; Dan Olteanu; Martin Theobald; Vasilis Vassalos,Abstract This report briefly summarizes the discussions carried out in the working group onclassification; representation and modeling of uncertain data. The discussion was dividedinto two subgroups: the first subgroup studied how different representation and modelingalternatives currently proposed can fit in a bigger picture of theory and technologyinteraction; while the second subgroup focused on contrasting current systemimplementations and the reasons behind such diverse class of available prototypes. Wesummarize the findings of these two groups and the future steps suggested by groupmembers.,Dagstuhl Seminar Proceedings,2009,*
Integrated DB&IR Semi-Structured Text Retrieval,Ralf Schenkel; Martin Theobald,An example of index oriented towards human navigation is the web directory. In such anindex; links to sites are organized into hierarchical categories; according to the sites'contents. In web directories; normally the tasks of collecting and categorizing pages arecarried out under supervision of human editors. An example of index not oriented to humansis a hidden list of metadata. Metadata are data about data. As a mean of assisting a searchengine to locate content or an information entity; they can be used to describe that content orentity. While not visible to humans; this information can provide contextual clues to automaticalgorithms used by search engines.,*,2009,*
Efficient phrase matching and proximity-based ranking for XML full-text search.,Yuliya Akkuzhyna; Gerhard Weikum; Martin Theobald,Abstract For a significant amount of queries posed to search engines; the proximity of thequeried terms is no less important then their frequency in the document. This work aims toadapt TopX; a full-text top-k search engine over semi-structured data developed at the Max-Planck Institute for Informatics; to proximity-aware indexing and retrieval. The existing indexstructure was extended and implemented for phrase and proximity search with regard tosemi-structured data. The integration of proximity search into top-k algorithm can potentiallybreak monotonic score aggregation which is necessary for early pruning of resultcandidates. The challenge of this work is to keep the top-k style processing of the queries; ieuse the benefit of early termination. The changes in behavior of ranking after proximity-aware scoring was studied and experimentally evaluated. For evaluation we use the …,*,2009,*
08421 Working Group: Lineage/Provenance,Anish Das Sarma; Amol Deshpande; Thomas Hubauer; Ihab F Ilyas; Birgitta König-Ries; Matthias Renz; Martin Theobald,Abstract The following summary tries to capture a collection of state-of-the-art techniquesand challenges for future work on lineage management in uncertain and probabilisticdatabases that we discussed in our working group. It was one half of a larger committee thatwe had initially formed; which then got split into two groups---one focusing on lineage as ameans of explanation of data; and one focusing more on lineage usage in probabilisticdatabases (see also the" Explanation" working group report for more details on the firstsubgroup).,Dagstuhl Seminar Proceedings,2009,*
08421 Working Group: Classification; Representation and Modeling.,Anish Das Sarma; Ander de Keijzer; Amol Deshpande; Peter J Haas; Ihab F Ilyas; Christoph Koch; Thomas Neumann; Dan Olteanu; Martin Theobald; Vasilis Vassalos,*,Uncertainty Management in Information Systems,2008,*
Working Group Report: Lineage/Provenance,Anish Das Sarma; Amol Deshpande; Thomas Hubauer; Ihab F Ilyas; Birgitta König-Ries; Matthias Renz; Martin Theobald,The following summary tries to capture a collection of state-of-the-art techniques andchallenges for future work on lineage management in uncertain and probabilistic databasesthat we discussed in our working group. It was one half of a larger committee that we hadinitially formed; which then got split into two groups---one focusing on lineage as a means ofexplanation of data; and one focusing more on lineage usage in probabilistic databases(see also the" Explanation" working group report for more details on the first subgroup).,*,2008,*
Data Modifications and Versioning in Trio,Anish Das Sarma; Martin Theobald; Jennifer Widom,*,*,2008,*
of Proceedings: Preproceedings of INEX 2007,Andreas Broschart; Ralf Schenkel; Martin Theobald; Gerhard Weikum,Abstract/Description: This paper describes the setup and results of the Max-Planck-Institut f{\" u} r Informatik's contributions for the INEX 2007 AdHoc Track task. The runs wereproduced with TopX; a search engine for ranked retrieval of XML data that supports aprobabilistic scoring model for full-text content conditions and tag-term combinations; pathconditions as exact or relaxable constraints; and ontology-based relaxation of terms and tagnames.,*,2007,*
of Book: Dynamics of Search Engines: An Introduction,Jens Graupmann; Michael Biwer; Christian Zimmer; Patrick Zimmer; Matthias Bender; Martin Theobald; Gerhard Weikum,*,*,2007,*
of Proceedings: String Processing and Information Retrieval: 14th International Symposium; SPIRE 2007,Ralf Schenkel; Andreas Broschart; Seungwon Hwang; Martin Theobald; Gerhard Weikum,Abstract/Description: In addition to purely occurrence-based relevance models; termproximity has been frequently used to enhance retrieval quality of keyword-oriented retrievalsystems. While there have been approaches on effective scoring functions that incorporateproximity; there has not been much work on algorithms or access methods for their efficientevaluation. This paper presents an efficient evaluation framework including a proximityscoring function integrated within a top-k query engine for text retrieval. We proposeprecomputed and materialized index structures that boost performance. The increasedretrieval effectiveness and efficiency of our framework are demonstrated through extensiveexperiments on a very large text benchmark collection. In combination with static indexpruning for the proximity lists; our algorithm achieves an improvement of two orders of …,*,2007,*
Efficient and Versatile Top-k Query Processing for Text; Structured; and Semistructured Data,Martin Theobald,Abstract TopX is a top-k retrieval engine for text and XML data. Unlike Boolean engines; itstops query processing as soon as it can safely determine the k top-ranked result objectsaccording to a monotonous score aggregation function with respect to a multidimensionalquery. The main contributions of the thesis unfold into four main points; confirmed byprevious publications at international conferences or workshops:,*,2006,*
of Proceedings: Advances in XML Information Retrieval and Evaluation; 4th International Workshop of the Initiative for the Evaluation of XML Retrieval; INEX 2005,Ralf Schenkel; Martin Theobald,*,*,2006,*
of Proceedings: Advances in Information Retrieval; 28th European Conference on IR Research; ECIR 2006,Ralf Schenkel; Martin Theobald,Abstract/Description: Keyword-based queries are an important means to retrieve informationfrom XML collections with unknown or complex schemas. Relevance Feedback integratesrelevance information provided by a user to enhance retrieval quality. For keyword-basedXML queries; feedback engines usually generate an expanded keyword query from thecontent of elements marked as relevant or nonrelevant. This approach that is inspired by text-based IR completely ignores the semistructured nature of XML. This paper makes theimportant step from pure content-based to structural feedback. It presents a framework thatexpands a keyword query into a full-fledged content-and-structure query. Extensiveexperiments with the established INEX benchmark and our TopX search engine show thefeasibility of our approach.,*,2006,*
TopX: Efficient Top-k Query Processing for Text; Semistructured; and Structured Data,Martin Theobald; Gerhard Weikum; Norbert Fuhr,Zusammenfassung TopX is a top-$ k $ retrieval engine for text and XML data. UnlikeBoolean engines; it stops query processing as soon as it can safely determine the $ k $ top-ranked result objects according to a monotonous score aggregation function with respect toa multidimensional query. The main contributions of the thesis unfold into four main points;confirmed by previous publications at international conferences or workshops:\begin{itemize}\item Top-$ k $ query processing with probabilistic guarantees.\item Index-accessoptimized top-$ k $ query processing.\item Dynamic and self-tuning; incremental queryexpansion for top-$ k $ query processing.\item Efficient support for ranked XML retrieval andfull-text search.\end {itemize} Our experiments demonstrate the viability and improvedefficiency of our approach compared to existing related work for a broad variety of …,*,2005,*
of Proceedings: Knowledge discovery in databases: PKDD 2005: 9th European Conference on Principles and Practice of Knowledge Discovery in Databases,Dimitrios Mavroeidis; George Tsatsaronis; Michalis Vazirgiannis; Martin Theobald; Gerhard Weikum,Abstract/Description: The introduction of hierarchical thesauri (HT) that contain significantsemantic information; has led researchers to investigate their potential for improvingperformance of the text classification task; extending the traditional “bag of words”representation; incorporating syntactic and semantic relationships among words. In thispaper we address this problem by proposing a Word Sense Disambiguation (WSD)approach based on the intuition that word proximity in the document implies proximity also inthe HT graph. We argue that the high precision exhibited by our WSD algorithm in varioushumanly-disambiguated benchmark datasets; is appropriate for the classification task.Moreover; we define a semantic kernel; based on the general concept of GVSM kernels; thatcaptures the semantic relations contained in the hierarchical thesaurus. Finally; we …,*,2005,*
of Proceedings: Conceptual modeling; ER 2004: 23rd International Conference on Conceptual Modeling,Gerhard Weikum; Jens Graupmann; Ralf Schenkel; Martin Theobald,Abstract/Description: The envisioned Semantic Web aims to provide richly annotated andexplicitly structured Web pages in XML; RDF; or description logics; based upon underlyingontologies and thesauri. Ideally; this should enable a wealth of query processing andsemantic reasoning capabilities using XQuery and logical inference engines. However; webelieve that the diversity and uncertainty of terminologies and schema-like annotations willmake precise querying on a Web scale extremely elusive if not hopeless; and the sameargument holds for large-scale dynamic federations of Deep Web sources. Therefore;ontology-based reasoning and querying needs to be enhanced by statistical means; leadingto relevanceranked lists as query results. This paper presents steps towards such a"statistically semantic" Web and outlines technical challenges. We discuss how statistically …,*,2004,*
of Proceedings: Proceedings 2004 VLDB Conference: The 30th International Conference on Very Large Databases (VLDB),Martin Theobald; Gerhard Weikum; Ralf Schenkel,Abstract/Description: Top-k queries based on ranking elements of multidimensional datasetsare a fundamental building block for many kinds of information discovery. The best knowngeneral-purpose algo-rithm for evaluating top-k queries is Fagin's threshold algorithm (TA).Since the user's goal behind top-k queries is to identify one or a few relevant and novel dataitems; it is intriguing to use approximative variants of TA to reduce run-time costs. This paperintroduces a family of approximative top-k algorithms based on probabilistic arguments.When scanning index lists of the underlying multidimensional data space in descendingorder of local scores; various forms of convolution and derived bounds are employed topredict when it is safe; with high probability; to drop candidate items and to prune the indexscans. The precision and the efficiency of the developed methods are experimentally …,*,2004,*
of Proceedings: Proceedings of the 29th International Conference on Very Large Data Bases (VLDB-03),Jens Graupmann; Sergej Sizov; Martin Theobald,Document title: From Focused Crawling to Expert Information: an Application Framework for WebExploration and Portal Generation Authors: Graupmann; Jens; Sizov; Sergej; Theobald; MartinEditors: Freytag; Johann Christoph; Lockemann; Peter C.; Abiteboul; Serge; Carey; Michael J.;Selinger; Patricia G.; Heuer; Andreas Document type: Conference-Paper Language: EnglishAudience: Experts Only External Publication Status: published Title of Proceedings: Proceedingsof the 29th International Conference on Very Large Data Bases (VLDB-03) Place ofConference/Meeting: Berlin Place of Publication: San Francisco; USA Last Change of theResource (YYYY-MM-DD): 2004-07-05 (Start) Date of Conference/Meeting (YYYY-MM-DD):2003-09-09 Date of Publication (YYYY-MM-DD): 2003 Review Status: not specified IntendedEducational Use: No Publisher: Morgan Kaufmann Communicated by: Gerhard Weikum …,*,2003,*
BINGO! Ein fokussierender Crawler zur Generierung personalisierter Ontologien,Martin Theobald; Stefan Siersdorfer; Sergej Sizov; Informationssysteme Prof Dr-Ing G Weikum,Page 1. BINGO! Ein fokussierender Crawler zur Generierung personalisierter Ontologien MartinTheobald Stefan Siersdorfer; Sergej Sizov Universität des Saarlandes Lehrstuhl für Datenbankenund Informationssysteme Prof. Dr.-Ing. G. Weikum 2. Oktober 2002 Page 2. FokussierendesCrawling zur Ontologiegenerierung ∎ Aufbau und Erweiterung benutzerdefinierter Ontologienüber spezifischen Themenhierarchien („Yahoo-Style“) ∎ Automatisches Aktualisieren und Filternhierarchischer Themenstrukturen unter Ausnutzung unterschiedlicher Relevanzkriterien(SVM-Konfidenz; Autorität; Cosinus-Maß) ∎ Automatisierte Bearbeitung von Expertenqueriesbzw. Vervollständigung vorhandener Ergebnismengen („Nadel im Heuhaufen“) BINGO!„Bookmark-Induced Gathering of !nformation“ ROOT Mountainbiking Garda-See OTHERSVirtuelles Bookmark-Dokument mit spezifischen Keywords …,*,2002,*
Program Committee Workshop on Management of Uncertain Data,Nilesh Dalvi; Alex Dekhtyar; Maarten Fokkinga; Ander de Keijzer; Maurice van Keulen; Thomas Lukasiewicz; Gabriella Pasi; Sunil Prabhakar; Martin Theobald; Guy De Tré; Jef Wijsen; Hainaut Vladimir Zadorozhny; Carlo Batini; IRISA Laure Berti; France Tiziana Catarci; Ahmed K Elmagarmid; Suzanne Embury; Alvaro Fernandes; Helena Galhardas; Michael Gertz; Raghav Kaushik; Chen Li; Andrea Maurino; Felix Naumann,The ability to detect and correct errors in the data; and more broadly to develop techniquesfor data quality assessment; has long been recognized as critical to the functionality of alarge number of applications; in areas ranging from business management to data-intensivescience. While many of the technical issues associated with data quality have been knownfor quite some time; novel applications still pose original challenges; while advances in datamanagement technology offer ideas for novel approaches. The sixth in a workshop seriesdedicated specifically to problems of Quality in Databases; QDB'08 is a qualified forum forpresenting and discussing novel ideas and solutions related to the problems of assessing;monitoring; improving; and maintaining the quality of data. Previous editions of the workshopwere co-located with top-level data management conferences; namely SIGMOD and …,*,*,*
INEX REPORT,P Bellot; A Doucet; S Geva; S Gurajada; J Kamps; G Kazai; M Koolen; M Landoni; M Marx; A Mishra; V Moriceau; J Mothe; M Preminger; G Ramírez; M Sanderson; E Sanjuan; F Scholer; A Schuh; X Tannier; M Theobald; A Trotman; Q Wang,Abstract INEX investigates focused retrieval from structured documents by providing largetest collections of structured documents; uniform evaluation measures; and a forum fororganizations to compare their results. This paper reports on the INEX'12 evaluationcampaign; which consisted of a five tracks: Linked Data; Relevance Feedback; SnippetRetrieval; Social Book Search; and Tweet Contextualization. INEX'12 was an exciting yearfor INEX in which we joined forces with CLEF and for the first time ran our workshop as partof the CLEF labs in order to facilitate knowledge transfer between the evaluation forums. 1,*,*,*
Dissociation-based Optimization in Probabilistic Databases,Maarten Van den Heuvel; Floris Geerts; Martin Theobald,Page 1. Dissociation-based Optimization in Probabilistic Databases Maarten Van den Heuvel1;Floris Geerts1; Martin Theobald2 1Universiteit Antwerpen; Belgium 2Ulm University; GermanyPage 2. Contents • Introduction • Issues with safety • Dissociation: make (probabilistically) unsafequeries safe • Top-k: using summaries to speed up inference in safe queries Page 3. IntroductionWhat is the director that is most likely to have directed a movie starring an award winning actor?PlayedIn Movie Actor P Star Wars Ewan McGregor 0.9 Star Wars Samuel L. Jackson 0.7 StarTrek Samuel L. Jackson 0.2 WonBy Actor Prize P Ewan McGregor Oscar 0.9 Samuel L. JacksonGrammy 0.8 DirectedBy Director Movie P George Lucas Star Wars 0.9 JJ Abrahms Star Trek0.8 George Lucas Star Trek 0.1 Top-1 query Page 4. Introduction Q(X):- DirectedBy(X; Y);PlayedIn(Y; Z); WonBy(Z; U) PlayedIn Movie Actor P …,*,*,*
Core Lecture" Information Retrieval and Data Mining" WS 2011/12,Martin Theobald; Pauli Miettinen,The list of students having qualified for the final exam (inluding the number of bonuses) is availablehere [PDF]. If you do not see your matriculation number in the list but you think you should beon this list; or if you think your bonus points are wrong; please contact the lecturers … The resultsof the third short test can be found here [PDF]. The limits for passing the test and for obtaininga bonus are 8 and 15 points; respectively … The results of the second short test can be foundhere [PDF]. The limits for passing the test and for obtaining a bonus are 8 and 15 points;respectively … The results of the first short test can be found here [PDF]. The limits for passingthe test and for obtaining a bonus were 8 and 15 points; respectively … Detailed schedule withslides and links to further literature … The assignments to tutoring groups are nowavailable. (Last change: 2011.11.02 16:02. No more changes are possible.),*,*,*
Authors’ Addresses Maximilian Dylla Max-Planck-Institut für Informatik Campus E1. 4 D-66123 Saarbrücken,Iris Miliaraki; Martin Theobald,Abstract In this paper; we investigate a novel approach of computing confidence bounds fortop-k ranking queries in probabilistic databases with non-materialized views. Unlike priorapproaches; we present an exact pruning algorithm for finding the top-ranked queryanswers according to their marginal probabilities without the need to first materialize allanswer candidates via the views. Specifically; we consider conjunctive queries over multiplelevels of select-project-join views; the latter of which are cast into Datalog rules; where alsothe rules themselves may be uncertain; ie; be valid with some degree of confidence. To ourknowledge; this work is the first to address integrated data and confidence computations inthe context of probabilistic databases by considering confidence bounds over partiallyevaluated query answers with first-order lineage formulas. We further extend our query …,*,*,*
Program Committees,Carlo Batini; IRISA Laure Berti; France Tiziana Catarci; Ahmed K Elmagarmid; Suzanne Embury; Alvaro Fernandes; Helena Galhardas; Michael Gertz; Raghav Kaushik; Chen Li; Andrea Maurino; Felix Naumann; Nilesh Dalvi; Alex Dekhtyar; Maarten Fokkinga; Ander de Keijzer; Maurice van Keulen; Thomas Lukasiewicz; Sunil Prabhakar; Martin Theobald; Guy De Tr&; Jef Wijsen; Hainaut Vladimir Zadorozhny; AnHai Doan,*,*,*,*
A Two-Tiered Index Architecture for Scalable RDF Processing,Sairam Gurajada; Martin Theobald,Page 1. A Two-Tiered Index Architecture for Scalable RDF Processing Sairam Gurajada andMartin Theobald {gurajada;mtb} @ mpi-inf.mpg.de “Vincent 'Onofrio” Vincent_donofrioLaw_&_order_criminal_intent tv_show Chris_not h Sex_and_the_city The_thirteenth_floo r “1999”movie matrix has_name starred_in starred_in released_in is_a is_a released_in is_a is_astarred_in starred_in similar_plot_as Who acted in a “tv-show” as well as in a “movie”? VincentDonofrio is one answer ?x ?name ?y tv_show ?z movie has_name starred_in starred_in is_ais_a RDF (Graph structured knowledge base) SPARQL Query Introduction Motivation How toscale RDF System? • Over 30 Billion triples in the linked data cloud • Distributed ApproachChallenges • Minimize inter-node communication by effective partitioning and replicationapproaches • Parallel query processing and effecient load balancing Existing Approaches …,*,*,*
Program Committee Chairs,Mei Hsu; Alfons Kemper; Timos Sellis; Ashraf Aboulnaga; Christian Jensen; Malu Castellanos; Justin Levandoski; Elisa Bertino; Christoph Freytag; Serge Abiteboul; Patrick Valduriez; Yannis Kotidis; Andy Pavlo; Yufei Tao; Nikolaus Augsten; Vassilis Christophides; Martin Theobald; Jiawei Han; Talel Abdessalem; Azza Abouzied; Foto Afrati; Reza Akbarinia; Bernd Amann; Arvind Arasu; Denilson Barbosa; Ken Barker; Xiao Bai; Christian Bizer; Spyros Blanas; Panagiotis Bouros,Program Committee Chairs Mei Hsu (HP Labs; USA) Alfons Kemper (Technische UniversitätMünchen; Germany) Timos Sellis (Swinburne University of Technology; Australia) … ProgramCommittee Area Chairs Cloud Computing and Database-as-a-Service Ashraf Aboulnaga (QatarComputing Research Institute; Qatar) Big Data and Data-Warehousing System ArchitecturesChristian Jensen (Aalborg University; Denmark) Data Integration; Metadata Management; andInteroperability Malu Castellanos (HP Labs; USA) Modern Hardware and In-Memory DatabaseArchitecture and Systems Justin Levandoski (Microsoft Research; USA) Privacy; Security; andTrust Elisa Bertino (Purdue University; USA) Query Processing; Indexing; and Optimization ChristophFreytag (Humboldt University; Germany) Social Networks; Social Web; Graph; and PersonalInformation Management Serge Abiteboul (INRIA; France) Crowdsourcing; Distributed …,*,*,*
UViz: A Web 2.0 Visualization Engine for Query-Driven Reasoning in Uncertain RDF Knowledge Bases,Timm Meiser; Martin Theobald,*,*,*,*
IO-Top-k at TREC 2006: Terabyte Track,Holger Bast Debapriyo Majumdar Ralf Schenkel; Martin Theobald; Gerhard Weikum,This paper describes the setup and results of our contribu- tion to the TREC 2006 TerabyteTrack. Our implemen- tation was based on the algorithms proposed in [1] “IO- Top-k: Index-AccessOptimized Top-K Query Processing; VLDB'06”; with a main focus on the efficiency track …1. INTRODUCTION IO-Top-k [1] extends the family of threshold algorithms (TA) [3; 4; 8] with asuite of new strategies. To retrieve the best-scoring (so-called top-k) answers to a multi-keywordquery under a monotonic aggregation of per-keyword scores; TA-style algorithms perform indexscans (so-called sorted accesses) over precomputed index lists; one for each keyword in thequery; which are sorted in descending order of per- keyword scores. The key point of TA is thatit aggregates scores on the fly; thus computes a lower bound for the total score of the currentrank-k result document and an upper bound for the total scores of all other candidate …,*,*,*
Workshop Officers,Tiziana Catarci; Yannis Ioannidis; Vassilis Christophides; Georgia Koutrika; Grigoris Antoniou; Ricardo Baeza-Yates; Wolf Tilo Balke; Jan Chomicki; Paolo Ciaccia; Ling Feng; Irene Fundulaki; Werner Kiessling; Masaru Kitsuregawa; Nikos Koudas; Alexandros Labrinidis; Carlo Meghini; Massimo Melucci; Bamshad Mobasher; Wolfgang Nejdl; Moira Norrie; Christos Papatheodorou; Evi Pitoura; Guillaume Raschia; Nicolas Spyratos; Yannis Stavrakas; Martin Theobald; Panayiotis Tsaparas,*,*,*,*
Report on INEX 2008,Gianluca Demartini Ludovic Denoyer Antoine Doucet; Khairun Nisa Fachry; Patrick Gallinari; Shlomo Geva; Wei-Che Huang; Tereza Iofciu; Jaap Kamps; Gabriella Kazai; Marijn Koolen; Monica Landoni; Ragnar Nordlie; Nils Pharo; Ralf Schenkel; Martin Theobald; Andrew Trotman; Arjen P de Vries; Alan Woodley; Jianhan Zhu,Abstract INEX investigates focused retrieval from structured documents by providing largetest collections of structured documents; uniform evaluation measures; and a forum fororganizations to compare their results. This paper reports on the INEX 2008 evaluationcampaign; which consisted of a wide range of tracks: Ad hoc; Book; Efficiency; EntityRanking; Interactive; QA; Link the Wiki; and XML Mining.,*,*,*
DELIS-TR-597,Martin Theobald; Holger Bast; Debapriyo Majumdar; Ralf Schenkel,*,*,*,*
Supplementary Material for “Match Graph Construction for Large Image Databases”,Kwang In Kim; James Tompkin; Martin Theobald; Jan Kautz; Christian Theobalt,This appendix presents additional discussion on several aspects of the proposed algorithm.Sec. A presents a modification of our algorithm which enables users to reflect localconnectivity in link prediction. The remaining sections focus on the label propagationapplication. Sec. B and C discuss functionalities of active label acquisition and adding newimages to the match graph while Sec. D discusses our error correction schemes for labelpropagation; which rely on an external database. Label propagation in videos is discussedin Sec. E. Finally; Sec. F briefly discuss evaluation of label propagation and future work.,*,*,*
Efficient Query Processing in Probabilistic-Temporal Databases,Maximillian Dylla; Iris Miliaraki; Martin Theobald,*,*,*,*
Working Group: Lineage/Provenance,Anish Das Sarma; Amol Deshpande; Thomas Hubauer; Ihab Ilyas; Birgitta König-Ries; Matthias Renz; Martin Theobald,Abstract. The following summary tries to capture a collection of state-of-the-art techniquesand challenges for future work on lineage management in uncertain and probabilisticdatabases that we discussed in our working group. It was one half of a larger committee thatwe had initially formed; which then got split into two groups—one focusing on lineage as ameans of explanation of data; and one focusing more on lineage usage in probabilisticdatabases (see also the “Explanation” working group report for more details on the firstsubgroup).,*,*,*
