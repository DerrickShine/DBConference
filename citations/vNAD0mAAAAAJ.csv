Yago: a core of semantic knowledge,Fabian M Suchanek; Gjergji Kasneci; Gerhard Weikum,Abstract We present YAGO; a light-weight and extensible ontology with high coverage andquality. YAGO builds on entities and relations and currently contains more than 1 millionentities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomicrelations between entities (such as HASONEPRIZE). The facts have been automaticallyextracted from Wikipedia and unified with WordNet; using a carefully designed combinationof rule-based and heuristic methods described in this paper. The resulting knowledge baseis a major step beyond WordNet: in quality by adding knowledge about individuals likepersons; organizations; products; etc. with their semantic relationships-and in quantity byincreasing the number of facts by more than an order of magnitude. Our empirical evaluationof fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean …,Proceedings of the 16th international conference on World Wide Web,2007,2477
The LRU-K page replacement algorithm for database disk buffering,Elizabeth J O'neil; Patrick E O'neil; Gerhard Weikum,Abstract This paper introduces a new approach to database disk buffering; called the LRU-Kmethod. The basic idea of LRU-K is to keep track of the times of the last K references topopular database pages; using this information to statistically estimate the interarrival timesof references on a page by page basis. Although the LRU-K approach performs optimalstatistical inference under relatively standard assumptions; it is fairly simple and incurs littlebookkeeping overhead. As we demonstrate with simulation experiments; the LRU-Kalgorithm surpasses conventional buffering algorithms in discriminating between frequentlyand infrequently referenced pages. In fact; LRU-K can approach the behavior of bufferingalgorithms in which page sets with known access frequencies are manually assigned todifferent buffer pools of specifically tuned sizes. Unlike such customized buffering …,ACM SIGMOD Record,1993,1081
Transactional information systems: theory; algorithms; and the practice of concurrency control and recovery,Gerhard Weikum; Gottfried Vossen,Transactional Information Systems is the long-awaited; comprehensive work from leadingscientists in the transaction processing field. Weikum and Vossen begin with a broad look atthe role of transactional technology in today's economic and scientific endeavors; then delveinto critical issues faced by all practitioners; presenting today's most effective techniques forcontrolling concurrent access by multiple clients; recovering from system failures; andcoordinating distributed transactions. The authors emphasize formal models that are easilyapplied across fields; that promise to remain valid as current technologies evolve; and thatlend themselves to generalization and extension in the development of new classes ofnetwork-centric; functionally rich applications. This book's purpose and achievement is thepresentation of the foundations of transactional systems as well as the practical aspects of …,*,2001,878
Yago: A large ontology from wikipedia and wordnet,Fabian M Suchanek; Gjergji Kasneci; Gerhard Weikum,Abstract This article presents YAGO; a large ontology with high coverage and precision.YAGO has been automatically derived from Wikipedia and WordNet. It comprises entitiesand relations; and currently contains more than 1.7 million entities and 15 million facts.These include the taxonomic Is-A hierarchy as well as semantic relations between entities.The facts for YAGO have been extracted from the category system and the infoboxes ofWikipedia and have been combined with taxonomic relations from WordNet. Type checkingtechniques help us keep YAGO's precision at 95%—as proven by an extensive evaluationstudy. YAGO is based on a clean logical model with a decidable consistency. Furthermore; itallows representing n-ary relations in a natural way while maintaining compatibility withRDFS. A powerful query model facilitates access to YAGO's data.,Web Semantics: Science; Services and Agents on the World Wide Web,2008,748
YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia,Johannes Hoffart; Fabian M Suchanek; Klaus Berberich; Gerhard Weikum,Abstract We present YAGO2; an extension of the YAGO knowledge base; in which entities;facts; and events are anchored in both time and space. YAGO2 is built automatically fromWikipedia; GeoNames; and WordNet. It contains 447 million facts about 9.8 million entities.Human evaluation confirmed an accuracy of 95% of the facts in YAGO2. In this paper; wepresent the extraction methodology; the integration of the spatio-temporal dimension; andour knowledge representation SPOTL; an extension of the original SPO-triple model to timeand space.,Artificial Intelligence,2013,700
Robust disambiguation of named entities in text,Johannes Hoffart; Mohamed Amir Yosef; Ilaria Bordino; Hagen Fürstenau; Manfred Pinkal; Marc Spaniol; Bilyana Taneva; Stefan Thater; Gerhard Weikum,Abstract Disambiguating named entities in natural-language text maps mentions ofambiguous names onto canonical entities like people or places; registered in a knowledgebase such as DBpedia or YAGO. This paper presents a robust method for collectivedisambiguation; by harnessing context from knowledge bases and using a new form ofcoherence graph. It unifies prior approaches into a comprehensive framework that combinesthree measures: the prior probability of an entity being mentioned; the similarity between thecontexts of a mention and a candidate entity; as well as the coherence among candidateentities for all mentions together. The method builds a weighted graph of mentions andcandidate entities; and computes a dense subgraph that approximates the best joint mention-entity mapping. Experiments show that the new method significantly outperforms prior …,Proceedings of the Conference on Empirical Methods in Natural Language Processing,2011,572
RDF-3X: a RISC-style engine for RDF,Thomas Neumann; Gerhard Weikum,Abstract RDF is a data representation format for schema-free structured information that isgaining momentum in the context of Semantic-Web corpora; life sciences; and also Web 2.0platforms. The" pay-as-you-go" nature of RDF and the flexible pattern-matching capabilitiesof its query language SPARQL entail efficiency and scalability challenges for complexqueries including long join paths. This paper presents the RDF-3X engine; animplementation of SPARQL that achieves excellent performance by pursuing a RISC-stylearchitecture with a streamlined architecture and carefully designed; puristic data structuresand operations. The salient points of RDF-3X are: 1) a generic solution for storing andindexing RDF triples that completely eliminates the need for physical-design tuning; 2) apowerful yet simple query processor that leverages fast merge joins to the largest …,Proceedings of the VLDB Endowment,2008,532
The RDF-3X engine for scalable management of RDF data,Thomas Neumann; Gerhard Weikum,Abstract RDF is a data model for schema-free structured information that is gainingmomentum in the context of Semantic-Web data; life sciences; and also Web 2.0 platforms.The" pay-as-you-go" nature of RDF and the flexible pattern-matching capabilities of its querylanguage SPARQL entail efficiency and scalability challenges for complex queries includinglong join paths. This paper presents the RDF-3X engine; an implementation of SPARQL thatachieves excellent performance by pursuing a RISC-style architecture with streamlinedindexing and query processing. The physical design is identical for all RDF-3X databasesregardless of their workloads; and completely eliminates the need for index tuning byexhaustive indexes for all permutations of subject-property-object triples and their binaryand unary projections. These indexes are highly compressed; and the query processor …,The VLDB Journal—The International Journal on Very Large Data Bases,2010,460
Concepts and applications of multilevel transactions and open nested transactions,Gerhard Weikum; Hans-Jörg Schek,This chapter gives an overview on multilevel transactions and its generalization toward opennested transactions. The main features of these transaction models are the following: rst;semantic properties of operations can be exploited to relax the isolation of concurrenttransactions; second; as a consequence; atomicity is achieved by compensation rather thanstate-based undo; and third; subtransactions can be made persistent independently of theircommit state; that is; global visibility of their updates. Advanced transaction models and newcorrectness criteria for transaction executions have been proposed for the following reasons(and possibly further reasons that are not mentioned here): 1. to provide better support forlong-lived activities in advanced DBMS applications; 2. to relax the classical ACID paradigm;for example; provide more exibility as to when updates are made visible to concurrent …,*,1992,391
Data partitioning and load balancing in parallel disk systems,Peter Scheuermann; Gerhard Weikum; Peter Zabback,Abstract Parallel disk systems provide opportunities for exploiting I/O parallelism in twopossible ways; namely via inter-request and intra-request parallelism. In this paper; wediscuss the main issues in performance tuning of such systems; namely striping and loadbalancing; and show their relationship to response time and throughput. We outline the maincomponents of an intelligent; self-reliant file system that aims to optimize striping by takinginto account the requirements of the applications; and performs load balancing by judiciousfile allocation and dynamic redistributions of the data when access patterns change. Oursystem uses simple but effective heuristics that incur only little overhead. We presentperformance experiments based on synthetic workloads and real-life traces.,The VLDB Journal—The International Journal on Very Large Data Bases,1998,350
Principles and realization strategies of multilevel transaction management,Gerhard Weikum,Abstract One of the demands of database system transaction management is to achieve ahigh degree of concurrency by taking into consideration the semantics of high-leveloperations. On the other hand; the implementation of such operations must pay attention toconflicts on the storage representation levels below. To meet these requirements in alayered architecture; we propose a multilevel transaction management utilizing layer-specific semantics. Based on the theoretical notion of multilevel serializability; a family ofconcurrency control strategies is developed. Suitable recovery protocols are investigated foraborting single transactions and for restarting the system after a crash. The choice of levelsinvolved in a multilevel transaction strategy reveals an inherent trade-off between increasedconcurrency and growing recovery costs. A series of measurements has been performed …,ACM Transactions on Database Systems (TODS),1991,342
Klee: A framework for distributed top-k query algorithms,Sebastian Michel; Peter Triantafillou; Gerhard Weikum,Abstract This paper addresses the efficient processing of top-k queries in wide-areadistributed data repositories where the index lists for the attribute values (or text terms) of aquery are distributed across a number of data peers and the computational costs includenetwork latency; bandwidth consumption; and local peer work. We present KLEE; a novelalgorithmic framework for distributed top-k queries; designed for high performance andflexibility. KLEE makes a strong case for approximate top-k algorithms over widelydistributed data sources. It shows how great gains in efficiency can be enjoyed at low result-quality penalties. Further; KLEE affords the query-initiating peer the flexibility to trade-offresult quality and expected performance and to trade-off the number of communicationphases engaged during query execution versus network bandwidth performance. We …,Proceedings of the 31st international conference on Very large data bases,2005,297
From centralized workflow specification to distributed workflow execution,Peter Muth; Dirk Wodtke; Jeanine Weissenfels; Angelika Kotz Dittrich; Gerhard Weikum,Abstract Current workflow management systems fall short of supporting large-scaledistributed; enterprise-wide applications. We present a scalable; rigorously foundedapproach to enterprise-wide workflow management; based on the distributed execution ofstate and activity charts. By exploiting the formal semantics of state and activity charts; wedevelop an algorithm for transforming a centralized state and activity chart into a provablyequivalent partitioned one; suitable for distributed execution. A synchronization scheme isdeveloped that guarantees an execution equivalent to a non-distributed one. This basicsolution is further refined in order to reduce communication overhead and exploit parallelismbetween partitions whenever possible. The developed synchronization schemes arecompared in terms of the number and size of synchronization messages.,Journal of Intelligent Information Systems,1998,288
Top-k query evaluation with probabilistic guarantees,Martin Theobald; Gerhard Weikum; Ralf Schenkel,Abstract Top-k queries based on ranking elements of multidimensional datasets are afundamental building block for many kinds of information discovery. The best known general-purpose algorithm for evaluating top-k queries is Fagin's threshold algorithm (TA). Since theuser's goal behind top-k queries is to identify one or a few relevant and novel data items; it isintriguing to use approximate variants of TA to reduce run-time costs. This paper introducesa family of approximate top-k algorithms based on probabilistic arguments. When scanningindex lists of the underlying multidimensional data space in descending order of localscores; various forms of convolution and derived bounds are employed to predict when it issafe; with high probability; to drop candidate items and to prune the index scans. Theprecision and the efficiency of the developed methods are experimentally evaluated …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,283
YAGO2: exploring and querying world knowledge in time; space; context; and many languages,Johannes Hoffart; Fabian M Suchanek; Klaus Berberich; Edwin Lewis-Kelham; Gerard De Melo; Gerhard Weikum,Abstract We present YAGO2; an extension of the YAGO knowledge base with focus ontemporal and spatial knowledge. It is automatically built from Wikipedia; GeoNames; andWordNet; and contains nearly 10 million entities and events; as well as 80 million factsrepresenting general world knowledge. An enhanced data representation introduces timeand location as first-class citizens. The wealth of spatio-temporal information in YAGO canbe explored either graphically or through a special time-and space-aware query language.,Proceedings of the 20th international conference companion on World wide web,2011,267
Naga: Searching and ranking knowledge,Gjergji Kasneci; Fabian M Suchanek; Georgiana Ifrim; Maya Ramanath; Gerhard Weikum,The Web has the potential to become the world's largest knowledge base. In order tounleash this potential; the wealth of information available on the Web needs to be extractedand organized. There is a need for new querying techniques that are simple and yet moreexpressive than those provided by standard keyword-based search engines. Searching forknowledge rather than Web pages needs to consider inherent semantic structures likeentities (person; organization; etc.) and relationships (isA; locatedIn; etc.). In this paper; wepropose NAGA; a new semantic search engine. NAGA builds on a knowledge base; which isorganized as a graph with typed edges; and consists of millions of entities and relationshipsextracted from Web-based corpora. A graph-based query language enables the formulationof queries with additional semantic information. We introduce a novel scoring model …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,252
List decoding of error-correcting codes: winning thesis of the 2002 ACM doctoral dissertation competition,Venkatesan Guruswami,How can one exchange information e? ectively when the medium of com-nication introduceserrors? This question has been investigated extensively starting with the seminal works ofShannon (1948) and Hamming (1950); and has led to the rich theory of “error-correctingcodes”. This theory has traditionally gone hand in hand with the algorithmic theory of“decoding” that tackles the problem of recovering from the errors e? ciently. This thesispresents some spectacular new results in the area of decoding algorithms for error-correctingcodes. Speci? cally; itshowshowthenotionof “list-decoding” can be applied torecover from far more errors; for a wide variety of err-correcting codes; than achievablebefore. A brief bit of background: error-correcting codes are combinatorial str-tures that showhow to represent (or “encode”) information so that it is-silient to a moderate number of …,*,2004,252
SOFIE: a self-organizing framework for information extraction,Fabian M Suchanek; Mauro Sozio; Gerhard Weikum,Abstract This paper presents SOFIE; a system for automated ontology extension. SOFIE canparse natural language documents; extract ontological facts from them and link the facts intoan ontology. SOFIE uses logical reasoning on the existing knowledge and on the newknowledge in order to disambiguate words to their most probable meaning; to reason on themeaning of text patterns and to take into account world knowledge axioms. This allowsSOFIE to check the plausibility of hypotheses and to avoid inconsistencies with the ontology.The framework of SOFIE unites the paradigms of pattern matching; word sensedisambiguation and ontological reasoning in one unified model. Our experiments show thatSOFIE delivers high-quality output; even from unstructured Internet documents.,Proceedings of the 18th international conference on World wide web,2009,247
The index-based XXL search engine for querying XML data with relevance ranking,Anja Theobald; Gerhard Weikum,Abstract Query languages for XML such as XPath or XQuery support Boolean retrieval: aquery result is a (possibly restructured) subset of XML elements or entire documents thatsatisfy the search conditions of the query. This search paradigm works for highly schematicXML data collections such as electronic catalogs. However; for searching information inopen environments such as the Web or intranets of large corporations; ranked retrieval ismore appropriate: a query result is a rank list of XML elements in descending order of(estimated) relevance. Web search engines; which are based on the ranked retrievalparadigm; do; however; not consider the additional information and rich annotationsprovided by the structure of XML documents and their element names. This paper presentsthe XXL search engine that supports relevance ranking on XML data. XXL is particularly …,International Conference on Extending Database Technology,2002,246
Rethinking Database System Architecture: Towards a Self-Tuning RISC-Style Database System.,Surajit Chaudhuri; Gerhard Weikum,Abstract Database technology is one of the cornerstones for the new millennium's ITlandscape. However; database systems as a unit of code packaging and deployment are ata crossroad: commercial systems have been adding features for a long time and have nowreached complexity that makes them a difficult choice; in terms of their" gain/pain ratio"; as acentral platform for value-added information services such as ERP or e-commerce. It iscritical that database systems be easy to manage; predictable in their performancecharacteristics; and ultimately self-tuning. For this elusive goal; RISC-style simplification ofserver functionality and interfaces is absolutely crucial. We suggest a radical architecturaldeparture in which database technology is packaged into much smaller RISC-style datamanagers with lean; specialized APIs; and with built-in self-assessment and auto-tuning …,VLDB,2000,246
Scalable join processing on very large RDF graphs,Thomas Neumann; Gerhard Weikum,Abstract With the proliferation of the RDF data format; engines for RDF query processing arefaced with very large graphs that contain hundreds of millions of RDF triples. This paperaddresses the resulting scalability problems. Recent prior work along these lines hasfocused on indexing and other physical-design issues. The current paper focuses on joinprocessing; as the fine-grained and schema-relaxed use of RDF often entails star-and chain-shaped join queries with many input streams from index scans. We present two contributionsfor scalable join processing. First; we develop very light-weight methods for sidewaysinformation passing between separate joins at query run-time; to provide highly effectivefilters on the input streams of joins. Second; we improve previously proposed algorithms forjoin-order optimization by more accurate selectivity estimations for very large RDF graphs …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,240
Self-tuning database technology and information services: from wishful thinking to viable engineering,Gerhard Weikum; Axel Moenkeberg; Christof Hasse; Peter Zabback,The COMFORT project was started in 1990; and it was then expected that automatic tuningcould be achieved with a few simple principles. While the feedback control loop frameworkprovides useful guidance; the difficult problems are in the details of the various tuningissues. For robust solutions; workload statistics and mathematical models are key assets;and for viable engineering; these must be carefully designed to ensure acceptableoverhead. The field; in general; has made significant progress towards self-tuning databasetechnology; but there is no breakthrough. The biggest challenges that the researchcommunity should address as high-priority problems are the interactions of different systemcomponents and their tuning knobs; and the interference between different workloadclasses. For tackling this complexity; it is believed that a drastic simplification of today's …,*,2002,230
PATTY: a taxonomy of relational patterns with semantic types,Ndapandula Nakashole; Gerhard Weikum; Fabian Suchanek,Abstract This paper presents PATTY: a large resource for textual patterns that denote binaryrelations between entities. The patterns are semantically typed and organized into asubsumption taxonomy. The PATTY system is based on efficient algorithms for frequentitemset mining and can process Web-scale corpora. It harnesses the rich type system andentity population of large knowledge bases. The PATTY taxonomy comprises 350;569pattern synsets. Random-sampling-based evaluation shows a pattern accuracy of 84.7%.PATTY has 8;162 subsumptions; with a random-sampling-based precision of 75%. ThePATTY resource is freely available for interactive access and download.,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,2012,227
The Lowell database research self-assessment,Serge Abiteboul; Rakesh Agrawal; Phil Bernstein; Mike Carey; Stefano Ceri; Bruce Croft; David DeWitt; Mike Franklin; Hector Garcia Molina; Dieter Gawlick; Jim Gray; Laura Haas; Alon Halevy; Joe Hellerstein; Yannis Ioannidis; Martin Kersten; Michael Pazzani; Mike Lesk; David Maier; Jeff Naughton; Hans Schek; Timos Sellis; Avi Silberschatz; Mike Stonebraker; Rick Snodgrass; Jeff Ullman; Gerhard Weikum; Jennifer Widom; Stan Zdonik,A group of senior database researchers gathers every few years to assess the state of databaseresearch and to point out problem areas that deserve additional focus. This article summarizesthe discussion and conclusions of the sixth such meeting in Lowell; MA; in May 2003. It followsa number of earlier reports with similar goals; including [1; 2; 5-7] … Continuing thistradition; 25 senior database researchers representing a broad cross section of the field in termsof research interests; affiliations; and geography gathered in Lowell for two days of intensivediscussion on where the database field is and where it should be going … Several importantobservations came out of this meeting. Information management continues to be a critical componentof most complex software systems. We recommend that database researchers increase theirfocus on the integration of text; data; code; and streams; fusion of information from …,Communications of the ACM,2005,203
The Mentor project: Steps towards enterprise-wide workflow management,Dirk Wodtke; Jeanine Weißenfels; Gerhard Weikum; Angelika Kotz Dittrich,Enterprise-wide workflow management where workflows may span multiple organizationalunits require particular consideration of scalability; heterogeneity; and availability issues.The Mentor project; introduced in this paper; aims to reconcile a rigorous workflowspecification method with a distributed middleware architecture as a step towards enterprise-wide solutions. The project uses the formalism of state and activity charts and a commercialtool; Statemate; for workflow specification. A first prototype of Mentor has been built whichallows executing specifications in a distributed manner. A major contribution of this paper isthe method for transforming a centralized state chart specification into a form that isamenable to a distributed execution and to incorporate the necessary synchronizationbetween different processing entities. Fault tolerance issues are addressed by coupling …,Data Engineering; 1996. Proceedings of the Twelfth International Conference on,1996,201
Client-server computer system with application recovery of server applications and client applications,*,A client-server computer system has one or more clients connected to one or more servers.During request/reply interactions; a client-side application sends a request for services (eg;read a file; return some information; update a database record; process data; etc.) to theserver. A server-side application request program processes the request; prepares a reply tothe request; and returns the reply to the client-side application. The server runs a resourcemanager to log operations and data pages in a manner that enables application anddatabase recovery. Among other tasks; the server's resource manager creates a stable logfile that can be used to help recover the client-side application in the event of a systemcrash. To capture the client-server interaction; the server's resource manager records thereply in the log buffer and commits the reply record to the stable log before the reply is …,*,2001,194
Combining linguistic and statistical analysis to extract relations from web documents,Fabian M Suchanek; Georgiana Ifrim; Gerhard Weikum,Abstract The World Wide Web provides a nearly endless source of knowledge; which ismostly given in natural language. A first step towards exploiting this data automatically couldbe to extract pairs of a given semantic relation from text documents-for example all pairs of aperson and her birthdate. One strategy for this task is to find text patterns that express thesemantic relation; to generalize these patterns; and to apply them to a corpus to find newpairs. In this paper; we show that this approach profits significantly when deep linguisticstructures are used instead of surface text patterns. We demonstrate how linguistic structurescan be represented for machine learning; and we provide a theoretical analysis of thepattern matching approach. We show the benefits of our approach by extensive experimentswith our prototype system L EILA.,Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,2006,186
A formal foundation for distributed workflow execution based on state charts,Dirk Wodtke; Gerhard Weikum,Abstract This paper provides a formal foundation for distributed workflow executions. Thestate chart formalism is adapted to the needs of a workflow model in order to establish abasis for both correctness reasoning and run-time support for complex and large-scaleworkflow applications. To allow for the distributed execution of a workflow across differentworkflow servers; which is required for scalability and organizational decentralization; amethod for the partitioning of workflow specifications is developed. It is proven that thepartitioning preserves the original state chart's behavior.,International Conference on Database Theory,1997,186
The DASDBS project: Objectives; experiences; and future prospects,H-J Schek; H-B Paul; Marc H.  Scholl; Gerhard Weikum,A retrospective of the Darmstadt database system project; also known as DASDBS; ispresented. The project is aimed at providing data management support for advancedapplications; such as geo-scientific information systems and office automation. Similar to thedichotomy of RSS and RDS in System R; a layered architectural approach was pursued: astorage management kernel serves as the lowest common denominator of the requirementsof the various applications classes; and a family of application-oriented front-ends providessemantically richer functions on top of the kernel. The lessons that were learned frombuilding the DASDBS system are discussed. Particular emphasis is placed on the followingissues: the role of nested relations; the experiences with using object buffers for coupling thesystem with the programming-language environment and the learning process in …,IEEE Transactions on Knowledge and Data Engineering,1990,181
Efficient top-k querying over social-tagging networks,Ralf Schenkel; Tom Crecelius; Mouna Kacimi; Sebastian Michel; Thomas Neumann; Josiane X Parreira; Gerhard Weikum,Abstract Online communities have become popular for publishing and searching content; aswell as for finding and connecting to other users. User-generated content includes; forexample; personal blogs; bookmarks; and digital photos. These items can be annotated andrated by different users; and these social tags and derived user-specific scores can beleveraged for searching relevant content and discovering subjectively interesting items.Moreover; the relationships among users can also be taken into consideration for rankingsearch results; the intuition being that you trust the recommendations of your close friendsmore than those of your casual acquaintances. Queries for tag or keyword combinations thatcompute and rank the top-k results thus face a large variety of options that complicate thequery processing and pose efficiency challenges. This paper addresses these issues by …,Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,2008,180
An efficient and versatile query engine for TopX search,Martin Theobald; Ralf Schenkel; Gerhard Weikum,Abstract This paper presents a novel engine; coined TopX; for efficient ranked retrieval ofXML documents over semistructured but nonschematic data collections. The algorithmfollows the paradigm of threshold algorithms for top-k query processing with a focus oninexpensive sequential accesses to index lists and only a few judiciously scheduled randomaccesses. The difficulties in applying the existing top-k algorithms to XML data lie in 1) theneed to consider scores for XML elements while aggregating them at the document level; 2)the combination of vague content conditions with XML path conditions; 3) the need to relaxquery conditions if too few results satisfy all conditions; and 4) the selectivity estimation forboth content and structure conditions and their impact on evaluation strategies. TopXaddresses these issues by precomputing score and path information in an appropriately …,Proceedings of the 31st international conference on Very large data bases,2005,174
Scalable knowledge harvesting with high precision and high recall,Ndapandula Nakashole; Martin Theobald; Gerhard Weikum,Abstract Harvesting relational facts from Web sources has received great attention forautomatically constructing large knowledge bases. Stateof-the-art approaches combinepattern-based gathering of fact candidates with constraint-based reasoning. However; theystill face major challenges regarding the trade-offs between precision; recall; and scalability.Techniques that scale well are susceptible to noisy patterns that degrade precision; whiletechniques that employ deep reasoning for high precision cannot cope with Web-scale data.This paper presents a scalable system; called PROSPERA; for high-quality knowledgeharvesting. We propose a new notion of ngram-itemsets for richer patterns; and use MaxSat-based constraint reasoning on both the quality of patterns and the validity of fact candidates.We compute pattern-occurrence statistics for two benefits: they serve to prune the …,Proceedings of the fourth ACM international conference on Web search and data mining,2011,168
Graph-based text classification: learn from your neighbors,Ralitsa Angelova; Gerhard Weikum,Abstract Automatic classification of data items; based on training samples; can be boostedby considering the neighborhood of data items in a graph structure (eg; neighboringdocuments in a hyperlink environment or co-authors and their publications for bibliographicdata entries). This paper presents a new method for graph-based classification; withparticular emphasis on hyperlinked text documents but broader applicability. Our approachis based on iterative relaxation labeling and can be combined with either Bayesian or SVMclassifiers on the feature spaces of the given data items. The graph neighborhood is takeninto consideration to exploit locality patterns while at the same time avoiding overfitting. Incontrast to prior work along these lines; our approach employs a number of noveltechniques: dynamically inferring the link/class pattern in the graph in the run of the …,Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,2006,168
HOPI: An efficient connection index for complex XML document collections,Ralf Schenkel; Anja Theobald; Gerhard Weikum,Abstract In this paper we present HOPI; a new connection index for XML documents basedon the concept of the 2–hop cover of a directed graph introduced by Cohen et al. In contrastto most of the prior work on XML indexing we consider not only paths with child or parentrelationships between the nodes; but also provide space–and time–efficient reachabilitytests along the ancestor; descendant; and link axes to support path expressions withwildcards in our XXL search engine. We improve the theoretical concept of a 2–hop cover bydeveloping scalable methods for index creation on very large XML data collections with longpaths and extensive cross–linkage. Our experiments show substantial savings in the queryperformance of the HOPI index over previously proposed index structures in combinationwith low space requirements.,International Conference on Extending Database Technology,2004,168
Merging application-centric and data-centric approaches to support transaction-oriented multi-system workflows,Yuri Breitbart; Andrew Deacon; H-J Schek; Amit Sheth; Gerhard Weikum,Abstract Workflow management is primarily concerned with dependencies between thetasks of a workflow; to ensure correct control flow and data flow. Transaction management;on the other hand; is concerned with preserving data dependencies by preventing executionof conflicting operations from multiple; concurrently executing tasks or transactions. In thispaper we argue that many applications will be served better if the properties of transactionand workflow models are supported by an integrated architecture. We also presentpreliminary ideas towards such an architecture.,ACM Sigmod Record,1993,168
Io-top-k: Index-access optimized top-k query processing,Holger Bast; Debapriyo Majumdar; Ralf Schenkel; Martin Theobald; Gerhard Weikum,Abstract Top-k query processing is an important building block for ranked retrieval; withapplications ranging from text and data integration to distributed aggregation of network logsand sensor data. Top-k queries operate on index lists for a query's elementary conditionsand aggregate scores for result candidates. One of the best implementation methods in thissetting is the family of threshold algorithms; which aim to terminate the index scans as earlyas possible based on lower and upper bounds for the final scores of result candidates. Thisprocedure performs sequential disk accesses for sorted index scans; but also has the optionof performing random accesses to resolve score uncertainty. This entails scheduling for thetwo kinds of accesses: 1) the prioritization of different index lists in the sequential accesses;and 2) the decision on when to perform random accesses and for which candidates. The …,Proceedings of the 32nd international conference on Very large data bases,2006,167
Probabilistic ranking of database query results,Surajit Chaudhuri; Gautam Das; Vagelis Hristidis; Gerhard Weikum,Abstract We investigate the problem of ranking answers to a database query when manytuples are returned. We adapt and apply principles of probabilistic models from InformationRetrieval for structured data. Our proposed solution is domain independent. It leveragesdata and workload statistics and correlations. Our ranking functions can be furthercustomized for different applications. We present results of preliminary experiments whichdemonstrate the efficiency as well as the quality of our ranking system.,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,166
A language modeling approach for temporal information needs,Klaus Berberich; Srikanta Bedathur; Omar Alonso; Gerhard Weikum,Abstract This work addresses information needs that have a temporal dimension conveyedby a temporal expression in the user's query. Temporal expressions such as “in the 1990s”are frequent; easily extractable; but not leveraged by existing retrieval models. Onechallenge when dealing with them is their inherent uncertainty. It is often unclear whichexact time interval a temporal expression refers to. We integrate temporal expressions into alanguage modeling approach; thus making them first-class citizens of the retrieval modeland considering their inherent uncertainty. Experiments on the New York Times AnnotatedCorpus using Amazon Mechanical Turk to collect queries and obtain relevanceassessments demonstrate that our approach yields substantial improvements in retrievaleffectiveness.,European Conference on Information Retrieval,2010,165
The Claremont report on database research,Rakesh Agrawal; Anastasia Ailamaki; Philip A Bernstein; Eric A Brewer; Michael J Carey; Surajit Chaudhuri; AnHai Doan; Daniela Florescu; Michael J Franklin; Hector Garcia-Molina; Johannes Gehrke; Le Gruenwald; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; Hank F Korth; Donald Kossmann; Samuel Madden; Roger Magoulas; Beng Chin Ooi; Tim O'Reilly; Raghu Ramakrishnan; Sunita Sarawagi; Michael Stonebraker; Alexander S Szalay; Gerhard Weikum,Abstract In late May; 2008; a group of database researchers; architects; users and punditsmet at the Claremont Resort in Berkeley; California to discuss the state of the research fieldand its impacts on practice. This was the seventh meeting of this sort in twenty years; andwas distinguished by a broad consensus that we are at a turning point in the history of thefield; due both to an explosion of data and usage scenarios; and to major shifts in computinghardware and platforms. Given these forces; we are at a time of opportunity for researchimpact; with an unusually large potential for influential results across computing; thesciences and society. This report details that discussion; and highlights the group'sconsensus view of new focus areas; including new database engine architectures;declarative programming languages; the interplay of structured and unstructured data …,ACM Sigmod Record,2008,163
Adding relevance to XML,Anja Theobald; Gerhard Weikum,Abstract XML query languages proposed so far are limited to Boolean retrieval in the sensethat query results are sets of qualifying XML elements or subgraphs. This search paradigm isintriguing for “closed” collections of XML documents such as e-commerce catalogs; but weargue that it is inadequate for searching the Web where we would prefer ranked lists ofresults based on relevance estimation. IR-style Web search engines; on the other hand; areincapable of exploiting the additional information made explicit in the structure; elementnames; and attributes of XML documents. In this paper we present a compact querylanguage; coined XXL for “flexible XML search language”; that reconciles both searchparadigms by combining XML graph pattern matching with relevance estimations andproducing ranked lists of XML subgraphs as search results. The paper describes the …,International Workshop on the World Wide Web and Databases,2000,161
Minerva: Collaborative p2p search,Matthias Bender; Sebastian Michel; Peter Triantafillou; Gerhard Weikum; Christian Zimmer,Abstract This paper proposes the live demonstration of a prototype of MINERVA; a novelP2P Web search engine. The search engine is layered on top of a DHT-based overlaynetwork that connects an a-priori unlimited number of peers; each of which maintains apersonal local database and a local search facility. Each peer posts a small amount ofmetadata to a physically distributed directory that is used to efficiently select promising peersfrom across the peer population that can best locally execute a query. The proposeddemonstration serves as a proof of concept for P2P Web search by deploying the project onstandard notebook PCs and also invites everybody to join the network by instantly installinga small piece of software from a USB memory stick.,Proceedings of the 31st international conference on Very large data bases,2005,160
Embedded systems design: the ARTIST roadmap for research and development,Bruno Bouyssounouse,Embedded systems now include a very large proportion of the advanced products designedin the world; spanning transport (avionics; space; automotive; trains); electrical andelectronic appliances (cameras; toys; televisions; home appliances; audio systems; andcellular phones); process control (energy production and distribution; factory automation andoptimization); telecommunications (satellites; mobile phones and telecom networks); andsecurity (e-commerce; smart cards); etc. The extensive and increasing use of embeddedsystems and their integration in everyday products marks a significant evolution ininformation science and technology. We expect that within a short timeframe embeddedsystems will be a part of nearly all equipment designed or manufactured in Europe; the USA;and Asia. There is now a strategic shift in emphasis for embedded systems designers …,*,2005,150
Towards self-tuning memory management for data servers,Gerhard Weikum; Arnd Christian  Koenig; Achim Kraiss; Markus Sinnwell,Abstract Although today's computers provide huge amounts of main memory; the ever-increasing load of large data servers; imposed by resource-intensive decision-supportqueries and accesses to multimedia and other complex data; often leads to memorycontention and may result in severe performance degradation. Therefore; careful tuning ofmemory mangement is crucial for heavy-load data servers. This paper gives an overview ofself-tuning methods for a spectrum of memory management issues; ranging from traditionalcaching to exploiting distributed memory in a server cluster and speculative prefetching in aWeb-based system. The common; fundamental elements in these methods include on-lineload tracking; near-future access prediction based on stochastic models and the availableon-line statistics; and dynamic and automatic adjustment of control parameters in a …,IEEE Data Eng. Bull.,1999,150
Integrating DB and IR technologies: What is the sound of one hand clapping?,Surajit Chaudhuri; Raghu Ramakrishnan; Gerhard Weikum,Abstract Databases (DB) and information retrieval (IR) have evolved as separate fields.However; modern applications such as customer support; health care; and digital librariesrequire capabilities for both data and text management. In such settings; traditional DBqueries; in SQL or XQuery; are not flexible enough to handle applicationspecific scoring andranking. IR systems; on the other hand; lack efficient support for handling structured parts ofthe data and metadata; and do not give the application developer adequate control over theranking function. This paper analyzes the requirements of advanced text-and data-richapplications for an integrated platform. The core functionality must be manageable; and theAPI should be easy to program against. A particularly important issue that we highlight ishow to reconcile flexibility in scoring and ranking models with optimizability; in order to …,CIDR,2005,148
From information to knowledge: harvesting entities and relationships from web sources,Gerhard Weikum; Martin Theobald,Abstract There are major trends to advance the functionality of search engines to a moreexpressive semantic level. This is enabled by the advent of knowledge-sharing communitiessuch as Wikipedia and the progress in automatically extracting entities and relationshipsfrom semistructured as well as natural-language Web sources. Recent endeavors of thiskind include DBpedia; EntityCube; KnowItAll; ReadTheWeb; and our own YAGO-NAGAproject (and others). The goal is to automatically construct and maintain a comprehensiveknowledge base of facts about named entities; their semantic classes; and their mutualrelations as well as temporal contexts; with high precision and high recall. This tutorialdiscusses state-of-the-art methods; research opportunities; and open challenges along thisavenue of knowledge harvesting.,Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2010,146
Probabilistic information retrieval approach for ranking of database query results,Surajit Chaudhuri; Gautam Das; Vagelis Hristidis; Gerhard Weikum,Abstract We investigate the problem of ranking the answers to a database query when manytuples are returned. In particular; we present methodologies to tackle the problem forconjunctive and range queries; by adapting and applying principles of probabilistic modelsfrom information retrieval for structured data. Our solution is domain independent andleverages data and workload statistics and correlations. We evaluate the quality of ourapproach with a user survey on a real database. Furthermore; we present andexperimentally evaluate algorithms to efficiently retrieve the top ranked results; whichdemonstrate the feasibility of our ranking system.,ACM Transactions on Database Systems (TODS),2006,146
Natural language questions for the web of data,Mohamed Yahya; Klaus Berberich; Shady Elbassuoni; Maya Ramanath; Volker Tresp; Gerhard Weikum,Abstract The Linked Data initiative comprises structured databases in the Semantic-Webdata model RDF. Exploring this heterogeneous data by structured query languages istedious and error-prone even for skilled users. To ease the task; this paper presents amethodology for translating natural language questions into structured SPARQL queriesover linked-data sources. Our method is based on an integer linear program to solve severaldisambiguation tasks jointly: the segmentation of questions into phrases; the mapping ofphrases to semantic entities; classes; and relations; and the construction of SPARQL triplepatterns. Our solution harnesses the rich type system provided by knowledge bases in theweb of linked data; to constrain our semantic-coherence objective function. We presentexperiments on both the question translation and the resulting query answering.,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,2012,142
Towards a universal wordnet by learning from combined evidence,Gerard De Melo; Gerhard Weikum,Abstract Lexical databases are invaluable sources of knowledge about words and theirmeanings; with numerous applications in areas like NLP; IR; and AI. We propose amethodology for the automatic construction of a large-scale multilingual lexical databasewhere words of many languages are hierarchically organized in terms of their meanings andtheir semantic relations to other words. This resource is bootstrapped from WordNet; a well-known English-language resource. Our approach extends WordNet with around 1.5 millionmeaning links for 800;000 words in over 200 languages; drawing on evidence extractedfrom a variety of resources including existing (monolingual) wordnets;(mostly bilingual)translation dictionaries; and parallel corpora. Graph-based scoring functions and statisticallearning techniques are used to iteratively integrate this information and build an output …,Proceedings of the 18th ACM conference on Information and knowledge management,2009,133
An optimality proof of the LRU-K page replacement algorithm,Elizabeth J O'neil; Patrick E O'Neil; Gerhard Weikum,Abstract This paper analyzes a recently published algorithm for page replacement inhierarchical paged memory systems [O'Neil et al. 1993]. The algorithm is called the LRU-Kmethod; and reduces to the well-known LRU (Least Recently Used) method for K= 1.Previous work [O'Neil et al. 1993; Weikum et al. 1994; Johnson and Shasha 1994] hasshown the effectiveness for K> 1 by simulation; especially in the most common case of K= 2.The basic idea in LRU-K is to keep track of the times of the last K references to memorypages; and to use this statistical information to rank-order the pages as to their expectedfuture behavior. Based on this the page replacement policy decision is made: which memory-resident page to replace when a newly accessed page must be read into memory. In thecurrent paper; we prove; under the assumptions of the independent reference model; that …,Journal of the ACM (JACM),1999,132
Star: Steiner-tree approximation in relationship graphs,Gjergji Kasneci; Maya Ramanath; Mauro Sozio; Fabian M Suchanek; Gerhard Weikum,Large graphs and networks are abundant in modern information systems: entity-relationshipgraphs over relational data or Web-extracted entities; biological networks; social onlinecommunities; knowledge bases; and many more. Often such data comes with expressivenode and edge labels that allow an interpretation as a semantic graph; and edge weightsthat reflect the strengths of semantic relations between entities. Finding close relationshipsbetween a given set of two; three; or more entities is an important building block for manysearch; ranking; and analysis tasks. From an algorithmic point of view; this translates intocomputing the best Steiner trees between the given nodes; a classical NP-hard problem. Inthis paper; we present a new approximation algorithm; coined STAR; for relationship queriesover large relationship graphs. We prove that for n query entities; STAR yields an O (log …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,131
Fast and accurate estimation of shortest paths in large graphs,Andrey Gubichev; Srikanta Bedathur; Stephan Seufert; Gerhard Weikum,Abstract Computing shortest paths between two given nodes is a fundamental operationover graphs; but known to be nontrivial over large disk-resident instances of graph data.While a number of techniques exist for answering reachability queries and approximatingnode distances efficiently; determining actual shortest paths (ie the sequence of nodesinvolved) is often neglected. However; in applications arising in massive online socialnetworks; biological networks; and knowledge graphs it is often essential to find out many; ifnot all; shortest paths between two given nodes. In this paper; we address this problem andpresent a scalable sketch-based index structure that not only supports estimation of nodedistances; but also computes corresponding shortest paths themselves. Generating theactual path information allows for further improvements to the estimation accuracy of …,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,130
The COMFORT automatic tuning project,Gerhard Weikum; Christof Hasse; Axel Mönkeberg; Peter Zabback,Abstract This paper reports on results and experiences from the COMFORT automatic tuningproject. The objective of the project has been to investigate architectural principles of self-tuning database and transaction processing systems; and to develop self-tuning methods forspecific performance tuning problems. A particular concern of the project has been to copewith workload dynamics and workload heterogeneity in multi-user systems. As a generalguideline; an adaptive feedback control approach has been adopted; where observations ofthe current load characteristics are used to predict performance trends and to drive thedynamic adjustment of tuning parameters. As examples of these general principles; thepaper discusses adaptive approaches to two specific tuning problems and the deelopedsolutions. First; we present a self-tuning load control method that copes with overload …,*,1994,130
Improving collection selection with overlap awareness in p2p search engines,Matthias Bender; Sebastian Michel; Peter Triantafillou; Gerhard Weikum; Christian Zimmer,Abstract Collection selection has been a research issue for years. Typically; in related work;precomputed statistics are employed in order to estimate the expected result quality of eachcollection; and subsequently the collections are ranked accordingly. Our thesis is that thissimple approach is insufficient for several applications in which the collections typicallyoverlap. This is the case; for example; for the collections built by autonomous peers crawlingthe web. We argue for the extension of existing quality measures using estimators of mutualoverlap among collections and present experiments in which this combination outperformsCORI; a popular approach based on quality estimation. We outline our prototypeimplementation of a P2P web search engine; coined MINERVA; that allows handling largeamounts of data in a distributed and self-organizing manner. We conduct experiments …,Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,2005,129
Dynamic file allocation in disk arrays,Gerhard Weikum; Peter Zabback; Peter Scheuermann,Abstract Large arrays of small disks are being considered as a promising approach to highperformance 1/0 architectures. In this paper we deal with the problem of data placement insuch a disk array. The prevalent approach is to decluster large files across a number of disksso as to minimize the access time to a file and balance the 1/0 load across the disks. Thedata placement problem entails determining the number of disks and the set of disks acrosswhich a file is declustered. Unlike previous work; this paper does not assume that all filesare allocated at the same time but rather considers dynamic file creations; This makes theplacement problem considerably harder because each placement decision has to take intoaccount the current allocation state and the access frequencies of the disks and the existingfiles. As a result; file creation may involve partial reorganization on one or more disks. The …,ACM SIGMOD Record,1991,128
XML-enabled workflow management for e-services across heterogeneous platforms,German Shegalov; Michael Gillmann; Gerhard Weikum,Abstract Advanced e-services require efficient; flexible; and easy-to-use workflowtechnology that integrates well with mainstream Internet technologies such as XML and Webservers. This paper discusses an XML-enabled architecture for distributed workflowmanagement that is implemented in the latest version of our Mentor-lite prototype system.The key asset of this architecture is an XML mediator that handles the exchange of businessand flow control data between workflow and business-object servers on the one hand andclient activities on the other via XML messages over http. Our implementation of the mediatorhas made use of Oracle's XSQL servlet. The major benefit of the advocated architecture isthat it provides seamless integration of client applications into e-service workflows withscalable efficiency and very little explicit coding; in contrast to an earlier; Java-based …,The VLDB Journal—The International Journal on Very Large Data Bases,2001,127
The SphereSearch engine for unified ranked retrieval of heterogeneous XML and web documents,Jens Graupmann; Ralf Schenkel; Gerhard Weikum,Abstract This paper presents the novel SphereSearch Engine that provides unified rankedretrieval on heterogeneous XML and Web data. Its search capabilities include vaguestructure conditions; text content conditions; and relevance ranking based on IR statisticsand statistically quantified ontological relationships. Web pages in HTML or PDF areautomatically converted into XML format; with the option of generating semantic tags bymeans of linguistic annotation tools. For Web data the XML-oriented query engine isleveraged to provide very rich search options that cannot be expressed in traditional Websearch engines: concept-aware and link-aware querying that takes into account the implicitstructure and context of Web pages. The benefits of the SphereSearch engine aredemonstrated by experiments with a large and richly tagged but non-schematic open …,Proceedings of the 31st international conference on Very large data bases,2005,126
Aida: An online tool for accurate disambiguation of named entities in text and tables,Mohamed Amir Yosef; Johannes Hoffart; Ilaria Bordino; Marc Spaniol; Gerhard Weikum,ABSTRACT We present AIDA; a framework and online tool for entity detection anddisambiguation. Given a natural-language text or a Web table; we map mentions ofambiguous names onto canonical entities like people or places; registered in a knowledgebase like DBpedia; Freebase; or YAGO. AIDA is a robust framework centred aroundcollective disambiguation exploiting the prominence of entities; similarity between thecontext of the mention and its candidates; and the coherence among candidate entities forall mentions. We have developed a Web-based online interface for AIDA where differentformats of inputs can be processed on the fly; returning proper entities and showingintermediate steps of the disambiguation process.,Proceedings of the VLDB Endowment,2011,125
A theoretical foundation of multi-level concurrency control,Gerhard Weikum,Until recently concurrency control theory has focussed on one-level systems; I. 8. regardingschedules simply as sequences of read and write operatlons on objects of one particularabstractlon level (/Pa79/;/BSW79/). usually pages or tuples. Nevertheless. real data basesystems sometlmes go beyond this classlcal view. a well-known eXarI'tple being System R.To Increase concurrency; locking Is applied twice within thls prototype. on tuples until end-of-transactlon and on pages for the scope of each tuple operatlon. Since page locks arereleased before the commitment of the complete transaction. the approach Is called 'opennested transactlon'(/Tr83/); A typical schedule acting on two layers would look like:,Proceedings of the fifth ACM SIGACT-SIGMOD symposium on Principles of database systems,1985,124
Efficient creation and incremental maintenance of the hopi index for complex xml document collections,Ralf Schenkel; Anja Theobald; Gerhard Weikum,The HOPI index; a connection index for XML documents based on the concept of a 2-hopcover; provides space-and time-efficient reachability tests along the ancestor; descendant;and link axes to support path expressions with wildcards in XML search engines. This paperpresents enhanced algorithms for building HOPI; shows how to augment the index withdistance information; and discusses incremental index maintenance. Our experiments showsubstantial improvements over the existing divide-and-conquer algorithm for index creation;low space overhead for including distance information in the index; and efficient updates.,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,117
The bag-of-opinions method for review rating prediction from sparse text patterns,Lizhen Qu; Georgiana Ifrim; Gerhard Weikum,Abstract The problem addressed in this paper is to predict a user's numeric rating in aproduct review from the text of the review. Unigram and n-gram representations of text arecommon choices in opinion mining. However; unigrams cannot capture importantexpressions like" could have been better"; which are essential for prediction models ofratings. N-grams of words; on the other hand; capture such phrases; but typically occur toosparsely in the training set and thus fail to yield robust predictors. This paper overcomes thelimitations of these two models; by introducing a novel kind of bag-of-opinionsrepresentation; where an opinion; within a review; consists of three components: a rootword; a set of modifier words from the same sentence; and one or more negation words.Each opinion is assigned a numeric score which is learned; by ridge regression; from a …,Proceedings of the 23rd International Conference on Computational Linguistics,2010,114
The YAGO-NAGA approach to knowledge discovery,Gjergji Kasneci; Maya Ramanath; Fabian Suchanek; Gerhard Weikum,Abstract This paper gives an overview on the YAGO-NAGA approach to informationextraction for building a conveniently searchable; large-scale; highly accurate knowledgebase of common facts. YAGO harvests infoboxes and category names of Wikipedia for factsabout individual entities; and it reconciles these with the taxonomic backbone of WordNet inorder to ensure that all entities have proper classes and the class system is consistent.Currently; the YAGO knowledge base contains about 19 million instances of binary relationsfor about 1.95 million entities. Based on intensive sampling; its accuracy is estimated to beabove 95 percent. The paper presents the architecture of the YAGO extractor toolkit; itsdistinctive approach to consistency checking; its provisions for maintenance and furthergrowth; and the query engine for YAGO; coined NAGA. It also discusses ongoing work on …,ACM SIGMOD Record,2009,114
x-RDF-3X: fast querying; high update rates; and consistency for RDF databases,Thomas Neumann; Gerhard Weikum,Abstract The RDF data model is gaining importance for applications in computationalbiology; knowledge sharing; and social communities. Recent work on RDF engines hasfocused on scalable performance for querying; and has largely disregarded updates. Inaddition to incremental bulk loading; applications also require online updates with flexiblecontrol over multi-user isolation levels and data consistency. The challenge lies in meetingthese requirements while retaining the capability for fast querying. This paper presents acomprehensive solution that is based on an extended deferred-indexing method withintegrated versioning. The version store enables time-travel queries that are efficientlyprocessed without adversely affecting queries on the current data. For flexible consistency;transactional concurrency control is provided with options for either snapshot isolation or …,Proceedings of the VLDB Endowment,2010,113
KORE: keyphrase overlap relatedness for entity disambiguation,Johannes Hoffart; Stephan Seufert; Dat Ba Nguyen; Martin Theobald; Gerhard Weikum,Abstract Measuring the semantic relatedness between two entities is the basis for numeroustasks in IR; NLP; and Web-based knowledge extraction. This paper focuses ondisambiguating names in a Web or text document by jointly mapping all names ontosemantically related entities registered in a knowledge base. To this end; we havedeveloped a novel notion of semantic relatedness between two entities represented as setsof weighted (multi-word) keyphrases; with consideration of partially overlapping phrases.This measure improves the quality of prior link-based models; and also eliminates the needfor (usually Wikipedia-centric) explicit interlinkage between entities. Thus; our method ismore versatile and can cope with long-tail and newly emerging entities that have few or nolinks associated with them. For efficiency; we have developed approximation techniques …,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,111
Workflow management with service quality guarantees,Michael Gillmann; Gerhard Weikum; Wolfgang Wonner,Abstract Workflow management systems (WFMS) that are geared for the orchestration ofbusiness processes across multiple organizations are complex distributed systems: theyconsist of multiple workflow engines; application servers; and communication middlewareservers such as ORBs; where each of these server types can be replicated on multiplecomputers for scalability and availability. Finding an appropriate system configuration withguaranteed application-specific quality of service in terms of throughput; response time; andtolerable downtime is a major challenge for human system administrators. This paperpresents a tool that largely automates the task of configuring a distributed WFMS. Based ona suite of mathematical models; the tool derives the necessary degrees of replication for thevarious server types in order to meet specified goals for performance and availability as …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,111
TopX: efficient and versatile top-k query processing for semistructured data,Martin Theobald; Holger Bast; Debapriyo Majumdar; Ralf Schenkel; Gerhard Weikum,Abstract Recent IR extensions to XML query languages such as Xpath 1.0 Full-Text or theNEXI query language of the INEX benchmark series reflect the emerging interest in IR-styleranked retrieval over semistructured data. TopX is a top-k retrieval engine for text andsemistructured data. It terminates query execution as soon as it can safely determine the ktop-ranked result elements according to a monotonic score aggregation function withrespect to a multidimensional query. It efficiently supports vague search on both content-andstructure-oriented query conditions for dynamic query relaxation with controllable influenceon the result ranking. The main contributions of this paper unfold into four main points:(1)fully implemented models and algorithms for ranked XML retrieval with XPath Full-Textfunctionality;(2) efficient and effective top-k query processing for semistructured data;(3) …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,109
A time machine for text search,Klaus Berberich; Srikanta Bedathur; Thomas Neumann; Gerhard Weikum,Abstract Text search over temporally versioned document collections such as web archiveshas received little attention as a research problem. As a consequence; there is no scalableand principled solution to search such a collection as of a specified time. In this work; weaddress this shortcoming and propose an efficient solution for time-travel text search byextending the inverted file index to make it ready for temporal search. We introduceapproximate temporal coalescing as a tunable method to reduce the index size withoutsignificantly affecting the quality of results. In order to further improve the performance oftime-travel queries; we introduce two principled techniques to trade off index size for itsperformance. These techniques can be formulated as optimization problems that can besolved to near-optimality. Finally; our approach is evaluated in a comprehensive series of …,Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,2007,107
Are Web Services the Next Revolution in e-Commerce?(Panel).,Shalom Tsur; Serge Abiteboul; Rakesh Agrawal; Umeshwar Dayal; Johannes Klein; Gerhard Weikum,1 Overview Electronic commerce in its different manifestations is spreading and rapidlyaffecting all of us. In most cases we; as individuals; come in touch with it when we pur-chasegoods or services over the web. This practice is usually referred to as" B2C" or business toconsumer interaction. Yet; this is only the tip of the iceberg: the real revolution (to use a goodclich© e) will come when businesses will electronically conduct their busi-ness with otherbusinesses over the web. This is known as" B2B" or business to business; and is expectedto introduce enormous efficiencies into the market place. Today; the trend is shifting fromB2C to B2B and a number of leading companies are already engaged in B2B activities:companies such as Ariba and CommerceOne offer to create and operate marketplaces.Other companies such as BEA; IBM; Vitria; Tibco and others offer the infrastructure …,VLDB,2001,106
MENTA: Inducing multilingual taxonomies from Wikipedia,Gerard de Melo; Gerhard Weikum,Abstract In recent years; a number of projects have turned to Wikipedia to establish large-scale taxonomies that describe orders of magnitude more entities than traditional manuallybuilt knowledge bases. So far; however; the multilingual nature of Wikipedia has largelybeen neglected. This paper investigates how entities from all editions of Wikipedia as wellas WordNet can be integrated into a single coherent taxonomic class hierarchy. We rely onlinking heuristics to discover potential taxonomic relationships; graph partitioning to formconsistent equivalence classes of entities; and a Markov chain-based ranking approach toconstruct the final taxonomy. This results in MENTA (Multilingual Entity Taxonomy); aresource that describes 5.4 million entities and is presumably the largest multilingual lexicalknowledge base currently available.,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,101
Semantic concurrency control in object-oriented database systems,Peter Muth; Thomas C Rakow; Gerhard Weikum; Peter Brossler; Christof Hasse,A locking protocol for object-oriented database systems (OODBSs) is presented. Theprotocol can exploit the semantics of methods invoked on encapsulated objects. Comparedto conventional page-oriented or record-oriented concurrency control protocols; theproposed protocol greatly improves the possible concurrency because commutative methodexecutions on the same object are not considered as a conflict. An OODBS applicationexample is presented. The principle of open-nested transactions is reviewed. It is shownthat; using the locking protocol in an open-nested transaction; the locks of a subtransactionare released when the subtransaction completes; and only a semantic lock is held further bythe parent of the subtransaction.,Data Engineering; 1993. Proceedings. Ninth International Conference on,1993,101
The Claremont report on database research,Rakesh Agrawal; Anastasia Ailamaki; Philip A Bernstein; Eric A Brewer; Michael J Carey; Surajit Chaudhuri; Anhai Doan; Daniela Florescu; Michael J Franklin; Hector Garcia-Molina; Johannes Gehrke; Le Gruenwald; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; Hank F Korth; Donald Kossmann; Samuel Madden; Roger Magoulas; Beng Chin Ooi; Tim O'Reilly; Raghu Ramakrishnan; Sunita Sarawagi; Michael Stonebraker; Alexander S Szalay; Gerhard Weikum,Here; we explore the conclusions of this self-assessment. It is by definition somewhatinward-focused but may be of interest to the broader computing community as both a windowinto upcoming directions in database research and a description of some of the community issuesand initiatives that surfaced. We describe the group's consensus view of new focus areas forresearch; including database engine architectures; declarative programming languages; interplayof structured data and free text; cloud data services; and mobile and virtual worlds. We also reporton discussions of the database community's growth and processes that may be of interest toother research areas facing similar challenges … Over the past 20 years; small groups of databaseresearchers have periodically gathered to assess the state of the field and propose directionsfor future research. 1;3;4;5;6;7 Reports of the meetings served to foster debate within the …,Communications of the ACM,2009,100
Unbundling transaction services in the cloud,David Lomet; Alan Fekete; Gerhard Weikum; Mike Zwilling,Abstract: The traditional architecture for a DBMS engine has the recovery; concurrencycontrol and access method code tightly bound together in a storage engine for records. Wepropose a different approach; where the storage engine is factored into two layers (each ofwhich might have multiple heterogeneous instances). A Transactional Component (TC)works at a logical level only: it knows about transactions and their" logical" concurrencycontrol and undo/redo recovery; but it does not know about page layout; B-trees etc. A DataComponent (DC) knows about the physical storage structure. It supports a record orientedinterface that provides atomic operations; but it does not know about transactions. Providingatomic record operations may itself involve DC-local concurrency control and recovery;which can be implemented using system transactions. The interaction of the mechanisms …,arXiv preprint arXiv:0909.1768,2009,99
4 Breast Cancer Diagnosis by Fuzzy CoCo,Carlos Pena Reyes,Although computerized tools for medical diagnosis have been developed since the early60s; their number and capabilities have grown impressively in the last years due; mainly; tothe availability of medical data and increased computing power. Most of these systems areconceived to provide high diagnostic performance. However; interest has recently shifted tosystems capable of providing; besides a correct diagnosis; insight on how the answer wasobtained. Thanks to their linguistic representation and their numeric behavior; fuzzy systemscan provide both performance and explanation.,Coevolutionary Fuzzy Modeling,2004,98
Multi-level recovery,Gerhard Weikum; Christof Hasse; Peter Broessler; Peter Muth,Abstract Multi-level transactions have received considerable attention as a framework forhigh-performance concurrency control methods. An inherent property of multi-leveltransactions is the need for compensating actions; since state-based recovery methods dono longer work correctly for transaction undo. The resulting requirement of operation loggingadds to the complexity of crash recovery. In addition; multi-level recovery algorithms have totake into account that high-level actions are not necessarily atomic; eg; if multiple pages areupdated in a single action. In this paper; we present a recovery algorithm for multi-leveltransactions. Unlike typical commercial database systems; we have striven for simplicityrather than employing special tricks. It is important to note; though; that simplicity is notachieved at the expense of performance. We show how a high-performance multi-level …,Proceedings of the ninth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1990,98
Multi-level transaction management; theoretical art or practical need?,Catriel Beeri; H-J Schek; Gerhard Weikum,Abstract A useful approach to the design and description of complex data managementsystems is the decomposition of a system into a hierarchically organized collection of levels.In such a system; transaction management is distributed among the levels. This paperpresents the fundamental theory of multi-level concurrency control and recovery. A model forthe computation of multi-level transactions is introduced by generalizing from the well knownsingle-level theory. Three basic principles; called commutation; reduction; and abstractionare explained. Using them enables one to explain and prove seemingly” tricky”implementation techniques as correct; by regarding them as multi-level algorithms. We showhow the theory helps to understand and explain in a systematic framework techniques thatare in use in today's DBMSs. We also discuss how and why multi-level algorithms may …,International Conference on Extending Database Technology,1988,97
An experimental study on distance-based graph drawing,Ulrik Brandes; Christian Pich,Abstract In numerous application areas; general undirected graphs need to be drawn; andforce-directed layout appears to be the most frequent choice. We present an extensiveexperimental study showing that; if the goal is to represent the distances in a graph well; acombination of two simple algorithms based on variants of multidimensional scaling is to bepreferred because of their efficiency; reliability; and even simplicity. We also hope thatdetails in the design of our study help advance experimental methodology in algorithmengineering and graph drawing; independent of the case at hand.,International Symposium on Graph Drawing,2008,95
Information retrieval and filtering over self-organising digital libraries,Paraskevi Raftopoulou; Euripides GM Petrakis; Christos Tryfonopoulos; Gerhard Weikum,Abstract We present i ClusterDL; a self-organising overlay network that supports informationretrieval and filtering functionality in a digital library environment. i ClusterDL is able tohandle huge amounts of data provided by digital libraries in a distributed and self-organisingway. The two-tier architecture and the use of semantic overlay networks provide aninfrastructure for creating large networks of digital libraries that require minimumadministration; yet offer a rich set of tools to the end-user. We present the main componentsof our architecture; the protocols that regulate peer interactions; and an experimentalevaluation that shows the efficiency; and the retrieval and filtering effectiveness of ourapproach.,International Conference on Theory and Practice of Digital Libraries,2008,93
Exploiting social relations for query expansion and result ranking,Matthias Bender; Tom Crecelius; Mouna Kacimi; Sebastian Michel; Thomas Neumann; Josiane Xavier Parreira; Ralf Schenkel; Gerhard Weikum,Online communities have recently become a popular tool for publishing and searchingcontent; as well as for finding and connecting to other users that share common interests.The content is typically user-generated and includes; for example; personal blogs;bookmarks; and digital photos. A particularly intriguing type of content is user-generatedannotations (tags) for content items; as these concise string descriptions allow forreasonings about the interests of the user who created the content; but also about the userwho generated the annotations. This paper presents a framework to cast the different entitiesof such networks into a unified graph model representing the mutual relationships of users;content; and tags. It derives scoring functions for each of the entities and relations. We haveperformed an experimental evaluation on two real-world datasets (crawled from deli. cio …,Data engineering workshop; 2008. ICDEW 2008. IEEE 24th International Conference on,2008,93
LEILA: Learning to extract information by linguistic analysis,Fabian M Suchanek; Georgiana Ifrim; Gerhard Weikum,Abstract One of the challenging tasks in the context of the Semantic Web is to automaticallyextract instances of binary relations from Web documents–for example all pairs of a personand the corresponding birthdate. In this paper; we present LEILA; a system that can extractinstances of arbitrary given binary relations from natural language Web documents–withouthuman interaction. Different from previous approaches; LEILA uses a deep syntacticanalysis. This results in consistent improvements over comparable systems (such as egSnowball or TextToOnto).,Proceedings of the 2nd Workshop on Ontology Learning and Population: Bridging the Gap between Text and Knowledge,2006,93
Timely yago: harvesting; querying; and visualizing temporal knowledge from wikipedia,Yafang Wang; Mingjie Zhu; Lizhen Qu; Marc Spaniol; Gerhard Weikum,Abstract Recent progress in information extraction has shown how to automatically buildlarge ontologies from high-quality sources like Wikipedia. But knowledge evolves over time;facts have associated validity intervals. Therefore; ontologies should include time as a first-class dimension. In this paper; we introduce Timely YAGO; which extends our previouslybuilt knowledge base YAGO with temporal aspects. This prototype system extracts temporalfacts from Wikipedia infoboxes; categories; and lists in articles; and integrates these into theTimely YAGO knowledge base. We also support querying temporal facts; by temporalpredicates in a SPARQL-style language. Visualization of query results is provided in order tobetter understand of the dynamic nature of knowledge.,Proceedings of the 13th International Conference on Extending Database Technology,2010,87
Language-model-based ranking for queries on RDF-graphs,Shady Elbassuoni; Maya Ramanath; Ralf Schenkel; Marcin Sydow; Gerhard Weikum,Abstract The success of knowledge-sharing communities like Wikipedia and the advances inautomatic information extraction from textual and Web sources have made it possible tobuild large" knowledge repositories" such as DBpedia; Freebase; and YAGO. Thesecollections can be viewed as graphs of entities and relationships (ER graphs) and can berepresented as a set of subject-property-object (SPO) triples in the Semantic-Web datamodel RDF. Queries can be expressed in the W3C-endorsed SPARQL language or bysimilarly designed graph-pattern search. However; exact-match query semantics often fallshort of satisfying the users' needs by returning too many or too few results. Therefore; IR-style ranking models are crucially needed. In this paper; we propose a language-model-based approach to ranking the results of exact; relaxed and keyword-augmented graph …,Proceedings of the 18th ACM conference on Information and knowledge management,2009,86
Exploiting Structure; Annotation; and Ontological Knowledge for Automatic Classification of XML Data.,Martin Theobald; Ralf Schenkel; Gerhard Weikum,ABSTRACT This paper investigates how to automatically classify schemaless XML data intoa user-defined topic directory. The main focus is on constructing appropriate feature spaceson which a classifier operates. In addition to the usual text-based term frequency vectors; westudy XML twigs and tag paths as extended features that can be combined with text termoccurrences in XML elements. Moreover; we show how to leverage ontological backgroundinformation; more specifically; the WordNet thesaurus; for the construction of moreexpressive feature spaces. For efficiency our implementation computes featuresincrementally and caches ontology entries. Our experiments demonstrate the improvedaccuracy of automatic classification based on the enhanced feature spaces.,WebDB,2003,86
Enterprise-wide workflow management based on state and activity charts,Peter Muth; Dirk Wodtke; Jeanine Weissenfels; Gerhard Weikum; Angelika Kotz Dittrich,Abstract This paper presents an approach towards the specification; verification; anddistributed execution of workflows based on state and activity charts. The formal foundationof state and activity charts is exploited at three levels. At the specification level; the formalismenforces precise descriptions of business processes while also allowing subsequentrefinements. In addition; precise specifications based on other methods can be automaticallyconverted into state and activity charts. At the level of verification; state charts are amenableto the efficient method of model checking; in order to verify particularly critical workflowproperties. Finally; at the execution level; a state chart specification forms the basis for theautomatic generation of modules that can be directly executed in a distributed manner.Within the MENTOR project; a coherent prototype system has been built that comprises …,*,1998,84
Efficient and self-tuning incremental query expansion for top-k query processing,Martin Theobald; Ralf Schenkel; Gerhard Weikum,Abstract We present a novel approach for efficient and self-tuning query expansion that isembedded into a top-k query processor with candidate pruning. Traditional query expansionmethods select expansion terms whose thematic similarity to the original query terms isabove some specified threshold; thus generating a disjunctive query with much higherdimensionality. This poses three major problems: 1) the need for hand-tuning the expansionthreshold; 2) the potential topic dilution with overly aggressive expansion; and 3) thedrastically increased execution cost of a high-dimensional query. The method developed inthis paper addresses all three problems by dynamically and incrementally merging theinverted lists for the potential expansion terms with the lists for the original query terms. Apriority queue is used for maintaining result candidates; the pruning of candidates is …,Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,2005,82
Towards a unified theory of concurrency control and recovery,Hans-Jörg Schek; Gerhard Weikum; Haiyan Ye,Abstract The classical theory of transaction management is based on two different andindependent criteria for the correct execution of transactions. The first criterion; serializability;ensures correct execution of parallel transactions under the assumption that no failuresoccur. The second criterion; strictness; ensures correct recovery from failures. In this paperwe develop a unified model that allows reasoning about the correctness of concurrencycontrol and recovery within the same framework. We introduce the correctness criteria of(prefix-) reducibility and (prefix-) expanded serializability and investigate their relationshipsto the classical criteria. An important advantage of our model is that it captures scheduleswith semantically rich ADT actions in addition to classical read/write schedules.,Proceedings of the twelfth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1993,81
Combining histograms and parametric curve fitting for feedback-driven query result-size estimation,Arnd Christian König; Gerhard Weikum,Note: OCR errors may be found in this Reference List extracted from the full text article. ACMhas opted to expose the complete List rather than only correct and linked references … {1} DanielBarbará; William DuMouchel; Christos Faloutsos; Peter J. Haas; Joseph M. Hellerstein; YannisE. Ioannidis; HV Jagadish; Theodore Johnson; Raymond T. Ng; Viswanath Poosala; KennethA. Ross; Kenneth C. Sevcik: The New Jersey Data Reduction Report. IEEE Data Eng. Bull.20(4): 3-45(1997) … {5} C. de Boor. A practical guide to splines. Springer-Verlag; 1978 …{9} PB Gibbons and Y. Matias. Synopsis data structures for massive data sets. Tech. report; BellLabs; 1998 … {11} Phillip B. Gibbons; S. Acharya; Y. Bartal; Y. Matias; S. Muthukishnan; V.Poosala; S. Ramaswamy; and T. Suel. AQUA: System and techniques for approximate queryanswering. Tech. report; Bell Labs; 1998.,Proceedings of the 25th International Conference on Very Large Data Bases,1999,79
Architectural issues of transaction management in multi-layered systems,Gerhard Weikum; Hans-Jörg Schek,Abstract The internal structure of current data b; lse systems is ideally characterized by ahierarchy of multiple layers. Each layer offers certain specific objects and operations on itsinterface. Within this framework we investigate the transaction management aspects I It isshown that the System R kind of concurrency control can be generalized and an appropriaterecovery method can be found by introducing a type of open nested transactions which arestrongly tied to architectural layers. Especially. our approach includes application-specificlevels on top of a data base kernel system. Up to now; most of the preprocessor solutions forso-called“non-standard” applications that have been proposed simply ignore aspects ofconcurrency control and recovery. We sketch different possibilities to realize transactionmanagement in such a layered environment. 1; lntrodution and RB Two different …,*,1984,76
Report on the DB/IR panel at sigmod 2005,Sihem Amer-Yahia; Pat Case; Thomas Rölleke; Jayavel Shanmugasundaram; Gerhard Weikum,Abstract This paper summarizes the salient aspects of the SIGMOD 2005 panel on"Databases and Information Retrieval: Rethinking the Great Divide". The goal of the panelwas to discuss whether we should rethink data management systems architectures to trulymerge Database (DB) and Information Retrieval (IR) technologies. The panel had very highattendance and generated lively discussions.,ACM SIGMOD Record,2005,75
Digital library information-technology infrastructures,Yannis Ioannidis; David Maier; Serge Abiteboul; Peter Buneman; Susan Davidson; Edward Fox; Alon Halevy; Craig Knoblock; Fausto Rabitti; Hans Schek; Gerhard Weikum,Abstract This paper charts a research agenda on systems-oriented issues in digital libraries.It focuses on the most central and generic system issues; including system architecture; user-level functionality; and the overall operational environment. With respect to user-levelfunctionality; in particular; it abstracts the overall information lifecycle in digital libraries tofive major stages and identifies key research problems that require solution in each stage.Finally; it recommends an explicit set of activities that would help achieve the research goalsoutlined and identifies several dimensions along which progress of the digital library fieldcan be evaluated.,International journal on digital libraries,2005,75
Efficient text proximity search,Ralf Schenkel; Andreas Broschart; Seungwon Hwang; Martin Theobald; Gerhard Weikum,Abstract In addition to purely occurrence-based relevance models; term proximity has beenfrequently used to enhance retrieval quality of keyword-oriented retrieval systems. Whilethere have been approaches on effective scoring functions that incorporate proximity; therehas not been much work on algorithms or access methods for their efficient evaluation. Thispaper presents an efficient evaluation framework including a proximity scoring functionintegrated within a top-k query engine for text retrieval. We propose precomputed andmaterialized index structures that boost performance. The increased retrieval effectivenessand efficiency of our framework are demonstrated through extensive experiments on a verylarge text benchmark collection. In combination with static index pruning for the proximitylists; our algorithm achieves an improvement of two orders of magnitude compared to a …,International Symposium on String Processing and Information Retrieval,2007,74
Yago2: a spatially and temporally enhanced knowledge base from wikipedia,Johannes Hoffart; Fabian M Suchanek; Klaus Berberich; Gerhard Weikum,Abstract We present YAGO2; an extension of the YAGO knowledge base; in which entities;facts; and events are anchored in both time and space. YAGO2 is built automatically fromWikipedia; GeoNames; and WordNet. It contains 447 million facts about 9.8 million entities.Human evaluation confirmed an accuracy of 95% of the facts in YAGO2. In this paper; wepresent the extraction methodology; the integration of the spatio-temporal dimension; andour knowledge representation SPOTL; an extension of the original SPO-triple model to timeand space.,*,2012,73
Distributed file organization with scalable cost/performance,Radek Vingralek; Yuri Breitbart; Gerhard Weikum,Abstract This paper presents a distributed file organization for record-structured; disk-resident files with key-based exact-match access. The file is organized into buckets that arespread across multiple servers; where a server may hold multiple buckets. Client requestsare serviced by mapping keys onto buckets and looking up the corresponding server in anaddress table. Dynamic growth in terms of file size and access load is supported by bucketsplits and migration onto other existing or newly acquired servers. The significant andchallenging problem addressed here is how to achieve scalability so that both the file sizeand the client throughput can be scaled up by linearly increasing the number of servers anddynamically redistributing data. Unlike previous work with similar objectives; our dataredistribution considers explicitly the cost/performance ratio of the system by aiming to …,ACM SIGMOD Record,1994,73
Efficient transparent application recovery in client-server information systems,David Lomet; Gerhard Weikum,Abstract Database systems recover persistent data; providing high database availability.However; database applications; typically residing on client or “middle-tier” application-server machines; may lose work because of a server failure. This prevents the masking ofserver failures from the human user and substantially degrades application availability. Thispaper aims to enable high application availability with an integrated method for databaseserver recovery and transparent application recovery in a client-server system. Theapproach; based on application message logging; is similar to earlier work on distributedsystem fault tolerance. However; we exploit advanced database logging and recoverytechniques and request/reply messaging properties to significantly improve efficiency.Forced log I/Os; frequently required by other methods; are usually avoided. Restart time …,ACM SIGMOD Record,1998,70
Recovery guarantees for general multi-tier applications,Roger Barga; David Lomet; Gerhard Weikum,Database recovery does not mask failures to applications and users. Recovery is neededthat considers data; messages and application components. Special cases have beenstudied; but clear principles for recovery guarantees in general multi-tier applications suchas Web-based e-services are missing. We develop a framework for recovery guarantees thatmasks almost all failures. The main concept is an interaction contract between twocomponents; a pledge as to message and state persistence; and contract release. Contractsare composed into system-wide agreements so that a set of components is provablyrecoverable with exactly-once message delivery and execution; except perhaps for crash-interrupted user input or output. Our implementation techniques reduce the data loggingcost; allow effective log truncation; and provide independent recovery for critical server …,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,69
The BINGO! System for Information Portal Generation and Expert Web Search.,Sergej Sizov; Martin Theobald; Stefan Siersdorfer; Gerhard Weikum; Jens Graupmann; Michael Biwer; Patrick Zimmer,Abstract This paper presents the BINGO! focused crawler; an advanced tool for informationportal generation and expert Web search. In contrast to standard search engines such asGoogle which are solely based on precomputed index structures; a focused crawlerinterleaves crawling; automatic classification; link analysis and assessment; and textfiltering. A crawl is started from a user-provided set of training data and aims to collectcomprehensive results for the given topics. The focused crawling paradigm has beenaround for a few years and many of our techniques are adopted from the informationretrieval and machine learning literature. BINGO! is a system-oriented effort to integrate asuite of techniques into a comprehensive and versatile tool. The paper discusses its overallarchitecture and main components; important lessons from early experimentation and the …,CIDR,2003,68
Fine-grained semantic typing of emerging entities,Ndapandula Nakashole; Tomasz Tylenda; Gerhard Weikum,Abstract Methods for information extraction (IE) and knowledge base (KB) construction havebeen intensively studied. However; a largely under-explored case is tapping into highlydynamic sources like news streams and social media; where new entities are continuouslyemerging. In this paper; we present a method for discovering and semantically typing newlyemerging out-of-KB entities; thus improving the freshness and recall of ontology-based IEand improving the precision and semantic rigor of open IE. Our method is based on aprobabilistic model that feeds weights into integer linear programs that leverage typesignatures of relational phrases and type correlation or disjointness constraints. Ourexperimental evaluation; based on crowdsourced user studies; show our method performingsignificantly better than prior work.,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2013,67
Ming: mining informative entity relationship subgraphs,Gjergji Kasneci; Shady Elbassuoni; Gerhard Weikum,Abstract Many modern applications are faced with the task of knowledge discovery in entity-relationship graphs; such as domain-specific knowledge bases or social networks. Miningan" informative" subgraph that can explain the relations between k (>= 2) given entities ofinterest is a frequent knowledge discovery scenario on such graphs. We present MING; aprincipled method for extracting an informative subgraph for given query nodes. MING buildson a new notion of informativeness of nodes. This is used in a random-walk-with-restartsprocess to compute the informativeness of entire subgraphs.,Proceedings of the 18th ACM conference on Information and knowledge management,2009,67
Database and information-retrieval methods for knowledge discovery,Gerhard Weikum; Gjergji Kasneci; Maya Ramanath; Fabian Suchanek,Our aim here is to advocate for the integration of database systems (DB) and information-retrieval(IR) methods to address applications that are emerging from the ongoing explosion and diversificationof digital information. One grand goal of such an endeavor is the automatic building and maintenanceof a comprehensive knowledge base of facts from encyclopedic sources and the scientificliterature. Facts should be represented in terms of typed entities and relationships and allowexpressive queries that return ranked results with precision in an efficient and scalablemanner. We thus explore how DB and IR methods might contribute toward this ambitiousgoal … DB and IR are separate fields in computer science due to historical accident. Both investigateconcepts; models; and computational methods for managing large amounts of complexinformation; though each began almost 40 years ago with very different application areas …,Communications of the ACM,2009,66
p2pDating: Real life inspired semantic overlay networks for web search,Josiane Xavier Parreira; Sebastian Michel; Gerhard Weikum,Abstract We consider a network of autonomous peers forming a logically global butphysically distributed search engine; where every peer has its own local collectiongenerated by independently crawling the Web. A challenging task in such systems is toefficiently route user queries to peers that can deliver high quality results and be able to rankthese returned results; thus satisfying the users' information need. However; the probleminherent with this scenario is selecting a few promising peers out of an a priori unlimitednumber of peers. In recent research a rather strict notion of semantic overlay networks hasbeen established. In most approaches; peers are connected to other peers based on a rigidsemantic profile by clustering them based on their contents. In contrast; our strategy followsthe spirit of peer autonomy and creates semantic overlay networks based on the notion of …,Information Processing & Management,2007,66
Performance evaluation of an adaptive and robust load control method for the avoidance of data contention thrashing,Axel Mönkeberg; Gerhard Weikum,*,VLDB,1992,66
DB&IR: both sides now,Gerhard Weikum,Database systems (DB) and information retrieval (IR) are two separate fields of computerscience by historical accident. Both study concepts; models; and computational methods formanaging large amounts of complex information; but thirty or forty years ago they startedwith very different application areas as major motivations and technology drivers: accountingsystems (online reservations; banking; etc.) for DB; and library systems (bibliographiccatalogs; patent collections; etc.) for IR. Thus; the two directions and their researchcommunities emphasized very different aspects of information management: dataconsistency; precise query processing; and efficiency on the DB side [53]; and textunderstanding; statistical ranking models; and user satisfaction on the IR side [35; 47].Decades later; there is now rapidly growing awareness of the needs for integrating DB …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,65
Intelligent search on XML data: applications; languages; models; implementations; and benchmarks,Henk Blanken,Recently; we have seen a steep increase in the popularity and adoption of XML; in areassuch as traditional databases; e-business; the scientific environment; and on the web.Querying XML documents and data efficiently is a challenging issue; this book approachessearch on XML data by combining content-based methods from information retrieval andstructure-based XML query methods and presents the following parts: applications; querylanguages; retrieval models; implementing intelligent XML systems; and evaluation. Toappreciate the book; basic knowledge of traditional database technology; informationretrieval; and XML is needed. The book is ideally suited for courses or seminars at thegraduate level as well as for education of research and development professionals workingon Web applications; digital libraries; database systems; and information retrieval.,*,2003,64
Discovering emerging entities with ambiguous names,Johannes Hoffart; Yasemin Altun; Gerhard Weikum,Abstract Knowledge bases (KB's) contain data about a large number of people;organizations; and other entities. However; this knowledge can never be complete due tothe dynamics of the ever-changing world: new companies are formed every day; new songsare composed every minute and become of interest for addition to a KB. To keep up with thereal world's entities; the KB maintenance process needs to continuously discover newlyemerging entities in news and other Web streams. In this paper we focus on the most difficultcase where the names of new entities are ambiguous. This raises the technical problem todecide whether an observed name refers to a known entity or represents a new entity. Thispaper presents a method to solve this problem with high accuracy. It is based on a newmodel of measuring the confidence of mapping an ambiguous mention to an existing …,Proceedings of the 23rd international conference on World wide web,2014,62
LINDA: distributed web-of-data-scale entity matching,Christoph Böhm; Gerard de Melo; Felix Naumann; Gerhard Weikum,Abstract Linked Data has emerged as a powerful way of interconnecting structured data onthe Web. However; the cross-linkage between Linked Data sources is not as extensive asone would hope for. In this paper; we formalize the task of automatically creating" sameAs"links across data sources in a globally consistent manner. Our algorithm; presented in amulti-core as well as a distributed version; achieves this link generation by accounting forjoint evidence of a match. Experiments confirm that our system scales beyond 100 millionentities and delivers highly accurate results despite the vast heterogeneity and dauntingscale.,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,62
See what's enBlogue: real-time emergent topic identification in social media,Foteini Alvanaki; Sebastian Michel; Krithi Ramamritham; Gerhard Weikum,Abstract With the increasing popularity of Web 2.0 streams; people become overwhelmed bythe available information. This is partly countered by tagging blog posts and tweets; so thatusers can filter messages according to their tags. However; this is insufficient for detectingnewly emerging topics that are not reflected by a single tag but are rather expressed byunusual tag combinations. This paper presents enBlogue; an approach for automaticallydetecting such emergent topics. EnBlogue uses a time-sliding window to compute statisticsabout tags and tag-pairs. These statistics are then used to identify unusual shifts incorrelations; most of the time caused by real-world events. We analyze the strength of theseshifts and measure the degree of unpredictability they include; used to rank tag-pairsexpressing emergent topics. Additionally; this" indicator of surprise" is carried over to …,Proceedings of the 15th International Conference on Extending Database Technology,2012,62
The LHAM log-structured history data access method,Peter Muth; Patrick O'Neil; Achim Pick; Gerhard Weikum,Abstract. Numerous applications such as stock market or medical information systemsrequire that both historical and current data be logically integrated into a temporal database.The underlying access method must support different forms of “time-travel” queries; themigration of old record versions onto inexpensive archive media; and high insertion andupdate rates. This paper presents an access method for transaction-time temporal data;called the log-structured history data access method (LHAM) that meets these demands. Thebasic principle of LHAM is to partition the data into successive components based on thetimestamps of the record versions. Components are assigned to different levels of a storagehierarchy; and incoming data is continuously migrated through the hierarchy. The paperdiscusses the LHAM concepts; including concurrency control and recovery; our full …,The VLDB Journal,2000,61
Snowball: Scalable storage on networks of workstations with balanced load,Radek Vingralek; Yuri Breitbart; Gerhard Weikum,Abstract Networks of workstations are an emerging architectural paradigm for high-performance parallel and distributed systems. Exploiting networks of workstations formassive data management poses exciting challenges. We consider here the problem ofmanaging record-structured data in such an environment. For example; managingcollections of HTML documents on a cluster of WWW servers is an important application forwhich our approach provides support. The records are accessed by a dynamically growingset of clients based on a search key (eg; a URL). To scale up the throughput of clientaccesses with approximately constant response time; the records and thus also their accessload are dynamically redistributed across a growing set of workstations. The paperaddresses two problems of realistic workloads: skewed access frequencies to the records …,Distributed and Parallel Databases,1998,60
Searching RDF Graphs with SPARQL and Keywords.,Shady Elbassuoni; Maya Ramanath; Ralf Schenkel; Gerhard Weikum,Abstract The proliferation of knowledge-sharing communities like Wikipedia and theadvances in automated information extraction from Web pages enable the construction oflarge knowledge bases with facts about entities and their relationships. The facts can berepresented in the RDF data model; as so-called subject-property-object triples; and canthus be queried by structured query languages like SPARQL. In principle; this allows precisequerying in the database spirit. However; RDF data may be highly diverse and queries mayreturn way too many results; so that ranking by informativeness measures is crucial to avoidoverwhelming users. Moreover; as facts are extracted from textual contexts or havecommunity-provided annotations; it can be beneficial to consider also,IEEE Data Eng. Bull.,2010,59
P2P Content Search: Give the Web Back to the People.,Matthias Bender; Sebastian Michel; Peter Triantafillou; Gerhard Weikum; Christian Zimmer,ABSTRACT The proliferation of peer-to-peer (P2P) systems has come with variouscompelling applications including file sharing based on distributed hash tables (DHTs) orother kinds of overlay networks. Searching the content of files (especially Web Search)requires multi-keyword querying with scoring and ranking. Existing approaches have no wayof taking into account the correlation between the keywords in the query. This paperpresents our solution that incorporates the queries and behavior of the users in the P2Pnetwork such that interesting correlations can be inferred.,IPTPS,2006,59
Word sense disambiguation for exploiting hierarchical thesauri in text classification,Dimitrios Mavroeidis; George Tsatsaronis; Michalis Vazirgiannis; Martin Theobald; Gerhard Weikum,Abstract The introduction of hierarchical thesauri (HT) that contain significant semanticinformation; has led researchers to investigate their potential for improving performance ofthe text classification task; extending the traditional “bag of words” representation;incorporating syntactic and semantic relationships among words. In this paper we addressthis problem by proposing a Word Sense Disambiguation (WSD) approach based on theintuition that word proximity in the document implies proximity also in the HT graph. Weargue that the high precision exhibited by our WSD algorithm in various humanly-disambiguated benchmark datasets; is appropriate for the classification task. Moreover; wedefine a semantic kernel; based on the general concept of GVSM kernels; that captures thesemantic relations contained in the hierarchical thesaurus. Finally; we conduct …,European Conference on Principles of Data Mining and Knowledge Discovery,2005,59
The MINERVA Project: Database Selection in the Context of P2P Search.,Matthias Bender; Sebastian Michel; Gerhard Weikum; Christian Zimmer,Abstract: This paper presents the MINERVA project that protoypes a distributed searchengine based on P2P techniques. MINERVA is layered on top of a Chord-style overlaynetwork and uses a powerful crawling; indexing; and search engine on every autonomouspeer. We formalize our system model and identify the problem of efficiently selectingpromising peers for a query as a pivotal issue. We revisit existing approaches to thedatabase selection problem and adapt them to our system environment. Measurements areperformed to compare different selection strategies using real-world data. The experimentsshow significant performance differences between the strategies and prove the importanceof a judicious peer selection strategy. The experiments also present first evidence that asmall number of carefully selected peers already provide the vast majority of all relevant …,BTW,2005,58
T-rank: Time-aware authority ranking,Klaus Berberich; Michalis Vazirgiannis; Gerhard Weikum,Abstract Analyzing the link structure of the web for deriving a page's authority and impliedimportance has deeply affected the way information providers create and link content; theranking in web search engines; and the users' access behavior. Due to the enormousdynamics of the web; with millions of pages created; updated; deleted; and linked to everyday; timeliness of web pages and links is a crucial factor for their evaluation. Users areinterested in important pages (ie; pages with high authority score) but are equally interestedin the recency of information. Time–and thus the freshness of web content and link structure–emanates as a factor that should be taken into account in link analysis when computing theimportance of a page. So far only minor effort has been spent on the integration of temporalaspects into link analysis techniques. In this paper we introduce T-Rank; a link analysis …,International Workshop on Algorithms and Models for the Web-Graph,2004,58
Recovery guarantees for internet applications,Roger Barga; David Lomet; German Shegalov; Gerhard Weikum,Abstract Internet-based e-services require application developers to deal explicitly withfailures of the underlying software components; for example web servers; servlets; browsersessions; and so forth. This complicates application programming; and may expose failuresto end users. This paper presents a framework for an application-independent infrastructurethat provides recovery guarantees and masks almost all system failures; thus relieving theapplication programmer from having to deal with these failures---by making applications"stateless." The main concept is an interaction contract between two components regardingmessage and state preservation. The framework provides comprehensive recoveryencompassing data; messages; and the states of application components. We describetechniques to reduce logging cost; allow effective log truncation; and permit independent …,ACM Transactions on Internet Technology (TOIT),2004,58
People on drugs: credibility of user statements in health communities,Subhabrata Mukherjee; Gerhard Weikum; Cristian Danescu-Niculescu-Mizil,Abstract Online health communities are a valuable source of information for patients andphysicians. However; such user-generated resources are often plagued by inaccuracies andmisinformation. In this work we propose a method for automatically establishing thecredibility of user-generated medical statements and the trustworthiness of their authors byexploiting linguistic cues and distant supervision from expert sources. To this end weintroduce a probabilistic graphical model that jointly learns user trustworthiness; statementcredibility; and language objectivity. We apply this methodology to the task of extracting rareor unknown side-effects of medical drugs---this being one of the problems where large scalenon-expert data has the potential to complement expert medical knowledge. We show thatour method can reliably extract side-effects and filter out false statements; while …,Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,2014,57
Gathering and ranking photos of named entities with high precision; high recall; and diversity,Bilyana Taneva; Mouna Kacimi; Gerhard Weikum,Abstract Knowledge-sharing communities like Wikipedia and automated extraction methodslike those of DBpedia enable the construction of large machine-processible knowledgebases with relational facts about entities. These endeavors lack multimodal data like photosand videos of people and places. While photos of famous entities are abundant on theInternet; they are much harder to retrieve for less popular entities such as notable computerscientists or regionally interesting churches. Querying the entity names in image searchengines yields large candidate lists; but they often have low precision and unsatisfactoryrecall. Our goal is to populate a knowledge base with photos of named entities; with highprecision; high recall; and diversity of photos for a given entity. We harness relational factsabout entities for generating expanded queries to retrieve different candidate lists from …,Proceedings of the third ACM international conference on Web search and data mining,2010,55
Time-aware authority ranking,Klaus Berberich; Michalis Vazirgiannis; Gerhard Weikum,The link structure of the web is analyzed to measure the authority of pages; which can betaken into account for ranking query results. Due to the enormous dynamics of the web; withmillions of pages created; updated; deleted; and linked to every day; temporal aspects ofweb pages and links are crucial factors for their evaluation. Users are interested in importantpages (ie; pages with high authority score) but are equally interested in the recency ofinformation. Time—and thus the freshness of web content and link structure—emanates as afactor that should be taken into account in link analysis when computing the importance of apage. So far only minor effort has been spent on the integration of temporal aspects into link-analysis techniques. In this paper we introduce T-Rank Light and T-Rank; two link-analysisapproaches that take into account the temporal aspects freshness (ie; timestamps of most …,Internet Mathematics,2005,55
Robust question answering over the web of linked data,Mohamed Yahya; Klaus Berberich; Shady Elbassuoni; Gerhard Weikum,Abstract Knowledge bases and the Web of Linked Data have become important assets forsearch; recommendation; and analytics. Natural-language questions are a user-friendlymode of tapping this wealth of knowledge and data. However; question answeringtechnology does not work robustly in this setting as questions have to be translated intostructured queries and users have to be careful in phrasing their questions. This paperadvocates a new approach that allows questions to be partially translated into relaxedqueries; covering the essential but not necessarily all aspects of the user's input. Tocompensate for the omissions; we exploit textual sources associated with entities andrelational facts. Our system translates user questions into an extended form of structuredSPARQL queries; with text predicates attached to triple patterns. Our solution is based on …,Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,2013,54
Webchild: Harvesting and organizing commonsense knowledge from the web,Niket Tandon; Gerard de Melo; Fabian Suchanek; Gerhard Weikum,Abstract This paper presents a method for automatically constructing a large commonsenseknowledge base; called WebChild; from Web contents. WebChild contains triples thatconnect nouns with adjectives via fine-grained relations like hasShape; hasTaste;evokesEmotion; etc. The arguments of these assertions; nouns and adjectives; aredisambiguated by mapping them onto their proper WordNet senses. Our method is based onsemi-supervised Label Propagation over graphs of noisy candidate assertions. Weautomatically derive seeds from WordNet and by pattern matching from Web text collections.The Label Propagation algorithm provides us with domain sets and range sets for 19different relations; and with confidence-ranked assertions between WordNet senses. Large-scale experiments demonstrate the high accuracy (more than 80 percent) and coverage …,Proceedings of the 7th ACM international conference on Web search and data mining,2014,53
Knowledge harvesting in the big-data era,Fabian Suchanek; Gerhard Weikum,Abstract The proliferation of knowledge-sharing communities such as Wikipedia and theprogress in scalable information extraction from Web and text sources have enabled theautomatic construction of very large knowledge bases. Endeavors of this kind includeprojects such as DBpedia; Freebase; KnowItAll; ReadTheWeb; and YAGO. These projectsprovide automatically constructed knowledge bases of facts about named entities; theirsemantic classes; and their mutual relationships. They contain millions of entities andhundreds of millions of facts about them. Such world knowledge in turn enables cognitiveapplications and knowledge-centric services like disambiguating natural-language text;semantic search for entities and relations in Web and enterprise data; and entity-orientedanalytics over unstructured contents. Prominent examples of how knowledge bases can …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,53
Hyena: Hierarchical type classification for entity names,Mohamed Amir Yosef; Sandro Bauer; Johannes Hoffart; Marc Spaniol; Gerhard Weikum,ABSTRACT Inferring lexical type labels for entity mentions in texts is an important asset forNLP tasks like semantic role labeling and named entity disambiguation (NED). Prior workhas focused on flat and relatively small type systems where most entities belong to exactlyone type. This paper addresses very fine-grained types organized in a hierarchicaltaxonomy; with several hundreds of types at different levels. We present HYENA for multi-label hierarchical classification. HYENA exploits gazetteer features and accounts for the jointevidence for types at different levels. Experiments and an extrinsic study on NEDdemonstrate the practical viability of HYENA.,Proceedings of COLING 2012: Posters,2012,53
EnBlogue: emergent topic detection in web 2.0 streams,Foteini Alvanaki; Michel Sebastian; Krithi Ramamritham; Gerhard Weikum,Abstract Emergent topics are newly arising themes in news; blogs; or tweets; often impliedby interesting and unexpected correlations of tags or entities. We present the enBloguesystem for emergent topic detection. The name enBlogue reflects the analogy with emergingtrends in fashion often referred to as en Vogue. EnBlogue continuously monitors Web 2.0streams and keeps track of sudden changes in tag correlations which can be adjusted usingpersonalization to reflect particular user interests. We demonstrate enBlogue with severalreal-time monitoring scenarios as well as with time lapse on archived data.,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,53
Ferrari: Flexible and efficient reachability range assignment for graph indexing,Stephan Seufert; Avishek Anand; Srikanta Bedathur; Gerhard Weikum,In this paper; we propose a scalable and highly efficient index structure for the reachabilityproblem over graphs. We build on the well-known node interval labeling scheme where theset of vertices reachable from a particular node is compactly encoded as a collection of nodeidentifier ranges. We impose an explicit bound on the size of the index and flexibly assignapproximate reachability ranges to nodes of the graph such that the number of index probesto answer a query is minimized. The resulting tunable index structure generates a betterrange labeling if the space budget is increased; thus providing a direct control over the tradeoff between index size and the query processing performance. By using a fast recursivequerying method in conjunction with our index structure; we show that; in practice;reachability queries can be answered in the order of microseconds on an off-the-shelf …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,52
Fast logistic regression for text categorization with variable-length n-grams,Georgiana Ifrim; Gökhan Bakir; Gerhard Weikum,Abstract A common representation used in text categorization is the bag of words model(aka. unigram model). Learning with this particular representation involves typically somepreprocessing; eg stopwords-removal; stemming. This results in one explicit tokenization ofthe corpus. In this work; we introduce a logistic regression approach where learning involvesautomatic tokenization. This allows us to weaken the a-priori required knowledge about thecorpus and results in a tokenization with variable-length (word or character) n-grams asbasic tokens. We accomplish this by solving logistic regression using gradient ascent in thespace of all ngrams. We show that this can be done very efficiently using a branch andbound approach which chooses the maximum gradient ascent direction projected onto asingle dimension (ie; candidate feature). Although the space is very large; our method …,Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,2008,52
Discovering and exploiting keyword and attribute-value co-occurrences to improve P2P routing indices,Sebastian Michel; Matthias Bender; Nikos Ntarmos; Peter Triantafillou; Gerhard Weikum; Christian Zimmer,Abstract Peer-to-Peer (P2P) search requires intelligent decisions for query routing: selectingthe best peers to which a given query; initiated at some peer; should be forwarded forretrieving additional search results. These decisions are based on statistical summaries foreach peer; which are usually organized on a per-keyword basis and managed in adistributed directory of routing indices. Such architectures disregard the possiblecorrelations among keywords. Together with the coarse granularity of per-peer summaries;which are mandated for scalability; this limitation may lead to poor search result quality. Thispaper develops and evaluates two solutions to this problem; sk-STAT based on single-keystatistics only; and mk-STAT based on additional multi-key statistics. For both cases; hashsketch synopses are used to compactly represent a peer's data items and are efficiently …,Proceedings of the 15th ACM international conference on Information and knowledge management,2006,52
Efficient and decentralized pagerank approximation in a peer-to-peer web search network,Josiane Xavier Parreira; Debora Donato; Sebastian Michel; Gerhard Weikum,Abstract PageRank-style (PR) link analyses are a cornerstone of Web search engines andWeb mining; but they are computationally expensive. Recently; various techniques havebeen proposed for speeding up these analyses by distributing the link graph among multiplesites. However; none of these advanced methods is suitable for a fully decentralized PRcomputation in a peer-to-peer (P2P) network with autonomous peers; where each peer canindependently crawl Web fragments according to the user's thematic interests. In such asetting the graph fragments that different peers have locally available or know about mayarbitrarily overlap among peers; creating additional complexity for the PR computation. Thispaper presents the JXP algorithm for dynamically and collaboratively computing PR scoresof Web pages that are arbitrarily distributed in a P2P network. The algorithm runs at every …,Proceedings of the 32nd international conference on Very large data bases,2006,52
Towards guaranteed quality and dependability of information services,Gerhard Weikum,Abstract The impressive advances in global networking and information technology providegreat opportunities for all kinds of ubiquitous information services; ranging from digitallibraries and information discovery to virtual-enterprise workflows and electronic commerce.However; many of these services too often exhibit rather poor quality and are thusunsuitable for mission-critical applications. In this paper I would like to encourage moreintensive research efforts towards service quality guarantees; the ultimate goal being theability to construct and deploy truly dependable systems with provable correctness;continuous availability; and predictable performance. The paper aims to sort out some of theissues towards these elusive goals; mainly by discussing a case study on workflowmanagement. I will point out various assets that can be leveraged; most notably; from …,*,1999,52
Disk scheduling for mixed-media workloads in a multimedia server,Y Rompogiannakis; Guido Nerjes; Peter Muth; Michael Paterakis; Peter Triantafillou; Gerhard Weikum,310st mutimedia appfic~. ons require storage and rm” ev~ of hge amounts of continuous andficrete data at v~ l high rat~ Disk driv= should be servicing such tid workloads achieving lowrqonse times for discrete requests; while guaruntea” ng the uninterrupted delivery of coIti.nuous data Disk scheduling algorithms for J- workloa&; &hough they play a central role inthis task have been overlooked by related mutimedia r~ earch efforts; which so far haveJmstfy concentrated on the scheduling of coI*” nuous requ- 03dv.~ e focus of this paper is one~ iti disk WO scheduling algorithms for _ workbads in a mutimedia storage server. Wepropose novel algorithms; a tonomy of r~~ mzt algorithms; and stud~'their p~ ormancethrough~ m. mentatiom Our resuti show that our proposed dgo~ hms offer drasticimprovements in discrete request average response times; low r~ onse-time vm” abifity …,Proceedings of the sixth ACM international conference on Multimedia,1998,52
Stochastic service guarantees for continuous data on multi-zone disks,Guido Nerjes; Peter Muth; Gerhard Weikum,Abstract Continuous data types like video and audio require the real-time delivery of datafragments fmm a server's disks to the client at which the data is displayed. This paperdevelops a stochastic model for analyzing the rate at which data fragments arrive too late atthe client and thus cause display “glitches”. The model is based on deriving the Laplace-Stieltjes transform of the service time distribution for batched disk service under a multi-userload of concurrently served continuous-data streams; and applying Chemoff bounds to thetail of the service time distribution and the resulting distribution of the glitch rate per stream.The results from the model provide the basis for configuring a server and exerting anadmission contxol such that the admitted streams suffer no mom than a specified (small) rateof glitches with a specified (very high) probability. The model cunsiders variable display …,Proceedings of the sixteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1997,51
Semantics-based multilevel transaction management in federated systems,Andrew Deacon; H-J Schek; Gerhard Weikum,A federated database management system (FDBMS) is a special type of distributeddatabase system that enables existing local databases; in a heterogeneous environment; tomaintain a high degree of autonomy. One of the key problems in this setting is thecoexistence of local transactions and global transactions; where the latter access andmanipulate data of multiple local databases. In modeling FDBMS transaction executions theauthors propose a more realistic model than the traditional read/write model; in their model alocal database exports high-level operations which are the only operations distributedglobal transactions can execute to access data in the shared local databases. Suchrestrictions are not unusual in practice as; for example; no airline or bank would ever permitforeign users to execute ad hoc queries against their databases for fear of compromising …,Data Engineering; 1994. Proceedings. 10th International Conference,1994,51
Web search clickstreams,Nils Kammenhuber; Julia Luxenburger; Anja Feldmann; Gerhard Weikum,Abstract Search engines are a vital part of the Web and thus the Internet infrastructure.Therefore understanding the behavior of users searching the Web gives insights into trends;and enables enhancements of future search capabilities. Possible data sources for studyingWeb search behavior are either server-side logs or client-side logs. Unfortunately; currentserver-side logs are hard to obtain as they are considered proprietary by the search engineoperators. Therefore we in this paper present a methodology for extracting client-side logsfrom the traffic exchanged between a large user group and the Internet. The added benefit ofour methodology is that we do not only extract the search terms; the query sequences; andsearch results of each individual user but also the full clickstream; ie; the result pages usersview and the subsequently visited hyperlinked pages. We propose a finite-state Markov …,Proceedings of the 6th ACM SIGCOMM conference on Internet measurement,2006,50
Integrated document caching and prefetching in storage hierarchies based on Markov-chain predictions,Achim Kraiss; Gerhard Weikum,Abstract. Large multimedia document archives may hold a major fraction of their data intertiary storage libraries for cost reasons. This paper develops an integrated approach to thevertical data migration between the tertiary; secondary; and primary storage in that itreconciles speculative prefetching; to mask the high latency of the tertiary storage; with thereplacement policy of the document caches at the secondary and primary storage level; andalso considers the interaction of these policies with the tertiary and secondary storagerequest scheduling. The integrated migration policy is based on a continuous-time Markovchain model for predicting the expected number of accesses to a document within aspecified time horizon. Prefetching is initiated only if that expectation is higher than those ofthe documents that need to be dropped from secondary storage to free up the necessary …,The VLDB Journal,1998,50
Set-oriented disk access to large complex objects,Gerhard Weikum,The use of set-oriented disk access whereby a variable-sized set of pages can be fetched orflushed to disk in a single call to the I/O system is proposed. This solution provides fastaccess to variable-length complex objects; yet retains the advantages of a page-structuredbuffer pool with a conventional frame size. A set-oriented I/O manager has beenimplemented in the Darmstadt database kernel system using the data-chained I/O method.Performance management indicate considerable enhancement of throughput as well asresponse time. In the experiments; set-oriented disk access for very large objects performedup to 25 times faster than conventional I/O.,Data Engineering; 1989. Proceedings. Fifth International Conference on,1989,50
Longitudinal analytics on web archive data: it's about time!,Gerhard Weikum; Nikos Ntarmos; Marc Spaniol; Peter Triantafillou; András A Benczúr; Scott Kirkpatrick; Philippe Rigaux; Mark Williamson,ABSTRACT Organizations like the Internet Archive have been capturing Web contents overdecades; building up huge repositories of time-versioned pages. The timestamp annotationsand the sheer volume of multi-modal content constitutes a gold mine for analysts of all sorts;across different application areas; from political analysts and marketing agencies toacademic researchers and product developers. In contrast to traditional data analytics onclick logs; the focus is on longitudinal studies over very long horizons. This longitudinalaspect affects and concerns all data and metadata; from the content itself; to the indices andthe statistical metadata maintained for it. Moreover; advanced analysts prefer to deal withsemantically rich entities like people; places; organizations; and ideally relationships suchas company acquisitions; instead of; say; Web pages containing such references. For …,CIDR,2011,49
Social Wisdom for Search and Recommendation.,Ralf Schenkel; Tom Crecelius; Mouna Kacimi; Thomas Neumann; Josiane Xavier Parreira; Marc Spaniol; Gerhard Weikum,Abstract Social-tagging communities offer great potential for smart recommendation and“socially enhanced” searchresult ranking. Beyond traditional forms of collaborativerecommendation that are based on the item-user matrix of the entire community; a specificopportunity of social communities is to reflect the different degrees of friendships and mutualtrust; in addition to the behavioral similarities among users. This paper presents a frameworkfor harnessing such social relations for search and recommendation. The framework isimplemented in the SENSE prototype system; and its usefulness is demonstrated inexperiments with an excerpt of the librarything community data.,IEEE Data Eng. Bull.,2008,49
Peer-to-peer information search: Semantic; social; or spiritual?,Matthias Bender; Tom Crecelius; Mouna Kacimi; Sebastian Michel; Josiane Xavier Parreira; Gerhard Weikum,Abstract We consider the network structure and query processing capabilities of socialcommunities like bookmarks and photo sharing communities such as del. icio. us or flickr. Acommon feature of all these networks is that the content is generated by the users and thatusers create social links with other users. The evolving network naturally resembles a peer-to-peer system; where the peers correspond to users. We consider the problem of queryrouting in such a peer-to-peer setting where peers are collaborating to form a distributedsearch engine. We have identified three query routing paradigms: semantic routing basedon query-to-content similarities; social routing based on friendship links within thecommunity; and spiritual routing based on user-to-user similarities such as shared interestsor similar behavior. We discuss how these techniques can be integrated into an existing …,IEEE Data Eng. Bull.,2007,49
The mentor architecture for enterprise-wide workflow management,Jeanine Weissenfels; Dirk Wodtke; Gerhard Weikum; Angelika Kotz Dittrich,*,Sh96,1996,49
Conflict-driven load control for the avoidance of data-contention thrashing,Axel Mönkeberg; Gerhard Weikum,A conflict-driven approach to automatic load control is presented. Various definitions ofconflict rate are investigated as to whether they are suitable as a control metric. Evidence isprovided that there exists at least one suitable metric and a single value; called the criticalconflict rate; that indicates data-contention (DC) thrashing regardless of the number or typesof transactions in the system. Based on this observation; an algorithm is developed thatadmits new transactions and/or cancels running transactions depending on the currentconflict rate. The algorithm and its various substrategies for transaction admission andtransaction cancellation are evaluated under several sorts of overload situations. Simulationexperiments with this algorithm have shown fairly good results; ie DC thrashing wasprevented in overload situations without overly limiting the achievable throughput under …,Data Engineering; 1991. Proceedings. Seventh International Conference on,1991,49
Query relaxation for entity-relationship search,Shady Elbassuoni; Maya Ramanath; Gerhard Weikum,Abstract Entity-relationship-structured data is becoming more important on the Web. Forexample; large knowledge bases have been automatically constructed by informationextraction from Wikipedia and other Web sources. Entities and relationships can berepresented by subject-property-object triples in the RDF model; and can then be preciselysearched by structured query languages like SPARQL. Because of their Boolean-matchsemantics; such queries often return too few or even no results. To improve recall; it is thusdesirable to support users by automatically relaxing or reformulating queries in such a waythat the intention of the original user query is preserved while returning a sufficient numberof ranked results. In this paper we describe comprehensive methods to relax SPARQL-liketriple-pattern queries in a fully automated manner. Our framework produces a set of …,Extended Semantic Web Conference,2011,48
Workflow history management in virtual enterprises using a light-weight workflow management system,Peter Muth; Jeanine Weissenfels; Michael Gillmann; Gerhard Weikum,Enterprise-spanning workflows require workflow management systems that can be tailoredto specific application needs; as well as enhanced support for interoperability betweendifferent workflow management systems. In virtual enterprises; the interoperability problem isnot limited to workflow execution; but also requires facilities like worklist management andhistory management to be interoperable. We present a lightweight system architecture;consisting of a small system kernel; on top of which extensions such as history managementand worklist management are themselves implemented as workflows. The functionality ofthe kernel; such as distributed workflow execution and interoperability interfaces; isavailable for all extensions. We show the feasibility of our approach by presenting theimplementation of history management in our workflow specification language; based on …,Research Issues on Data Engineering: Information Technology for Virtual Enterprises; 1999. RIDE-VE'99. Proceedings.; Ninth International Workshop on,1999,48
Untangling the cross-lingual link structure of Wikipedia,Gerard de Melo; Gerhard Weikum,Abstract Wikipedia articles in different languages are connected by interwiki links that areincreasingly being recognized as a valuable source of cross-lingual information.Unfortunately; large numbers of links are imprecise or simply wrong. In this paper;techniques to detect such problems are identified. We formalize their removal as anoptimization task based on graph repair operations. We then present an algorithm withprovable properties that uses linear programming and a region growing technique to tacklethis challenge. This allows us to transform Wikipedia into a much more consistentmultilingual register of the world's entities and concepts.,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,2010,47
Semantic similarity search on semistructured data with the XXL search engine,Ralf Schenkel; Anja Theobald; Gerhard Weikum,Abstract Query languages for XML such as XPath or XQuery support Boolean retrieval: aquery result is a (possibly restructured) subset of XML elements or entire documents thatsatisfy the search conditions of the query. This search paradigm works for highly schematicXML data collections such as electronic catalogs. However; for searching information inopen environments such as the Web or intranets of large corporations; ranked retrieval ismore appropriate: a query result is a ranked list of XML elements in descending order of(estimated) relevance. Web search engines; which are based on the ranked retrievalparadigm; do; however; not consider the additional information and rich annotationsprovided by the structure of XML documents and their element names. This article presentsthe XXL search engine that supports relevance ranking on XML data. XXL is particularly …,Information Retrieval,2005,47
Federated transaction management with snapshot isolation,Ralf Schenkel; Gerhard Weikum; Norbert Weißenberg; Xuequn Wu,Abstract Federated transaction management (also known as multidatabase transactionmanagement in the literature) is needed to ensure the consistency of data that is distributedacross multiple; largely autonomous; and possibly heterogeneous component databasesand accessed by both global and local transactions. While the global atomicity of suchtransactions can be enforced by using a standardized commit protocol like XA or its CORBAcounterpart OTS; global serializability is not self-guaranteed as the underlying componentsystems may use a variety of potentially incompatible local concurrency control protocols.The problem of how to achieve global serializability; by either constraining the componentsystems or implementing additional global protocols at the federation level; has beenintensively studied in the literature; but did not have much impact on the practical side. A …,*,2000,47
Unifying concurrency control and recovery of transactions,Gustavo Alonso; Radek Vingralek; Divyakant Agrawal; Yuri Breitbart; Amr El Abbadi; Hans-J Schek; Gerhard Weikum,Abstract Transaction management in shared databases is generally viewed as acombination of two problems; concurrency control and recovery; which have beenconsidered as orthogonal problems. Consequently; the correctness criteria derived for theseproblems are incomparable. Recently a unified theory of concurrency control and recoveryhas been introduced that is based on commutativity and performs transaction recovery bysubmitting inverse operations for operations of aborted transactions. In this paper weprovide a constructive correctness criterion that leads to the design of unified protocols thatguarantee atomicity and serializability.,Information Systems,1994,47
Uniform object management,George Copeland; Michael Franklin; Gerhard Weikum,Abstract Most real-world applications require a capability for both general-purposeprogramming and database transactions on persistent data. Unfortunately; theimplementation techniques for these capabilities are notoriously incompatible. Programminglanguages stress memory-resident transient data with a rich collection of data types; whiledatabase systems stress disk-resident persistent data with a limited collection of data types.Even in object-oriented database systems; combining these capabilities is traditionally doneusing a two-level storage model in which storage formats are quite different. This approachsuffers from the performance overhead required to translate data between these two levels.This paper describes the steps we have taken toward improving the simplicity and efficiencyof applications by merging programming-language and database object management …,International Conference on Extending Database Technology,1990,47
Harvesting facts from textual web sources by constrained label propagation,Yafang Wang; Bin Yang; Lizhen Qu; Marc Spaniol; Gerhard Weikum,Abstract There have been major advances on automatically constructing large knowledgebases by extracting relational facts from Web and text sources. However; the world isdynamic: periodic events like sports competitions need to be interpreted with their respectivetimepoints; and facts such as coaching a sports team; holding political or business positions;and even marriages do not hold forever and should be augmented by their respectivetimespans. This paper addresses the problem of automatically harvesting temporal facts withsuch extended time-awareness. We employ pattern-based gathering techniques for factcandidates and construct a weighted pattern-candidate graph. Our key contribution is asystem called PRAVDA based on a new kind of label propagation algorithm with ajudiciously designed loss function; which iteratively processes the graph to label good …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,45
Data quality in web archiving,Marc Spaniol; Dimitar Denev; Arturas Mazeika; Gerhard Weikum; Pierre Senellart,Abstract Web archives preserve the history of Web sites and have high long-term value formedia and business analysts. Such archives are maintained by periodically re-crawlingentire Web sites of interest. From an archivist's point of view; the ideal case to ensure highestpossible data quality of the archive would be to" freeze" the complete contents of an entireWeb site during the time span of crawling and capturing the site. Of course; this is practicallyinfeasible. To comply with the politeness specification of a Web site; the crawler needs topause between subsequent http requests in order to avoid unduly high load on the site's httpserver. As a consequence; capturing a large Web site may span hours or even days; whichincreases the risk that contents collected so far are incoherent with the parts that are still tobe crawled. This paper introduces a model for identifying coherent sections of an archive …,Proceedings of the 3rd workshop on Information credibility on the web,2009,45
The XXL search engine: Ranked retrieval of XML data using indexes and ontologies,Anja Theobald; Gerhard Weikum,XML is becoming the standard for integrating and exchanging data over the Internet andwithin intranets; covering the complete spectrum from largely unstructured; ad hoedocuments to highly structured; schematic data. For searching information in openenvironments such as the Web or intranets of large corporations; ranked retrieval is moreappropriate: a query result is a rank list of XML elements in descending order of relevance.We have developed a core language; coined XXL for" flexible X ML search language"[1]; forranked retrieval of XML data using regular element path expressions and search conditionsover element contents. For similarity search we have introduced a new operator"~"; whichcan be used for both element content comparisons and approximate matching of elementnames. On the XML Shakespeare play collection; for example; we can search for scenes …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,45
Foundations of Automated Database Tuning.,Surajit Chaudhuri; Gerhard Weikum,But cache bookkeeping has time and space overhead:• O (log M) time for priority queuemaintenance• M*> M entries in cache directory to remember k last accesses to M* pages+overhead acceptable for improved cache hit rate+ add 'l bookkeeping memory is small anduncritical to tune→ improved implementations: 2Q; ARC,ICDE,2006,44
What workflow technology can do for electronic commerce,Peter Muth; Jeanine Weissenfels; Gerhard Weikum,Abstract Electronic Commerce (EC) is a rapidly growing research and development area ofvery high practical relevance. A major challenge in successfully designing EC applicationsis to identify existing building-block technologies and integrate them into a commonapplication framework. We argue that workflow management technology should be a keybuilding block for EC applications. Workflow technology aims to provide as much computersupport as possible to the modeling; execution; supervision; and possibly reengineering ofbusiness processes. Hence; it is beneficial to model major parts of EC applications also asworkflows. We identify properties and requirements of workflows specific to EC; anddescribe solutions which can already be provided by current workflow managementtechnology. In addition; we point out necessary extensions to the state of the art of …,Proceedings of the EURO-MED NET Conference,1998,44
Adaptive load balancing in disk arrays,Peter Scheuermann; Gerhard Weikum; Peter Zabback,Abstract Large arrays of small disks axe providing an attractive approach for highperformance I/O systems. In order to make effective use of disk arrays and other multi-diskarchitectures; it is necessary to develop intelligent software tools that allow automatic tuningof the disk arrays to varying workloads. In this paper we describe an adaptive method fordata allocation and load balancing in disk arrays. Our method deals with dynamicallychanging access frequencies of files by reallocating file extents; thus” cooling down” hotdisks. In addition; the method takes into account the fact that some files may exhibitperiodical access patterns; and considers explicitly the cost of performing the” cooling”operations. Preliminary performance studies based on real-life I/O traces demonstrate theeffectivity of this approach.,International Conference on Foundations of Data Organization and Algorithms,1993,44
Active knowledge: dynamically enriching RDF knowledge bases by web services,Nicoleta Preda; Gjergji Kasneci; Fabian M Suchanek; Thomas Neumann; Wenjun Yuan; Gerhard Weikum,Abstract The proliferation of knowledge-sharing communities and the advances ininformation extraction have enabled the construction of large knowledge bases using theRDF data model to represent entities and relationships. However; as the Web and its latentlyembedded facts evolve; a knowledge base can never be complete and up-to-date. On theother hand; a rapidly increasing suite of Web services provide access to timely and high-quality information; but this is encapsulated by the service interface. We propose to leveragethe information that could be dynamically obtained from Web services in order to enrich RDFknowledge bases on the fly whenever the knowledge base does not suffice to answer a userquery. To this end; we develop a sound framework for appropriately generating queries toencapsulated Web services and efficient algorithms for query execution and result …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,43
Matching task profiles and user needs in personalized web search,Julia Luxenburger; Shady Elbassuoni; Gerhard Weikum,Abstract Personalization has been deemed one of the major challenges in informationretrieval with a significant potential for providing better search experience to individualusers. Especially; the need for enhanced user models better capturing elements such asusers' goals; tasks; and contexts has been identified. In this paper; we introduce a statisticallanguage model for user tasks representing different granularity levels of a user profile;ranging from very specific search goals to broad topics. We propose a personalizationframework that selectively matches the actual user information need with relevant past usertasks; and allows to dynamically switch the course of personalization from re-finding veryprecise information to biasing results to general user interests. In the extreme; our model isable to detect when the user's search and browse history is not appropriate for aiding the …,Proceedings of the 17th ACM conference on Information and knowledge management,2008,43
Performance and availability assessment for the configuration of distributed workflow management systems,Michael Gillmann; Jeanine Weissenfels; Gerhard Weikum; Achim Kraiss,Abstract Workflow management systems (WFMSs) that are geared for the orchestration ofenterprise-wide or even “virtual-enterprise”-style business processes across multipleorganizations are complex distributed systems. They consist of multiple workflow engines;application servers; and ORB-style communication servers. Thus; deriving a suitableconfiguration of an entire distributed WFMS for a given application workload is a difficulttask. This paper presents a mathematically based method for configuring a distributedWFMS such that the application's demands regarding performance and availability can bemet while aiming to minimize the total system costs. The major degree of freedom that theconfiguration method considers is the replication of the underlying software components;workflow engines and application servers of different types as well as the communication …,International Conference on Extending Database Technology,2000,43
A decidability result about sufficient-completeness of axiomatically specified abstract data types,Tobias Nipkow; Gerhard Weikum,Abstract The problem of deciding whether an axiomatic specification of an abstract data typeis sufficiently-complete is known to be in general unsolvable. Regarding axioms as directedrewrite rules instead of symmetric equations a specification defines a reduction relation onterms. It is proved that in the subclass of left-linear axiomatic specifications the property ofsufficient-completeness is decidable; if the corresponding reduction relation is normalizingand confluent. The presented algorithm can also be used to determine a set of constructorsfor a specified data type.,*,1982,43
TopX and XXL at INEX 2005,Martin Theobald; Ralf Schenkel; Gerhard Weikum,Abstract We participated with two different and independent search engines in this year'sINEX round: The XXL Search Engine and the TopX engine. As this is the first participation forTopX; this paper focuses on the design principles; scoring; query evaluation and results ofTopX. We shortly discuss the results with XXL afterwards.,International Workshop of the Initiative for the Evaluation of XML Retrieval,2005,42
Bookmark-driven Query Routing in Peer-to-Peer Web Search.,Matthias Bender; Sebastian Michel; Gerhard Weikum; Christian Zimmer,Abstract: We consider the problem of collaborative Web search and query routing strategiesin a peer-to-peer (P2P) environment. In our architecture every peer has a full-fledged searchengine with a (thematically focused) crawler and a local index whose contents may betailored to the user's specific interest profile. Peers are autonomous and post meta-information about their bookmarks and index lists to a global directory; which is efficientlyimplemented in a decentralized manner using Chordstyle distributed hash tables. A queryposed by one peer is first evaluated locally; if the result is unsatisfactory the query isforwarded to selected peers. These peers are chosen based on a benefit/cost measurewhere benefit reflects the thematic similarity of peers' interest profiles; derived frombookmarks; and cost captures estimated peer load and response time. The meta …,Workshop on Peer-to-Peer Information Retrieval,2004,42
Counting at large: Efficient cardinality estimation in internet-scale data networks,Nikos Ntarmos; Peter Triantafillou; Gerhard Weikum,Counting in general; and estimating the cardinality of (multi-) sets in particular; is highlydesirable for a large variety of applications; representing a foundational block for the efficientdeployment and access of emerging internetscale information systems. Examples of suchapplications range from optimizing query access plans in internet-scale databases; toevaluating the significance (rank/score) of various data items in information retrievalapplications. The key constraints that any acceptable solution must satisfy are:(i) efficiency:the number of nodes that need be contacted for counting purposes must be small in order toenjoy small latency and bandwidth requirements;(ii) scalability; seemingly contradicting theefficiency goal: arbitrarily large numbers of nodes nay need to add elements to a (multi-) set;which dictates the need for a highly distributed solution; avoiding server-based scalability …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,41
Integrating light-weight workflow management systems within existing business environments,Peter Muth; Jeanine Weissenfels; Michael Gillmann; Gerhard Weikum,Workflow management systems (WfMSs) support the efficient; largely automated executionof business processes. However; using a WfMS typically requires implementing theapplication's control flow exclusively by the WfMS. This approach is powerful if the controlflow is specified and implemented from scratch; but it has severe drawbacks if a WfMS is tobe integrated within environments with existing solutions for implementing control flow.Usually; the existing solutions are too complex to be substituted by the WfMS all at once.Hence; the WfMS must support an incremental integration; ie the reuse of existingimplementations of control flow as well as their incremental substitution. Extending theWfMS's functionality according to future application needs; eg by worklist and historymanagement; must also be possible. In particular; at the beginning of an incremental …,Data Engineering; 1999. Proceedings.; 15th International Conference on,1999,41
Multi-level transactions and open nested transactions,Gerhard Weikum; Hans-J Schek,*,Data Engineering,1991,41
Design; implementation; and performance of the LHAM log-structured history data access method,Peter Muth; Patrick O'neil; Achim Pick; Gerhard Weikum,Abstract Numerous applications such as stock market or medical information systemsrequire that both historical and current data be logically integrated into a temporal database.The underlying access method must support different forms of “time-travel” queries; themigration of old record versions onto inexpensive archive media; and high insert and updaterates. This paper introduces a new access method for transaction-time temporal data; calledthe Logstructured History Data Access Method (LHAM) that meets these demands. Thebasic principle of LHAM is to partition the data into successive components based on thetimestamps of the record versions. Components are assigned to different levels of a storagehierarchy; and incoming data is continuously migrated through the hierarchy. The paperdiscusses the LHAM concepts; including concurrency control and recovery; our full …,VLDB,1998,40
Disk cooling in parallel disk systems,Gerhard Weikum; Peter Zabback; P Scheuermann,Abstract Parallel disk systems provide opportunities for high performance I/O by supportingefficiently intra-request and inter-request parallelism. We review briefly the components ofan intelligent file manager that performs striping on an individual file basis and achievesload balancing by judicious file allocation and dynamic redistribution of the data. The mainpart of the paper discusses our" disk cooling" procedure for dynamic redistribution of datawhich is based on reallocation of file fragments. We show that this heuristic method achievesexcellent load balance in the presence of evolving access patterns. We report on two sets ofexperiments: a synthetic experiment which exhibits a self-similar skew in the data accesspatterns and a trace-based experiment where we study the impact of the file fragment sizeon the cooling procedure. 1 Introduction Parallel disk systems are of great importance to …,Institute of Electrical and Electronics Engineers Data Engineering Bulletin,1994,40
Multi-level transaction management for complex objects: Implementation; performance; parallelism,Gerhard Weikum; Christof Hasse,Abstract Multi-level transactions are a variant of open-nested transactions in which thesubtransactions correspond to operations at different levels of a layered system architecture.They allow the exploitation of semantics of high-level operations to increase concurrency. Asa consequence; undoing a transaction requires compensation of completedsubtransactions. In addition; multi-level recovery methods must take into consideration thathigh-level operations are not necessarily atomic if multiple pages are updated in a singlesubtransaction. This article presents algorithms for multi-level transaction management thatare implemented in the database kernel system (DASDBS). In particular; we show that multi-level recovery can be implemented in an efficient way. We discuss performancemeasurements using a synthetic benchmark for processing complex objects in a multi …,The VLDB Journal—The International Journal on Very Large Data Bases,1993,40
Minerva∞: A scalable efficient peer-to-peer search engine,Sebastian Michel; Peter Triantafillou; Gerhard Weikum,Abstract The promises inherent in users coming together to form data sharing networkcommunities; bring to the foreground new problems formulated over such dynamic; evergrowing; computing; storage; and networking infrastructures. A key open challenge is toharness these highly distributed resources toward the development of an ultra scalable;efficient search engine. From a technical viewpoint; any acceptable solution must fullyexploit all available resources dictating the removal of any centralized points of control;which can also readily lead to performance bottlenecks and reliability/availability problems.Equally importantly; however; a highly distributed solution can also facilitate pluralism ininforming users about internet content; which is crucial in order to preclude the formation ofinformation-resource monopolies and the biased visibility of content from economically …,Proceedings of the ACM/IFIP/USENIX 2005 International Conference on Middleware,2005,39
Extraction of temporal facts and events from Wikipedia,Erdal Kuzey; Gerhard Weikum,Abstract Recently; large-scale knowledge bases have been constructed by automaticallyextracting relational facts from text. Unfortunately; most of the current knowledge bases focuson static facts and ignore the temporal dimension. However; the vast majority of facts areevolving with time or are valid only during a particular time period. Thus; time is a significantdimension that should be included in knowledge bases. In this paper; we introduce acomplete information extraction framework that harvests temporal facts and events from semi-structured data and free text of Wikipedia articles to create a temporal ontology. First; weextend a temporal data representation model by making it aware of events. Second; wedevelop an information extraction method which harvests temporal facts and events fromWikipedia infoboxes; categories; lists; and article titles in order to build a temporal …,Proceedings of the 2nd Temporal Web Analytics Workshop,2012,38
Deep answers for naturally asked questions on the web of data,Mohamed Yahya; Klaus Berberich; Shady Elbassuoni; Maya Ramanath; Volker Tresp; Gerhard Weikum,Abstract We present DEANNA; a framework for natural language question answering overstructured knowledge bases. Given a natural language question; DEANNA translatesquestions into a structured SPARQL query that can be evaluated over knowledge basessuch as Yago; Dbpedia; Freebase; or other Linked Data sources. DEANNA analyzesquestions and maps verbal phrases to relations and noun phrases to either individualentities or semantic classes. Importantly; it judiciously generates variables for target entitiesor classes to express joins between multiple triple patterns. We leverage the semantic typesystem for entities and use constraints in jointly mapping the constituents of the question torelations; classes; and entities. We demonstrate the capabilities and interface of DEANNA;which allows advanced users to influence the translation process and to see how the …,Proceedings of the 21st international conference on World Wide Web,2012,38
Fast Software Encryption: 16th International Workshop; FSE 2009 Leuven; Belgium; February 22-25; 2009 Revised Selected Papers,Jean-Philippe Aumasson; Itai Dinur; Willi Meier; Adi Shamir; Orr Dunkelman,*,*,2009,38
Iqn routing: Integrating quality and novelty in p2p querying and ranking,Sebastian Michel; Matthias Bender; Peter Triantafillou; Gerhard Weikum,Abstract We consider a collaboration of peers autonomously crawling the Web. A pivotalissue when designing a peer-to-peer (P2P) Web search engine in this environment is queryrouting: selecting a small subset of (a potentially very large number of relevant) peers tocontact to satisfy a keyword query. Existing approaches for query routing work well ondisjoint data sets. However; naturally; the peers' data collections often highly overlap; aspopular documents are highly crawled. Techniques for estimating the cardinality of theoverlap between sets; designed for and incorporated into information retrieval engines arevery much lacking. In this paper we present a comprehensive evaluation of appropriateoverlap estimators; showing how they can be incorporated into an efficient; iterativeapproach to query routing; coined Integrated Quality Novelty (IQN). We propose to further …,International Conference on Extending Database Technology,2006,38
A reproducible benchmark for p2p retrieval,Thomas Neumann; Matthias Bender; Sebastian Michel; Gerhard Weikum; Philippe Bonnet; Ioana Manolescu,Abstract With the growing popularity of information retrieval (IR) in distributed systems and inparticular {P2P} Web search; a huge number of protocols and prototypes have beenintroduced in the literature. However; nearly every paper considers a different benchmark forits experimental evaluation; rendering their mutual comparison and the quantification ofperformance improvements an impossible task. We present a standardized; generalpurpose benchmark for {P2P IR} systems that finally makes this possible. We start bypresenting a detailed requirement analysis for such a standardized benchmark frameworkthat allows for reproducible and comparable experimental setups without sacrificingflexibility to suit different system models. We further suggest Wikipedia as a publicly-available and all-purpose document corpus and finally introduce a simple but yet flexible …,Untitled Event,2006,38
Implementation and performance of multi-level transaction management in a multidatabase environment,Werner Schaad; H-J Schek; Gerhard Weikum,The 2PC (two-phase commit) protocol together with strict 2PL (two-phase locking) can beconsidered as a de-facto standard for distributed transaction processing. However; 2PC isoften unacceptable from a performance and execution-autonomy point of view. Multileveltransaction management offers an alternative solution that allows early commits ofsubtransactions and thus improves performance and execution autonomy. While the theoryof this approach is well established; a practical evaluation in a multi-database environmentis still lacking. This paper describes a prototype implementation and initial measurementsthat compare transaction processing with 2PC versus distributed multi-level transactionmanagement. Although we used SQL in our examples; the concepts presented areapplicable to any notion of distributed object management system.,Research Issues in Data Engineering; 1995: Distributed Object Management; Proceedings. RIDE-DOM'95. Fifth International Workshop on,1995,38
A multi-level transaction approach to federated dbms transaction management,H-J Schek; Gerhard Weikum; Werner Schaad,Transaction management in federated database management systems (FDBMS) iscompared with multi-level transaction management. An FDBMS has global and localtransaction management and can therefore be viewed as a multi-level system. In order tocope properly with local transactions; a dynamic conflict relation between globalsubtransactions is introduced. By exploiting the knowledge about the context of the conflictsbetween global subtransactions; the authors derive a weaker form of multi-levelserializability that is equivalent to quasi-serializability. The approach is further extended toexploit the semantics of the high-level operations that correspond to global subtransactions;even in the presence of local transactions. As a consequence; aborts of global transactionsmust be implemented by compensating subtransactions; and complete serializability is …,Interoperability in Multidatabase Systems; 1991. IMS'91. Proceedings.; First International Workshop on,1991,38
NAGA: harvesting; searching and ranking knowledge,Gjergji Kasneci; Fabian M Suchanek; Georgiana Ifrim; Shady Elbassuoni; Maya Ramanath; Gerhard Weikum,Abstract The presence of encyclopedic Web sources; such as Wikipedia; the Internet MovieDatabase (IMDB); World Factbook; etc. calls for new querying techniques that are simpleand yet more expressive than those provided by standard keyword-based search engines.Searching for explicit knowledge needs to consider inherent semantic structures involvingentities and relationships. In this demonstration proposal; we describe a semantic searchsystem named NAGA. NAGA operates on a knowledge graph; which contains millions ofentities and relationships derived from various encyclopedic Web sources; such as the onesabove. NAGA's graph-based query language is geared towards expressing queries withadditional semantic information. Its scoring model is based on the principles of generativelanguage models; and formalizes several desiderata such as confidence; informativeness …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,37
Bridging the Terminology Gap in Web Archive Search.,Klaus Berberich; Srikanta J Bedathur; Mauro Sozio; Gerhard Weikum,Page 1. Bridging the Terminology Gap in Web Archive Search Klaus Berberich; Srikanta Bedathur;Mauro Sozio; Gerhard Weikum Max-Planck Institute for Informatics; Saarbrücken; Germany Page2. Bridging the Terminology Gap in Web Archives (Klaus Berberich) ∎ http://www.liwa-project.eu ∎ European Union FP7 project that develops next generation web archiving technologiesPage 3. Bridging the Terminology Gap in Web Archives (Klaus Berberich) Web Archives ∎Archived contents increasingly made available on the Web – Web content increasingly archived ∎Web archives play an important role in providing access and preserving our cultural heritagehttp://archives.timesonline.co.uk Issues since 1785 digitized http://archive.org/web 150B webpages archived since 1996 Page 4. Bridging the Terminology Gap in Web Archives (KlausBerberich) What is the Terminology Gap? ∎ Terminology evolves constantly …,WebDB,2009,36
STICS: searching with strings; things; and cats,Johannes Hoffart; Dragan Milchevski; Gerhard Weikum,Abstract This paper describes an advanced search engine that supports users in queryingdocuments by means of keywords; entities; and categories. Users simply type words; whichare automatically mapped onto appropriate suggestions for entities and categories. Basedon named-entity disambiguation; the search engine returns documents containing thequery's entities and prominent entities from the query's categories.,Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval,2014,35
Language as a foundation of the Semantic Web,Gerard De Melo; Gerhard Weikum,ABSTRACT This paper aims to show how language-related knowledge may serve as afundamental building block for the Semantic Web. We present a system of URIs for terms;languages; scripts; and characters; which are not only highly interconnected but also linkedto a great variety of resources on the Web. Additional mapping heuristics may then be usedto derive new links.,Proceedings of the 2007 International Conference on Posters and Demonstrations-Volume 401,2008,35
Near-optimal dynamic replication in unstructured peer-to-peer networks,Mauro Sozio; Thomas Neumann; Gerhard Weikum,Abstract Replicating data in distributed systems is often needed for availability andperformance. In unstructured peer-to-peer networks; with epidemic messaging for queryrouting; replicating popular data items is also crucial to ensure high probability of finding thedata within a bounded search distance from the requestor. This paper considers suchnetworks and aims to maximize the probability of successful search. Prior work along theselines has analyzed the optimal degrees of replication for data items with non-uniform butglobal request rates; but did not address the issue of where replicas should be placed andwas very very limited in the capabilities for handling heterogeneity and dynamics of networkand workload. This paper presents the integrated P2R2 algorithm for dynamic replicationthat addresses all these issues; and determines both the degrees of replication and the …,Proceedings of the twenty-seventh ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2008,35
TOB: Timely Ontologies for Business Relations.,Qi Zhang; Fabian M Suchanek; Lihua Yue; Gerhard Weikum,ABSTRACT In this paper we present a suite of methods for extracting temporal relations fromsemi-structured and textual Web sources. We particularly address the needs for building andmaintaining business ontologies; where the time aspects of relations between companies;between companies and products; and between companies and customers are important.For example; the date on which a company acquired another company or when a new CEOtook over is crucial information for business-intelligence applications. Our methods aregeared for extracting business relations and their time information from three kinds ofsources: Wikipedia infoboxes; Reuter's news feeds; and news pages provided by Google.All techniques are integrated into the TOB framework for timely business ontologies. Ourexperiments show that we can achieve fairly high precision for the extracted information.,WebDB,2008,35
Query-log based authority analysis for web information search,Julia Luxenburger; Gerhard Weikum,Abstract The ongoing explosion of web information calls for more intelligent andpersonalized methods towards better search result quality for advanced queries. Query logsand click streams obtained from web browsers or search engines can contribute to betterquality by exploiting the collaborative recommendations that are implicitly embedded in thisinformation. This paper presents a new method that incorporates the notion of query nodesinto the PageRank model and integrates the implicit relevance feedback given by clickstreams into the automated process of authority analysis. This approach generalizes the well-known random-surfer model into a random-expert model that mimics the behavior of anexpert user in an extended session consisting of queries; query refinements; and result-navigation steps. The enhanced PageRank scores; coined QRank scores; can be …,International Conference on Web Information Systems Engineering,2004,35
Combining information extraction and human computing for crowdsourced knowledge acquisition,Sarath Kumar Kondreddi; Peter Triantafillou; Gerhard Weikum,Automatic information extraction (IE) enables the construction of very large knowledgebases (KBs); with relational facts on millions of entities from text corpora and Web sources.However; such KBs contain errors and they are far from being complete. This motivates theneed for exploiting human intelligence and knowledge using crowd-based humancomputing (HC) for assessing the validity of facts and for gathering additional knowledge.This paper presents a novel system architecture; called Higgins; which shows how toeffectively integrate an IE engine and a HC engine. Higgins generates game questionswhere players choose or fill in missing relations for subject-relation-object triples. Forgenerating multiple-choice answer candidates; we have constructed a large dictionary ofentity names and relational phrases; and have developed specifically designed statistical …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,34
Ranking database query results using probabilistic models from information retrieval,*,A system and methods rank results of database queries. An automated approach for rankingdatabase query results is disclosed that leverages data and workload statistics andassociations. Ranking functions are based upon the principles of probabilistic models fromInformation Retrieval that are adapted for structured data. The ranking functions areencoded into an intermediate knowledge representation layer. The system is generic; as theranking functions can be further customized for different applications. Benefits of thedisclosed system and methods include the use of adapted probabilistic information retrieval(PIR) techniques that leverage relational/structured data; such as columns; to providenatural groupings of data values. This permits the inference and use of pair-wiseassociations between data values across columns; which are usually not possible with …,*,2008,34
Global document frequency estimation in peer-to-peer web search,Matthias Bender; Sebastian Michel; Peter Triantafillou; Gerhard Weikum,This paper presents an efficient solution for the problem of estimating global documentfrequencies in a large-scale P2P network with very high dynamics where peers can join andleave the network on short notice. In particular; the developed method takes into account thefact that the local document collections of autonomous peers may arbitrarily overlap; so thatglobal counting needs to be duplicateinsensitive. The method is based on hash sketches asa technique for compact data synopses. Experimental studies demonstrate the estimator'saccuracy; scalability; and ability to cope with high dynamics. Moreover; the benefit forranking P2P search results is shown by experiments with real-world Web data and queries.,Proc. of the 9th Int. Workshop on the web and databases,2006,34
Database foundations for scalable RDF processing,Katja Hose; Ralf Schenkel; Martin Theobald; Gerhard Weikum,Abstract As more and more data is provided in RDF format; storing huge amounts of RDFdata and efficiently processing queries on such data is becoming increasingly important.The first part of the lecture will introduce state-of-the-art techniques for scalably storing andquerying RDF with relational systems; including alternatives for storing RDF; efficient indexstructures; and query optimization techniques. As centralized RDF repositories havelimitations in scalability and failure tolerance; decentralized architectures have beenproposed. The second part of the lecture will highlight system architectures and strategiesfor distributed RDF processing. We cover search engines as well as federated queryprocessing; highlight differences to classic federated database systems; and discussefficient techniques for distributed query processing in general and for RDF data in …,Proceedings of the 7th international conference on Reasoning web: semantic technologies for the web of data,2011,33
Combining text and linguistic document representations for authorship attribution,Andreas Kaster; Stefan Siersdorfer; Gerhard Weikum,ABSTRACT In this paper; we provide several alternatives to the classical Bag-Of-Wordsmodel for automatic authorship attribution. To this end; we consider linguistic and writingstyle information such as grammatical structures to construct different documentrepresentations. Furthermore we describe two techniques to combine the obtainedrepresentations: combination vectors and ensemble based meta classification. Ourexperiments show the viability of our approach.,SIGIR workshop: stylistic analysis of text for information access,2005,33
COMPASS: A concept-based Web search engine for HTML; XML; and deep Web data,Jens Graupmann; Michael Biwer; Christian Zimmer; Patrick Zimmer; Matthias Bender; Martin Theobald; Gerhard Weikum,Today's web search engines are still following the paradigm of keyword-based search.Although this is the best choice for large scale search engines in terms of throughput andscalability; it inherently limits the ability to accomplish more meaningful query tasks. XMLquery engines (eg; based on XQuery or XPath); on the other hand; have powerful querycapabilities; but at the same time their dedication to XML data with a global schema is theirweakness; because most web information is still stored in diverse formats and does notconform to common schemas. Typical web formats include static HTML pages or pages thatare generated dynamically from underlying database systems; accessible only throughportal interfaces. We have developed an expressive style of conceptbased and context-aware querying with relevance ranking that encompasses different; non-schematic data …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,33
Vertical data migration in large near-line document archives based on markov-chain predictions,Achim Kraiss; Gerhard Weikum,Abstract Large multimedia document archives hold most of their data in near-line tertiarystorage libraries for cost reasons. This paper develops an integrated approach to the verticaldata migration hetween the tertiary and secondary storage in that it reconciles speculativepreloading; to mask the high latency of the tertiary storage; with the replacement policy of thesecondary storage. In addition; it considers the interaction of these policies with the tertiarystorage scheduling and controls preloading aggressiveness by taking contention for tertiarystorage drives into account. The integrated migration policy is based on a continuous-timeMarkov-chain (CTMC) model; fijr predicting the expected number of accesses to a documentwithin a specified time horizon. The parameters of the CTMC model; the probabilities of co-accessing certain documents and the interaction times between successive accesses; are …,In Proceedings of 23rd International Conference on Very Large Data Bases,1997,32
Discovering and exploring relations on the web,Ndapandula Nakashole; Gerhard Weikum; Fabian Suchanek,Abstract We propose a demonstration of PATTY; a system for learning semanticrelationships from the Web. PATTY is a collection of relations learned automatically fromtext. It aims to be to patterns what WordNet is to words. The semantic types of PATTYrelations enable advanced search over subject-predicate-object data. With the ongoingtrends of enriching Web data (both text and tables) with entity-relationship-oriented semanticannotations; we believe a demo of the PATTY system will be of interest to the databasecommunity.,Proceedings of the VLDB Endowment,2012,31
“Catch me if you can”: Visual Analysis of Coherence Defects in Web Archiving,Marc Spaniol; Arturas Mazeika; Dimitar Denev; Gerhard Weikum,ABSTRACT The World Wide Web is a continuously evolving network of contents (eg Webpages; images; sound files; etc.) and an interconnecting link structure. Hence; an archivistmay never be sure if the contents collected so far are still consistent with those contents sheneeds to retrieve next. Therefore; questions arise about detecting; measuring them and–finally–understanding coherence defects. To this end; visualization strategies are beingpresented that might be applied on different level of granularities: working with (in the idealcase) properly set last-modified timestamps; based on metadata extracted from the crawlerin accelerated crawl-revisit pairs; or from the Internet Archive's WARC files. In order to helpthe archivist in understanding the nature of these defects; this paper investigates means forvisualizing change behavior and archive coherence.,The 9 th International Web Archiving Workshop (IWAW 2009) Corfu; Greece; September/October; 2009 Workshop Proceedings,2009,31
Efficient peer-to-peer semantic overlay networks based on statistical language models,Alessandro Linari; Gerhard Weikum,Abstract In this paper we address the query routing problem in peer-to-peer (P2P)information retrieval. Our system builds up on the idea of a Semantic Overlay Network(SON); in which each peer becomes neighbor of a small number of peers; chosen amongthose that are most similar to it. Peers in the network are represented by a statisticalLanguage Model derived from their local data collections but; instead of using the non-metric Kullback-Leibler divergence to compute the similarity between them; we use asymmetrized and" metricized" related measure; the square root of the Jensen-Shannondivergence; which let us map the problem to a metric search problem. The search strategyexploits the triangular inequality to efficiently prune the search space and relies on a priorityqueue to visit the most promising peers first. To keep communications costs low and to …,Proceedings of the international workshop on Information retrieval in peer-to-peer networks,2006,31
Transductive learning for text classification using explicit knowledge models,Georgiana Ifrim; Gerhard Weikum,Abstract We present a generative model based approach for transductive learning for textclassification. Our approach combines three methodological ingredients: learning frombackground corpora; latent variable models for decomposing the topic-word space into topic-concept and concept-word spaces; and explicit knowledge models (light-weight ontologies;thesauri; eg WordNet) with named concepts for populating latent variables. The combinationhas synergies that can boost the combined performance. This paper presents the theoreticalmodel and extensive experimental results on three data collections. Our experiments showimproved classification results over state-of-the-art classification techniques such as theSpectral Graph Transducer and Transductive Support Vector Machines; particularly for thecase of sparse training.,European Conference on Principles of Data Mining and Knowledge Discovery,2006,31
Deriving a Web-Scale Common Sense Fact Database.,Niket Tandon; Gerard De Melo; Gerhard Weikum,Abstract The fact that birds have feathers and ice is cold seems trivially true. Yet; mostmachine-readable sources of knowledge either lack such common sense facts entirely orhave only limited coverage. Prior work on automated knowledge base construction haslargely focused on relations between named entities and on taxonomic knowledge; whiledisregarding common sense properties. In this paper; we show how to gather large amountsof common sense facts from Web n-gram data; using seeds from the ConceptNet collection.Our novel contributions include scalable methods for tapping onto Web-scale data and anew scoring model to determine which patterns and facts are most reliable. Theexperimental results show that this approach extends ConceptNet by many orders ofmagnitude at comparable levels of precision.,AAAI,2011,30
SHARC: framework for quality-conscious web archiving,Dimitar Denev; Arturas Mazeika; Marc Spaniol; Gerhard Weikum,Abstract Web archives preserve the history of born-digital content and offer great potentialfor sociologists; business analysts; and legal experts on intellectual property andcompliance issues. Data quality is crucial for these purposes. Ideally; crawlers should gathersharp captures of entire Web sites; but the politeness etiquette and completenessrequirement mandate very slow; long-duration crawling while Web sites undergo changes.This paper presents the SHARC framework for assessing the data quality in Web archivesand for tuning capturing strategies towards better quality with given resources. We definequality measures; characterize their properties; and derive a suite of quality-consciousscheduling strategies for archive crawling. It is assumed that change rates of Web pages canbe statistically predicted based on page types; directory depths; and URL names. We …,Proceedings of the VLDB Endowment,2009,30
Best-effort top-k query processing under budgetary constraints,Michal Shmueli-Scheuer; Chen Li; Yosi Mass; Haggai Roitman; Ralf Schenkel; Gerhard Weikum,We consider a novel problem of top-k query processing under budget constraints. Weprovide both a framework and a set of algorithms to address this problem. Existingalgorithms for top-k processing are budget-oblivious; ie; they do not take budget constraintsinto account when making scheduling decisions; but focus on the performance to computethe final top-k results. Under budget constraints; these algorithms therefore often returnresults that are a lot worse than the results that can be achieved with a clever; budget-awarescheduling algorithm. This paper introduces novel algorithms for budget-aware top-kprocessing that produce results that have a significantly higher quality than those of state-of-the-art budget-oblivious solutions.,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,30
DB&IR integration: report on the Dagstuhl seminar,Sihem Amer-Yahia; Djoerd Hiemstra; Thomas Roelleke; Divesh Srivastava; Gerhard Weikum,Abstract This paper is based on a five-day workshop on" Ranked XML Querying" that tookplace in Schloss Dagstuhl in Germany in March 2008 and was attended by 27 people fromthree different research communities: database systems (DB); information retrieval (IR); andWeb. The seminar title was interpreted in an IR-style" andish" sense (it covered also subsetsof {Ranking; XML; Querying}; with larger sets being favored) rather than the DB-style strictlyconjunctive manner. So in essence; the seminar really addressed the integration of DB andIR technologies with Web 2.0 being an important target area.,ACM SIGIR Forum,2008,30
The juxtaposed approximate pagerank method for robust pagerank approximation in a peer-to-peer web search network,Josiane Xavier Parreira; Carlos Castillo; Debora Donato; Sebastian Michel; Gerhard Weikum,Abstract We present Juxtaposed approximate PageRank (JXP); a distributed algorithm forcomputing PageRank-style authority scores of Web pages on a peer-to-peer (P2P) network.Unlike previous algorithms; JXP allows peers to have overlapping content and requires no apriori knowledge of other peers' content. Our algorithm combines locally computed authorityscores with information obtained from other peers by means of random meetings among thepeers in the network. This computation is based on a Markov-chain state-lumping technique;and iteratively approximates global authority scores. The algorithm scales with the numberof peers in the network and we show that the JXP scores converge to the true PageRankscores that one would obtain with a centralized algorithm. Finally; we show how to deal withmisbehaving peers by extending JXP with a reputation model.,The VLDB Journal—The International Journal on Very Large Data Bases,2008,30
Enhancing concurrency in layered systems,Gerhard Weikum,Abstract To enhance concurrency in a layered system architecture; a multi-level transactionapproach is proposed. Low-level operations issued by some higher level action are treatedas a layer-specific subtransaction. Thus; according to the rule of level-by-level serializability;semantic knowledge about application-specific actions can be utilized; and still single high-level actions may be interleaved on a deeper layer. In the case of a nested two-phaselocking protocol this means that low-level locks are released prematurely and only semanticlocks are held until the end of an application transaction. Due to this characteristic ofsubtransactions; transaction aborts have to be performed by compensating low-levelchanges through inverse high-level operations rather than backing them out on the pagelevel. Besides developing implementation guidelines for a multi-level concurrency control …,*,1989,30
Stochastic performance guarantees for mixed workloads in a multimedia information system,Guido Nerjes; Peter Muth; Gerhard Weikum,We present an approach to stochastic performance guarantees for multimedia servers withmixed workloads. Advanced multimedia applications such as digital libraries or teleteachingexhibit a mixed workload with accesses to both" continuous" and conventional;" discrete"data; where the fractions of continuous data and discrete data requests vary over time. Weassume that a server shares all disks among continuous and discrete data; and we developa stochastic performance model for the resulting mixed workload; using a combination ofanalytic and simulation based modeling. Based on this model we devise a round basedscheduling scheme with stochastic performance guarantees: for continous data requests; webound the probability that" glitches" occur and for discrete data requests; we bound theprobability that the response time exceeds a certain tolerance threshold. We present early …,Research Issues in Data Engineering; 1997. Proceedings. Seventh International Workshop on,1997,29
Finet: Context-aware fine-grained named entity typing,Luciano del Corro; Abdalghani Abujabal; Rainer Gemulla; Gerhard Weikum,Abstract We propose FINET; a system for detecting the types of named entities in shortinputs—such as sentences or tweets—with respect to WordNet's super fine-grained typesystem. FINET generates candidate types using a sequence of multiple extractors; rangingfrom explicitly mentioned types to implicit types; and subsequently selects the mostappropriate using ideas from word-sense disambiguation. FINET combats data scarcity andnoise from existing systems: It does not rely on supervision in its extractors and generatestraining data for type selection from WordNet and other resources. FINET supports the mostfine-grained type system so far; including types with no annotated training data. Ourexperiments indicate that FINET outperforms state-of-the-art methods in terms of recall;precision; and granularity of extracted types.,*,2015,27
A unified approach to concurrency control and transaction recovery,Gustavo Alonso; Radek Vingralek; Divyakant Agrawal; Yuri Breitbart; Amr El Abbadi; Hans Schek; Gerhard Weikum,Abstract In this paper; we have addressed an open problem posed by [SWY93]: how tocharacterize the class of histories PRED in a constructive way so that unified schedulingprotocols can be derived from it. We have slightly modified the original definitions ofexpanded histories and PRED to account for certain executions; and we have provided anequivalent class; SOT; with a more constructive definition. This new class is used as thebasis for several protocols that implement unified concurrency control and recovery in anefficient manner. So far; our model is restricted to read and write operations. However; boththe model and the developed protocols can be generalized to transactions with semanticallyrich operations where recovery is based on compensating operations.,International Conference on Extending Database Technology,1994,27
Tuning of striping units in disk-array-based file systems,Gerhard Weikum; Peter Zabback,Striping files across the disks of a disk array is a promising approach to improve the I/Operformance of data management systems. An important tuning parameter of this method isthe striping unit that is; the maximum number of logically consecutive blocks that areallocated on one disk. The striping unit determines the degree of parallelism in servicing arequest by multiple disks; and its affects the achievable throughput of I/O requests. Since agood choice of a file's striping unit depends on the file's access characteristics such asaverage request size; it is proposed that file-specific striping units be chosen rather thanchoosing the same global striping unit for all files. The paper presents a method for tuningfile-specific striping units; based on the access characteristics of the individual files and thethroughput requirements of the application. Performance experiments are presented …,Research Issues on Data Engineering; 1992: Transaction and Query Processing; Second International Workshop on,1992,27
Harmony and dissonance: organizing the people's voices on political controversies,Rawia Awadallah; Maya Ramanath; Gerhard Weikum,Abstract The wikileaks documents about the death of Osama Bin Laden and the debatesabout the economic crisis in Greece and other European countries are some of thecontroversial topics being played on the news everyday. Each of these topics has manydifferent aspects; and there is no absolute; simple truth in answering questions such as:should the EU guarantee the financial stability of each member country; or should thecountries themselves be solely responsible? To understand the landscape of opinions; itwould be helpful to know which politician or other stakeholder takes which position-supportor opposition-on these aspects of controversial topics.,Proceedings of the fifth ACM international conference on Web search and data mining,2012,26
Fast software encryption,Orr Dunkelman,Fast Software Encryption 2009 was the 16th in a series of workshops on symmetric keycryptography. Starting from 2002; it is sponsored by the International Association forCryptologic Research (IACR). FSE 2009 was held in Leuven; Belgium; after previousvenues held in Cambridge; UK (1993; 1996); Leuven; Belgium (1994; 2002); Haifa; Israel(1997); Paris; France (1998; 2005); Rome; Italy (1999); New York; USA (2000); Yokohama;Japan (2001); Lund; Sweden (2003); New Delhi; India (2004); Graz; Austria (2006);Luxembourg; Luxembourg (2007); and Lausanne; Switzerland (2008). The workshop's maintopic is symmetric key cryptography; including the design of fast and secure symmetric keyprimitives; such as block ciphers; stream ciphers; hash functions; message authenticationcodes; modes of operation and iteration; as well as the theoretical foundations of these …,*,2009,26
Learning word-to-concept mappings for automatic text classification,Georgiana Ifrim; Martin Theobald; Gerhard Weikum,Abstract For both classification and retrieval of natural language text documents; thestandard document representation is a term vector where a term is simply a morphologicalnormal form of the corresponding word. A potentially better approach would be to map everyword onto a concept; the proper word sense and use this additional information in thelearning process. In this paper we address the problem of automatically classifying naturallanguage text documents. We investigate the effect of word to concept mappings and wordsense disambiguation techniques on improving classification accuracy. We use the WordNetthesaurus as a background knowledge base and propose a generative language modelapproach to document classification. We show experimental results comparing theperformance of our model with Naive Bayes and SVM classifiers.,Learning in Web Search Workshop; ICML,2005,26
Integrating snapshot isolation into transactional federations,Ralf Schenkel; Gerhard Weikum,Abstract This Paper reconsiders the Problem of transactional federations; more specificallythe concurrency control issue; with particular consideration of component Systems thatprovide only snapshot isolation; which is the default setting in Oracle and widely used inpractice. The Paper derives criteria and practical protocols for guaranteeing globalserializability at the federation level. The Paper generalizes the well-known ticket methodand develops novel federation-level graph testing methods to incorporate sub-serializabilitycomponent Systems like Oracle. These contributions are embedded in a practical projectthat built a CORBA-based federated database architecture suitable for modern Internet-orIntranet-based applications such as electronie commerce. This prototype System; whichincludes a federated transaction manager coined Trafic (Transactional Federation of …,International Conference on Cooperative Information Systems,2000,26
AIDA-light: High-Throughput Named-Entity Disambiguation.,Dat Ba Nguyen; Johannes Hoffart; Martin Theobald; Gerhard Weikum,ABSTRACT To advance the Web of Linked Data; mapping ambiguous names in structuredand unstructured contents onto knowledge bases would be a vital asset. State-of-the-artmethods for Named Entity Disambiguation (NED) face major tradeoffs regardingefficiency/scalability vs. accuracy. Fast methods use relatively simple context features andavoid computationally expensive algorithms for joint inference. While doing very well onprominent entities in clear input texts; these methods achieve only moderate accuracy whenfed with difficult inputs. On the other hand; methods that rely on rich context features andjoint inference for mapping names onto entities pay the price of being much slower. Thispaper presents AIDA-light which achieves high accuracy on difficult inputs while also beingfast and scalable. AIDA-light uses a novel kind of two-stage mapping algorithm. It first …,LDOW,2014,25
The SHARC framework for data quality in Web archiving,Dimitar Denev; Arturas Mazeika; Marc Spaniol; Gerhard Weikum,Abstract Web archives preserve the history of born-digital content and offer great potentialfor sociologists; business analysts; and legal experts on intellectual property andcompliance issues. Data quality is crucial for these purposes. Ideally; crawlers should gathercoherent captures of entire Web sites; but the politeness etiquette and completenessrequirement mandate very slow; long-duration crawling while Web sites undergo changes.This paper presents the SHARC framework for assessing the data quality in Web archivesand for tuning capturing strategies toward better quality with given resources. We definedata quality measures; characterize their properties; and develop a suite of quality-conscious scheduling strategies for archive crawling. Our framework includes single-visitand visit---revisit crawls. Single-visit crawls download every page of a site exactly once in …,The VLDB Journal—The International Journal on Very Large Data Bases,2011,25
Distributed hash sketches: Scalable; efficient; and accurate cardinality estimation for distributed multisets,Nikos Ntarmos; Peter Triantafillou; Gerhard Weikum,Abstract Counting items in a distributed system; and estimating the cardinality of multisets inparticular; is important for a large variety of applications and a fundamental building block foremerging Internet-scale information systems. Examples of such applications range fromoptimizing query access plans in peer-to-peer data sharing; to computing the significance(rank/score) of data items in distributed information retrieval. The general formal problemaddressed in this article is computing the network-wide distinct number of items with someproperty (eg; distinct files with file name containing “spiderman”) where each node in thenetwork holds an arbitrary subset; possibly overlapping the subsets of other nodes. The keyrequirements that a viable approach must satisfy are:(1) scalability towards very largenetwork size;(2) efficiency regarding messaging overhead;(3) load balance of storage …,ACM Transactions on Computer Systems (TOCS),2009,25
Computational Science and Its Applications-ICCSA 2005,Osvaldo Gervasi; Marina L Gavrilova; Vipin Kumar; Antonio Laganà; Heow Pueh Lee; Youngsong Mun; David Taniar; Chih Jeng Kenneth Tan,This five-volume set was compiled following the 2006 International Conference onComputational Science and its Applications; ICCSA 2006; held in Glasgow; UK; during May8–11; 2006. It represents the outstanding collection of almost 664 refereed papers selectedfrom over 2;450 submissions to ICCSA 2006. Computational science has firmly establisheditself as a vital part of many scientific investigations; affecting researchers and practitionersin areas ranging from applications such as aerospace and automotive; to emergingtechnologies such as bioinformatics and nanotechnologies; to core disciplines such asmathematics; physics; and chemistry. Due to the shear size of many challenges incomputational science; the use of supercomputing; parallel processing; and sophisticatedalgorithms is inevitable and becomes a part of fundamental theoretical research as well …,Conference proceedings ICCSA,2005,25
Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data,Zachary G Ives; Yannis Papakonstantinou; Alon Halevy,*,*,2003,25
The Web in 2010: Challenges and opportunities for database research,Gerhard Weikum,Abstract The impressive advances in global networking and information technology providegreat opportunities for all kinds of Web-based information services; ranging from digitallibraries and information discovery to virtual-enterprise workflows and electronic commerce.However; many of these services still exhibit rather poor quality in terms of unacceptableperformance during load peaks; frequent and long outages; and unsatisfactory searchresults. F or the next decade; the overriding goal of database research should be to providemeans for building zero-administration; self-tuning information services with predictableresponse time; virtually continuous availability; and; ultimately;“moneyback” service-qualityguarantees. A particularly challenging aspect of this theme is the quality of search results indigital libraries; scientific data repositories; and on the Web. To aim for more intelligent …,*,2001,25
The mentor workbench for enterprise-wide workflow management,Dirk Wodtke; Jeanine Weissenfels; Gerhard Weikum; Angelika Kotz Dittrich; Peter Muth,Abstract MENTOR (“Middleware for Enterprise-Wide Workflow Management”) is a jointproject of the University of the Saarland; the Union Bank of Switzerland; and ETH Zurich [1;2; 3]. The focus of the project is on enterprise-wide workflow management. Workflows in thiscategory may span multiple organizational units each unit having its own workflow server;involve a variety of heterogeneous information systems; and require many thousands ofclients to interact with the workflow management system (WFMS). The project aims todevelop a scalable and highly available environment for the execution and monitoring ofworkflows; seamlessly integrated with a specification and verification environment. For thespecification of workflows; MENTOR utilizes the formalism of state and activity charts. Themathematical rigor of the specification method establishes a basis for both correctness …,ACM SIGMOD Record,1997,25
Task-aware search personalization,Julia Luxenburger; Shady Elbassuoni; Gerhard Weikum,Abstract Search personalization has been pursued in many ways; in order to provide betterresult rankings and better overall search experience to individual users [5]. However; blindlyapplying personalization to all user queries; for example; by a background model derivedfrom the user's long-term query-and-click history; is not always appropriate for aiding theuser in accomplishing her actual task. User interests change over time; a user sometimesworks on very different categories of tasks within a short timespan; and history-basedpersonalization may impede a user's desire of discovering new topics. In this paper wepropose a personalization framework that is selective in a twofold sense. First; it selectivelyemploys personalization techniques for queries that are expected to benefit from priorhistory information; while refraining from undue actions otherwise. Second; we introduce …,Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,2008,24
Classification and Intelligent Search on Information in XML.,Norbert Fuhr; Gerhard Weikum,XML will be the method of choice for representing all kinds of documents in productcatalogs; digital libraries; scientific data repositories; and across the Web. This observationcreates high expectations that XML will be a major catalyst in constructing the “SemanticWeb”. However; merely casting all documents into XML format does not necessarily make adocument's semantics explicit and more amenable for effective information searching.Rather; to fully leverage XML on a global scale; significant progress is needed on thefollowing issues: 1. providing an easy-to-use yet powerful and efficient search language thatcombines concepts from current XML pattern-matching languages (eg; XPath; XQuery; etc.)with ontology-backed information-retrievalstyle search result ranking; 2. extracting moresemantics from existing document collections by constructing structural and ontological …,IEEE Data Eng. Bull.,2002,24
Transactional information systems,Gottfried Vossen,*,*,2001,24
Spezifikation; Verifikation und verteilte Ausführung von Workflows in MENTOR,Gerhard Weikum; Dirk Wodtke; Angelika Kotz Dittrich; Peter Muth; Jeanine Weißenfels,Zusammenfassung. Der Artikel diskutiert die Spezifikation; Verifikation und verteilteAusführung von Workflows mit Hilfe von State-und Activitycharts. Die formale Fundierungder State-und Activitycharts wird auf drei Ebenen ausgenutzt. Auf der Spezifikationsebeneerzwingt der verwendete Formalismus präzise Beschreibungen von Geschäftsprozessen;läßt jedoch ebenfalls spätere Verfeinerungen zu. Darüber hinaus können Spezifikationen;die mit anderen Methoden erstellt wurden; weitgehend automatisch auf State-undActivitycharts abgebildet werden. Auf der Ebene der Verifikation kritischer Workflow-Eigenschaften eröffnen Statecharts die Anwendung der effizienten Methode desModellprüfens. Auf der Ausführungsebene schließlich können aus einer Spezifikationverteilt ausführbare Komponenten generiert werden. Im Rahmen des MENTOR-Projekts …,Informatik Forschung und Entwicklung,1997,24
A cost-model-based online method for distributed caching,Markus Sinnwell; Gerhard Weikum,Presents a method for distributed caching to exploit the aggregate memory of networks ofworkstations in data-intensive applications. In contrast to prior work; the approach is basedon a detailed cost model as the basis for optimizing the placement of variable-size dataobjects in a distributed; possibly heterogeneous two-level storage hierarchy. To address theonline problem with a priori unknown and evolving workload parameters; the methodemploys dynamic load tracking procedures and an approximative; low-overhead version ofthe cost model for continuous reoptimization steps that are embedded in the decisions of theunderlying local cache managers. The method is able to automatically find a good tradeoffbetween an" egoistic" and an" altruistic" behavior of the network nodes; and proves itspractical viability in a detailed simulation study under a variety of workload and system …,Data Engineering; 1997. Proceedings. 13th International Conference on,1997,24
Load control in scalable distributed file structures,Yuri Breitbart; Radek Vingralek; Gerhard Weikum,Abstract The paper presents a family of distributed file structures; coined DiFS; for recordstructured; disk resident files with key based exact or interval match access. The file isorganized into buckets that are spread among multiple servers; where a server may holdseveral buckets. Client requests are serviced by mapping keys onto buckets and looking upthe corresponding server in an address table. Dynamic growth; in terms of file size andaccess load; is supported by bucket splits and bucket migrations onto the existing or newlycreated servers. The major problem that we are addressing is achieving scalability in thesense that both the file size and the client throughput can be scaled up by linearly increasingthe number of servers and dynamically redistributing the data. Unlike previous work withsimilar objectives; our data redistribution considers explicitly the cost/performance ratio of …,Distributed and Parallel Databases,1996,24
Knowlywood: Mining activity knowledge from hollywood narratives,Niket Tandon; Gerard de Melo; Abir De; Gerhard Weikum,Abstract Despite the success of large knowledge bases; one kind of knowledge that has notreceived attention so far is that of human activities. An example of such an activity isproposing to someone (to get married). For the computer; knowing that this involves twoadults; often but not necessarily a woman and a man; that it often takes place in someromantic location; that it typically involves flowers or jewelry; and that it is usually followed bykissing; is a valuable asset for tasks like natural language dialog; scene understanding; orvideo search. This corresponds to the challenging task of acquiring semantic frames thatcapture human activities; their participating agents; and their typical spatio-temporalcontexts. This paper presents a novel approach that taps into movie scripts and othernarrative texts. We develop a pipeline for semantic parsing and knowledge distillation; to …,Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,2015,23
A fresh look on knowledge bases: Distilling named events from news,Erdal Kuzey; Jilles Vreeken; Gerhard Weikum,Abstract Knowledge bases capture millions of entities such as people; companies or movies.However; their knowledge of named events like sports finals; political scandals; or naturaldisasters is fairly limited; as these are continuously emerging entities. This paper presents amethod for extracting named events from news articles; reconciling them into canonicalizedrepresentation; and organizing them into fine-grained semantic classes to populate aknowledge base. Our method captures similarity measures among news articles in a multi-view attributed graph; considering textual contents; entity occurrences; and temporalordering. For distilling canonicalized events from this raw data; we present a novel graphcoarsening algorithm based on the information-theoretic principle of minimum descriptionlength. The quality of our method is experimentally demonstrated by extracting …,Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,2014,23
Coupling label propagation and constraints for temporal fact extraction,Yafang Wang; Maximilian Dylla; Marc Spaniol; Gerhard Weikum,Abstract The Web and digitized text sources contain a wealth of information about namedentities such as politicians; actors; companies; or cultural landmarks. Extracting thisinformation has enabled the automated construction of large knowledge bases; containinghundred millions of binary relationships or attribute values about these named entities.However; in reality most knowledge is transient; ie changes over time; requiring a temporaldimension in fact extraction. In this paper we develop a methodology that combines labelpropagation with constraint reasoning for temporal fact extraction. Label propagationaggressively gathers fact candidates; and an Integer Linear Program is used to clean outfalse hypotheses that violate temporal constraints. Our method is able to improve on recallwhile keeping up with precision; which we demonstrate by experiments with biography …,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,2012,23
Interesting-phrase mining for ad-hoc text analytics,Srikanta Bedathur; Klaus Berberich; Jens Dittrich; Nikos Mamoulis; Gerhard Weikum,Abstract Large text corpora with news; customer mail and reports; or Web 2.0 contributionsoffer a great potential for enhancing business-intelligence applications. We propose aframework for performing text analytics on such data in a versatile; efficient; and scalablemanner. While much of the prior literature has emphasized mining keywords or tags in blogsor social-tagging communities; we emphasize the analysis of interesting phrases. Theseinclude named entities; important quotations; market slogans; and other multi-word phrasesthat are prominent in a dynamically derived ad-hoc subset of the corpus; eg; being frequentin the subset but relatively infrequent in the overall corpus. We develop preprocessing andindexing methods for phrases; paired with new search techniques for the top-k mostinteresting phrases in ad-hoc subsets of the corpus. Our framework is evaluated using a …,Proceedings of the VLDB Endowment,2010,23
Distributed top-k aggregation queries at large,Thomas Neumann; Matthias Bender; Sebastian Michel; Ralf Schenkel; Peter Triantafillou; Gerhard Weikum,Abstract Top-k query processing is a fundamental building block for efficient ranking in alarge number of applications. Efficiency is a central issue; especially for distributed settings;when the data is spread across different nodes in a network. This paper introduces noveloptimization methods for top-k aggregation queries in such distributed environments. Theoptimizations can be applied to all algorithms that fall into the frameworks of the prior TPUTand KLEE methods. The optimizations address three degrees of freedom: 1) hierarchicallygrouping input lists into top-k operator trees and optimizing the tree structure; 2) computingdata-adaptive scan depths for different input sources; and 3) data-adaptive sampling of asmall subset of input sources in scenarios with hundreds or thousands of query-relevantnetwork nodes. All optimizations are based on a statistical cost model that utilizes local …,Distributed and Parallel Databases,2009,23
Component-based Software Engineering: 11th International Symposium; CBSE 2008; Karlsruhe; Germany; October 14-17; 2008; Proceedings,Michel RV Chaudron; Clemens Szyperski; Ralf H Reussner,On behalf of the Organizing Committee we are pleased to present the p-ceedings of the2008 Symposium on Component-Based Software Engineering (CBSE). CBSE is concernedwith the development of software-intensivesystems from independently developed software-building blocks (components); the-velopment of components; and system maintenance andimprovement by means of component replacement and customization. CBSE 2008 was the11th in a series of events that promote a science and technology foundation for achievingpredictable quality in software systems through the use of software component technologyand its associated software engineering practices. Wewerefortunateto haveadedicatedProgramCommitteecompris…internationallyrecognizedresearchersandin…. Wewouldlike to thank the members of theProgram Committee and associated reviewers for their contribution in making this …,*,2008,23
Approximate information filtering in peer-to-peer networks,Christian Zimmer; Christos Tryfonopoulos; Klaus Berberich; Manolis Koubarakis; Gerhard Weikum,Abstract Most approaches to information filtering taken so far have the underlying hypothesisof potentially delivering notifications from every information producer to subscribers. Thisexact publish/subscribe model creates an efficiency and scalability bottleneck; and might noteven be desirable in certain applications. The work presented here puts forward MAPS; anovel approach to support approximate information filtering in a peer-to-peer environment.In MAPS a user subscribes to and monitors only carefully selected data sources; andreceives notifications about interesting events from these sources only. This way scalabilityis enhanced by trading recall for lower message traffic. We define the protocols of a peer-to-peer architecture especially designed for approximate information filtering; and introducenew node selection strategies based on time series analysis techniques to improve data …,International Conference on Web Information Systems Engineering,2008,23
FluxCapacitor: efficient time-travel text search,Klaus Berberich; Srikanta Bedathur; Thomas Neumann; Gerhard Weikum,Abstract An increasing number of temporally versioned text collections is available todaywith Web archives being a prime example. Search on such collections; however; is often notsatisfactory and ignores their temporal dimension completely. Time-travel text search solvesthis problem by evaluating a keyword query on the state of the text collection as of a user-specified time point. This work demonstrates our approach to efficient time-travel text searchand its implementation in the FLUXCAPACITOR prototype.,Proceedings of the 33rd international conference on Very large data bases,2007,23
Benchmarking and configuration of workflow management systems,Michael Gillmann; Ralf Mindermann; Gerhard Weikum,Abstract Workflow management systems (WFMS) are a cornerstone of mission-criticial;possibly cross-organizational business processes. For large-scale applications both theirperformance and availability are crucial factors; and the system needs to be properlyconfigured to meet the application demands. Despite ample work on scalable systemarchitectures for workflow management; the literature has neglected the important issues ofhow to systematically measure the performance of a given system configuration and how todetermine viable configurations without resorting to expensive trial-and-error or guesswork.This paper proposes a synthetic benchmark for workflow management systems; based onthe TPC-C order-entry benchmark; a complete e-commerce workflow is specified in a system-independent form. This workflow benchmark; which stresses all major components of a …,International Conference on Cooperative Information Systems,2000,23
Scheduling strategies for mixed workloads in multimedia information servers,Guido Nerjes; Peter Muth; Michael Paterakis; Yannis Romboyannakis; Peter Triantafillou; Gerhard Weikum,In contrast to pure video servers; advanced applications such as digital libraries orteleteaching exhibit a mixed workload with massive access to conventional;" discrete" datasuch as text documents; images and indexes as well as requests for" continuous data". Inaddition to the service quality guarantees for continuous data requests; quality-consciousapplications require that the response time of the discrete data requests stay below someuser-tolerance threshold. We study the impact of different disk scheduling policies on theservice quality for both continuous and discrete data. We identify a number of critical issues;present a framework for describing the various policies in terms of few parameters andfinally provide experimental results; based on a detailed simulation testbed; that comparedifferent scheduling policies.,Research Issues In Data Engineering; 1998.'Continuous-Media Databases and Applications'. Proceedings.; Eighth International Workshop on,1998,23
Extending transaction management to capture more consistency with better performance,Gerhard Weikum,Abstract This paper surveys recent work on extended transaction management. It focusseson transaction–oriented applications in large distributed and heterogeneous informationsystems where applications often span multiple autonomous databases. In such anenvironment; the challenge is to reconcile data consistency and acceptable performance tothe best extent possible; which may imply trading off consistency versus performance in acontrolled manner. One approach is to decompose a long user interaction into severalindependently executed transactions and to provide an additional control sphere as the”glue” between the transactions. This additional control sphere is usually referred to as anactivity or a transactional workflow. Workflow management serves to specify and enforcecertain types of execution dependencies between the transactions of a workflow; and thus …,Invited Paper; 9th French Database Conference; Toulouse,1993,23
Open nested transactions in federated database systems,Gerhard Weikum; Andrew Deacon; Werner Schaad; Hans-Joerg Schek,The increasing demand for interoperability of existing information systems requires supportfor global transactions in federations of largely autonomous databases. In the past fewyears; substantial research has been carried out on this problem of federated transactionmanagement. The following general model of a federated database system has evolvedfrom this research 4]. A federated database system is composed of a number of pre-existinglocal databases (LDBs); which are managed by the same or by di erent database systems.An LDB is a collection of data and pre-existing applications. The applications run localtransactions (LTs) that access only the LDB. The federated system serves to support newapplications that span multiple LDBs. These applications run global transactions (GTs) thatconsist of multiple global subtransactions (GSTs) each of which accesses exactly one …,IEEE Data Eng. Bull.,1993,23
A Performance Evaluation of Multi-Level Transaction Management.,Christof Hasse; Gerhard Weikum,Abstract Multi-level transactions are a variant of open nested transactions in which thesubtransactions correspond to operations at different levels of a layered system architecture.The point of multi-level transactions is that the semantics of high-level operations can beexploited in order to increase concurrency. As a consequence; undoing a transactionrequires compensation of completed subtransactions. In addition; multi-level recoverymethods have to take into account that highlevel operations are not necessarily atomic ifmultiple pages are updated in a single subtransaction. This paper presents a performanceevaluation of the multi-level transaction management that is implemented in the databasekernel system DASDBS. In particular; it is shown that multi-level recovery can beimplemented in an efficient way. We discuss performance measurements; using a …,VLDB,1991,23
Yago-qa: Answering questions by structured knowledge queries,Peter Adolphs; Martin Theobald; Ulrich Schafer; Hans Uszkoreit; Gerhard Weikum,We present a natural-language question-answering system that gives access to theaccumulated knowledge of one of the largest community projects on the Web-Wikipedia-viaan automatically acquired structured knowledge base. Key to building such a system is toestablish mappings from natural language expressions to semantic representations. Wepropose to acquire these mappings by data-driven methods-corpus harvesting andparaphrasing and present a preliminary empirical study that demonstrates the viability of ourmethod.,Semantic Computing (ICSC); 2011 Fifth IEEE International Conference on,2011,22
Incorporating terminology evolution for query translation in text retrieval with association rules,Amal C Kaluarachchi; Aparna S Varde; Srikanta Bedathur; Gerhard Weikum; Jing Peng; Anna Feldman,Abstract Time-stamped documents such as newswire articles; blog posts and other web-pages are often archived online. When these archives cover long spans of time; theterminology within them could undergo significant changes. Hence; when users posequeries pertaining to historical information; over such documents; the queries need to betranslated; taking into account these temporal changes; to provide accurate responses tousers. For example; a query on Sri Lanka should automatically retrieve documents with itsformer name Ceylon. We call such concepts SITACs; ie; Semantically Identical TemporallyAltering Concepts. In order to discover SITACs; we propose an approach based on a novelframework constituting an integration of natural language processing; association rulemining; and contextual similarity as a learning technique. The proposed approach has …,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,22
Applications of Graph Transformations with Industrial Relevance: Third International Symposium; AGTIVE 2007; Kassel; Germany; October 10-12; 2007; Revised Sel...,Andy Schürr; Manfred Nagl; Albert Zündorf,This book constitutes the thoroughly refereed post-conference proceedings of the ThirdInternational Symposium on Applications of Graph Transformations; AGTIVE 2007; held inKassel; Germany; in October 2007. The 30 revised full papers presented together with 2invited papers were carefully selected from numerous submissions during two rounds ofreviewing and improvement. The papers are organized in topical sections on graphtransformation applications; meta-modeling and domain-specific language; new graphtransformation approaches; program transformation applications; dynamic system modeling;model driven software development applications; queries; views; and modeltransformations; as well as new pattern matching and rewriting concepts. The volumemoreover contains 4 papers resulting from the adjacent graph transformation tool contest …,*,2008,22
Progress in Pattern Recognition; Image Analysis and Applications: 13th Iberoamerican Congress on Pattern Recognition; CIARP 2008; Havana; Cuba; September 9...,José Ruiz-Shulcloper; Walter Kropatsch,This book constitutes the refereed proceedings of the 13th Iberoamerican Congress onPattern Recognition; CIARP 2008; held in Havana; Cuba; in September 2008. The 93revised full papers presented together with 3 keynote articles were carefully reviewed andselected from 182 submissions. The papers are organized in topical sections on signalanalysis for characterization and filtering; analysis of shape and texture; analysis of speechand language; data mining; clustering of images and documents; statistical patternrecognition; classification and description of objects; classification and edition; geometricimage analysis; neural networks; computer vision; image coding; associative memories andneural networks; interpolation and video tracking; images analysis; music and speechanalysis; as well as classifier combination and document filtering.,*,2008,22
Databases and Web 2.0 panel at VLDB 2007,Sihem Amer-Yahia; Volker Markl; Alon Halevy; AnHai Doan; Gustavo Alonso; Donald Kossmann; Gerhard Weikum,Abstract Web 2.0 refers to a set of technologies that enables indviduals to create and sharecontent on the Web. The types of content that are shared on Web 2.0 are quite varied andinclude photos and videos (eg; Flickr; YouTube); encyclopedic knowledge (eg; Wikipedia);the blogosphere; social book-marking and even structured data (eg; Swivel; Many-eyes).One of the important distinguishing features of Web 2.0 is the creation of communities ofusers. Online communities such as LinkedIn; Friendster; Facebook; MySpace and Orkutattract millions of users who build networks of their contacts and utilize them for social andprofessional purposes. In a nutshell; Web 2.0 offers an architecture of participation anddemocracy that encourages users to add value to the application as they use it.,ACM SIGMOD Record,2008,22
BuzzRank… and the trend is your friend,Klaus Berberich; Srikanta Bedathur; Michalis Vazirgiannis; Gerhard Weikum,Abstract Ranking methods like PageRank assess the importance of Web pages based onthe current state of the rapidly evolving Web graph. The dynamics of the resultingimportance scores; however; have not been considered yet; although they provide the key toan understanding of the Zeitgeist on the Web. This paper proposes the BuzzRank methodthat quantifies trends in time series of importance scores and is based on a relevant growthmodel of importance scores. We experimentally demonstrate the usefulness of BuzzRank ona bibliographic dataset.,Proceedings of the 15th international conference on World Wide Web,2006,22
Flexible worklist management in a light-weight workflow management system,Jeanine Weissenfels; Peter Muth; Gerhard Weikum,Abstract Building a universal workflow management system; if feasible at all; is achievableonly at the expense of creating a heavy-weight system that needs to provide a very broadspectrum of functionality. For many application scenarios; such a system is too expensive;creates too much overhead or simply does not perform well enough. In this paper; weadvocate a light-weight system architecture; consisting of a small system kernel on top ofwhich extensions to the kernel are implemented as workflows themselves. This approachprovides a seamless integration of kernel extensions like worklist management; historymanagement etc.. The functionality of the kernel such as distributed workflow execution andinteroperability interfaces can be used by all extensions. We show the feasibility of ourapproach by presenting the implementation of worklist management in our workflow …,Proceeding on the EDBT Workshop on Workflow Management Systems,1998,22
Entity timelines: visual analytics and named entity evolution,Arturas Mazeika; Tomasz Tylenda; Gerhard Weikum,Abstract The constantly evolving Web reflects the evolution of society. Knowledge aboutentities (people; companies; political parties; etc.) evolves over time. Facts add up (eg;awards; lawsuits; divorces); change (eg; spouses; CEOs; political positions); and evencease to exist (eg; countries split into smaller or join into bigger ones). Analytics of theevolution of the entities poses many challenges including extraction; disambiguation; andcanonization of entities from large text collections as well as introduction of specific analysisand interactivity methods for the evolving entity data. In this demonstration proposal; weconsider a novel problem of the evolution of named entities. To this end; we have extracted;disambiguated; canonicalized; and connected named entities with the YAGO ontology. Toanalyze the evolution we have developed a visual analytics system. Careful …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,21
Advances in Cryptology-ASIACRYPT 2008: 14th International Conference on the Theory and Application of Cryptology and Information Security; Melbourne; Australi...,Josef Pawel Pieprzyk,This book constitutes the refereed proceedings of the 14th International Conference on theTheory and Application of Cryptology and Information Security; ASIACRYPT 2008; held inMelbourne; Australia; in December 2008. The 33 revised full papers presented together withthe abstract of 1 invited lecture were carefully reviewed and selected from 208 submissions.The papers are organized in topical sections on muliti-party computation; cryptographicprotocols; cryptographic hash functions; public-key cryptograhy; lattice-based cryptography;private-key cryptograhy; and analysis of stream ciphers.,*,2008,21
Database selection and result merging in p2p web search,Sergey Chernov; Pavel Serdyukov; Matthias Bender; Sebastian Michel; Gerhard Weikum; Christian Zimmer,Abstract Intelligent Web search engines are extremely popular now. Currently; onlycommercial centralized search engines like Google can process terabytes of Web data.Alternative search engines fulfilling collaborative Web search on a voluntary basis areusually based on a blooming Peer-to-Peer (P2P) technology. In this paper; we investigatethe effectiveness of different database selection and result merging methods in the scope ofP2P Web search engine Minerva. We adapt existing measures for database selection andresults merging; all directly derived from popular document ranking measures; to addressthe specific issues of P2P Web search. We propose a general approach to both tasks basedon the combination of pseudo-relevance feedback methods. From experiments with TRECWeb data; we observe that pseudo-relevance feedback improves quality of distributed …,*,2007,21
JXP: Global Authority Scores in a P2P Network.,Josiane Xavier Parreira; Gerhard Weikum,ABSTRACT This document presents the JXP algorithm for dynamically and collaborativelycomputing PageRank-style authority scores of Web pages distributed in a P2P network. Inthe architecture that we pursue; every peer crawls and indexes Web fragments at itsdiscretion; driven by the thematic profile or overlay neighborhood of the peer. The JXPalgorithm runs at every peer; and is initialized by a local authority computation on the basisof the locally available Web fragment. Peers collaborate by periodically “meeting” with otherpeers in the network. Whenever two peers meet they exchange their local information anduse this new information to improve their local authority scores. Even though only localcomputations are performed; the JXP scores approximate the global importance of pages inthe entire network. The storage demand of each peer is linear in the number of Web …,WebDB,2005,21
A database striptease or how to manage your personal databases,Martin Kersten; Gerhard Weikum; Michael Franklin; Daniel Keim; Alex Buchmann; Surajit Chaudhuri,This chapter discusses how to manage one's personal databases or database striptease.The database management problem has been entering everyone's life. To realize itspresence it suffices to sit down and tally the electronic data sources crucial for survival in themodern society. As long as data sources are independent; devices are never replaced; nordo new devices enter the realm of existence; it will survive easily in the digital jungle.However; life runs a different course. Each time one meets a new person; one may have tosynchronize several databases with his address information. The limitations of the humanbrain to cope with the information overload calls upon better support to “remember” where;what and when has been accumulated in the fabric of data sources making up theenvironment. Buying a new PDA surely means a re-organization and possibly retyping …,*,2003,21
Erweiterbarkeit; Kooperation; Föderation von Datenbanksystemen,Hans-Jörg Schek; Gerhard Weikum,Zusammenfassung Wir diskutieren die Rolle der Erweiterbarkeit; Kooperation undFöderation von Datenbanksystemen in ihrem Wechselspiel mit verschiedenenAnwendungssystemen. Erweiterbarkeit vereinfacht die Zusammenarbeit zwischenAnwendungssystem und Datenbanksystem. Wir zeigen auf; inwieweit durch den Ansatzextern definierter Typen (EDTs) und Verallgemeinerungen ein wünschenswerter Grad anAutonomie bei den Anwendungssystemen belassen wird und inwieweit die Verwendungvon Datenbanksystem-Funktionalität dennoch möglich ist. Kooperation vonDatenbanksystemen ist die Integration von Sub-Datenbanksystemen zu einem Super-Datenbanksystem sowohl in Bezug auf die Objektverwaltung (globales Schema) als auch inBezug auf die Transaktionsverwaltung. Wir stellen hier dar; welche Konsequenzen das …,*,1991,21
Knowledge bases in the age of big data analytics,Fabian M Suchanek; Gerhard Weikum,Abstract This tutorial gives an overview on state-of-the-art methods for the automaticconstruction of large knowledge bases and harnessing them for data and text analytics. Itcovers both big-data methods for building knowledge bases and knowledge bases beingassets for big-data applications. The tutorial also points out challenges and researchopportunities.,Proceedings of the VLDB Endowment,2014,20
Acquiring Comparative Commonsense Knowledge from the Web.,Niket Tandon; Gerard De Melo; Gerhard Weikum,Abstract Applications are increasingly expected to make smart decisions based on whathumans consider basic commonsense. An often overlooked but essential form ofcommonsense involves comparisons; eg the fact that bears are typically more dangerousthan dogs; that tables are heavier than chairs; or that ice is colder than water. In this paper;we first rely on open information extraction methods to obtain large amounts of comparisonsfrom the Web. We then develop a joint optimization model for cleaning and disambiguatingthis knowledge with respect to WordNet. This model relies on integer linear programmingand semantic coherence scores. Experiments show that our model outperforms strongbaselines and allows us to obtain a large knowledge base of disambiguated commonsenseassertions.,AAAI,2014,20
Graffiti: graph-based classification in heterogeneous networks,Ralitsa Angelova; Gjergji Kasneci; Gerhard Weikum,Abstract We address the problem of multi-label classification in heterogeneous graphs;where nodes belong to different types and different types have different sets of classificationlabels. We present a novel approach that aims to classify nodes based on theirneighborhoods. We model the mutual influence of nodes as a random walk in which therandom surfer aims at distributing class labels to nodes while walking through the graph.When viewing class labels as “colors”; the random surfer is essentially spraying differentnode types with different color palettes; hence the name Graffiti of our method. In contrast toprevious work on topic-based random surfer models; our approach captures and exploits themutual influence of nodes of the same type based on their connections to nodes of othertypes. We show important properties of our algorithm such as convergence and scalability …,World Wide Web,2012,20
IQ: The Case for Iterative Querying for Knowledge.,Yosi Mass; Maya Ramanath; Yehoshua Sagiv; Gerhard Weikum,ABSTRACT Large knowledge bases; the Linked Data cloud; and Web 2.0 communitiesopen up new opportunities for deep question answering to support the advancedinformation needs of knowledge workers like students; journalists; or business analysts. Thiscalls for going beyond keyword search; towards more expressive ways of entity-relationship-oriented querying with graph constraints or even full-fledged languages like SPARQL (overgraph-structured; schema-less data). However; a neglected aspect of this active researchdirection is the need to support also query refinements; relaxations; and interactiveexploration; as single-shot queries are often insufficient for the users' tasks. This paperaddresses this issue by discussing the paradigm of Iterative Querying; IQ for short. Wepresent two instantiations for IQ; one based on keyword search over labeled graphs …,CIDR,2011,20
Advances in Biometrics: Third International Conferences; ICB 2009; Alghero; Italy; June 2-5; 2009; Proceedings,Massimo Tistarelli; Mark S Nixon,rd It is a pleasure and an honour both to organize ICB 2009; the 3 IAPR/IEEE Inter-tionalConference on Biometrics. This will be held 2–5 June in Alghero; Italy; hosted by theComputer Vision Laboratory; University of Sassari. The conference series is the premierforum for presenting research in biometrics and its allied technologies: the generation ofnew ideas; new approaches; new techniques and new evaluations. The ICB seriesoriginated in 2006 from joining two highly reputed conferences: Audio and Video BasedPersonal Authentication (AVBPA) and the International Conference on BiometricAuthentication (ICBA). Previous conferences were held in Hong Kong and in Korea. This isthe first time the ICB conference has been held in Europe; and by Programme Committee;arrangements and by the quality of the papers; ICB 2009 will continue to maintain the …,*,2009,20
EntityAuthority: Semantically Enriched Graph-Based Authority Propagation.,Julia Stoyanovich; Srikanta J Bedathur; Klaus Berberich; Gerhard Weikum,ABSTRACT This paper pursues the recently emerging paradigm of searching for entities thatare embedded in Web pages. We utilize informationextraction techniques to identify entitycandidates in documents; map them onto entries in a richly structured ontology; and derive ageneralized data graph that encompasses Web pages; entities; and ontological conceptsand relationships. We exploit this combination of pages and entities for a novel kind ofsearch-result ranking; coined EntityAuthority; in order to improve the quality of keywordqueries that return either pages or entities. To this end; we utilize the mutual reinforcementbetween authoritative pages and important entities. This resembles the HITS method forWeb-graph link analysis and recently proposed ObjectRank methods; but our approachoperates on a much richer; typed graph structure with different kinds of nodes and also …,WebDB,2007,20
Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining,J Leskovec; J Kleinberg; C Faloutsos,*,*,2005,20
EOS: exactly-once E-service middleware,German Shegalov; Gerhard Weikum; Roger Barga; David Lomet,This chapter aims to place failure handling logic into a generic Internet middlewareframework so that failures are masked from application programs (and users). Applicationprograms are; thus; relieved from handling message timeouts and other exceptions causedby system failures. Today's Web-based e-services do not handle system failures well. One ofthe most prominent examples is unintentional purchase of multiple copies of the same itemin an online store. This may happen when the user sees a browser timeout for the final“checkout”(“place order”) request caused by a short outage or overload of the network or thebackend servers (typically during peak load). Whereas the request may have beensuccessfully; although slowly; processed; the user may attempt to send the checkout requestonce again; for example; by hitting the browser “refresh” button; unintentionally buying …,*,2002,20
Cross-document co-reference resolution using sample-based clustering with knowledge enrichment,Sourav Dutta; Gerhard Weikum,Abstract Identifying and linking named entities across information sources is the basis ofknowledge acquisition and at the heart of Web search; recommendations; and analytics. Animportant problem in this context is cross-document co-reference resolution (CCR):computing equivalence classes of textual mentions denoting the same entity; within andacross documents. Prior methods employ ranking; clustering; or probabilistic graphicalmodels using syntactic features and distant features from knowledge bases. However; thesemethods exhibit limitations regarding run-time and robustness.,Transactions of the Association for Computational Linguistics,2015,19
KnowLife: a knowledge graph for health and life sciences,Patrick Ernst; Cynthia Meng; Amy Siu; Gerhard Weikum,Knowledge bases (KB's) contribute to advances in semantic search; Web analytics; andsmart recommendations. Their coverage of domain-specific knowledge is limited; though.This demo presents the KnowLife portal; a large KB for health and life sciences;automatically constructed from Web sources. Prior work on biomedical ontologies hasfocused on molecular biology: genes; proteins; and pathways. In contrast; KnowLife is a one-stop portal for a much wider range of relations about diseases; symptoms; causes; riskfactors; drugs; side effects; and more. Moreover; while most prior work relies on manuallycurated sources as input; the KnowLife system taps into scientific literature as well as onlinecommunities. KnowLife uses advanced information extraction methods to populate therelations in the KB. This way; it learns patterns for relations; which are in turn used to …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,19
Towards universal multilingual knowledge bases,Gerard de Melo; Gerhard Weikum,Abstract Lexical; ontological; as well as encyclopedic knowledge is increasingly beingencoded in machine-readable form. This paper deals with knowledge representation inmultilingual settings. It begins by proposing a generic graph-based knowledge baseframework; and then; in three case studies; explains how preexisting knowledge can be castinto this framework. The first case study involves enriching WordNet with information abouthuman languages and their relationships. The second study shows how machine learningtechniques can be used to bootstrap a large-scale multilingual version of WordNet wheresemantic relationships between terms in many languages are captured. The final studyexamines how information can be extracted from Wiktionary to produce a lexical network ofetymological and derivational relationships between words.,Principles; Construction; and Applications of Multilingual Wordnets. Proceedings of the 5th Global WordNet Conference (GWC 2010),2010,19
Overview of webclef 2007,Valentin Jijkoun; Maarten De Rijke,Abstract This paper describes the WebCLEF 2007 task. The task definition—which goesbeyond traditional navigational queries and is concerned with undirected information searchgoals—combines insights gained at previous editions of WebCLEF and of the WiQA pilotthat was run at CLEF 2006. We detail the task; the assessment procedure and the resultsachieved by the participants.,Workshop of the Cross-Language Evaluation Forum for European Languages,2007,19
Towards a statistically semantic web,Gerhard Weikum; Jens Graupmann; Ralf Schenkel; Martin Theobald,Abstract The envisioned Semantic Web aims to provide richly annotated and explicitlystructured Web pages in XML; RDF; or description logics; based upon underlying ontologiesand thesauri. Ideally; this should enable a wealth of query processing and semanticreasoning capabilities using XQuery and logical inference engines. However; we believethat the diversity and uncertainty of terminologies and schema-like annotations will makeprecise querying on a Web scale extremely elusive if not hopeless; and the same argumentholds for large-scale dynamic federations of Deep Web sources. Therefore; ontology-basedreasoning and querying needs to be enhanced by statistical means; leading to relevance-ranked lists as query results. This paper presents steps towards such a “statisticallysemantic” Web and outlines technical challenges. We discuss how statistically quantified …,International Conference on Conceptual Modeling,2004,19
Ontology-enabled XML search,Ralf Schenkel; Anja Theobald; Gerhard Weikum,Abstract XML is rapidly evolving towards the standard for data integration and exchangeover the Internet and within intranets; covering the complete spectrum from largelyunstructured; ad hoc documents to highly structured; schematic data. However; establishedXML query languages like XML-QL [96] or XQuery [34] cannot cope with the rapid growth ofinformation in open environments such as the Web or intranets of large corporations; as theyare bound to boolean retrieval and do not provide any relevance ranking for the (typicallynumerous) results. Recent approaches such as XIRQL [128] or our own system XXL [295;296] that are driven by techniques from information retrieval overcome the latter problem byconsidering the relevance of each potential hit for the query and returning the results in aranked order; using similarity measures like the cosine measure. But they are still tied to …,*,2003,19
Towards response time guarantees for e-service middleware,Achim Kraiss; Frank Schoen; Gerhard Weikum; Uwe Deppisch,Quality of service (QoS) is a hot topic for Internet-based e-commerce and other e-services.However; it is much more talked about than it is really provided by deployed systems.Application service providers such as Akamai or performance rating companies such asKeynote contribute to a paradigm shift from 'best effort'to performance guarantees; but thesetrends still disregard a number of critical issues. First; they mostly focus on mean responsetime averaged over long time periods like weeks or months; but in high-end applicationssuch as banking or online stock brokerage the variance and the tail of the response timedistribution are equally important and it is often the peak load during the busiest hour thatmatters most. Second; advanced e-services aim to provide differentiated QoS for variousclasses of customers and requests. This requires a judicious prioritization with regard to …,IEEE Data Eng. Bull.,2001,19
Pros and cons of operating system transactions for data base systems,Gerhard Weikum,Recently; several proposals have been made to integrate the well-known notion oftransaction into operating systems. A general transaction service could serve M a commonbasis for various client types; including data base systems. In this paper; bene5ts as we5 asdrawbacks of such an operating system facility are diiussed. A msjor contribution is toinvestigate the suitability of three basic kinds of DBS server concepts with regard to theutilization of OS transactions. To overcome some of the problems of building DBStransactions on top of OS transections; we propose a multi-level trausaetion methodology.Single DBS requests are handled as transactions by the OS layer. Management of entireDBS transactions is done in the DBS supported by basic OS services. This multi-leveltransaction management method seems to have certain performance 8dv8ntsges over …,Proceedings of 1986 ACM Fall joint computer conference,1986,19
Relationship queries on extended knowledge graphs,Mohamed Yahya; Denilson Barbosa; Klaus Berberich; Qiuyue Wang; Gerhard Weikum,Abstract Entity search over text corpora is not geared for relationship queries where answersare tuples of related entities and where a query often requires joining cues from multipledocuments. With large knowledge graphs; structured querying on their relational facts is analternative; but often suffers from poor recall because of mismatches between user queriesand the knowledge graph or because of weakly populated relations. This paper presents theTriniT search engine for querying and ranking on extended knowledge graphs that combinerelational facts with textual web contents. Our query language is designed on the paradigmof SPO triple patterns; but is more expressive; supporting textual phrases for each of theSPO arguments. We present a model for automatic query relaxation to compensate formismatches between the data and a user's query. Query answers--tuples of entities--are …,Proceedings of the Ninth ACM International Conference on Web Search and Data Mining,2016,18
PolariCQ: Polarity classification of political quotations,Rawia Awadallah; Maya Ramanath; Gerhard Weikum,Abstract We consider the problem of automatically classifying quotations about politicaldebates into both topic and polarity. These quotations typically appear in news media andonline forums. Our approach maps quotations onto one or more topics in a category systemof political debates; containing more than a thousand fine-grained topics. To overcome thedifficulty that pro/con classification faces due to the brevity of quotations and sparseness offeatures; we have devised a model of quotation expansion that harnesses antonyms fromthesauri like WordNet. We developed a suite of statistical language models; judiciouslycustomized to our settings; and use these to define similarity measures for unsupervised orsupervised classifications. Experiments show the effectiveness of our method.,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,18
OpinioNetIt: Understanding the Opinions-People network for politically controversial topics,Rawia Awadallah; Maya Ramanath; Gerhard Weikum,Abstract The wikileaks documents or the economic crises in Ireland and Portugal are someof the controversial topics being played on the news everyday. Each of these topics hasmany different aspects; and there is no absolute; simple truth in answering questions suchas: should the EU guarantee the financial stability of each member country; or should thecountries themselves be solely responsible? To understand the landscape of opinions; itwould be helpful to know which politician or other stakeholder takes which position-supportor opposition-on these aspects of controversial topics. In this paper; we describe our system;named OpinioNetIt (pronounced similar to" opinionated"); which aims to automatically derivea map of the opinions-people network from news and other Web documents. We build thisnetwork as follows. First; we make use of a small number of generic seeds to identify …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,18
MinervaDL: An architecture for information retrieval and filtering in distributed digital libraries,Christian Zimmer; Christos Tryfonopoulos; Gerhard Weikum,Abstract We present MinervaDL; a digital library architecture that supports approximateinformation retrieval and filtering functionality under a single unifying framework. Thearchitecture of MinervaDL is based on the peer-to-peer search engine Minerva; and is ableto handle huge amounts of data provided by digital libraries in a distributed and self-organizing way. The two-tier architecture and the use of the distributed hash table as therouting substrate provides an infrastructure for creating large networks of digital libraries withminimal administration costs. We discuss the main components of this architecture; presentthe protocols that regulate node interactions; and experimentally evaluate our approach.,International Conference on Theory and Practice of Digital Libraries,2007,18
Automatic tuning of data placement and load balancing in disk arrays,Peter Scheuermann; Gerhard Weikum; Peter Zabback,Abstract Large arrays of small disks are providing an attractive approach for highperformance I/O systems. They allow for low-cost; reliable storage and can achieve higherthroughput compared to large disks. However; in order to make effective use of thecommercially available architectures; it is necessary to develop intelligent software tools thatallow automatic tuning of the disk arrays to varying workloads. In this paper we describe anintegrated set of algorithms and the implementation of a file manager for automatic filepartitioning and allocation and for load balancing on disk arrays. Our approach consists ofmodular building blocks that can be invoked independently of each other; thus; algorithmsfor file allocation and disk load balancing can be used regardless of whether striping isemployed or not. Our heuristic method for file partitioning aims to determine the optimal …,*,1992,18
Konzeption und Realisierung einer mengenorientierten Seitenschnittstelle zum effizienten Zugriff auf Komplexe Objekte,Gerhard Weikum; Bernd Neumann; H-B Paul,Zusammenfassung Eine Facette Komplexer Objekte; wie sie derzeit im Zusammenhang mitDatenbanksystemen für Non-Standard-Anwendungen diskutiert werden; ist; daß sie extremgroß werden können. Beim Zugriff auf ganze Objekte bzw. große Subobjekte entsteht inherkömmlichen Systemarchitekturen ein Engpaß an der Seitenschnittstelle; wenn diese sehroft wiederholt aufgerufen werden muß. Wir schlagen eine mengenorientierteSeitenschnittstelle vor; die es erlaubt; ganze Mengen von Seiten auf einmal im Puffer zufixieren oder zum Zurückschreiben freizugeben. Auf diese Weise wird ein seitenstrukturierterObjektpuffer für variabel lange; sehr große Komplexe Speicher-Objekte realisiert. DieMengenorientierung sollte bis zur I/O-Schnittstelle des Betriebssystems durchgereichtwerden; um ein optimales Plattenzugriffsverhalten zu erzielen. Messungen mit der …,*,1987,18
KnowLife: a versatile approach for constructing a large knowledge graph for biomedical sciences,Patrick Ernst; Amy Siu; Gerhard Weikum,Biomedical knowledge bases (KB's) have become important assets in life sciences. Priorwork on KB construction has three major limitations. First; most biomedical KBs are manuallybuilt and curated; and cannot keep up with the rate at which new findings are published.Second; for automatic information extraction (IE); the text genre of choice has been scientificpublications; neglecting sources like health portals and online communities. Third; mostprior work on IE has focused on the molecular level or chemogenomics only; like protein-protein interactions or gene-drug relationships; or solely address highly specific topics suchas drug effects. We address these three limitations by a versatile and scalable approach toautomatic KB construction. Using a small number of seed facts for distant supervision ofpattern-based extraction; we harvest a huge number of facts in an automated manner …,BMC bioinformatics,2015,17
Evin: Building a knowledge base of events,Erdal Kuzey; Gerhard Weikum,Abstract We present EVIN: a system that extracts named events from news articles;reconciles them into canonicalized events; and organizes them into semantic classes topopulate a knowledge base. EVIN exploits different kinds of similarity measures amongnews; referring to textual contents; entity occurrences; and temporal ordering. Thesesimilarities are captured in a multi-view attributed graph. To distill canonicalized events;EVIN coarsens the graph by iterative merging based on a judiciously designed loss function.To infer semantic classes of events; EVIN uses statistical language models. EVIN provides aGUI that allows users to query the constructed knowledge base of events; and to explore it ina visual manner.,Proceedings of the 23rd International Conference on World Wide Web,2014,17
Anonymity and censorship resistance in unstructured overlay networks,Michael Backes; Marek Hamerlik; Alessandro Linari; Matteo Maffei; Christos Tryfonopoulos; Gerhard Weikum,Abstract This paper presents Clouds; a peer-to-peer protocol that guarantees bothanonymity and censorship resistance in semantic overlay networks. The design of such aprotocol needs to meet a number of challenging goals: enabling the exchange of encryptedmessages without assuming previously shared secrets; avoiding centralised infrastructures;like trusted servers or gateways; and guaranteeing efficiency without establishing directconnections between peers. Anonymity is achieved by cloaking the identity of protocolparticipants behind groups of semantically close peers. Censorship resistance isguaranteed by a cryptographic protocol securing the anonymous communication betweenthe querying peer and the resource provider. Although we instantiate our technique onsemantic overlay networks to exploit their retrieval capabilities; our framework is general …,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2009,17
The Atomic Manifesto.,Cliff B Jones; David B Lomet; Alexander B Romanovsky; Gerhard Weikum,Page 1. The Atomic Manifesto Cliff Jones (University of Newcastle upon Tyne; UKcliff.jones@ncl.ac.uk) David Lomet (Microsoft Research; USA lomet@microsoft.com) AlexanderRomanovsky (University of Newcastle upon Tyne; UK alexander.romanovsky@ncl.ac.uk) GerhardWeikum (MPI Saarbruecken; Germany weikum@mpi-sb.mpg.de) Dagstuhl Seminar (OrganizerAuthors) and Alan Fekete; Marie-Claude Gaudel; Henry F. Korth; Rogerio de Lemos; Eliot Moss;Ravi Rajwar; Krithi Ramamritham; Brian Randell; Luis Rodrigues Dagstuhl Seminar (ParticipantAuthors). Abstract: This paper is a manifesto for future research on “atomicity” in its many guisesand is based on a five-day workshop on “Atomicity in System Design and Execution” that tookplace in Schloss Dagstuhl in Germany in April 2004 …,J. UCS,2005,17
A framework for the physical design problem for data synopses,Arnd Christian König; Gerhard Weikum,Abstract Maintaining statistics on multidimensional data distributions is crucial for predictingthe run-time and result size of queries and data analysis tasks with acceptable accuracy. Tothis end a plethora of techniques have been proposed for maintaining a compact data“synopsis” on a single table; ranging from variants of histograms to methods based onwavelets and other transforms. However; the fundamental question of how to reconcile thesynopses for large information sources with many tables has been largely unexplored. Thispaper develops a general framework for reconciling the synopses on many tables; whichmay come from different information sources. It shows how to compute the optimalcombination of synopses for a given workload and a limited amount of available memory.The practicality of the approach and the accuracy of the proposed heuristics are …,International Conference on Extending Database Technology,2002,17
J-NERD: joint named entity recognition and disambiguation with rich linguistic features,Dat Ba Nguyen; Martin Theobald; Gerhard Weikum,Abstract Methods for Named Entity Recognition and Disambiguation (NERD) perform NERand NED in two separate stages. Therefore; NED may be penalized with respect to precisionby NER false positives; and suffers in recall from NER false negatives. Conversely; NEDdoes not fully exploit information computed by NER such as types of mentions. This paperpresents J-NERD; a new approach to perform NER and NED jointly; by means of aprobabilistic graphical model that captures mention spans; mention types; and the mappingof mentions to entities in a knowledge base. We present experiments with different kinds oftexts from the CoNLL'03; ACE'05; and ClueWeb'09-FACC1 corpora. J-NERD consistentlyoutperforms state-of-the-art competitors in end-to-end NERD precision; recall; and F1.,Transactions of the Association for Computational Linguistics,2016,16
Knowledge harvesting from text and web sources,Fabian Suchanek; Gerhard Weikum,The proliferation of knowledge-sharing communities such as Wikipedia and the progress inscalable information extraction from Web and text sources has enabled the automaticconstruction of very large knowledge bases. Recent endeavors of this kind includeacademic research projects such as DBpedia; KnowItAll; Probase; ReadTheWeb; andYAGO; as well as industrial ones such as Freebase and Trueknowledge. These projectsprovide automatically constructed knowledge bases of facts about named entities; theirsemantic classes; and their mutual relationships. Such world knowledge in turn enablescognitive applications and knowledge-centric services like disambiguating natural-languagetext; deep question answering; and semantic search for entities and relations in Web andenterprise data. Prominent examples of how knowledge bases can be harnessed include …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,16
CATE: context-aware timeline for entity illustration,Tran Anh Tuan; Shady Elbassuoni; Nicoleta Preda; Gerhard Weikum,Abstract Wikipedia has become one of the most authoritative information sources on theWeb. Each article in Wikipedia provides a portrait of a certain entity. However; such a portraitis far from complete. An informative portrait of an entity should also reveal the context theentity belongs to. For example; for a person; major historical; political and cultural events thatcoincide with her life are important and should be included in that person's portrait. Similarly;the person's interactions with other people are also important. All this information should besummarized and presented in an appealing and interactive visual interface that enablesusers to quickly scan the entity's portrait. We demonstrate CATE which is a system thatutilizes Wikipedia to create a portrait of a given entity of interest. We provide a visualizationtool that summarizes the important events related to the entity. The novelty of our …,Proceedings of the 20th international conference companion on World wide web,2011,16
Einstein: physicist or vegetarian? summarizing semantic type graphs for knowledge discovery,Tomasz Tylenda; Mauro Sozio; Gerhard Weikum,Abstract The Web and; in particular; knowledge-sharing communities such as Wikipediacontain a huge amount of information encompassing disparate and diverse fields.Knowledge bases such as DBpedia or Yago represent the data in a concise and morestructured way bearing the potential of bringing database tools to Web Search. The wealth ofdata; however; poses the challenge of how to retrieve important and valuable information;which is often intertwined with trivial and less important details. This calls for an efficient andautomatic summarization method. In this demonstration proposal; we consider the novelproblem of summarizing the information related to a given entity; like a person or anorganization. To this end; we utilize the rich type graph that knowledge bases provide foreach entity; and define the problem of selecting the best cost-restricted subset of types as …,Proceedings of the 20th international conference companion on World wide web,2011,16
Language-model-based pro/con classification of political text,Rawia Awadallah; Maya Ramanath; Gerhard Weikum,Abstract Given a controversial political topic; our aim is to classify documents debating thetopic into pro or con. Our approach extracts topic related terms; pro/con related terms; andpairs of topic related and pro/con related terms and uses them as the basis for constructing apro query and a con query. Following standard LM techniques; a document is classified aspro or con depending on which of the query likelihoods is higher for the document. Ourexperiments show that our approach is promising.,Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval,2010,16
Anonymous and censorship resistant content sharing in unstructured overlays,Michael Backes; Marek Hamerlik; Alessandro Linari; Matteo Maffei; Christos Tryfonopoulos; Gerhard Weikum,Abstract Semantic overlay networks are an instance of unstructured overlays; where peersthat are semantically; thematically; or socially close are organized into groups to exploitsimilarities at query time. In this work we present Clouds; a novel P2P search infrastructurefor providing anonymous and censorship resistant search functionality in such networks.Although we utilize semantic overlays to exploit their retrieval capabilities; our framework isgeneral and can be applied to any unstructured overlay.,Proceedings of the twenty-seventh ACM symposium on Principles of distributed computing,2008,16
Architectural alternatives for information filtering in structured overlays,Christos Tryfonopoulos; Christian Zimmer; Gerhard Weikum; Manolis Koubarakis,Content providers are naturally distributed and produce large amounts of new informationevery day. Peer-to-peer information filtering is a promising approach that offers scalability;adaptivity to high dynamics; and failure resilience. The authors developed two approachesthat utilize the chord distributed hash table as the routing substrate; but one stressesretrieval effectiveness; whereas the other relaxes recall guarantees to achieve lowermessage traffic and thus better scalability. This article highlights the two approaches' maincharacteristics; presents the issues and trade-offs involved in their design; and comparesthem in terms of scalability; efficiency; and filtering effectiveness.,IEEE Internet Computing,2007,16
Computing trusted authority scores in peer-to-peer web search networks,Josiane Xavier Parreira; Debora Donato; Carlos Castillo; Gerhard Weikum,Abstract Peer-to-peer (P2P) networks have received great attention for sharing andsearching information in large user communities. The open and anonymous nature of P2Pnetworks is one of its main strengths; but it also opens doors to manipulation of theinformation and of the quality ratings. In our previous work (JX Parreira; D. Donato; S. Micheland G. Weikum in VLDB 2006) we presented the JXP algorithm for distributed computingPageRank scores for information units (Web pages; sites; peers; social groups; etc.) within alink-or endorsement-based graph structure. The algorithm builds on local authoritycomputations and bilateral peer meetings with exchanges of small data structures that arerelevant for gradually learning about global properties and eventually converging towardsglobal authority rankings.,Proceedings of the 3rd international workshop on Adversarial information retrieval on the web,2007,16
Applied algebra; algebraic algorithms and error-correcting codes,Hsiao-Feng Lu,The AAECC Symposia Series was started in 1983 by Alain Poli (Toulouse); who; togetherwith R. Desq; D. Lazard and P. Camion; organized the first conference. Originally theacronym AAECC meant “Applied Algebra and Error-Correcting Codes.” Over the years itsmeaning has shifted to “Applied Algebra; Algebraic Algorithms and Error-Correcting Codes;”reflecting the growing importance of complexity; particularly for decoding algorithms. Duringthe AAECC-12 symposium the conference committee decided to enforce the theory andpractice of the coding side as well as the cryptographic aspects. Algebra was conserved; asin the past; but slightly more oriented to algebraic geometry codes; finite fields; complexity;polynomials; and graphs.,*,2007,16
From the KERNEL to the COSMOS: The database research group at ETH Zurich,Hans-Jörg Schek; Marc H Scholl; Gerhard Weikum,Abstract This report describes COSMOS; the research program of the database researchgroup at ETH Zurich. These activities are a natural follow-on of DASDBS; the DarmstadtData base System project at the Technical University of Darmstadt. While most emphasis inDASDBS was on a database kernel system; the project at ETH focuses on the coopera tionbetween a database system and its environment. The environment consists of cli ents askingfor database service and other systems offering service to the database system. Theresearch objective is the exploration of the architecture of a COoperative System for theManagement of ObjectS (COSMOS). In short; we are on the way" from the kernel to thecosmos".,*,1990,16
YAGO: A multilingual knowledge base from wikipedia; wordnet; and geonames,Thomas Rebele; Fabian Suchanek; Johannes Hoffart; Joanna Biega; Erdal Kuzey; Gerhard Weikum,Abstract YAGO is a large knowledge base that is built automatically from Wikipedia;WordNet and GeoNames. The project combines information from Wikipedias in 10 differentlanguages into a coherent whole; thus giving the knowledge a multilingual dimension. It alsoattaches spatial and temporal information to many facts; and thus allows the user to querythe data over space and time. YAGO focuses on extraction quality and achieves a manuallyevaluated precision of 95%. In this paper; we explain how YAGO is built from its sources;how its quality is evaluated; how a user can access it; and how other projects utilize it.,International Semantic Web Conference,2016,15
Harpy: Hypernyms and alignment of relational paraphrases,Adam Grycner; Gerhard Weikum,Abstract Collections of relational paraphrases have been automatically constructed fromlarge text corpora; as a WordNet counterpart for the realm of binary predicates and theirsurface forms. However; these resources fall short in their coverage of hypernymy links(subsumptions) among the synsets of phrases. This paper closes this gap by computing ahigh-quality alignment between the relational phrases of the Patty taxonomy; one of thelargest collections of this kind; and the verb senses of WordNet. To this end; we devisejudicious features and develop a graph-based alignment algorithm by adapting andextending the SimRank random-walk method. The resulting taxonomy of relational phrasesand verb senses; coined HARPY; contains 20;812 synsets organized into a Directed AcyclicGraph (DAG) with 616;792 hypernymy links. Our empirical assessment; indicates that the …,Proceedings of COLING 2014; the 25th International Conference on Computational Linguistics: Technical Papers,2014,15
Time-based exploration of news archives,Omar Alonso; Klaus Berberich; Srikanta Bedathur; Gerhard Weikum,ABSTRACT In this paper; we present NEAT; a prototype system that provides an explorationinterface to news archive search. Our prototype visualizes search results making use of twokinds of temporal information; namely; news articles' publication dates but also theircontained temporal expressions. The displayed timelines are annotated with major events;harvested using crowdsourcing; to make it easier for users to put the shown search resultsinto context. The prototype has been fully implemented and deployed on the New YorkTimes Annotated Corpus.,HCIR 2010,2010,15
Find your advisor: robust knowledge gathering from the web,Ndapandula Nakashole; Martin Theobald; Gerhard Weikum,Abstract We present a robust method for gathering relational facts from the Web; based onmatching generalized patterns which are automatically learned from seed facts for relationsof interest. Our approach combines these generalized patterns for high recall informationextraction with a rule-based; declarative reasoning approach to also ensure high precision.Newly extracted candidate facts are assigned statistical weights which reflect the strengthsof the patterns used to extract them. For checking the plausibility of candidate facts withrespect to existing knowledge and competing hypotheses; we use an efficient algorithm forweighted Max-Sat over propositional-logic clauses. In contrast to prior work on reasoning-based information extraction; we employ richer statistics and smart pruning to bound thenumber of grounded rules passed on to the Max-Sat solver.,Procceedings of the 13th International Workshop on the Web and Databases,2010,15
Adaptive personalization of web search,Shady Elbassuoni; Julia Luxenburger; Gerhard Weikum,Abstract An often stated problem in the state-of-the-art web search is its lack of useradaptation; as all users are presented with the same search results for a given query string.A user submitting an ambiguous query such as” java” with a strong interest in traveling mightappreciate finding pages related to the Indonesian island Java. However; if the same usersearched for programming tutorials a few minutes ago; the situation would be completelydifferent; and call for programming-related results. Furthermore suppose our sample usersearches for” java hashmap”. Again imposing her interest into traveling might this time havethe contrary effect and even harm the result quality. Thus the effectiveness of apersonalization of web search shows high variance in performance depending on the query;the user and the search context. To this end; carefully choosing the right personalization …,*,2008,15
GeoSensor networks,Silvia Nittel,Gaussian processes (GPs) are local approximation techniques that model spatial data byplacing (and updating) priors on the covariance structures underlying the data. Originallydeveloped for geo-spatial contexts; they are also applicable in general contexts that involvecomputing and modeling with multi-level spatial aggregates; eg; modeling a configurationspace for crystallographic design; casting folding energies as a function of a protein'scontact map; and formulation of vaccination policies taking into account social dynamics ofindividuals. Typically; we assume a parametrized covariance structure underlying the data tobe modeled. We estimate the covariance parameters conditional on the locations for whichwe have observed data; and use the inferred structure to make predictions at new locations.GPs have a probabilistic basis that allow us to estimate variances at unsampled locations …,Encyclopedia of GIS,2008,15
Clustered scheduling algorithms for mixed-media disk workloads in a multimedia server,Elias Balafoutis; Michael Paterakis; Peter Triantafillou; Guido Nerjes; Peter Muth; Gerhard Weikum,Abstract Divisible load scenarios occur in modern media server applications since mostmultimedia applications typically require access to continuous and discrete data. A highperformance Continuous Media (CM) server greatly depends on the ability of its disk IOsubsystem to serve both types of workloads efficiently. Disk scheduling algorithms for mixedmedia workloads; although they play a central role in this task; have been overlooked byrelated research efforts. These algorithms must satisfy several stringent performance goals;such as achieving low response time and ensuring fairness; for the discrete-data workload;while at the same time guaranteeing the uninterrupted delivery of continuous data; for thecontinuous-data workload. The focus of this paper is on disk scheduling algorithms for mixedmedia workloads in a multimedia information server. We propose novel algorithms …,Cluster Computing,2003,15
The BINGO! focused crawler: From bookmarks to archetypes,Sergej Sizov; Stefan Siersdorfer; Martin Theobald; Gerhard Weikum,The BINGO! system implements an approach to focused crawling that aims to overcome thelimitations of the initial training data. To this end; BINGO! identifies; among the crawled andpositively classified documents of a topic; characteristic" archetypes" and uses them forperiodically re-training the classifier; this way the crawler is dynamically adapted based onthe most significant documents seen so far. Two kinds of archetypes are considered: goodauthorities as determined by employing Kleinberg's link analysis algorithm; and documentsthat have been automatically classified with high confidence using a linear SVM classifier.,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,15
The COMFORT project,Gerhard Weikum; Christof Hasse; Axel Moenkeberg; Michael Rys; Peter Zabback,The COMFORT project; which is intended to automate the performance tuning of databasesystems; is discussed. The project addresses several important tuning issues of paralleldatabase systems in a multiuser environment; and it aims to develop general architecturalprinciples for a self-tuning database system. The overall goals and the rationale areexamined; and a brief overview of the current work is given.,Parallel and Distributed Information Systems; 1993.; Proceedings of the Second International Conference on,1993,15
I/O-Parallelitaet und Fehlertoleranz in Disk-Arrays. Teil 1: I/O-Parallelitaet,Gerhard Weikum; Peter Zabback,*,Informatik Spektrum,1993,15
UWN: A large multilingual lexical knowledge base,Gerard de Melo; Gerhard Weikum,Abstract We present UWN; a large multilingual lexical knowledge base that describes themeanings and relationships of words in over 200 languages. This paper explains how linkprediction; information integration and taxonomy induction methods have been used to buildUWN based on WordNet and extend it with millions of named entities from Wikipedia. Weadditionally introduce extensions to cover lexical relationships; frame-semantic knowledge;and language data. An online interface provides human access to the data; while a softwareAPI enables applications to look up over 16 million words and names.,Proceedings of the ACL 2012 System Demonstrations,2012,14
ANGIE: Active knowledge for interactive exploration,Nicoleta Preda; Fabian M Suchanek; Gjergji Kasneci; Thomas Neumann; Maya Ramanath; Gerhard Weikum,Abstract We present ANGIE; a system that can answer user queries by combiningknowledge from a local database with knowledge retrieved from Web services. If a userposes a query that cannot be answered by the local database alone; ANGIE calls theappropriate Web services to retrieve the missing information. This information is integratedseamlessly and transparently into the local database; so that the user can query and browsethe knowledge base while appropriate Web services are called automatically in thebackground.,Proceedings of the VLDB Endowment,2009,14
Automata; Languages and Programming: 36th International Colloquium; ICALP 2009; Rhodes; Greece; July 5-12; 2009; Proceedings,Susanne Albers; Alberto Marchetti-Spaccamela; Yossi Matias; Sotiris Nikoletseas; Wolfgang Thomas,ICALP 2009; the 36th edition of the International Colloquium on Automata; Languages andProgramming; was held on the island of Rhodes; July 6–10; 2009. ICALP is a series ofannual conferences of the European Association for Theoretical Computer Science (EATCS)which ﬁrst took place in 1972. This year; the ICALP program consisted of the establishedtrack A (focusing on algorithms; complexity and games) and track B (focusing on logic;automata; semantics and theory of programming); and of the recently introduced track C (in2009 focusing on foundations of networked computation). In response to the call for papers;the Program Committee received 370 submissions: 223 for track A; 84 for track B and 63 fortrack C. Out of these; 108 papers were selected for inclusion in the scientiﬁc program: 62papers for track A; 24 for track B and 22 for track C. The selection was made by the …,*,2009,14
Exploiting correlated keywords to improve approximate information filtering,Christian Zimmer; Christos Tryfonopoulos; Gerhard Weikum,Abstract Information filtering; also referred to as publish/subscribe; complements one-timesearching since users are able to subscribe to information sources and be notified whenevernew documents of interest are published. In approximate information filtering only selectedinformation sources; that are likely to publish documents relevant to the user interests in thefuture; are monitored. To achieve this functionality; a subscriber exploits statistical metadatato identify promising publishers and index its continuous query only in those publishers. Thestatistics are maintained in a directory; usually on a per-keyword basis; thus disregardingpossible correlations among keywords. Using this coarse information; poor publisherselection may lead to poor filtering performance and thus loss of interesting documents. 1Based on the above observation; this work extends query routing techniques from the …,Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,2008,14
On the utility of automatically generated wordnets,Gerard De Melo; Gerhard Weikum,Abstract. Lexical resources modelled after the original Princeton Word-Net are beingcompiled for a considerable number of languages; however most have yet to reach acomparable level of coverage. In this paper; we show that automatically built wordnets;created from an existing wordnet in conjunction with translation dictionaries; are a suitablealternative for many applications; despite the errors introduced by the automatic buildingprocedure. Apart from analysing the resources directly; we conducted tests on semanticrelatedness assessment and cross-lingual text classification with very promising results.,Proc. Global WordNet Conference,2008,14
Transactional Information Systems: Theory,Gerhard Weikum; Gottfried Vossen,*,Algorithms; and the Practice of Concurrency Control and Recovery,2002,14
Mentor: Entwurf einer Workflow-Management-Umgebung basierend auf State-und Activitycharts,Dirk Wodtke; Angelika Kotz Dittrich; Peter Muth; Markus Sinnwell; Gerhard Weikum,Zusammenfassung Der Aspekt der Steuerung von Arbeitsabläufen rückt heute inUnternehmen-gerade auch im Dienstleistungsbereich-immer mehr in den Vordergrund.Eine möglichst optimale Strukturierung und Abwicklung von Unternehmensprozessen spielt;etwa im Rahmen des Business Process Reengineering; für die Wirtschaftlichkeit eine großeRolle. Die aus einzelnen Arbeitsschritten zusammengesetzten Arbeitsabläufe werden auchals „Workflows „bezeichnet. Beider computerunterstützten Abwicklung von Workflows sollendie einzelnen Arbeitsschritte mittels beliebiger Datenbanksysteme und/oder andererSystemkomponenten (Dokumentenarchive; Applikationsprogramme etc.) ausgeführtwerden; die Workflow-Management-Umgebung soll die Infrastruktur zur koordinierten undfehlertoleranten Ausführung in einer unternehmensweit verteilten und hochgradig …,*,1995,14
Principles and Realisation Strategies of Multi-Level Transaction Management,Gerhard Weikum,*,*,1987,14
As time goes by: comprehensive tagging of textual phrases with temporal scopes,Erdal Kuzey; Vinay Setty; Jannik Strötgen; Gerhard Weikum,Abstract Temporal expressions (TempEx's for short) are increasingly important in search;question answering; information extraction; and more. Techniques for identifying andnormalizing explicit temporal expressions work well; but are not designed for and cannotcope with textual phrases that denote named events; such as" Clinton's term as secretary ofstate". This paper addresses the problem of detecting such temponyms; inferring theirtemporal scopes; and mapping them to events in a knowledge base if present there. Wepresent methods for this kind of temponym resolution; using an entity-and TempEx-orienteddocument model and the Yago knowledge base for distant supervision. We develop a familyof Integer Linear Programs for jointly inferring temponym mappings to the timeline andknowledge base. This enriches the document representation and also extends the …,Proceedings of the 25th International Conference on World Wide Web,2016,13
Crowdsourced entity markup,Lili Jiang; Yafang Wang; Johannes Hoffart; Gerhard Weikum,Abstract. Entities; such as people; places; products; etc.; exist in knowledge bases andlinked data; on one hand; and in web pages; news articles; and social media; on the otherhand. Entity markup; like Named Entities Recognition and Disambiguation (NERD); is theessential means for adding semantic value to unstructured web contents and this wayenabling the linkage between unstructured and structured data and knowledge collections.A major challenge in this endeavor lies in the dynamics of the digital contents about theworld; with new entities emerging all the time. In this paper; we propose a crowdsourcedframework for NERD; specifically addressing the challenge of emerging entities in socialmedia. Our approach combines NERD techniques with the detection of entity alias namesand with co-reference resolution in texts. We propose a linking-game based …,Proceedings of the 1st International Conference on Crowdsourcing the Semantic Web,2013,13
A weakly supervised model for sentence-level semantic orientation analysis with multiple experts,Lizhen Qu; Rainer Gemulla; Gerhard Weikum,Abstract We propose the weakly supervised Multi-Experts Model (MEM) for analyzing thesemantic orientation of opinions expressed in natural language reviews. In contrast to mostprior work; MEM predicts both opinion polarity and opinion strength at the level of individualsentences; such fine-grained analysis helps to understand better why users like or dislikethe entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason; MEM is weakly supervised:It starts with potentially noisy indicators obtained from coarse-grained training data (ie;document-level ratings); a small set of diverse base predictors; and; if available; smallamounts of fine-grained training data. We integrate these noisy indicators into a unifiedprobabilistic framework using ideas from ensemble learning and graph-based semi …,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,2012,13
Constructing and utilizing wordnets using statistical methods,Gerard de Melo; Gerhard Weikum,Abstract Lexical databases following the wordnet paradigm capture information aboutwords; word senses; and their relationships. A large number of existing tools and datasetsare based on the original WordNet; so extending the landscape of resources aligned withWordNet leads to great potential for interoperability and to substantial synergies. Wordnetsare being compiled for a considerable number of languages; however most have yet toreach a comparable level of coverage. We propose a method for automatically producingsuch resources for new languages based on WordNet; and analyse the implications of thisapproach both from a linguistic perspective as well as by considering natural languageprocessing tasks. Our approach takes advantage of the original WordNet in conjunction withtranslation dictionaries. A small set of training associations is used to learn a statistical …,Language Resources and Evaluation,2012,13
Computer Aided Verification: 21st International Conference; CAV 2009; Grenoble; France; June 26-July 2; 2009; Proceedings,Ahmed Bouajjani; Oded Maler,This volume contains the proceedings of the 21st International Conference on Computer-Aided Veri? cation (CAV) held in Grenoble; France; between June 28 and July 2; 2009. CAVis dedicated to the advancement of the theory and practice of computer-aided formalanalysis methods for hardware and software systems. Its scope ranges from theoreticalresults to concrete applications; with an emphasis on practical veri? cation tools and theunderlying algorithms and techniques. Everyinstanceofaconferenceisspecialinitso….ThisCAVisspecialfor at least two reasons:? rst; it took place in Grenoble; the place where theCAV series started 20 years ago. Secondly; there was a particularly large number of papersubmissions: 135 regular papers and 34 tool papers; summing up to 169 submissions. Theyall went through an active review process; with each …,*,2009,13
Trust in agent societies,Rino Falcone; K Suzanne Barber; Jordi Sabater-Mir; Munindar P Singh,*,11th International Workshop; TRUST 2008; Estoril; Portugal; May 12-13; 2008. Revised Selected and Invited Papers,2009,13
Electronic Goverment: 6th International Conference; EGOV 2007; Regensburg; Germany; September 3-7; 2007; Proceedings,Maria A Wimmer; Jochen Scholl,EGOV 2007 was the sixth edition of this highly successful series of annual int-nationalconferences dedicated to electronic government research and practice. Like all itspredecessors; EGOV 2007 achieved a remarkable number of paper submissions. Moreover;the quality of this year's submissions again superseded previous years' submissions. For thethird year in a row; the conference was anteceded by a doctoral colloquium; withapproximately 20 PhD projects d-cussed. The conference also provided a forum foracademic work in progress; for practitioner reports; and for workshops on specialty topics.Along with the International Conference on Digital Government Research (dg. o) intheUSAandthe e-GovernmentTrackatthe HawaiiInternationalC-ference on System Sciences(HICSS); the EGOV series of conferences has est-lished itself as the leading annual …,*,2007,13
A comparative study of pub/sub methods in structured P2P networks,Matthias Bender; Sebastian Michel; Sebastian Parkitny; Gerhard Weikum,Abstract Methods for publish/subscribe applications over P2P networks have been aresearch issue for a long time. Many approaches have been developed and evaluated; buttypically each based on different assumptions; which makes their mutual comparison verydifficult if not impossible. We identify two design patterns that can be used to implementpublish/subscribe applications over structured P2P networks and provide an analyticalanalysis of their complexity. Based on a characterization of different real-world usagescenarios we present evidence as to which approach is preferable for certain applicationclasses. Finally; we present simulation results that support our analysis.,*,2007,13
Goal-oriented methods and meta methods for document classification and their parameter tuning,Stefan Siersdorfer; Sergej Sizov; Gerhard Weikum,Abstract Automatic text classification methods come with various calibration parameterssuch as thresholds for probabilities in Bayesian classifiers or for hyperplane distances inSVM classifiers. In a given application context these parameters should be set so as to meetthe relative importance of various result quality metrics such as precision versus recall. Inthis paper we consider classifiers that can accept a document for a topic; reject it; or abstain.We aim to meet the application's goals in terms of accuracy (ie; avoid false acceptances orrejections) and loss (ie; limit the fraction of documents for which no decision is made). To thisend we investigate restrictive forms of Support Vector Machine classifiers and we developmeta methods that split the training data into subsets for independently trained classifiersand then combine the results of these classifiers. These techniques tend to improve …,Proceedings of the thirteenth ACM international conference on Information and knowledge management,2004,13
Incremental scheduling of mixed workloads in multimedia information servers,Guido Nerjes; Peter Muth; Michael Paterakis; Y Romboyannakis; Peter Triantafillou; Gerhard Weikum,Abstract In contrast to pure video servers; advanced multimedia applications such as digitallibraries or teleteaching exhibit a mixed workload with massive access toconventional;“discrete” data such as text documents; images and indexes as well asrequests for “continuous data”; like video and audio data. In addition to the service qualityguarantees for continuous data requests; quality-conscious applications require that theresponse time of the discrete data requests stay below some user-tolerance threshold. Inthis paper; we study the impact of different disk scheduling policies on the service quality forboth continuous and discrete data. We provide a framework for describing various policies interms of few parameters; and we develop a novel policy that is experimentally shown tooutperform all other policies.,*,2000,13
Taxonomic data integration from multilingual Wikipedia editions,Gerard de Melo; Gerhard Weikum,Abstract Information systems are increasingly making use of taxonomic knowledge aboutwords and entities. A taxonomic knowledge base may reveal that the Lago di Garda is a lakeand that lakes as well as ponds; reservoirs; and marshes are all bodies of water. As thenumber of available taxonomic knowledge sources grows; there is a need for techniques tointegrate such data into combined; unified taxonomies. In particular; the Wikipediaencyclopedia has been used by a number of projects; but its multilingual nature has largelybeen neglected. This paper investigates how entities from all editions of Wikipedia as wellas WordNet can be integrated into a single coherent taxonomic class hierarchy. We rely onlinking heuristics to discover potential taxonomic relationships; graph partitioning to formconsistent equivalence classes of entities; and a Markov chain-based ranking approach …,Knowledge and information systems,2014,12
Discovering semantic relations from the web and organizing them with patty,Ndapandula Nakashole; Gerhard Weikum; Fabian Suchanek,Abstract PATTY is a system for automatically distilling relational patterns from the Web; forexample; the pattern" X covered Y" between a singer and someone else's song. We haveextracted a large collection of such patterns and organized them in a taxonomic manner;similar in style to the WordNet thesaurus but capturing relations (binary predicates) insteadof concepts and classes (unary predicates). The patterns are organized by semantic typesand synonyms; and they form a hierarchy based on subsumptions. For example;" X coveredY" is subsumed by" X sang Y"; which in turn is subsumed by" X performed Y"(where X canbe any musician; not just a singer). In this paper we give an overview of the PATTY systemand the resulting collections of relational patterns. We discuss the four main components ofPATTY's architecture and a variety of use cases; including the paraphrasing of relations …,ACM SIGMOD Record,2013,12
Real-time population of knowledge bases: opportunities and challenges,Ndapandula Nakashole; Gerhard Weikum,Abstract Dynamic content is a frequently accessed part of the Web. However; mostinformation extraction approaches are batch-oriented; thus not effective for gathering rapidlychanging data. This paper proposes a model for fact extraction in real-time. Our modeladdresses the difficult challenges that timely fact extraction on frequently updated dataentails. We point out a naive solution to the main research question and justify the choiceswe make in the model we propose.,Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,2012,12
Personalizing the Search for Knowledge.,Minko Dudev; Shady Elbassuoni; Julia Luxenburger; Maya Ramanath; Gerhard Weikum,ABSTRACT Recent work on building semantic search engines has given rise to large graph-based knowledge repositories and facilities for querying them and more importantly; rankingthe results. While the ranking provided may prove to be acceptable in general; for a trulysatisfactory search experience; it is necessary to tailor the results according to the user'sinterest. In this paper; we address the issue of personalizing query results in the specificsetting of graph-based knowledge bases. In particular; we address two important issues: i)construction of the user profile based on the inference of the user's interest and ii) a formalmodel for personalized scoring which incorporates the user's interest. Preliminaryexperimental results show that our techniques are indeed promising.,PersDB,2008,12
Making SENSE: socially enhanced search and exploration,Tom Crecelius; Mouna Kacimi; Sebastian Michel; Thomas Neumann; Josiane Xavier Parreira; Ralf Schenkel; Gerhard Weikum,Abstract Online communities like Flickr; del. icio. us and YouTube have establishedthemselves as very popular and powerful services for publishing and searching contents;but also for identifying other users who share similar interests. In these communities; dataare usually annotated with carefully selected and often semantically meaningful tags;collaboratively chosen by the user who uploaded an item and other users who came acrossthe item. Items like urls or videos are typically retrieved by issueing queries that consist of aset of tags; returning items that have been frequently annotated with these tags. However;users often prefer a more personalized way of searching over such a'global'search;exploiting preferences of and connections between users. The SENSE system presented inthis demo supports hybrid personalization along two dimensions: in the social dimension …,Proceedings of the VLDB Endowment,2008,12
Comparing apples and oranges: normalized pagerank for evolving graphs,Klaus Berberich; Srikanta Bedathur; Gerhard Weikum; Michalis Vazirgiannis,Abstract PageRank is the best known technique for link-based importance ranking. Thecomputed importance scores; however; are not directly comparable across differentsnapshots of an evolving graph. We present an efficiently computable normalization forPageRank scores that makes them comparable across graphs. Furthermore; we show thatthe normalized PageRank scores are robust to non-local changes in the graph; unlike thestandard PageRank measure.,Proceedings of the 16th international conference on World Wide Web,2007,12
How naga uncoils: searching with entities and relations,Gjergji Kasneci; Fabain M Suchanek; Maya Ramanath; Gerhard Weikum,Abstract Current keyword-oriented search engines for theWorld WideWeb do not allowspecifying the semantics of queries. We address this limitation with NAGA 1; a new semanticsearch engine. NAGA builds on a large semantic knowledge base of binary relationships(facts) derived from the Web. NAGA provides a simple; yet expressive query language toquery this knowledge base. The results are then ranked with an intuitive scoring mechanism.We show the effectiveness and utility of NAGA by comparing its output with that of Googleonsome interesting queries.,Proceedings of the 16th international conference on World Wide Web,2007,12
Towards self-organizing query routing and processing for peer-to-peer web search,Gerhard Weikum; Peter Triantafillou; David Hales; Christian Schindelhauer,Abstract The peer-to-peer computing paradigm is an intriguing alternative to Google-stylesearch engines for querying and ranking Web content. In a network with many thousands ormillions of peers the storage and access load requirements per peer are much lighter thanfor a centralized Google-like server farm; thus more powerful techniques from informationretrieval; statistical learning; computational linguistics; and ontological reasoning can beemployed on each peer's local search engine for boosting the quality of search results. Inaddition; peers can dynamically collaborate on advanced and particularly difficult queries.Moroever; a peer-to-peer setting is ideally suited to capture local user behavior; like querylogs and click streams; and disseminate and aggregate this information in the network; at thediscretion of the corresponding user; in order to incorporate richer cognitive models. This …,Proceedings of the European Conference on Complex Systems (ECCS'05); Nov. 14th; Paris; France. Publishers: i6doc; Belgium,2005,12
BINGO!: Bookmark-Induced Gathering of Information.,Sergej Sizov; Martin Theobald; Stefan Siersdorfer; Gerhard Weikum,Abstract Focused (thematic) crawling is a relatively new; promising approach to improvingthe recall of expert search on the Web. It involves the automatic classiﬁcation of visiteddocuments into a user-or community-speciﬁc topic hierarchy (ontology). The quality of thetraining data for the classiﬁer is the most critical issue and potential bottleneck for theeffectivity and scale of a focused crawler This paper presents the BINGO! approach tofocused crawling that aims to overcome the limitations of the initial training data. To this end;BINGO! identiﬁes; among the crawled and positively classiﬁed documents of a topic;characteristic" archetypes" and uses them for periodically re-training the classiﬁer; this waythe crawler is dynamically adapted based on the most signiﬁcant documents seen so far.Two kinds of archetypes are considered: good authorities as determined by employing …,WISE,2002,12
A goal-driven auto-configuration tool for the distributed workflow management system mentorlite,Michael Gillmann; Jeanine Weissenfels; German Shegalov; Wolfgang Wonner; Gerhard Weikum,Abstract The Mentor-lite prototype has been developed within the research project“Architecture; Configuration; and Administration of Large Workflow Management Systems”funded by the German Science Foundation (DFG). It has evolved from its predecessorMentor [1]; but aims at a simpler architecture. The main goal of Mentor-lite has been to builda light-weight; extensible; and tailorable workflow management system (WFMS) with smallfootprint and easy-to-use administration capabilities. Our approach is to provide only kernelfunctionality inside the workflow engine; and consider system components like historymanagement and worklist management as extensions on top of the kernel. The key point toretain the light-weight nature is that these extensions are implemented as workflowsthemselves. The workflow specifications are interpreted at runtime; which is a crucial …,ACM SIGMOD Record,2000,12
The COMFORT project: A comfortable way to better performance,Gerhard Weikum; C Hasse; A Moenkeberg,*,*,1990,12
Ein Subsystem zur stabilen Speicherung versionenbehafteter; hierarchisch strukturierter Tupel,Uwe Deppisch; V Obermeit; H-B Paul; H-J Schek; Marc Scholl; Gerhard Weikum,Kurzfassung Das hier beschriebene Speichersystem unseres DarmstädterDatenbankkernsystems kann als Erweiterung der Dateiverwaltung von Betriebssystemenum Grundfunktionen eines Datenbanksystems betrachtet werden: Es dient der stabilenSpeicherung von strukturierten Datensätzen unter Einschluß einer Transaktionsverwaltungund optionaler Versionierung. Zur Strukturierung werden ausschließlich Relationen mitatomaren oder relationenwertigen Attributen zugelassen. Zur Manipulation erlauben wir imwesentlichen eine geschachtelte Projektion und mengenorientierte Änderungsoperationen.Eine solche Schnittstelle relativ weit unten in der Systemarchitektur erlaubt es. gezielt aufeinzelne Daten und gleichzeitig effizient auf Datenmengen zuzugreifen. Dementsprechendist die direkt darunterliegende Pufferschnittstelle (seiten-) mengenorientiert angelegt. Für …,*,1985,12
Commonsense in Parts: Mining Part-Whole Relations from the Web and Image Tags.,Niket Tandon; Charles Hariman; Jacopo Urbani; Anna Rohrbach; Marcus Rohrbach; Gerhard Weikum,Abstract Commonsense knowledge about part-whole relations (eg; screen partOf notebook)is important for interpreting user input in web search and question answering; or for objectdetection in images. Prior work on knowledge base construction has compiled part-wholeassertions; but with substantial limitations: i) semantically different kinds of part-wholerelations are conflated into a single generic relation; ii) the arguments of a part-wholeassertion are merely words with ambiguous meaning; iii) the assertions lack additionalattributes like visibility (eg; a nose is visible but a kidney is not) and cardinality information(eg; a bird has two legs while a spider eight); iv) limited coverage of only tens of thousandsof assertions. This paper presents a new method for automatically acquiring part-wholecommonsense from Web contents and image tags at an unprecedented scale; yielding …,AAAI,2016,11
Leveraging joint interactions for credibility analysis in news communities,Subhabrata Mukherjee; Gerhard Weikum,Abstract Media seems to have become more partisan; often providing a biased coverage ofnews catering to the interest of specific groups. It is therefore essential to identify credibleinformation content that provides an objective narrative of an event. News communities suchas digg; reddit; or newstrust offer recommendations; reviews; quality ratings; and furtherinsights on journalistic works. However; there is a complex interaction between differentfactors in such online communities: fairness and style of reporting; language clarity andobjectivity; topical perspectives (like political viewpoint); expertise and bias of communitymembers; and more. This paper presents a model to systematically analyze the differentinteractions in a news community between users; news; and sources. We develop aprobabilistic graphical model that leverages this joint interaction to identify 1) highly …,Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,2015,11
Relly: Inferring hypernym relationships between relational phrases,Adam Grycner; Gerhard Weikum; Jay Pujara; James Foulds; Lise Getoor,Abstract Relational phrases (eg;“got married to”) and their hypernyms (eg;“is a relative of”)are central for many tasks including question answering; open information extraction;paraphrasing; and entailment detection. This has motivated the development of severallinguistic resources (eg DIRT; PATTY; and WiseNet) which systematically collect andorganize relational phrases. These resources have demonstrable practical benefits; but areeach limited due to noise; sparsity; or size. We present a new general-purpose method;RELLY; for constructing a large hypernymy graph of relational phrases with high-qualitysubsumptions using collective probabilistic programming techniques. Our graph inductionapproach integrates small highprecision knowledge bases together with large automaticallycurated resources; and reasons collectively to combine these resources into a consistent …,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015,11
Werdy: Recognition and disambiguation of verbs and verb phrases with syntactic and semantic pruning,Luciano Del Corro; Rainer Gemulla; Gerhard Weikum,Abstract Word-sense recognition and disambiguation (WERD) is the task of identifying wordphrases and their senses in natural language text. Though it is well understood how todisambiguate noun phrases; this task is much less studied for verbs and verbal phrases. Wepresent Werdy; a framework for WERD with particular focus on verbs and verbal phrases.Our framework first identifies multi-word expressions based on the syntactic structure of thesentence; this allows us to recognize both contiguous and non-contiguous phrases. We thengenerate a list of candidate senses for each word or phrase; using novel syntactic andsemantic pruning techniques. We also construct and leverage a new resource of pairs ofsenses for verbs and their object arguments. Finally; we feed the so-obtained candidatesenses into standard word-sense disambiguation (WSD) methods; and boost their …,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),2014,11
Hyena-live: Fine-grained online entity type classification from natural-language text,Mohamed Amir Yosef; Sandro Bauer; Johannes Hoffart; Marc Spaniol; Gerhard Weikum,Abstract Recent research has shown progress in achieving high-quality; very fine-grainedtype classification in hierarchical taxonomies. Within such a multi-level type hierarchy withseveral hundreds of types at different levels; many entities naturally belong to multiple types.In order to achieve high-precision in type classification; current approaches are either limitedto certain domains or require time consuming multistage computations. As a consequence;existing systems are incapable of performing ad-hoc type classification on arbitrary inputtexts. In this demo; we present a novel Webbased tool that is able to perform domainindependent entity type classification under real time conditions. Thanks to its efficientimplementation and compacted feature representation; the system is able to process textinputs on-the-fly while still achieving equally high precision as leading state-ofthe-art …,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,2013,11
Big Data Methods for Computational Linguistics.,Gerhard Weikum; Johannes Hoffart; Ndapandula Nakashole; Marc Spaniol; Fabian M Suchanek; Mohamed Amir Yosef,Abstract Many tasks in computational linguistics traditionally rely on hand-crafted or curatedresources like thesauri or word-sense-annotated corpora. The availability of big data; fromthe Web and other sources; has changed this situation. Harnessing these assets requiresscalable methods for data and text analytics. This paper gives an overview on our recentwork that utilizes big data methods for enhancing semantics-centric tasks dealing withnatural language texts. We demonstrate a virtuous cycle in harvesting knowledge from largedata and text collections and leveraging this knowledge in order to improve the annotationand interpretation of language in Web pages and social media. Specifically; we show how tobuild large dictionaries of names and paraphrases for entities and relations; and how thesehelp to disambiguate entity mentions in texts.,IEEE Data Eng. Bull.,2012,11
DIDO: a disease-determinants ontology from web sources,Victoria Nebot Romero; Min Ye; Mario Albrecht; Jae-Hong Eom; Gehard Weikum,Abstract This paper introduces DIDO; a system providing convenient access to knowledgeabout factors involved in human diseases; automatically extracted from textual Web sources.The knowledge base is bootstrapped by integrating entities from hand-crafted sources likeMeSH and OMIM. As these are short on relationships between dierent types of biomedicalentities; DIDO employs flexible and robust pattern learning and constraint-based reasoningmethods to automatically extract new relational facts from textual sources. These facts canthen be iteratively added to the knowledge base. The result is a semantic graph of typedentities and relations between diseases; their symptoms; and their factors; with emphasis onenvironmental factors but covering also molecular determinants. We demonstrate the valueof DIDO for knowledge discovery about causal factors and properties of complex …,Proceedings of the 20th international conference companion on World wide web,2011,11
InZeit: efficiently identifying insightful time points,Vinay Setty; Srikanta Bedathur; Klaus Berberich; Gerhard Weikum,Abstract Web archives are useful resources to find out about the temporal evolution ofpersons; organizations; products; or other topics. However; even when advanced text searchfunctionality is available; gaining insights into the temporal evolution of a topic can be atedious task and often requires sifting through many documents. The demonstrated systemnamed InZeit (pronounced" insight") assists users by determining insightful time points for agiven query. These are the time points at which the top-k time-travel query result changessubstantially and for which the user should therefore inspect query results. InZeit determinesthe m most insightful time points efficiently using an extended segment tree for in-memorybookkeeping.,Proceedings of the VLDB Endowment,2010,11
NEAT: news exploration along time,Omar Alonso; Klaus Berberich; Srikanta Bedathur; Gerhard Weikum,Abstract There are a number of efforts towards building applications that leverage temporalinformation in documents. The demonstration of our NEAT (News Exploration Along Time)prototype system that we propose here; is an attempt towards building an intuitive andexploratory interface for search results over large news archives using timelines. Thedemonstration uses the New York Times Annotated Corpus as an illustrative example ofsuch a news archive. The NEAT system consists of two parts: the back-end server extractsand stores in an index all the temporal information from documents; and performs importantphrase discovery from sentences that have time-sensitive information. The front-end userinterface; anchors the results of a keyword search along the timeline where the user canexplore and browse results at different points in time. To aid in this exploration; the …,Proceedings of the 32nd European conference on Advances in Information Retrieval,2010,11
Yago2: A spatially and temporally enhanced knowledge base from wikipedia,Johannes Hoffart; Fabian Suchanek; Klaus Berberich; E Kelham; G de Melo; G Weikum; F Suchanek; Gjergji Kasneci; Maya Ramanath; Adam Pease,Deutsch. max planck institut informatik. YAGO2: A Spatially and Temporally EnhancedKnowledge Base from Wikipedia. Research Demo Downloads Statistics Publications PeopleLinking FAQ. Our publications about YAGO. Johannes Hoffart; Fabian Suchanek; KlausBerberich; Gerhard Weikum YAGO2: A Spatially and Temporally Enhanced Knowledge Basefrom Wikipedia (pdf) Special issue of the Artificial Intelligence Journal; 2012; Johannes Hoffart;Fabian M. Suchanek; Klaus Berberich; Edwin Lewis Kelham; Gerard de Melo; and GerhardWeikum YAGO2: Exploring and Querying World Knowledge in Time; Space; Context; andMany Languages (pdf) Demo paper in the proceedings of the 20th International World WideWeb Conference (WWW 2011) Hyderabad; India; 2011; …,Commun. ACM,2009,11
TopX@ INEX 2007,Andreas Broschart; Ralf Schenkel; Martin Theobald; Gerhard Weikum,Abstract This paper describes the setup and results of the Max-Planck-Institut für Informatik'scontributions for the INEX 2007 AdHoc Track task. The runs were produced with TopX; asearch engine for ranked retrieval of XML data that supports a probabilistic scoring model forfull-text content conditions and tag-term combinations; path conditions as exact or relaxableconstraints; and ontology-based relaxation of terms and tag names.,International Workshop of the Initiative for the Evaluation of XML Retrieval,2007,11
DelosDLMS: infrastructure and services for future digital library systems,Ceri Binding; Gert Brettlecker; Tiziana Catarci; Stavros Christodoulakis; Tom Crecelius; Nektarios Gioldasis; Hans-christian Jetter; Mouna Kacimi; Diego Milano; Paola Ranaldi; Harald Reiterer; Hans-jörg Schek; Heiko Schuldt; Douglas Tudhope; Gerhard Weikum,Abstract DelosDLMS is a prototype of a next-generation Digital Library (DL) managementsystem. It is the result of integrating various specialized DL services provided by partners ofthe DELOS network of excellence into the OSIRIS platform. OSIRIS is a middlewareenvironment for the reliable and scalable distributed execution of processes. Processes; inturn; are DL applications that are built from the specialized services available in theintegrated system. DelosDLMS provides support for content-based retrieval in image; audio;video; and 3D collections and combination of any of these media types with keywordqueries. It allows annotating retrieved information; provides a rich set of advanced graphicaluser interfaces to browse and explore large collections; and supports users in interactingwith the system using a speech interface and interactive paper. Finally; it can even be …,*,2007,11
On the usage of global document occurrences in peer-to-peer information systems,Odysseas Papapetrou; Sebastian Michel; Matthias Bender; Gerhard Weikum,Abstract There exist a number of approaches for query processing in Peer-to-Peerinformation systems that efficiently retrieve relevant information from distributed peers.However; very few of them take into consideration the overlap between peers: as the mostpopular resources (eg; documents or files) are often present at most of the peers; a largefraction of the documents eventually received by the query initiator are duplicates. Wedevelop a technique based on the notion of global document occurrences (GDO) that; whenprocessing a query; penalizes frequent documents increasingly as more and more peerscontribute their local results. We argue that the additional effort to create and maintain theGDO information is reasonably low; as the necessary information can be piggybacked ontothe existing communication. Early experiments indicate that our approach significantly …,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2005,11
Where the truth lies: Explaining the credibility of emerging claims on the web and social media,Kashyap Popat; Subhabrata Mukherjee; Jannik Strötgen; Gerhard Weikum,Abstract The web is a huge source of valuable information. However; in recent times; there isan increasing trend towards false claims in social media; other web-sources; and even innews. Thus; factchecking websites have become increasingly popular to identify suchmisinformation based on manual analysis. Recent research proposed methods to assessthe credibility of claims automatically. However; there are major limitations: most worksassume claims to be in a structured form; and a few deal with textual claims but require thatsources of evidence or counter-evidence are easily retrieved from the web. None of theseworks can cope with newly emerging claims; and no prior method can give user-interpretable explanations for its verdict on the claim's credibility. This paper overcomesthese limitations by automatically assessing the credibility of emerging claims; with …,Proceedings of the 26th International Conference on World Wide Web Companion,2017,10
Credibility assessment of textual claims on the web,Kashyap Popat; Subhabrata Mukherjee; Jannik Strötgen; Gerhard Weikum,Abstract There is an increasing amount of false claims in news; social media; and other websources. While prior work on truth discovery has focused on the case of checking factualstatements; this paper addresses the novel task of assessing the credibility of arbitraryclaims made in natural-language text-in an open-domain setting without any assumptionsabout the structure of the claim; or the community where it is made. Our solution is based onautomatically finding sources in news and social media; and feeding these into a distantlysupervised classifier for assessing the credibility of a claim (ie; true or fake). For inference;our method leverages the joint interaction between the language of articles about the claimand the reliability of the underlying web sources. Experiments with claims from the popularwebsite snopes. com and from reported cases of Wikipedia hoaxes demonstrate the …,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,2016,10
Probabilistic prediction of privacy risks in user search histories,Joanna Biega; Ida Mele; Gerhard Weikum,Abstract This paper proposes a new model of user-centric; global; probabilistic privacy;geared for today's challenges of helping users to manage their privacy-sensitive informationacross a wide variety of social networks; online communities; QA forums; and searchhistories. Our approach anticipates an adversary that harnesses global backgroundknowledge and rich statistics in order to make educated guesses; that is; probabilisticinferences at sensitive data. We aim for a tool that simulates such a powerful adversary;predicts privacy risks; and guides the user. In this paper; our framework is specialized for thecase of Internet search histories. We present preliminary experiments that demonstrate howestimators of global correlations among sensitive and non-sensitive key-value items can befed into a probabilistic graphical model in order to compute meaningful measures of …,Proceedings of the First International Workshop on Privacy and Secuirty of Big Data,2014,10
Finding images of difficult entities in the long tail,Bilyana Taneva; Mouna Kacimi; Gerhard Weikum,Abstract While images of famous people and places are abundant on the Internet; they aremuch harder to retrieve for less popular entities such as notable computer scientists orregionally interesting churches. Querying the entity names in image search engines yieldslarge candidate lists; but they often have low precision and unsatisfactory recall. In thispaper; we propose a principled model for finding images of rare or ambiguous namedentities. We propose a set of efficient; light-weight algorithms for identifying entity-specifickeyphrases from a given textual description of the entity; which we then use to scorecandidate images based on the matches of keyphrases in the underlying Web pages. Ourexperiments show the high precision-recall quality of our approach.,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,10
Formal Concept Analysis: 7th International Conference; ICFCA 2009 Darmstadt; Germany; May 21-24; 2009 Proceedings,Sébastien Ferré; Sebastian Rudolph,The discipline of formal concept analysis (FCA) is concerned with the form-ization ofconcepts and conceptual thinking. Built on the solid foundation of lattice and order theory;FCA is? rst and foremost a mathematical discipline. However; its motivation andguidingprinciples arebasedon strongphilosophical underpinnings. In practice; FCA provides apowerful framework for the qua-tative; formal analysis of data; as demonstrated by numerousapplications in diverse areas. Likewise; it emphasizes the aspect of human-centeredinformation processing by employing visualization techniques capable of revealing inherentstructure in data in an intuitively graspable way. FCA thereby contributes to structuring andnavigating the ever-growing amount of information available in our evolving informationsociety and supports the process of turning data into information and ultimately into …,*,2009,10
RoboCup 2008: Robot Soccer World Cup XII.,Hitoshi Matsubara; Alfredo Weitzenfeld; Changjiu Zhou,The 12th annual RoboCup International Symposium was held during July 15–18; 2008 inconjunction with RoboCup 2008 Competitions and Demonstrations. The symposiumrepresents the core meeting for the presentation and discussion of scientific contributions indiverse areas related to the main threads within RoboCupSoccer; RoboCupRescue;RoboCup@ Home and RoboCupJunior. Its scope encompassed; but was not restricted to;research and education activities within the fields of artificial intelligence and robotics. Afundamental aspect of RoboCup is promoting science and technology among youngstudents and researchers; in addition to providing a forum for discussion and excitementabout Robotics with practitioners from all over the world. Since its first edition in 1997 inNagoya; the RoboCup Competitions and Symposium have attracted an increasing …,*,2009,10
Automatic tuning of data synopses,Arnd Christian König; Gerhard Weikum,Abstract Maintaining statistics on multidimensional data distributions is crucial for predictingthe run-time and result size of queries and data analysis tasks with acceptable accuracy.Applications of such predictions include traditional query optimization; priority managementand resource scheduling for data mining tasks; as well as querying heterogeneous Webdata sources with diverse information quality. To this end a plethora of techniques havebeen proposed for maintaining a compact data “synopsis” on a single table; ranging fromvariants of histograms to methods based on wavelets and other transforms. However; thefundamental question of how to reconcile the synopses for large information sources withmany tables has been largely unexplored. This paper develops a general framework forreconciling the synopses on many tables; which may come from different information …,Information Systems,2003,10
Web & Datenbanken: Konzepte; Architekturen; Anwendungen,Gottfried Vossen,*,*,2003,10
The role of web services in information search,Jens Graupmann; Gerhard Weikum,Abstract State-of-the-art Web search engines are inherently limited in their abilities to searchinformation in Deep Web beyond portals. This paper discusses how Web services andSemantic-Web-style ontologies can be leveraged for a new kind of Deep Web search tool.We argue that keyword based search should be replaced by a more expressive style ofconcept based querying; and we outline how portals can be automatically wrapped into Webservices and how a focused crawl can automatically generate queries for exploringinformation beyond portal boundaries. As a first; simple demonstrator for the usefulness ofthe developed framework and tool suite; we briefly present an automatically generated metaportal about movies.,IEEE Data Eng. Bull.,2002,10
Types for Proofs and Programs: International Workshop; TYPES'95; Torino; Italy; June 5-8; 1995 Selected Papers,Stefano Berardi; Mario Coppo,This volume contains a refereed selection of revised full papers chosen from thecontributions presented during the Third Annual Workshop held under the auspices of theESPRIT Basic Research Action 6453 Types for Proofs and Programs. The workshop tookplace in Torino; Italy; in June 1995. Type theory is a formalism in which theorems and proofs;specifications and programs can be represented in a uniform way. The 19 papers includedin the book deal with foundations of type theory; logical frameworks; and implementationsand applications; all in all they constitute a state-of-the-art survey for the area of type theory.,*,1996,10
From relations and nested relations to object models,Hans-Jörg Schek; Marc H Scholl,Abstract: Relational; complex object; and object-oriented data models seem to establishthree diverging directions and it appears hopeless to arrive at a new higher-level unifyingdata model as a new platform for the non-traditional applications as well as for the classicalones. In this paper; however; we give some evidence that recent object models can beconsidered as an evolution from classical ones. Specifically; we describe a first evolutionarypath from relations through nested relations and complex objects to object networks. Asecond evolutionary path starts from the DBTG network model and shows how this is turnedinto an object network again. Thus; we conclude that the next generation object-orienteddata models should integrate concepts from these predecessors. This enables compatibility;coexistence; and cooperation between the different systems as well as a possible …,*,1991,10
Managing complex objects in the Darmstadt database kernel system,Uwe Deppisch; H-B Paul; H-J Schek; Gerhard Weikum,Abstract Complex objects are required in many new applications of databases. A commonfeature is that objects use other (sub-) objects for their description. So; retrieval or update ofcomplex objects may include some or all of their subobjects. The kernel of DASDBS; theDArmStadt DataBase System; is described which was designed and implemented with theobjective to provide a storage system for a variety of complex object notions. This paperconcentrates on the mapping of hierarchically structured complex records to sets of pagesand shows how set-orientation in the operations at the kernel interfaces can be preservedand transformed to operations on sets of pages.,*,1991,10
DASDBS: Konzepte und Architektur eines neuartigen Datenbanksystems,H-J Schek; Gerhard Weikum,*,Informatik,1987,10
The elephant in the room: getting value from Big Data,Serge Abiteboul; Luna Dong; Oren Etzioni; Divesh Srivastava; Gerhard Weikum; Julia Stoyanovich; Fabian M Suchanek,Big Data; and its 4 Vs–volume; velocity; variety; and veracity–have been at the forefront ofsocietal; scientific and engineering discourse. Arguably the most important 5th V; value; isnot talked about as much. How can we make sure that our data is not just big; but alsovaluable? WebDB 2015 1 has as its theme “Freshness; Correctness; Quality of Informationand Knowledge on the Web”. The workshop attracted 31 submissions; of which the best 9were selected for presentation at the workshop; and for publication in the proceedings. Toset the stage; we have interviewed several prominent members of the data managementcommunity; soliciting their opinions on how we can ensure that data is not just available inquantity; but also in quality. In this interview Serge Abiteboul; Oren Etzioni; DiveshSrivastava with Luna Dong; and Gerhard Weikum shared with us their motivation for …,Proceedings of the 18th international workshop on web and databases,2015,9
Aida-social: Entity linking on the social stream,Yusra Ibrahim; Mohamed Amir Yosef; Gerhard Weikum,Abstract Named Entity Linking (NEL) in microblogs is a challenging task due to the use ofcryptic abbreviations; insufficient contextual information; and the time-varying importance ofentities. We propose three techniques to target these challenges: Mention Normalization;Contextual Enrichment; and Temporal Entity Importance. By combining these noveltechniques; we achieve 13% improvement in precision over a state-of-the-art NEL tool.,Proceedings of the 7th International Workshop on Exploiting Semantic Annotations in Information Retrieval,2014,9
Gem-based entity-knowledge maintenance,Bilyana Taneva; Gerhard Weikum,Abstract Knowledge bases about entities have become a vital asset for Web search;recommendations; and analytics. Examples are Freebase being the core of the GoogleKnowledge Graph and the use of Wikipedia for distant supervision in numerous IR and NLPtasks. However; maintaining the knowledge about not so prominent entities in the long tail isoften a bottleneck as human contributors face the tedious task of continuously identifyingand reading relevant sources. To overcome this limitation and accelerate the maintenanceof knowledge bases; we propose an approach that automatically extracts; from the Web; keycontents for given input entities. Our method; called GEM; generates salient contents about agiven entity; using minimal assumptions about the underlying sources; while meeting theconstraint that the user is willing to read only a certain amount of information. Salient …,Proceedings of the 22nd ACM international conference on Information & Knowledge Management,2013,9
Opinions network for politically controversial topics,Rawia Awadallah; Maya Ramanath; Gerhard Weikum,Abstract This paper describes OpinioNetIt; a structured; faceted; knowledge-base ofopinions; and its use in political opinions analysis. OpinioNetIt consists of information aboutpeople; topics and opinions in the form of person; opinion; topic triples; indicating theopinion of a person on a topic. Our specific focus has been on acquiring opinions held byvarious stakeholders on politically controversial topics. Our system can be used for variouskinds of analysis; including; heat maps showing political bias; flip-flopping politicians;dissenters; etc. In this paper; we describe the architecture of our system; and how it can beused to address these various use-cases.,Proceedings of the first edition workshop on Politics; elections and data,2012,9
Pravda-live: interactive knowledge harvesting,Yafang Wang; Maximilian Dylla; Zhaochun Ren; Marc Spaniol; Gerhard Weikum,Abstract Acquiring high-quality (temporal) facts for knowledge bases is a labor-intensiveprocess. Although there has been recent progress in the area of semi-supervised factextraction; these approaches still have limitations; including a restricted corpus; a fixed set ofrelations to be extracted or a lack of assessment capabilities. In this paper we introducePRAVDA-live; a framework that overcomes these limitations and supports the entire pipelineof interactive knowledge harvesting. To this end; our demo exhibits fact extraction from ad-hoc corpus creation; via relation specification; labeling and assessment all the way to ready-to-use RDF exports.,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,9
Tracking entities in web archives: the LAWA project,Marc Spaniol; Gerhard Weikum,Abstract Web-preservation organization like the Internet Archive not only capture the historyof born-digital content but also reflect the zeitgeist of different time periods over more than adecade. This longitudinal data is a potential gold mine for researchers like sociologists;politologists; media and market analysts; or experts on intellectual property. The LAWAproject (Longitudinal Analytics of Web Archive data) is developing an Internet-basedexperimental testbed for large-scale data analytics on Web archive collections. Its emphasisis on scalable methods for this specific kind of big-data analytics; and software tools foraggregating; querying; mining; and analyzing Web contents over long epochs. In this paper;we highlight our research on {\em entity-level analytics} in Web archive data; which lifts Webanalytics from plain text to the entity-level by detecting named entities; resolving …,Proceedings of the 21st International Conference on World Wide Web,2012,9
Scalable spatio-temporal knowledge harvesting,Yafang Wang; Bin Yang; Spyros Zoupanos; Marc Spaniol; Gerhard Weikum,Abstract Knowledge harvesting enables the automated construction of large knowledgebases. In this work; we made a first attempt to harvest spatio-temporal knowledge from newsarchives to construct trajectories of individual entities for spatio-temporal entity tracking. Ourapproach consists of an entity extraction and disambiguation module and a fact generationmodule which produce pertinent trajectory records from textual sources. The evaluation onthe 20 years' New York Times news article corpus showed that our methods are effectiveand scalable.,Proceedings of the 20th international conference companion on World wide web,2011,9
SITAC: discovering semantically identical temporally altering concepts in text archives,Amal Kaluarachchi; Debjani Roychoudhury; Aparna S Varde; Gerhard Weikum,Abstract This paper demonstrates a system called SITAC based on our proposed approachto automate the discovery of concepts (called SITACs) in text sources that are identicalsemantically but alter their names over time. This system is developed to perform time-awaretranslation of queries over text corpora by incorporating terminology evolution; thusproviding more accurate responses to users; eg; query processing on Mumbai shouldautomatically take into account its former name Bombay. The SITAC system constitutes anovel collaborative framework of natural language processing; association rule mining andcontextual similarity.,Proceedings of the 14th International Conference on Extending Database Technology,2011,9
Temporal knowledge for timely intelligence,Gerhard Weikum; Srikanta Bedathur; Ralf Schenkel,Abstract Knowledge bases about entities and their relationships are a great asset forbusiness intelligence. Major advances in information extraction and the proliferation ofknowledge-sharing communities like Wikipedia have enabled ways for the largelyautomated construction of rich knowledge bases. Such knowledge about entity-orientedfacts can greatly improve the output quality and possibly also efficiency of processingbusiness-relevant documents and event logs. This holds for information within the enterpriseas well as in Web communities such as blogs. However; no knowledge base will ever befully complete and real-world knowledge is continuously changing: new facts supersede oldfacts; knowledge grows in various dimensions; and completely new classes; relation types;or knowledge structures will arise. This leads to a number of difficult research questions …,International Workshop on Business Intelligence for the Real-Time Enterprise,2010,9
Providing Multilingual; Multimodal Answers to Lexical Database Queries.,Gerard De Melo; Gerhard Weikum,Abstract Language users are increasingly turning to electronic resources to address theirlexical information needs; due to their convenience and their ability to simultaneouslycapture different facets of lexical knowledge in a single interface. In this paper; we discusstechniques to respond to a user's lexical queries by providing multilingual and multimodalinformation; and facilitating navigating along different types of links. To this end; structuredinformation from sources like WordNet; Wikipedia; Wiktionary; as well as Web services islinked and integrated to provide a multi-faceted yet consistent response to user queries. Themeanings of words in many different languages are characterized by mapping them toappropriate WordNet sense identifiers and adding multilingual gloss descriptions as well asexample sentences. Relationships are derived from WordNet and Wiktionary to allow …,LREC,2010,9
Statistical structures for Internet-scale data management,Nikos Ntarmos; Peter Triantafillou; Gerhard Weikum,Abstract Efficient query processing in traditional database management systems relies onstatistics on base data. For centralized systems; there is a rich body of research results onsuch statistics; from simple aggregates to more elaborate synopses such as sketches andhistograms. For Internet-scale distributed systems; on the other hand; statistics managementstill poses major challenges. With the work in this paper we aim to endow peer-to-peer datamanagement over structured overlays with the power associated with such statisticalinformation; with emphasis on meeting the scalability challenge. To this end; we firstcontribute efficient; accurate; and decentralized algorithms that can compute key aggregatessuch as Count; CountDistinct; Sum; and Average. We show how to construct several types ofhistograms; such as simple Equi-Width; Average-Shifted Equi-Width; and Equi-Depth …,The VLDB Journal,2009,9
Computational Science–ICCS 2009: 9th International Conference Baton Rouge; LA; USA; May 25-27; 2009 Proceedings,Gabrielle Allen; Jaroslaw Nabrzyski; Edward Seidel; Geert Dick van Albada; Jack Dongarra; Peter MA Sloot,“There is something fascinating about science. One gets such wholesale returns ofconjecture out of such a tri? ing investment of fact.” Mark Twain; Life on the Mississippi Thechallenges in succeeding with computational science are numerous and deeply a? ect alldisciplines. NSF's 2006 Blue Ribbon Panel of Simulation-Based 1 Engineering Science(SBES) states 'researchers and educators [agree]: com-tational and simulation engineeringsciences are fundamental to the security and welfare of the United States... We mustovercome di? culties inherent in multiscale modeling; the development of next-generationalgorithms; and the design... of dynamic data-driven application systems... We mustdetermine better ways to integrate data-intensive computing; visualization; and simulation.-portantly; wemustoverhauloureducationalsystemtofos… study... The payo? sformeeting …,*,2009,9
Smart Graphics: 10th International Symposium; SG 2009; Salamanca; Spain; Mai 28-30; 2009; Proceedings,Andreas Butz; Brian Fisher; Marc Christie; Antonio Krüger; Patrick Olivier; Roberto Therón,Smartgraphicsarepervasivein ourlives nowadays. Thewaysartistsand desi-ersproduceimagesthate? ectivelysupporthumancognitionandcommun… are continuouslychanging and evolving as they incorporate novel methods p-vided by the advances inscience and technology. As a counterpart; the radically new visions in most art forms havestimulated scientists to breath-taking levels of achievement. This symbiotic relationshipbetween art and science (and technology) is one of the foundations of the technologicalculture of contemporary society and is especiallyevidentinthecreationofsmartgraph….Suchaprocessrestsonadeep understanding of the fundamentals of perception and cognitionas they relate to interaction and communication technologies; together with arti? cialintelligence andcomputergraphicstechniques …,*,2009,9
Public Key Cryptography-PKC 2009: 12th International Conference on Practice and Theory in Public Key Cryptography Irvine; CA; USA; March 18-20; 2009; Procee...,Stanislaw Jarecki; Gene Tsudik,This book constitutes the refereed proceedings of the 12th International Conference onPractice and Theory in Public-Key Cryptography; PKC 2009; held in Irvine; CA; USA; inMarch 2009. The 28 revised full papers presented were carefully reviewed and selectedfrom 112 submissions. The papers are organized in topical sections on number theory;applications and protocols; multi-party protocols; identity-based encryption; signatures;encryption; new cryptosystems and optimizations; as well as group signatures andanonymous credentials.,*,2009,9
Harvesting; searching; and ranking knowledge on the web: invited talk,Gerhard Weikum,Abstract There are major trends to advance the functionality of search engines to a moreexpressive semantic level (eg;[2; 4; 6; 7; 8; 9; 13; 14; 18]). This is enabled by employinglarge-scale information extraction [1; 11; 20] of entities and relationships from semistructuredas well as natural-language Web sources. In addition; harnessing Semantic-Web-styleontologies [22] and reaching into Deep-Web sources [16] can contribute towards a grandvision of turning the Web into a comprehensive knowledge base that can be efficientlysearched with high precision. This talk presents ongoing research towards this objective;with emphasis on our work on the YAGO knowledge base [23; 24] and the NAGA searchengine [14] but also covering related projects. YAGO is a large collection of entities andrelational facts that are harvested from Wikipedia and WordNet with high accuracy and …,Proceedings of the Second ACM International Conference on Web Search and Data Mining,2009,9
Self-management technology in databases,Surajit Chaudhuri; Gerhard Weikum,The values in the relations of a relational database are elements of one or more underlyingsets called domains. In practical applications; a domain may be infinite; eg; the set of naturalnumbers. In this case; the value of a relational calculus query when applied to such adatabase may be infinite; eg;{njn! 10}. A query Q is called finite if the value of Q whenapplied to any database is finite. Even when the database domains are finite; all that isnormally known about them is that they are some finite superset of the values that occur inthe database. In this case; the value of a relational calculus query may depend on such anunknown domain; eg;{xj 8yR (x; y)}. A query Q is called domain independent if the value of Qwhen applied to any database is the same for any two domains containing the databasevalues or; equivalently; if the value of Q when applied to a database contains only values …,*,2009,9
Advances in Visual Computing: 4th International Symposium; ISVC 2008; Las Vegas; NV; USA; December 1-3; 2008; Proceedings,George Bebis; Richard Boyle; Bahram Parvin,It is with greatpleasure that we present the proceedings of the 4th International Symposiumon Visual Computing (ISVC 2008) in Las Vegas; Nevada. ISVC o? ers a common umbrellafor the four main areas of visual computing including vision; graphics; visualization; andvirtual reality. Its goal is to provide a forum for researchers; scientists; engineers andpractitioners throughout the world to present their latest research? ndings; ideas;developments and applications in the broader area of visual computing. This year; ISVCgrew signi? cantly; the programconsisted of 15 oralsessions; 1 poster session; 8 specialtracks; and 6 keynote presentations. The response to the call for papers was very strong; wereceived over 340 submissions for the main symposium from which we accepted 102 papersfor oral presentation and 70 papers for poster presentation. Special track papers were …,*,2008,9
A machine learning approach to building aligned wordnets,Gerard De Melo; Gerhard Weikum,Abstract WordNet is a lexical database describing English words and their senses. Wepropose a method for automatically producing similar resources for new languages bytaking advantage of the original WordNet in conjunction with translation dictionaries. A smallset of training mappings is used to learn a model for predicting associations between termsand senses. The associations are represented using a variety of scores that take intoaccount structural properties as well as semantic relatedness and corpus frequencyinformation. For evaluation; we created a German-language wordnet; and the data indicatea significantly better coverage and higher precision than previous heuristics. The resultingresources provide not only valuable information for monolingual NLP tasks but also enablea high degree of cross-lingual interoperability.,Programme Committee 7,2008,9
Efficient Time-Travel on Versioned Text Collections.,Klaus Berberich; Srikanta J Bedathur; Gerhard Weikum,Abstract: The availability of versioned text collections such as the Internet Archive opens upopportunities for time-aware exploration of their contents. In this paper; we propose time-travel retrieval and ranking that extends traditional keyword queries with a temporal contextin which the query should be evaluated. More precisely; the query is evaluated over allstates of the collection that existed during the temporal context. In order to support thesequeries; we make key contributions in (i) defining extensions to well-known relevancemodels that take into account the temporal context of the query and the version history ofdocuments;(ii) designing an immortal index over the full versioned text collection that avoidsa blowup in index size; and (iii) making the popular NRA algorithm for top-k queryprocessing aware of the temporal context. We present preliminary experimental analysis …,BTW,2007,9
Examinations on the automatic classification of lamellar graphite using the support vector machine,Kathrin Roberts; Gerhard Weikum; Frank Muecklich,*,Praktische Metallographie,2005,9
Towards Collaborative Search in Digital Libraries Using Peer-to-Peer Technology.,Matthias Bender; Sebastian Michel; Christian Zimmer; Gerhard Weikum,Abstract. We consider the problem of collaborative search across a large number of digitallibraries and query routing strategies in a peerto-peer (P2P) environment. Both digitallibraries and users are equally viewed as peers and; thus; as part of the P2P network. Oursystem provides a versatile platform for a scalable search engine combining local indexstructures of autonomous peers with a global directory based on a distributed hash table(DHT) as an overlay network.,DELOS Workshop: Digital Library Architectures,2004,9
Foundations of statistical natural language processing,Gerhard Weikum,It is a pleasure to write this review of an excellent textbook. Pedagogically this is a real gem;and it is a great source of teaching material for a variety of courses. I discovered the bookwhen I was looking for textbook material that I could use in my class on information retrieval(IR); which is an advanced undergraduate course at my university. Being myself a" coredatabase person"; I committed myself to teaching the IR class with the intention to learn anew subject and the expectation that there should be plenty of good textbooks to choosefrom. I easily found a few decent books; but none of them seemed adequate as the mainliterature for my class (which is also a matter of personal taste; however). I ended up takingmaterial from different sources; textbooks as well as conference and journal papers (thelatter especially for Web search); and the one source to which I referred most and which …,Acm Sigmod Record,2002,9
R-susceptibility: An ir-centric approach to assessing privacy risks for users in online communities,Joanna Asia Biega; Krishna P Gummadi; Ida Mele; Dragan Milchevski; Christos Tryfonopoulos; Gerhard Weikum,Abstract Privacy of Internet users is at stake because they expose personal information inposts created in online communities; in search queries; and other activities. An adversarythat monitors a community may identify the users with the most sensitive properties andutilize this knowledge against them (eg; by adjusting the pricing of goods or targeting ads ofsensitive nature). Existing privacy models for structured data are inadequate to captureprivacy risks from user posts. This paper presents a ranking-based approach to theassessment of privacy risks emerging from textual contents in online communities; focusingon sensitive topics; such as being depressed. We propose ranking as a means of modelinga rational adversary who targets the most afflicted users. To capture the adversary'sbackground knowledge regarding vocabulary and correlations; we use latent topic …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,8
Air quality assessment from social media and structured data: Pollutants and health impacts in urban planning,Xu Du; Onyeka Emebo; Aparna Varde; Niket Tandon; Sreyasi Nag Chowdhury; Gerhard Weikum,This paper describes our work on mining pollutant data to assess air quality in urban areas.Notable aspects of this work are that we mine social media and structured data in a domain-specific context; incorporate commonsense knowledge in mining media opinions and focuson the urban planning domain in a multicity environment. The results of mining are useful forpredictive analysis in urbanization. A significant contribution is that we provide usefulinformation on urban health impacts.,Data Engineering Workshops (ICDEW); 2016 IEEE 32nd International Conference on,2016,8
KOGNAC: efficient encoding of large knowledge graphs,Jacopo Urbani; Sourav Dutta; Sairam Gurajada; Gerhard Weikum,Abstract: Many Web applications require efficient querying of large Knowledge Graphs(KGs). We propose KOGNAC; a dictionary-encoding algorithm designed to improveSPARQL querying with a judicious combination of statistical and semantic techniques. InKOGNAC; frequent terms are detected with a frequency approximation algorithm andencoded to maximise compression. Infrequent terms are semantically grouped intoontological classes and encoded to increase data locality. We evaluated KOGNAC incombination with state-of-the-art RDF engines; and observed that it significantly improvesSPARQL querying on KGs with up to 1B edges. Subjects: Artificial Intelligence (cs. AI) Citeas: arXiv: 1604.04795 [cs. AI](or arXiv: 1604.04795 v2 [cs. AI] for this version) Submissionhistory From: Jacopo Urbani [view email][v1] Sat; 16 Apr 2016 20: 54: 12 GMT (291kb; D) …,arXiv preprint arXiv:1604.04795,2016,8
RDF Xpress: a flexible expressive RDF search engine,Shady Elbassuoni; Maya Ramanath; Gerhard Weikum,Abstract We demonstrate RDF Xpress; a search engine that enables users to effectivelyretrieve information from large RDF knowledge bases or Linked Data Sources. RDF Xpressprovides a search interface where users can combine triple patterns with keywords to formqueries. Moreover; RDF Xpress supports automatic query relaxation and returns a rankedlist of diverse query results.,Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,2012,8
Search for knowledge,Gerhard Weikum,Abstract There are major trends to advance the functionality of search engines to a moreexpressive semantic level. This is enabled by the advent of knowledge-sharing communitiessuch as Wikipedia and the progress in automatically extracting entities and relationshipsfrom semistructured as well as natural-language Web sources. In addition; Semantic-Web-style ontologies; structured Deep-Web sources; and Social-Web networks and taggingcommunities can contribute towards a grand vision of turning the Web into a comprehensiveknowledge base that can be efficiently searched with high precision. This vision and positionpaper discusses opportunities and challenges along this research avenue. The technicalissues to be looked into include knowledge harvesting to construct large knowledge bases;searching for knowledge in terms of entities and relationships; and ranking the results of …,*,2010,8
Extracting sense-disambiguated example sentences from parallel corpora,Gerard De Melo; Gerhard Weikum,Abstract Example sentences provide an intuitive means of grasping the meaning of a word;and are frequently used to complement conventional word definitions. When a word hasmultiple meanings; it is useful to have example sentences for specific senses (and hencedefinitions) of that word rather than indiscriminately lumping all of them together. In thispaper; we investigate to what extent such sense-specific example sentences can beextracted from parallel corpora using lexical knowledge bases for multiple languages as asense index. We use word sense disambiguation heuristics and a cross-lingual measure ofsemantic similarity to link example sentences to specific word senses. From the sentencesfound for a given sense; an algorithm then selects a smaller subset that can be presented toend users; taking into account both representativeness and diversity. Preliminary results …,Proceedings of the 1st Workshop on Definition Extraction,2009,8
Self-Sustaining Systems: First Workshop; S3 2008 Potsdam; Germany; May 15-16; 2008; Proceedings,Robert Hirschfeld; Kim Rose,The Workshop on Self-sustaining Systems (S3) is a forum for the discussion of topicsrelating to computer systems and languages that are able to bootstrap; implement; modify;and maintain themselves. One property of these systems is that their implementation isbased onsmall but powerfulabstractions; examples include (amongst others)Squeak/Smalltalk; COLA; Klein/Self; PyPy/Python; Rubinius/Ruby; andLisp. Suchsystemsaretheenginesoftheirownrepla…; giving researchers and developers great power to experiment with; and explore futuredirections from within; their own small language kernels. S3 took place on May 15–16; 2008at the Hasso-Plattner-Institute (HPI) in Potsdam; Germany. It was an exciting opportunity forresearchers and prac-tioners interested in self-sustaining systems to meet and share theirknowledge; experience; and ideas for future research and development. S3 provided an …,*,2008,8
The topx db&ir engine,Martin Theobald; Ralf Schenkel; Gerhard Weikum,Abstract This paper proposes a demo of the TopX search engine; an extensive frameworkfor unified indexing; querying; and ranking of large collections of unstructured;semistructured; and structured data. TopX integrates efficient algorithms for top-k-styleranked retrieval with powerful scoring models for text and XML documents; as well asdynamic and self-tuning query expansion based on background ontologies.,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,8
The atomic manifesto: A story in four quarks,Cliff Jones; David Lomet; Alexander Romanovsky; Gerhard Weikum; Alan Fekete; Marie-Claude Gaudel; Henry F Korth; Rogerio de Lemos; Eliot Moss; Ravi Rajwar; Krithi Ramamritham; Brian Randell; Luis Rodrigues,Abstract This paper is based on a five-day workshop on" Atomicity in System Design andExecution" that took place in Schloss Dagstuhl in Germany [5] in April 2004 and wasattended by 32 people from different scientific communities. The participants includedresearchers from the four areas of• database and transaction processing systems;• faulttolerance and dependable systems;• formal methods for system design and correctnessreasoning; and• to a smaller extent; hardware architecture and programming languages.,ACM SIGOPS Operating Systems Review,2005,8
Using restrictive classification and meta classification for junk elimination,Stefan Siersdorfer; Gerhard Weikum,Abstract This paper addresses the problem of performing supervised classification ondocument collections containing also junk documents. With” junk documents” we meandocuments that do not belong to the topic categories (classes) we are interested in. This typeof documents can typically not be covered by the training set; nevertheless in many realworld applications (eg classification of web or intranet content; focused crawling etc.) suchdocuments occur quite often and a classifier has to make a decision about them. We tacklethis problem by using restrictive methods and ensemble-based meta methods that maydecide to leave out some documents rather than assigning them to inappropriate classeswith low confidence. Our experiments with four different data sets show that the proposedtechniques can eliminate a relatively large fraction of junk documents while dismissing …,European Conference on Information Retrieval,2005,8
The MINERVA project: towards collaborative search in digital libraries using Peer-to-Peer technology,Matthias Bender; Sebastian Michel; Christian Zimmer; Gerhard Weikum,Abstract We consider the problem of collaborative search across a large number of digitallibraries and query routing strategies in a peer-to-peer (P2P) environment. Both digitallibraries and users are equally viewed as peers and; thus; as part of the P2P network. Oursystem provides a versatile platform for a scalable search engine combining local indexstructures of autonomous peers with a global directory based on a distributed hash table(DHT) as an overlay network. Experiments with the MINERVA prototype testbed study thebenefits and costs of P2P search for keyword queries.,*,2005,8
XXL@ INEX 2003,Ralf Schenkel; Anja Theobald; Gerhard Weikum,ABSTRACT Information retrieval on XML combines retrieval on content data (element andattribute values) with retrieval on structural data (element and attribute names). Standardquery languages for XML such as XPath or XQuery support Boolean retrieval: a query resultis a (possibly restructured) subset of XML elements or entire documents that satisfy thesearch conditions of the query. Such search conditions consist of regular path expressionsincluding wildcards for paths of arbitrary length and boolean content conditions. Wedeveloped a flexible XML search language called XXL for probabilistic ranked retrieval onXML data. XXL offers a special operator'∼'for specifying semantic similarity searchconditions on element names as well as element values. Ontological knowledge andappropriate index structures are necessary for semantic similarity search on XML data …,INEX 2003 Workshop Proceedings,2004,8
With heart towards response time guarantees for message-based e-services,Achim Kraiss; Frank Schoen; Gerhard Weikum; Uwe Deppisch,Abstract The HEART tool (Help for Ensuring Acceptable Response Times) has beendeveloped by the IT Research and Innovations department of Dresdner Bank for thecomputation of viable message prioritization in message-based eservices; such as stockbrokerage services where service requests of different customer classes with class-specificperformance goals have to be served by a server. HEART determines viable messageprioritizations in the sense that they satisfy the specified performance goals of customerclasses. In this paper; we describe the practical problem setting we address with HEARTand outline the functionality of HEART. The demo will show HEART's underlying concepts;its architecture and an example scenario.,International Conference on Extending Database Technology,2002,8
Benchmarking von Workflow-Management-Systemen (Kurzbeitrag),Michael Gillmann; Peter Muth; Gerhard Weikum; Jeanine Weissenfels,Zusammenfassung Heute verfügbare Workflow-Management-Systeme (WfMS) weisen inihrer Leistungsfähigkeit und Skalierbarkeit deutliche Defizite auf. Während anVerbesserungen intensiv gearbeitet wird; sollten Leistungsvergleiche zwischenverschiedenen WfMS generell auf eine systematische; objektive Basis gestellt werden. Zudiesem Zweck entwickelt dieser Kurzbeitrag einen Benchmark für WfMS; der—ausgehendvon den Basisaktivitäten des TPC-C-Benchmarks—in verschiedenen Ausbaustufen Stärkenund Schwächen von WfMS ausloten und systematisch vergleichen kann. Die Benchmark-Definition wägt sorgfältig ab zwischen Einfachheit; Realitätsnähe und Aussagekraft vonBenchmark-Messungen und ist in dem im Industriestandard UML verwendeten State-undActivity-Chart-Formalismus spezifiziert.,*,1999,8
LoT: Dynamic declustering of TSB-tree nodes for parallel access to temporal data,Peter Muth; Achim Kraiß; Gerhard Weikum,Abstract In this paper; we consider the problem of exploiting I/O parallelism for efficientaccess to transaction-time temporal databases. As temporal databases maintain historicalversions of records in addition to current ones; we consider range queries in both timedimension and key dimension. Multiple disks can be used to read sets of disk blocks inparallel; thereby improving the performance of such queries substantially. The problem is tofind an optimal declustering algorithm for spreading record versions across disks. Thesolution depends on the index structure used. We have adopted the time split B-tree; as itprovides efficient support for time range and key range queries. Our declustering methodcoined LoT (Local Balancing for TSB-trees) aims to decluster runs of logically consecutiveleaf nodes of a TSB-tree onto separate disks. The method is dynamic in the sense that it …,International Conference on Extending Database Technology,1996,8
Deeplife: An entity-aware search; analytics and exploration platform for health and life sciences,Patrick Ernst; Amy Siu; Dragan Milchevski; Johannes Hoffart; Gerhard Weikum,Abstract Despite the abundance of biomedical literature and health discussions in onlinecommunities; it is often tedious to retrieve informative contents for health-centric informationneeds. Users can query scholarly work in PubMed by keywords and MeSH terms; and resortto Google for everything else. This demo paper presents the DeepLife system; to overcomethe limitations of existing search engines for life science and health topics. DeepLifeintegrates large knowledge bases and harnesses entity linking methods; to support searchand exploration of scientific literature; newspaper feeds; and social media; in terms ofkeywords and phrases; biomedical entities; and taxonomic categories. It also providesfunctionality for entityaware text analytics over health-centric contents.,Proceedings of ACL-2016 System Demonstrations,2016,7
Experience-aware item recommendation in evolving review communities,Subhabrata Mukherjee; Hemank Lamba; Gerhard Weikum,Current recommender systems exploit user and item similarities by collaborative filtering.Some advanced methods also consider the temporal evolution of item ratings as a globalbackground process. However; all prior methods disregard the individual evolution of auser's experience level and how this is expressed in the user's writing in a reviewcommunity. In this paper; we model the joint evolution of user experience; interest in specificitem facets; writing style; and rating behavior. This way we can generate individualrecommendations that take into account the user's maturity level (eg; recommending artmovies rather than blockbusters for a cinematography expert). As only item ratings andreview texts are observables; we capture the user's experience and interests in a latentmodel learned from her reviews; vocabulary and writing style. We develop a generative …,Data Mining (ICDM); 2015 IEEE International Conference on,2015,7
Lights; camera; action: Knowledge extraction from movie scripts,Niket Tandon; Gerhard Weikum; Gerard de Melo; Abir De,Abstract With the success of large knowledge graphs; research on automatically acquiringcommonsense knowledge is revived. One kind of knowledge that has not received attentionis that of human activities. This paper presents an information extraction pipeline forsystematically distilling activity knowledge from a corpus of movie scripts. Our semanticframes capture activities together with their participating agents and their typical spatial;temporal and sequential contexts. The resulting knowledge base comprises about 250;000activities with links to specific movie scenes where they occur.,Proceedings of the 24th International Conference on World Wide Web,2015,7
Redescription mining over non-binary data sets using decision trees,Tetiana Zinchenko; Pauli Miettinen; Gerhard Weikum,Abstract Scientific data mining is aimed to extract useful information from huge data sets withthe help of computational efforts. Recently; scientists encounter an overload of data whichdescribe domain entities from different sides. Many of them provide alternative means toorganize information. And every alternative data set offers a different perspective onto thestudied problem. Redescription mining is tool with a goal of finding various descriptions ofthe same objects; ie giving information on entity from different perspectives. It is a tool forknowledge discovery which helps uniformly reason across data of diverse origin andintegrates numerous forms of characterizing data sets.,*,2014,7
Fast entity recognition in biomedical text,Amy Siu; Dat Ba Nguyen; Gerhard Weikum,ABSTRACT In biomedical text mining; entity recognition is often an early task in the pipelineof analyzing free text. MetaMap; the de facto standard software tool for this task; employsmuch Natural Language Processing (NLP) machinery to recognize entities in UMLS (UnifiedMedical Language System); the largest metathesaurus. Knowing that the NLP machinery istime-consuming; and that UMLS is rich in lexical variations; this work investigates whether afast; string similarity-based method can achieve results comparable to those of MetaMap.We implemented an NLP-light method that performs fast MinHash lookups via charactertrigram features. Starting with UMLS as the dictionary of entities; we select a subset whoseentity names are short and thus amenable to a string similarity-based approach. We appliedthe method to both scientific literature and layman-oriented texts from Internet health …,Workshop on Data Mining for Healthcare (DMH) at Knowledge Discovery and Data Mining (KDD),2013,7
Cross-lingual data quality for knowledge base acceleration across wikipedia editions,Júlia Göbölös-Szabó; Natalia Prytkova; Marc Spaniol; Gerhard Weikum,ABSTRACT Knowledge-sharing communities like Wikipedia and knowledge bases likeFreebase are expected to capture the latest facts about the real world. However; neither ofthese can keep pace with the rate at which events happen and new knowledge is reportedin news and social media. To narrow this gap; we propose an approach to accelerate theonline maintenance of knowledge bases. Our method; called LAIKA; is based on linkprediction. Wikipedia editions in different languages; Wikinews; and other news media comewith extensive but noisy interlinkage at the entity level. We utilize this input forrecommending; for a given Wikipedia article or knowledge-base entry; new categories;related entities; and cross-lingual interwiki links. LAIKA constructs a large graph from theavailable input and uses link-overlap measures and random-walk techniques to generate …,Proceedings of the 10^ th International Workshop on Quality in Databases (QDB 2012),2012,7
Big web analytics: Toward a virtual web observatory,Marc Spaniol; András Benczúr; Zsolt Viharos; Gerhard Weikum,Toggle navigation. HAL: HAL; HALSHS; TEL; MédiHAL; Liste des portails; AURéHAL; API;Documentation. Episciences.org; Sciencesconf.org; Support. Connexion: Connexion; Créerun compte; Mot de passe oublié ? Login oublié ? fr; en. Accueil; Dépôt; Consultation: Lesderniers dépôts; Par type de publication; Par discipline; Par année de publication; Parstructure de recherche; Les portails de l'archive; Les collections. Recherche; Documentation:Tutoriels; Compte et profil: Pourquoi créer un compte et un profil dans HAL; Créer son compteet son profil dans HAL; Modifier son compte ou son profil dans HAL; Modifier son mot depasse; Login ou mot de passe oublié; Les droits associés au profil. Déposer: Avant decommencer; Les types de publication acceptés …,Ercim News,2012,7
Database tuning using online algorithms,Nicolas Bruno; Surajit Chaudhuri; Gerhard Weikum,realized using such logical structures. For example; in tree based data acquisition protocols;a collection tree is built that is rooted at the data collection center such as the sink node [8].The dissemination of the data requests from the participating nodes and collection of datafrom the sensor nodes are accomplished using this tree. A cluster based data acquisitionmechanism has been proposed in [3]. As shown in Fig. 1; nodes are organized into a fixednumber of clusters; and nodes within each cluster dynamically elect a cluster head. The dataacquisition is carried out in two phases. In the first phase; cluster heads collect data fromtheir cluster nodes. In the second phase; cluster heads send collected data to the nodes thathave subscribed to the data. The cluster heads are re-elected to balance energyconsumption among the nodes in the cluster. Zhang et al.[13] have proposed an adaptive …,*,2009,7
Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,B Gallagher; H Tong; T Eliassi-Rad; C Faloutsos,*,*,2008,7
Node behavior prediction for large-scale approximate information filtering,Christian Zimmer; Christos Tryfonopoulos; Klaus Berberich; Gerhard Weikum; Manolis Koubarakis,ABSTRACT In this paper we investigate methods that allow us to identify the publishingbehavior of individual nodes in large-scale distributed information filtering systems. Thework presented here is based on our system MAPS (Minerva ApproximatePublish/Subscribe); a novel approach to support approximate information filteringfunctionality in a peer-to-peer environment. In MAPS; a user subscribes to and monitors onlycarefully selected publisher nodes; and receives notifications from these information sourcesonly. In this way; document-granularity dissemination is known from exact informationfiltering approaches is avoided; and the system is able to support very high publication rates.However this scalability benefits come at the cost of lower recall. To improve node selectionand thus recall; in previous work we have proposed a ranking method that predicts nodes' …,LSDS-IR,2007,7
Design alternatives for large-scale web search: Alexander was great; aeneas a pioneer; and anakin has the force,Matthias Bender; Sebastian Michel; Peter Triantafillou; Gerhard Weikum,ABSTRACT Indexing the Web and meeting the throughput; responsetime; and failure-resilience requirements of a search engine requires massive storage and computationalresources and a careful system design for scalability. This is exemplified by the big datacenters of the leading commercial search engines. Various proposals and debates haveappeared in the literature as to whether Web indexes can be implemented in a fullydistributed or even peer-to-peer manner without impeding scalability; and differentpartitioning strategies have been worked out. In this paper; we resume this ongoingdiscussion by analyzing the design space for distributed Web indexing; considering theinfluence of partitioning strategies as well as different storage technologies including Flash-RAM. We outline and discuss the pros and cons of three fundamental alternatives; and …,Proceedings of the 1st LSDS-IR Workshop,2007,7
Maps: approximate publish/subscribe functionality in peer-to-peer networks,Klaus Berberich; Manolis Koubarakis; Christos Tryfonopoulos; Gerhard Weikum; Christian Zimmer,Abstract Information filtering has been a research issue for years. In an information filteringscenario users information needs are expressed by user subscriptions; and users arenotified about published documents or events that match these interests. The combination ofthe publish/subscribe scenario with the peer-to-peer (P2P) approach of autonomous peersmakes high demands on the scalability and the efficiency of such a given highly distributednetwork. However; in many cases a subscriber is not interested in all the events that matchhis profile; but rather in a small representative set. In this paper; we present our approach ofan approximate publish/subscribe system; that relaxes the assumption for receivingnotifications from every information producer in the network. Our work builds upondistributed hash table technology to create and maintain a distributed global directory that …,Proceedings of the 1st international workshop on Advanced data processing in ubiquitous computing (ADPUC 2006),2006,7
Experiences with Building a Federated Transaction Manager based on CORBA OTS.,Ralf Schenkel; Gerhard Weikum,Abstract. Federated transaction management is needed to ensure the consistency of datathat is distributed across multiple; largely autonomous. and possibly heterogeneouscomponent databases and accessed by both global and local transactions. The problem ofhow to achieve global serializability has been intensively studied in the literature; but noneof the proposed approaches can be considered as a universally" best" strategy. Rather afederated transaction manager should support a suite of strategies and allow selecting themost suitable protocol for each application. Furthermore; it should not only concentrate onthe idealized correctness criterion of serializability. but also offer support for differentisolation levels. which are widely used in existing database systems. This paper presentsthe design and implementation of a CORBA-based customizable transaction manager for …,EFIS,1999,7
On mixed-workload multimedia storage servers with guaranteed performance and service quality,Guido Nerjes; Yannis Romboyannakis; Peter Muth; Michael Paterakis; Peter Triantafillou; Gerhard Weikum,Abstract An important issue in multimedia information systems that has receivedconsiderable attention is to provide performance and service quality guaranteesfor\continuous" streams of video/audio data; especially bounding the rate of non-timely datafragments; so-called\glitches"; under a given number of concurrently served streams. Anequally important but much harder problem that has been neglected so far is to extend suchguarantees to a mixed workload with both continuous-data streams and response-time-sensitive requests to conventional;\discrete" data. This paper develops an analyticperformance model for such a mixed workload. The model is a hierarchical one; where thehigher; macroscopic level addresses the mutual performance impacts of continuous-dataand discrete-data requests by means of an abstract Markov process model; and the lower …,Proceedings 3rd International Workshop on Multimedia Information Systems; Como; Italy,1997,7
What's different about garbage collection for persistent programming languages,M Franklin; G Copeland; G Weikum,*,*,1989,7
Making sense of entities and quantities in web tables,Yusra Ibrahim; Mirek Riedewald; Gerhard Weikum,Abstract HTML tables and spreadsheets on the Internet or in enterprise intranets oftencontain valuable information; but are created ad-hoc. As a result; they usually lacksystematic names for column headers and clear vocabulary for cell values. This limits the re-use of such tables and creates a huge heterogeneity problem when comparing oraggregating multiple tables.,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,2016,6
Exception-enriched rule learning from knowledge graphs,Mohamed H Gad-Elrab; Daria Stepanova; Jacopo Urbani; Gerhard Weikum,Abstract Advances in information extraction have enabled the automatic construction oflarge knowledge graphs (KGs) like DBpedia; Freebase; YAGO and Wikidata. These KGs areinevitably bound to be incomplete. To fill in the gaps; data correlations in the KG can beanalyzed to infer Horn rules and to predict new facts. However; Horn rules do not take intoaccount possible exceptions; so that predicting facts via such rules introduces errors. Toovercome this problem; we present a method for effective revision of learned Horn rules byadding exceptions (ie; negated atoms) into their bodies. This way errors are largely reduced.We apply our method to discover rules with exceptions from real-world KGs. Ourexperimental results demonstrate the effectiveness of the developed method and theimprovements in accuracy for KG completion by rule-based fact prediction.,International Semantic Web Conference,2016,6
Temponym tagging: Temporal scopes for textual phrases,Erdal Kuzey; Jannik Strötgen; Vinay Setty; Gerhard Weikum,Abstract For many NLP and IR applications; anchored temporal information extracted fromtextual documents is of utmost importance. Thus; temporal tagging--the extraction andnormalization of temporal expressions--has gained a lot of attention in recent years andseveral tools such as HeidelTime and SUTime are proposed. However; such tools do notaddress textual phrases with temporal scopes like" Clinton's time as First Lady". While suchphrases (so-called temponyms) are not temporal expressions per se; information about theirtemporal scopes can be helpful in many scenarios; eg; in the context of temporal informationretrieval. In this paper; we describe the integration of a wide range of temponyms to thepublicly available temporal tagger HeidelTime to include temponym tagging.,Proceedings of the 25th International Conference Companion on World Wide Web,2016,6
Aligning multi-cultural knowledge taxonomies by combinatorial optimization,Natalia Prytkova; Gerhard Weikum; Marc Spaniol,Abstract Large collections of digital knowledge have become valuable assets for search andrecommendation applications. The taxonomic type systems of such knowledge bases areoften highly heterogeneous; as they reflect different cultures; languages; and intentions ofusage. We present a novel method to the problem of multi-cultural knowledge alignment;which maps each node of a source taxonomy onto a ranked list of most suitable nodes in thetarget taxonomy. We model this task as combinatorial optimization problems; using integerlinear programming and quadratic programming. The quality of the computed alignments isevaluated; using large heterogeneous taxonomies about book categories.,Proceedings of the 24th International Conference on World Wide Web,2015,6
Discovering and disambiguating named entities in text,Johannes Hoffart,Die Erkennung von Entitäten wie Personen; Organisation; Liedern oder Orten in Texten istein wichtiger Baustein für semantische Suche; maschinelle Übersetzung undInformationsextraktion. Ein Kernproblem der Erkennung ist die Mehrdeutigkeit allerEigennamen. Diese erfordert robuste Methoden; um Eigennamen mit den passendenkanonischen Entitäten einer Wissensbasis zu verknüpfen. Zusätzlich müssenVerknüpfungsmethoden in dieser dynamischen; sich stetig wandelnden Welt vonunvollständigen Wissensbasen ausgehen; da ständig neue Entitäten entstehen. DieseDissertation entwickelt Methoden; Eigennamen zu erkennen und mit kanonischen Entitätenzu verknüpfen; und verbindet somit Texte mit Wissensbasen. Der erste Beitrag ist einerobuste Methode zur Verknüpfung von Eigennamen mit Entitäten; die auf einem …,*,2015,6
Adapting AIDA for Tweets.,Mohamed Amir Yosef; Johannes Hoffart; Yusra Ibrahim; Artem Boldyrev; Gerhard Weikum,*,# MSM,2014,6
AIDArabic A Named-Entity Disambiguation Framework for Arabic Text,Mohamed Amir Yosef; Marc Spaniol; Gerhard Weikum,Abstract There has been recently a great progress in the field of automatically generatedknowledge bases and corresponding disambiguation systems that are capable of mappingtext mentions onto canonical entities. Efforts like the before mentioned have enabledresearchers and analysts from various disciplines to semantically “understand” contents.However; most of the approaches have been specifically designed for the English languageand-in particular-support for Arabic is still in its infancy. Since the amount of Arabic Webcontents (eg in social media) has been increasing dramatically over the last years; we see agreat potential for endeavors that support an entity-level analytics of these data. To this end;we have developed a framework called AIDArabic that extends the existing AIDA system byadditional components that allow the disambiguation of Arabic texts based on an …,Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing (ANLP),2014,6
Knowledge Linking for Online Statistics,Marc Spaniol; Natalia Prytkova; Gerhard Weikum,Abstract The LAWA project investigates large-scale Web (archive) data along the temporaldimension. As a use case; we are studying Knowledge Linking for Online Statistics. Statisticportals such as eurostat's" Statistics Explained"( http://epp. eurostat. ec. europa.eu/statistics_explained/index. php/Main_Page) provide a wealth of articles constituting anencyclopedia of European statistics. Together with its statistical glossary; the huge amount ofnumerical data comes with a well-defined thesaurus. However; this data is not directly athands; when browsing Web data covering the topic. For instance; when reading newsarticles about the debate on renewable energy across Europe after the earthquake in Japanand the Fukushima accident; one would ideally be able to understand these discussionsbased on statistical evidence.,Proc. of WSC,2013,6
Susie: Search using services and information extraction,Nicoleta Preda; Fabian Suchanek; Wenjun Yuan; Gerhard Weikum,The API of a Web service restricts the types of queries that the service can answer. Forexample; a Web service might provide a method that returns the songs of a given singer; butit might not provide a method that returns the singers of a given song. If the user asks for thesinger of some specific song; then the Web service cannot be called-even though theunderlying database might have the desired piece of information. This asymmetry isparticularly problematic if the service is used in a Web service orchestration system. In thispaper; we propose to use on-the-fly information extraction to collect values that can be usedas parameter bindings for the Web service. We show how this idea can be integrated into aWeb service orchestration system. Our approach is fully implemented in a prototype calledSUSIE. We present experiments with real-life data and services to demonstrate the …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,6
Dictionary-based named entity recognition,Artem Boldyrev; Gerhard Weikum; Christian Theobalt,Author: Boldyrev; Artem et al.; Genre: Thesis; Published in Print: 2013;Title: Dictionary-based Named Entity Recognition.,*,2013,6
The SOLAR system for sharp Web archiving,Arturas Mazeika; Dimitar Denev; Marc Spaniol; Gerhard Weikum,ABSTRACT Web archives preserve the history of digital information on the Internet. They area great asset for media and business analysts and also for experts on intellectual property(eg; patent offices; IP lawyers) and Internet legislators (eg consumer services) to prove ordisprove certain allegations. To fulfill this purpose; archives should not only periodicallycrawl a Web site's pages but should also assure that the captured pages are a preciserepresentation of the Web site as of a single timepoint. This is a hard problem; since thepoliteness etiquette and completeness requirement of archive crawlers mandate slow; long-duration crawling while the Web site is changing. This paper presents the SOLAR(Scheduling of Downloads for Archiving of Web Sites) system for sharp Web archiving.SOLAR crawls all pages of a Web site and then re-crawls the visited pages forming visit …,Proceedings of the 10th international Web archiving workshop; IWAW,2010,6
Optimizing distributed top-k queries,Thomas Neumann; Matthias Bender; Sebastian Michel; Ralf Schenkel; Peter Triantafillou; Gerhard Weikum,Abstract Top-k query processing is a fundamental building block for efficient ranking in alarge number of applications. Efficiency is a central issue; especially for distributed settings;when the data is spread across different nodes in a network. This paper introduces noveloptimization methods for top-k aggregation queries in such distributed environments thatcan be applied to all algorithms that fall into the frameworks of the prior TPUT and KLEEmethods. The optimizations address 1) hierarchically grouping input lists into top-k operatortrees and optimizing the tree structure; and 2) computing data-adaptive scan depths fordifferent input sources. The paper presents comprehensive experiments with two differentreal-life datasets; using the ns-2 network simulator for a packet-level simulation of a largeInternet-style network.,International Conference on Web Information Systems Engineering,2008,6
Architecting Dependable Systems V,Rogério de Lemos; Felicita Giandomenico; Cristina Gacek; Henry Muccini; Marlon Vieira,This is the fifth book in a series on Architecting Dependable Systems we started six yearsago that brings together issues related to software architectures and the dependability ofsystems. This book includes expanded and peer-reviewed papers based on the selectedcontributions to two workshops; and a number of invited papers written by recognizedexperts in the area. The two workshops were: the Workshop on Architecting DependableSystems (WADS) organized at the 2007 International Conference on Dependable Systemsand Networks (DSN 2007); and the Third Workshop on the Role of Software Architecture forTesting and Analysis organized as part of a federated conference on Component-BasedSoftware Engineering and Software Architecture (CompArch 2007). Identification of thesystem structure (ie; architecture) early in its development process makes it easier for the …,*,2008,6
Fine-grained relevance feedback for XML retrieval,Hanglin Pan; Ralf Schenkel; Gerhard Weikum,ABSTRACT This demonstration presents an XML IR system that allows users to give feedbackof different granularities and types; using Dempster-Shafer theory of evidence to computeex- panded and reweighted queries … Categories and Subject Descriptors H.3.3 [InformationSearch and Retrieval]: Relevance feedback … Keywords relevance feedback; XML IR; dempstershafer theory … Introduction Relevance Feedback is an important way to enhance retrieval qualityby integrating relevance information provided by a user. In XML retrieval; existing feedback enginesusually generate an expanded keyword query from the content of el- ements marked as relevantor nonrelevant. This approach that is inspired by text-based IR completely ignores the semistructurednature of XML. The system presented in this demonstration exploits ad- vanced user feedbackbeyond simple per-result feedback. In- stead of supplying a single relevance value for a …,Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,2008,6
Flood little; cache more: effective result-reuse in P2P IR systems,Christian Zimmer; Srikanta Bedathur; Gerhard Weikum,Abstract State-of-the-art Peer-to-Peer Information Retrieval (P2P IR) systems suffer from theirlack of response time guarantee especially with scale. To address this issue; a number oftechniques for caching of multi-term inverted list intersections and query results have beenproposed recently. Although these enable speedy query evaluations with low networkoverheads; they fail to consider the potential impact of caching on result qualityimprovements. In this paper; we propose the use of a cache-aware query routing scheme;that not only reduces the response delays for a query; but also presents an opportunity toimprove the result quality while keeping the network usage low. In this regard; we makethree-fold contributions in this paper. First of all; we develop a cache-aware; multi-roundquery routing strategy that balances between query efficiency and result-quality. Next; we …,International Conference on Database Systems for Advanced Applications,2008,6
Tunable Word-Level Index Compression for Versioned Corpora,Klaus Berberich; Srikanta Bedathur; Gerhard Weikum,Abstract. This paper presents a tunable index compression scheme for supporting time-travel phrase queries over large versioned corpora such as web archives. Support forphrase queries makes maintenance of word positions necessary; thus increasing the indexsize significantly. We propose to fuse the word positions in many neighboring versions of adocument; and thus exploit the typically high level of redundancy and compressibility toshrink the index size. The resulting compression scheme called FUSION; can be tuned totrade off compression for query-processing overheads. Our experiments on the revisionhistory of Wikipedia demonstrate the effectiveness of our method.,Proc. of Workshop EIIR,2008,6
STAR: A System for Tuple and Attribute Ranking of Query Answers.,Nishant Kapoor; Gautam Das; Vagelis Hristidis; S Sudarshan; Gerhard Weikum,Abstract In recent years there has been a great deal of interest in developing effectivetechniques for ad-hoc search and retrieval in structured repositories such as relationaldatabases-eg; searching online databases of homes; used cars; and electronic goods. Inmany of these applications; the user often experiences “information overload”; which occurswhen the system responds to an under-specified user query by returning an overwhelmingnumber of tuples; each displayed with a huge number of features (or attributes). We havedeveloped a search and retrieval system that tackles this information overload problem fromtwo angles. First; we show how to automatically rank and display the top-n most relevanttuples. Second; our system offers techniques for ordering the attributes of the returned tuplesin decreasing order of “usefulness” and selects only a few of the most useful attributes to …,ICDE,2007,6
SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval; Amsterdam; The Netherla...,W Kraaij; AP de Vries; CLA Clarke; N Fuhr; N Kando,*,Proceedings of the 6th annual international ACM SIGIR conference on Research and development in information retrieval; SIGIR,2007,6
Topx–adhoc track and feedback task,Martin Theobald; Andreas Broschart; Ralf Schenkel; Silvana Solomon; Gerhard Weikum,Abstract This paper describes the setup and results of the Max-Planck-Institut für Informatik'scontributions for the INEX 2006 AdHoc Track and Feedback task. The runs were producedwith the TopX system; which is a top-k retrieval engine for text and XML data that uses acombination of BM25-based content and structural scores.,International Workshop of the Initiative for the Evaluation of XML Retrieval,2006,6
Refinement Techniques in Software Engineering: First Pernambuco Summer School on Software Engineering; PSSE 2004; Recife; Brazil; November 23-December...,Ana Cavalcanti; Augusto Sampaio; Jim Woodcock,The Pernambuco School on Software Engineering (PSSE) 2004 was the? rst in a series ofevents devoted to the study of advanced computer science and to the promotion ofinternational scienti? c collaboration. The main theme in 2004 was re? nement (or rei?cation). Re? nement describes the veri? able relationship between a speci? cation and itsimplementation; it also describes the process of discoveringappropriateimplementations;givena speci? cation. Thus; in oneway or another; re? nement is at the heart of theprogramming process; and so is the major daily activity ofeveryprofessionalsoftwareengineer. The Summer School and its proceedings were intendedto give a detailed tutorial introduction to the scienti? c basis of this activity. Theseproceedings record the contributions from the invited lecturers. Each chapter is the result …,*,2006,6
TopX: efficient and versatile top-k query processing for text; structured; and semistructured data,Martin Theobald,TopX is a top-k retrieval engine for text and XML data. Unlike Boolean engines; it stopsquery processing as soon as it can safely determine the k top-ranked result objectsaccording to a monotonous score aggregation function with respect to a multidimensionalquery. The main contributions of the thesis unfold into four main points; confirmed byprevious publications at international conferences or workshops:• Top-k query processingwith probabilistic guarantees.• Index-access optimized top-k query processing.• Dynamicand self-tuning; incremental query expansion for top-k query processing.• Efficient supportfor ranked XML retrieval and full-text search. Our experiments demonstrate the viability andimproved efficiency of our approach compared to existing related work for a broad variety ofretrieval scenarios.,*,2006,6
P2p directories for distributed web search: From each according to his ability; to each according to his needs,Matthias Bender; Sebastian Michel; Gerhard Weikum,A compelling application of peer-to-peer (P2P) system technology would be distributed Websearch; where each peer autonomously runs a search engine on a personalized localcorpus (eg; built from a thematically focused Web crawl) and peers collaborate by routingqueries to remote peers that can contribute many or particularly good results for thesespecific queries. Such systems typically rely on a decentralized directory; eg; built on top of adistributed hash table (DHT); that holds compact; aggregated statistical metadata about thepeers which is used to identify promising peers for a particular query. To support an a-prioriunlimited number of peers; it is crucial to keep the load on the distributed directory low.Moreover; each peer should ideally tailor its postings to the directory to reflect its particularstrengths; such as rich information about specialized topics that no or only few other …,Data Engineering Workshops; 2006. Proceedings. 22nd International Conference on,2006,6
Editorial Board,Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,Software development is being revolutionized. The heavy-weight processes of the 1980sand 1990s are being replaced by light-weight; so called agile processes. Agile processesmove the focus of software development back to what really matters: running software. Thisis only made possible by accepting that software development is a creative job done by;with; and for individual human beings. For this reason; agile software developmentencourages interaction; communication; and fun. This was the focus of the Fifth InternationalConference on Extreme Programming and Agile Processes in Software Engineering whichtook place between June 6 and June 10; 2004 at the conference center in Garmisch-Partenkirchen at the foot of the Bavarian Alps near Munich; Germany. In this way theconference provided a unique forum for industry and academic professionals to discuss …,*,2005,6
Classification and focused crawling for semistructured data,Martin Theobald; Ralf Schenkel; Gerhard Weikum,Abstract Despite the great advances in XML data management and querying; the currentlyprevalent XPath-or XQuery-centric approaches face severe limitations when applied to XMLdocuments in large intranets; digital libraries; federations of scientific data repositories; andultimately the Web. In such environments; data has much more diverse structure andannotations than in a business-data setting and there is virtually no hope for a commonschema or DTD that all the data complies with. Without a schema; however; databasestylequerying would often produce either empty result sets; namely; when queries are overlyspecific; or way too many results; namely; when search predicates are overly broad; thelatter being the result of the user not knowing enough about the structure and annotations ofthe data.,*,2003,6
Ähnlichkeitssuche auf XML-Daten,Sergej Sizov; Anja Theobald; Gerhard Weikum,Zusammenfassung Anfragesprachen für XML; wie zB XPATH oder XML-QL; unterstützenBoolesches Retrieval; Anfrageergebnisse sind dabei ungeordnete Mengen von XML-Elementen; die die regulären Suchmuster einer Anfrage erfüllen. Dieses Suchparadigma istfür stark schematisierte;“geschlossene “XML-Dokumentkollektionen; zB elektronischeKataloge; geeignet. Für die Suche nach Informationen im World Wide Web oder in “offenen“Umgebungen; zB Intranets großer Unternehmen; ist jedoch Ranked Retrieval vorzuziehen;Anfrageergebnisse sind dabei Ranglisten von XML-Elementen; die nach absteigenderRelevanz sortiert sind. Web-Suchmaschinen; die auf Information-Retrieval-Konzeptenbasieren; sind andererseits nicht in der Lage; die zusätzlichen Informationen; die sich ausder Struktur von XML-Dokumenten und der semantischen Annotation durch …,*,2001,6
Auto-tuned spline synopses for database statistics management,Arnd Christian König; Gerhard Weikum,ABSTRACT Data distribution statistics are vital for database systems and other data-miningplatforms in order to predict the running time of complex queries for data ltering andextraction. State-of-theart database systems are in exible in that they maintain histograms ona xed set of single attributes; each with a xed number of buckets regardless of the underlyingdistribution and precision requirements for selectivity estimation. Despite many proposals formore advanced types of" data synopses"; research seems to have ignored the critical tuningissue of deciding on which attribute combinations synopses should be built and how manybuckets (or; analogously; transform coe cients; etc.) these should have with a given amountof memory that is available for statistics management overall. This paper develops a methodfor the automatic tuning of variable-size spline-based data synopses for multidimensional …,10th International Conference on the Management of Data; Pune; India (COMAD),2000,6
Cardinal virtues: Extracting relation cardinalities from text,Paramita Mirza; Simon Razniewski; Fariz Darari; Gerhard Weikum,Abstract: Information extraction (IE) from text has largely focused on relations betweenindividual entities; such as who has won which award. However; some facts are never fullymentioned; and no IE method has perfect recall. Thus; it is beneficial to also tap contentsabout the cardinalities of these relations; for example; how many awards someone has won.We introduce this novel problem of extracting cardinalities and discusses the specificchallenges that set it apart from standard IE. We present a distant supervision method usingconditional random fields. A preliminary evaluation results in precision between 3% and55%; depending on the difficulty of relations.,arXiv preprint arXiv:1704.04455,2017,5
Across: a framework for multi-cultural interlinking of web taxonomies,Natalia Boldyrev; Marc Spaniol; Gerhard Weikum,Abstract The Web hosts a huge variety of multi-cultural taxonomies. They encompassproduct catalogs of e-commerce; general-purpose knowledge bases and numerous domain-specific category systems. The" common denominator" of all these is their enormousdiversity; which makes it infeasible to combine multiple taxonomies for ad-hoc tasks. Tosupport the alignment of independently created Web taxonomies; we introduce the ACROSSframework. For mapping categories across different taxonomies; ACROSS harnessesinstance-level features as well as distant supervision from an intermediate source likemultiple Wikipedia editions. Our experiments with heterogeneous taxonomies for differentdomains demonstrate the viability of our approach and improvement over state-of-the-artbaselines.,Proceedings of the 8th ACM Conference on Web Science,2016,5
The knowledge awakens: Keeping knowledge bases fresh with emerging entities,Johannes Hoffart; Dragan Milchevski; Gerhard Weikum; Avishek Anand; Jaspreet Singh,Abstract Entity search over news; social media and the Web allows users to preciselyretrieve concise information about specific people; organizations; movies and theircharacters; and other kinds of entities. This expressive search mode builds on two majorassets: 1) a knowledge base (KB) that contains the entities of interest and 2) entity markup inthe documents of interest derived by automatic disambiguation of entity names (NED) andlinking names to the KB. These prerequisites are not easily available; though; in theimportant case when a user is interested in a newly emerging entity (EE) such as newmovies; new songs; etc. Automatic methods for detecting and canonicalizing EEs are notnearly at the same level as the NED methods for prominent entities that have richdescriptions in the KB. To overcome this major limitation; we have developed an …,Proceedings of the 25th International Conference Companion on World Wide Web,2016,5
Named entity disambiguation for resource-poor languages,Mohamed H Gad-Elrab; Mohamed Amir Yosef; Gerhard Weikum,Abstract Named entity disambiguation (NED) is the task of linking ambiguous names innatural language text to canonical entities like people; organizations or places; registered ina knowledge base. The problem is well-studied for English text; but few systems haveconsidered resource-poor languages that lack comprehensive name-entity dictionaries;entity descriptions; and large annotated training corpora. In this paper we address the NEDproblem for languages with limited amount of annotated corpora as well as structuredresource such as Arabic. We present a method that leverages structured English resourcesto enrich the components of a language-agnostic NED system and enable effective NED forother languages. We achieve this by fusing data from several multilingual resources and theoutput of automatic translation/transliteration systems. We show the viability and quality of …,Proceedings of the Eighth Workshop on Exploiting Semantic Annotations in Information Retrieval,2015,5
Image Analysis and Recognition: 12th International Conference; ICIAR 2015; Niagara Falls; ON; Canada; July 22-24; 2015; Proceedings,Mohamed Kamel; Aurélio Campilho,This book constitutes the thoroughly refereed proceedings of the 12th InternationalConference on Image Analysis and Recognition; ICIAR 2015; held in Niagara Falls; ON;Canada; in July 2015. The 55 revised full papers and 5 short papers presented werecarefully reviewed and selected from 80 submissions. The papers are organized in thefollowing topical sections: image quality assessment; image enhancement; imagesegmentation; registration and analysis; image coding; compression and encryption;dimensionality reduction and classification; biometrics; face description; detection andrecognition; human activity recognition; robotics and 3D vision; medical image analysis; andapplications.,*,2015,5
C3EL: a joint model for cross-document co-reference resolution and entity linking,Sourav Dutta; Gerhard Weikum,Abstract Cross-document co-reference resolution (CCR) computes equivalence classesover textual mentions denoting the same entity in a document corpus. Named-entity linking(NEL) disambiguates mentions onto entities present in a knowledge base (KB) or mapsthem to null if not present in the KB. Traditionally; CCR and NEL have been addressedseparately. However; such approaches miss out on the mutual synergies if CCR and NELwere performed jointly. This paper proposes C3EL; an unsupervised framework combiningCCR and NEL for jointly tackling both problems. C3EL incorporates results from the CCRstage into NEL; and vice versa: additional global context obtained from CCR improves thefeature space and performance of NEL; while NEL in turn provides distant KB features foralready disambiguated mentions to improve CCR. The CCR and NEL steps are …,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015,5
Aesthetics: analytics with strings; things; and cats,Johannes Hoffart; Dragan Milchevski; Gerhard Weikum,Abstract This paper describes an advanced news analytics and exploration system thatallows users to visualize trends of entities like politicians; countries; and organizations incontinuously updated news articles. Our system improves state-of-the-art text analytics bylinking ambiguous names in news articles to entities in knowledge bases like Freebase;DBpedia or YAGO. This step enables indexing entities and interpreting the contents in termsof entities. This way; the analysis of trends and co-occurrences of entities gains accuracy;and by leveraging the taxonomic type hierarchy of knowledge bases; also in expressivenessand usability. In particular; we can analyze not only individual entities; but also categories ofentities and their combinations; including co-occurrences with informative text phrases. OurWeb-based system demonstrates the power of this approach by insightful anecdotic …,Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,2014,5
Database researchers: plumbers or thinkers?,Gerhard Weikum,Abstract DB researchers have traditionally focused on engine-centered issues such asindexing; query processing; and transactions. Data mining has broadened the community'sviewpoint towards algorithmic and statistical issues. However; DB research has always hada tendency to shy away from seemingly elusive long-term challenges with AI flavor. On theother hand; the current explosion of digital content in enterprises and the Internet; is mostlycaused by user-created information like text; tags; photos; videos; and not by seeing morewell-designed databases of the traditional kind. In this situation; I question the traditionalskepticism of DB researchers towards" AI-complete" problems and the DB community'sreluctance to embark on seemingly non-DB-ish grand challenges. Big questions that I seeas great opportunities also for DB research include: 1) automatic extraction of relational …,Proceedings of the 14th International Conference on Extending Database Technology,2011,5
Variants of ltl query checking,Hana Chockler; Arie Gurfinkel; Ofer Strichman,Abstract Given a model M and a temporal logic formula ϕ [?]; where? is a placeholder; thequery checking problem; as defined for the case of CTL by Chan in 2000; is to find thestrongest propositional formula f such that M⊧ ϕ [?← f]. The motivation for solving thisproblem is; among other things; to get insight on the model. We consider various objectivesto the LTL query-checking problem; and study the question of whether there is a bettersolution than simply enumerating all possible formulas (modulo logical equivalence). It turnsout that in most cases the answer is no; but there is one particular objective for which theanswer–in practice–is definitely yes. The solution is based on a reduction to a Pseudo-Boolean Solving problem.,Haifa Verification Conference,2010,5
Advances in Data Mining: Applications and Theoretical Aspects: 10th Industrial Conference; ICDM 2010; Berlin; Germany; July 12-14; 2010. Proceedings,Petra Perner,These are the proceedings of the tenth event of the Industrial Conference on Data MiningICDM held in Berlin (www. data-mining-forum. de). For this edition the Program Committeereceived 175 submissions. After the pe-review process; we accepted 49 high-quality papersfor oral presentation that are included in this book. The topics range from theoretical aspectsof data mining to app-cations of data mining such as on multimedia data; in marketing;finance and telec-munication; in medicine and agriculture; and in process control; industryand society. Extended versions of selected papers will appear in the international journalTrans-tions on Machine Learning and Data Mining (www. ibai-publishing. org/journal/mldm).Ten papers were selected for poster presentations and are published in the ICDM PosterProceeding Volume by ibai-publishing (www. ibai-publishing. org). In conjunction with …,*,2010,5
Answering web questions using structured data: dream or reality?,Fernando Pereira; Anand Rajaraman; Sunita Sarawagi; William Tunstall-Pedoe; Gerhard Weikum; Alon Halevy,Abstract The question of which role structured data can play in Web search has been raisedfrom the early days of the Web. On the one hand; structured data can be used to answerfactual queries. On the other; large amounts of structured data can be used to betterorganize web-content and therefore to improve search on a wide range of queries.,Proceedings of the VLDB Endowment,2009,5
Sanskrit Computational Linguistics: First and Second International Symposia Rocquencourt; France; October 29-31; 2007 Providence; RI; USA; May 15-17; 2008; Re...,Gérard Huet; Amba Kulkarni; Peter Scharf,This volume constitutes the thoroughly refereed post-conference proceedings of the Firstand Second International Symposia on Sanskrit Computational Linguistics; held inRocquencourt; France; in October 2007 and in Providence; RI; USA; in May 2008respectively. The 11 revised full papers of the first and the 12 revised papers of the secondsymposium presented with an introduction and a keynote talk were carefully reviewed andselected from the lectures given at both events. The papers address several topics such asthe structure of the Paninian grammatical system; computational linguistics; lexicography;lexical databases; formal description of sanskrit grammar; phonology and morphology;machine translation; philology; and OCR.,*,2009,5
Argumentation in Multi-Agent Systems: Fifth International Workshop; ArgMAS 2008; Estoril; Portugal; May 12; 2008; Revised Selected and Invited Papers,Iyad Rahwan; Pavlos Moraitis,During the last decade Argumentation has been gaining importance within ArtificialIntelligence especially in multi agent systems. Argumentation is a powerful mechanism formodelling the internal reasoning of an agent. It also provides tools for analysing; designingand implementing sophisticated forms of interaction among rational agents; thus makingimportant contributions to the theory and practice of multiagent dialogues. Applicationdomains include: nonmonotonic reasoning; legal disputes; business negotiation; labordisputes; team formation; scientific inquiry; deliberative democracy; ontology reconciliation;risk analysis; scheduling; and logistics. This volume presents the latest developments in thisarea at the interface of argumentation theory and multi agent systems. The 10 revised fullpapers presented together with 3 invited papers from the AAMAS 2008 conference were …,*,2009,5
Database tuning using combinatorial search,Surajit Chaudhuri; Vivek Narasayya; Gerhard Weikum,realized using such logical structures. For example; in tree based data acquisition protocols;a collection tree is built that is rooted at the data collection center such as the sink node [8].The dissemination of the data requests from the participating nodes and collection of datafrom the sensor nodes are accomplished using this tree. A cluster based data acquisitionmechanism has been proposed in [3]. As shown in Fig. 1; nodes are organized into a fixednumber of clusters; and nodes within each cluster dynamically elect a cluster head. The dataacquisition is carried out in two phases. In the first phase; cluster heads collect data fromtheir cluster nodes. In the second phase; cluster heads send collected data to the nodes thathave subscribed to the data. The cluster heads are re-elected to balance energyconsumption among the nodes in the cluster. Zhang et al.[13] have proposed an adaptive …,*,2009,5
Information Hiding: 10th International Workshop; IH 2008; Sana Barbara; CA; USA; May 19-21; 2008; Revised Selected Papers,Kaushal Solanki; Kenneth Sullivan; Upamanyu Madhow,th It is our great pleasure to present this volume of the proceedings of the 10 edition ofInformation Hiding (IH 2008). The conference was held in Santa Barbara-the Ame-canRiviera; California; USA; during May 19–21; 2008. It was organized by three SantaBarbarans on fire; from both industry (Mayachitra) and academia (UCSB). Over the years;Information Hiding (IH) has established itself as a premier forum for presenting researchcovering various aspects of information hiding. Continuing the tradition; this year; we providea balanced program including topics such as anonymity and privacy; forensics;steganography; watermarking; fingerprinting; other hiding domains; and novel applications.We received a total of 64 papers from all over the globe; and would like to take thisopportunity to thank all the authors who submitted their paper to IH 2008 and thus …,*,2008,5
E-Commerce and Web Technologies: 9th International Conference; EC-Web 2008 Turin; Italy; September 3-4; 2008; Proceedings,Giuseppe Psaila; Roland Wagner,The International Conference on E-commerce and Web Technologies (EC-Web) is a matureand well-established forum for researchers working in the area of electronic commerce andweb technologies. These are the proceedings of the ninth conference in the series; which;like previous EC-Web conferences; was co-located with DEXA; the International Conferenceon Database and Expert Systems Applications; which; this year; took place in Turin; Italy.One key feature of EC-Web is its two-fold nature: it brings together both papers proposingtechnological solutions for e-commerce and the World Wide Web; and papers concerningthe management of e-commerce; such as web marketing; the impact of e-commerce onbusiness processes and organizations; the analysis of case studies; as well as socialaspects of e-commerce (to understand the impact of e-commerce solutions on day-to-day …,*,2008,5
Index-based snippet generation,Gabriel Manolache,Abstract Ranked result lists with query-dependent snippets have become state of the art intext search. They are typically implemented by searching; at query time; for occurrences ofthe query words in the top-ranked documents. This document-based approach has threeinherent problems:(i) when a document is indexed by terms which it does not contain literally(eg; related words or spelling variants); localization of the corresponding snippets becomesproblematic;(ii) each query operator (eg; phrase or proximity search) has to be implementedtwice; on the index side in order to compute the correct result set; and on the snippetgeneration side to generate the appropriate snippets; and (iii) in a worst case; the wholedocument needs to be scanned for occurrences of the query words; which is problematic forvery long documents. This thesis presents an alternative index-based approach that …,Master's Thesis,2008,5
Mapping Roget's Thesaurus and WordNet to French.,Gerard De Melo; Gerhard Weikum,Abstract Roget's Thesaurus and WordNet are very widely used lexical reference works. Wedescribe an automatic mapping procedure that effectively produces French translations ofthe terms in these two resources. Our approach to the challenging task of disambiguation isbased on structural statistics as well as measures of semantic relatedness that are utilized tolearn a classification model for associations between entries in the thesaurus and Frenchterms taken from bilingual dictionaries. By building and applying such models; we haveproduced French versions of Roget's Thesaurus and WordNet with a considerable level ofaccuracy; which can be used for a variety of different purposes; by humans as well as incomputational applications.,LREC,2008,5
Programming Languages and Systems,G Ramalingam,*,6th Asian Symposium; APLAS,2008,5
Exploiting community behavior for enhanced link analysis and Web search,Julia Luxenburger; Gerhard Weikum,Abstract Methods for Web link analysis and authority ranking such as PageRank are basedon the assumption that a user endorses a Web page when creating a hyperlink to this page.There is a wealth of additional user-behavior information that could be considered forimproving authority analysis; for example; the history of queries that a user community posedto a search engine over an extended time period; or observations about which query-resultpages were clicked on and which ones were not clicked on after a user saw the summarysnippets of the top-10 results. We study enhancements of link analysis methods byincorporating additional user assessments based on query logs and click streams; includingnegative feedback when a query-result page does not satisfy the user demand or is evenperceived as spam. Our methods use various novel forms of Markov models whose states …,Dagstuhl Seminar Proceedings,2007,5
Rank synopses for efficient time travel on the web graph,Klaus Berberich; Srikanta Bedathur; Gerhard Weikum,The World Wide Web is increasingly becoming the key source of information pertaining notonly to business and entertainment but also to a spectrum of sciences; culture; and politics.However; the Web has an even greater source of information within it–evolutionary history ofits structure and content. It not only captures the evolution of digital content but embodies thenear-term history of our society; economy; and science. Although efforts such as the InternetArchive [1] are archiving a large fraction of the Web; there is a serious lack of tools that aredesigned for the effective search over these Web archives. Time travel queries are aimed atsupporting the evolutionary (temporal) analysis over Web archives extending the power ofWeb search-engines. Specifically; a time travel query Q is defined as a pair〈 Qir; Qtc〉;where Qir is the IR-style keyword query and Qtc is the target temporal context. For …,Proceedings of the 15th ACM international conference on Information and knowledge management,2006,5
IQN Routing: Integrating Quality and Novelty for P2P Web Search,Sebastian Michel; Matthias Bender; Peter Triantafillou; Gerhard Weikum,Page 1 …,Proceedings of the EDBT,2006,5
Challenges of distributed search across digital libraries,Matthias Bender; Sebastian Michel; Gerhard Weikum; Christian Zimmer,Abstract. We present the MINERVA1 project that tackles the problem of collaborative searchacross a large number of digital libraries. The search engine is layered on top of a Chord-style peer-to-peer overlay network that connects an a-priori unlimited number of peers ordigital libraries. Each library posts a small amount of metadata to a conceptually global; butphysically distributed directory. This directory is used to efficiently select promising librariesto execute a query based on their local data. The paper discusses current challengesregarding replication; caching and proactive dissemination; query routing based on localuser profiles such as bookmarks; and benefit/cost models for query routing.,Future Digital Library Management Systems: System Architecture and Information Access,2005,5
Peer-to-Peer-Technologie für unternehmensweites und organisationsübergreifendes Workflow-Management.,Matthias Bender; Steffen Kraus; Florian Kupsch; German Shegalov; Gerhard Weikum; Dirk Werth; Christian Zimmer,Workflow-Management ist eine reife Technologie; ihre Erfolgsbilanz beim möglichen Einsatzfür die Steuerung unternehmensweiter und organisationsübergreifender Geschäftsprozesseist aber eher bescheiden. Zwei neuere Technologien; die die Einsetzbarkeit von Workflow-Management vereinfachen und verbessern könnten; sind Web Services (WS) und Peer-to-Peer-Architekturen (P2P)[ACK04; Or01; Le03a]. Durch WS können Applikationen und ganzeSub-Workflows über Unternehmensgrenzen hinweg einheitlich gekapselt werden. P2P-Architekturen; für File-Sharing und Publish-Subscribe-Anwendungen sehr erfolgreich;könnten komplex strukturierte und organisatorisch schwierig zu vereinheitlichende Prozesseüber lose gekoppelte Systeme dezentralisiert abwickeln; ohne in die Engpässe zentralerProzessintegration und Systemadministration zu laufen.,GI Jahrestagung (2),2004,5
A Performance Model of Mixed-Workload Multimedia Information Servers.,Guido Nerjes; Peter Muth; Gerhard Weikum,Abstract Advanced multimedia applications such as digital libraries or teleteaching exhibit amixed workload with accesses to both” continuous” data (eg; video) and conventional;”discrete” data (eg; text/image documents). As the fractions of continuous-data versusdiscrete-data requests vary over time; we consider a multimedia storage server with bothclasses of data spread across all disks for dynamic load sharing. This paper develops astochastic model for predicting the performance of mixed multimedia workloads on a givensystem configuration. It focuses on analyzing the response-time distribution for discrete-datarequests under potential contention with continuous-data requests. We derive an upperbound for the probability that the response time of a discrete-data request exceeds aspecified tolerance threshold. Experimental results from detailed simulation studies …,MMB,1999,5
Performance Assessment and Configuration of Enterprise-Wide Workflow Management Systems.,Michael Gillmann; Jeanine Weissenfels; Gerhard Weikum; Achim Kraiss,The main goal of workflow management systems (WFMS) is to support the efficient; largelyautomated execution of business processes. Large enterprises demand the reliableexecution of a wide variety of workflow types. For some of these workflow types; theavailability of the components of the underlying; often distributed WFMS is crucial; for otherworkflow types; high throughput and short response times are required. However; finding aconfiguration of the WFMS (eg; with replicated components) that meets all requirements is anon-trivial problem. Moreover; it may be necessary to adapt the configuration over time dueto changes of the workflow load; eg; upon adding new workflow types. Therefore; it is notsufficient to find an appropriate initial configuration; it should rather be possible toreconfigure the WFMS dynamically. The first step towards a dynamic configuration tool is …,Enterprise-Wide and Cross-Enterprise Workflow Management,1999,5
Uniform Object Management; 2nd Int,G Copeland; M Franklin; G Weikum,*,Conf. Extending Database Technology,1990,5
Pros and Cons of Operating System Transactions for Data Base Systems; ACM,G Weikum,*,IEEE Fall Joint Computer Conf,1986,5
Automated template generation for question answering over knowledge graphs,Abdalghani Abujabal; Mohamed Yahya; Mirek Riedewald; Gerhard Weikum,Abstract Templates are an important asset for question answering over knowledge graphs;simplifying the semantic parsing of input utterances and generating structured queries forinterpretable answers. State-of-the-art methods rely on hand-crafted templates with limitedcoverage. This paper presents QUINT; a system that automatically learns utterance-querytemplates solely from user questions paired with their answers. Additionally; QUINT is ableto harness language compositionality for answering complex questions without having anytemplates for the entire question. Experiments with different benchmarks demonstrate thehigh quality of QUINT.,Proceedings of the 26th international conference on world wide web,2017,4
Ten Years of Knowledge Harvesting: Lessons and Challenges.,Gerhard Weikum; Johannes Hoffart; Fabian M Suchanek,Abstract This article is a retrospective on the theme of knowledge harvesting: automaticallyconstructing large highquality knowledge bases from Internet sources. We draw on ourexperience in the Yago-Naga project over the last decade; but consider other projects aswell. The article discusses lessons learned on the architecture of a knowledge harvestingsystem; and points out open challenges and research opportunities.,IEEE Data Eng. Bull.,2016,4
Credible review detection with limited information using consistency features,Subhabrata Mukherjee; Sourav Dutta; Gerhard Weikum,Abstract Online reviews provide viewpoints on the strengths and shortcomings ofproducts/services; influencing potential customers' purchasing decisions. However; theproliferation of non-credible reviews—either fake (promoting/demoting an item); incompetent(involving irrelevant aspects); or biased—entails the problem of identifying credible reviews.Prior works involve classifiers harnessing rich information about items/users—which mightnot be readily available in several domains—that provide only limited interpretability as towhy a review is deemed non-credible. This paper presents a novel approach to address theabove issues. We utilize latent topic models leveraging review texts; item ratings; andtimestamps to derive consistency features without relying on item/user histories; unavailablefor “long-tail” items/users. We develop models; for computing review credibility scores to …,Joint European Conference on Machine Learning and Knowledge Discovery in Databases,2016,4
Exploratory querying of extended knowledge graphs,Mohamed Yahya; Klaus Berberich; Maya Ramanath; Gerhard Weikum,Abstract Knowledge graphs (KGs) are important assets for search; analytics; andrecommendations. However; querying a KG to explore entities and discover facts is difficultand tedious; even for users with skills in SPARQL. First; users are not familiar with thestructure and labels of entities; classes and relations. Second; KGs are bound to beincomplete; as they capture only major facts about entities and their relationships and missout on many of the more subtle aspects. We demonstrate TriniT; a system that facilitatesexploratory querying of large KGs; by addressing these issues of" vocabulary" mismatch andKG incompleteness. TriniT supports query relaxation rules that are invoked to allow forrelevant answers which are not found otherwise. The incompleteness issue is addressed byextending a KG with additional text-style token triples obtained by running Open IE on …,Proceedings of the VLDB Endowment,2016,4
Context-Sensitive Auto-Completion for Searching with Entities and Categories,Andreas Schmidt; Johannes Hoffart; Dragan Milchevski; Gerhard Weikum,Abstract When searching in a document collection by keywords; good auto-completionsuggestions can be derived from query logs and corpus statistics. On the other hand; whenquerying documents which have automatically been linked to entities and semanticcategories; auto-completion has not been investigated much. We have developed asemantic auto-completion system; where suggestions for entities and categories arecomputed in real-time from the context of already entered entities or categories and fromentity-level co-occurrence statistics for the underlying corpus. Given the huge size of theknowledge bases that underlie this setting; a challenge is to compute the best suggestionsfast enough for interactive user experience. Our demonstration shows the effectiveness ofour method; and its interactive usability.,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,4
Disambiguation of entities in MEDLINE abstracts by combining MeSH terms with knowledge,Amy Siu; Patrick Ernst; Gerhard Weikum,Abstract Entity disambiguation in the biomedical domain is an essential task in any textmining pipeline. Much existing work shares one limitation; in that their model trainingprerequisite and/or runtime computation are too expensive to be applied to all ambiguousentities in real-time. We propose an automatic; light-weight method that processes MEDLINEabstracts at largescale and with high-quality output. Our method exploits MeSH terms andknowledge in UMLS to first identify unambiguous anchor entities; and then disambiguateremaining entities via heuristics. Experiments showed that our method is 79.6% and 87.7%accurate under strict and relaxed rating schemes; respectively. When compared toMetaMap's disambiguation; our method is one order of magnitude faster with a slightadvantage in accuracy.,Proceedings of the 15th Workshop on Biomedical Natural Language Processing,2016,4
Commonsense knowledge in domain-specific knowledge bases,A Varde; N Tandon; S Nag Chowdhury; G Weikum,Common sense knowledge (CSK) is so intuitive that it is often the hardest to capture and usefor various real world applications. Prior work at MPII includes extracting and harnessingcommon sense knowledge in various contexts. This entails deriving a fact database forcommon sense on a Web scale; getting comparative common sense knowledge;discovering CSK from movie scripts and mining on specific activities that are CSK-based [7;8; 9]. Notable among these is the WebChild project that involves creating a hugecommonsense knowledge base from Web contents [7]. It includes a commonsense browserthrough which users can search information about various realworld concepts; their commonproperties and related terms along with illustrations. This project propels future research inseveral avenues such as expanding the scope of the “activities” captured therein …,*,2015,4
Spotting knowledge base facts in web texts,Tomasz Tylenda; Sarath Kumar Kondreddi; Gerhard Weikum,Abstract Knowledge bases (KB) such as DBpedia; YAGO and Freebase have beenconstructed by harvesting facts from high-quality data sources and incorporating communitycontributions. Accurately detecting occurrences of these KB facts in complementary sources(sources other than where they were extracted from) is crucial for fact validity assessmentsand deriving occurrence statistics. In this paper we consider fact spotting–the task ofautomatically discovering the mentions of KB facts in text documents. Our fact spottingmethodology follows a two-stage approach. First; we perform similarity-based matching ofnoun phrases with labels of KB entities and dependency path structures with patterns of KBrelations. Next; we perform joint matching of mentions; entities; paths; relations; and theirtextual locations by encoding them into variables of an integer linear program. We …,Proceedings of the 4th Workshop on Automated Knowledge Base Construction,2014,4
Spotting facts in the wild,Tomasz Tylenda; Yafang Wang; Gerhard Weikum,ABSTRACT Knowledge bases have become key assets for many tasks related to searchand analytics. Retrieving textual evidence for the facts about an entity–in news; socialmedia; or other web contents–is important for assessing the truth of statements; gatheringstatistics about the saliency of facts; and as a basis for summarizing documents or entirecorpora. This paper addresses the problem of spotting occurrences of known facts in textualdocuments; reporting the presence or absence of facts. Our solution is based on richdictionaries of paraphrases for entity names and binary relations; and uses heuristic rules forhigh precision and recall. We evaluated our method by finding facts from Freebase in acorpus of stylistically diverse biographies from the web.,Workshop on Automatic Creation and Curation of Knowledge Bases,2014,4
A unified probabilistic approach for semantic clustering of relational phrases,Adam Grycner; Gerhard Weikum; Jay Pujara; James Foulds; Lise Getoor,Abstract The task of finding synonymous relational phrases is important in natural languageunderstanding problems such as question answering and paraphrase detection. While thistask has been addressed by many previous systems; each of these existing approaches islimited either in expressivity or in scalability. To address this challenge; we present a large-scale statistical relational method for clustering relational phrases using Probabilistic SoftLogic (PSL)[1]. To assess the quality of our approach; we evaluated it relative to a set ofbaseline methods. The proposed technique was found to outperform the baselines for bothclustering and link prediction; and was shown to be scalable enough to be applied to200;000 relational phrases.,AKBC-14,2014,4
Transactions on Large-Scale Data-and Knowledge-Centered Systems VIII: Special Issue on Advances in Data Warehousing and Knowledge Discovery,Abdelkader Hameurlain; Josef Küng; Roland Wagner; Alfredo Cuzzocrea; Umeshwar Dayal,Data warehousing and knowledge discovery is an extremely active research area where anumber of methodologies and paradigms converge; with coverage of both theoretical issuesand practical solutions. The area of data warehousing and knowledge discovery has beenwidely accepted as a key technology for enterprises and organizations; as it allows them toimprove their abilities in data analysis; decision support; and the automatic extraction ofknowledge from data. With the exponentially growing amount of information to be includedin the decision-making process; the data to be considered are becoming more and morecomplex in both structure and semantics. As a consequence; novel developments; both atthe methodological level; eg; complex analytics over data; and at the infrastructural level; eg;cloud computing architectures; are necessary. Orthogonal to the latter aspects; the …,*,2013,4
Runtime Verification,Martin Leucker,Starting from a definition of runtime verification; we develop a taxonomy that explains thedifferent aspects of runtime verification. We explain the core idea of runtime verification byshowing how monitors can be attached to existing programs; be used to verify certainaspects of the underlying program as well as be used to guide the program execution. Themain part of the presentation deals with synthesis techniques that; starting from a high levelcorrectness specifications; derive suitable monitors automatically. We start with propertiesexpressed in linear temporal logic (LTL); first with a semantics on finite traces and thenextended to a semantics over infinite traces.,Modeling and Verifying Parallel Processes,2012,4
Indexing strategies for constrained shortest paths over large social networks,Philipp Klodt; Gerhard Weikum; Srikanta Bedathur; Stephan Seufert,Abstract In this work we introduced the label-constrained shortest path problem as anextension to the shortest path problem that allows a shortest path query to specify whichedge labels are allowed on shortest paths. Furthermore we analyse its theoretical difficultyfor exact indexing strategies and come to the conclusion that exact indexing is hard forgraphs with not trivially small label sets. We then give simple and efficient extensions of theSketch algorithms from [1] to support these queries and analyse the performance of ouralgorithms on different constraint sizes. We come to the conclusion that the recall especiallyfor small constraint sizes needs to be improved and provide a simple but powerful extensionof the indexing step that increases recall significantly.,Universitat des Saarlandes,2011,4
Solving linear programs in mapreduce,Mahdi Ebrahimi; Gerhard Weikum; Rainer Gemulla,Abstract Most interesting discrete optimization problems are NP-hard; thus no efficientalgorithm to find optimal solution to such problems is likely to exist. Linear programmingplays a central role in design and analysis of many approximation algorithms. However;linear program instances in real-world applications grow enormously. In this thesis; we studythe Awerbuch-Khandekar parallel algorithm for approximating linear programs; providestrategies for efficient realization of the algorithm in MapReduce; and discuss methods toimprove its performance in practice. Further; we characterize numerical properties of thealgorithm by comparing it with partially-distributed optimization methods. Finally; weevaluate the algorithm on a weighted maximum satisfiability problem generated by SOFIEknowledge extraction framework on the complete Academic Corpus.,*,2011,4
Bonsai: Growing interesting small trees,Stephan Seufert; Srikanta Bedathur; Julián Mestre; Gerhard Weikum,Graphs are increasingly used to model a variety of loosely structured data such as biologicalor social networks and entity-relationships. Given this profusion of large-scale graph data;efficiently discovering interesting substructures buried within is essential. Thesesubstructures are typically used in determining subsequent actions; such as conductingvisual analytics by humans or designing expensive biomedical experiments. In suchsettings; it is often desirable to constrain the size of the discovered results in order to directlycontrol the associated costs. In this paper; we address the problem of finding cardinality-constrained connected sub trees in large node-weighted graphs that maximize the sum ofweights of selected nodes. We provide an efficient constant-factor approximation algorithmfor this strongly NP-hard problem. Our techniques can be applied in a wide variety of …,Data Mining (ICDM); 2010 IEEE 10th International Conference on,2010,4
Temporal search in web archives,Klaus Lorenz Berberich,Webarchive bezeichnen einerseits Archive ursprünglich im Web veröffentlichter Inhalte (z. B.das Internet Archive); andererseits Archive; die vor langer Zeit veröffentlichter Inhalte imWeb zugreifbar machen (z. B. das Archiv von The Times). Ein gewachsenes Bewusstein;dass originär digitale Inhalte bewahrenswert sind; sowie verbesserteDigitalisierungsverfahren haben dazu geführt; dass Anzahl und Umfang von Webarchivenzugenommen haben. Um das volle Potenzial von Webarchiven auszuschöpfen; bedarf esdurchdachter Suchverfahren. Diese Arbeit befasst sich mit drei relevanten Teilproblemenund leistet die folgenden Beiträge:-Vorstellung des Time-Travel Inverted indeX (TTIX) alseine Erweiterung des invertierten Index; um Zeitreise-Textsuche auf Webarchiven effizientzu unterstützen.-Eine neue Methode zur automatischen Umformulierung von …,*,2010,4
Database and Expert Systems Applications: 20th International Conference; DEXA 2009; Linz; Austria; August 31-September 4; 2009; Proceedings,Sourav S Bhowmick; Josef Küng; Roland Wagner,This book constitutes the refereed proceedings of the 20th International Conference onDatabase and Expert Systems Applications; DEXA 2009; held in Linz; Austria; inAugust/September 2009. The 35 revised full papers and 35 short papers presented werecarefully reviewed and selected from 202 submissions. The papers are organized in topicalsections on XML and databases; Web; semantics and ontologies; temporal; spatial; and highdimensional databases; database and information system architecture; performance andsecurity; query processing and optimisation; data and information integration and quality;data and information streams; data mining algorithms; data and information modelling;information retrieval and database systems; and database and information systemarchitecture and performance.,*,2009,4
Emerging Trends in Visual Computing: LIX Fall Colloquium; ETVC 2008; Palaiseau; France; November 18-20; 2008; Revised Selected and Invited Papers,Frank Nielsen,Volume Editor Frank Nielsen Ecole Polytechnique; LIX Route de Saclay; 91128 PalaiseauCedex; France E-mail: nielsen@ lix. polytechnique. fr and Sony Computer ScienceLaboratories; Inc. 3-14-13 Higashi Gotanda 3F; 141-0022 Shinagawa-ku; Tokyo; JapanE-mail: Frank. Nielsen@ acm. org Library of Congress Control Number: Applied for CR SubjectClassification (1998): I. 4; I. 5; I. 2.10; I. 3.3; I. 3.5; I. 3.7; I. 2.6; F. 2; G. 1.2 LNCS Sublibrary: SL6–Image Processing; Computer Vision; Pattern Recognition; and Graphics ISSN 0302-9743ISBN-10 3-642-00825-9 Springer Berlin Heidelberg New York ISBN-13 978-3-642-00825-2Springer Berlin Heidelberg New York This work is subject to copyright. All rights arereserved; whether the whole or part of the material is concerned; specifically the rights oftranslation; reprinting … Preface ETVC 2008; the fall colloquium of the computer …,*,2009,4
Balancing Agility and Formalism in Software Engineering: Second IFIP TC 2 Central and East European Conference on Software Engineering Techniques; CEE-SE...,JR Nawrocki,The origins of CEE-SET go back to the end of the 1990s; when the Polish Inf-mationProcessing Society together with other partners organized the Software EngineeringEducation Symposium; SEES 1998; sponsored by CEPIS; and the Polish Conference onSoftware Engineering; KKIO 1999 (the latter has become anannualevent). AfewyearslaterKKIOchangedtoaninternatio…on Software Engineering Techniques; SET 2006; sponsored by Technical C-mittee 2(Software: Theory and Practice) of the International Federation for Information Processing;IFIP [http://www. i? p. org/]. In 2007 the conference got a new name: second IFIP TC2 Centraland East-European Conference on So-ware Engineering Techniques; CEE-SET 2007. Ittook place in Poznan; Poland; and lasted for three days; from October 10 to 12; 2007 (thedetails are on the conference web page http://www. cee-set. org/2007). The conference …,*,2008,4
Languages and Compilers for Parallel Computing,José Nelson Amaral,In 2008 the Workshop on Languages and Compilers for Parallel Computing left the USA tocelebrate its 21st anninversary in Edmonton; Alberta; Canada. Following its long-established tradition; the workshop focused on topics at the frontier of research anddevelopment in languages; optimizing compilers; applications; and programming models forhigh-performance computing. While LCPC continues to focus on parallel computing; the2008 edition included the presentation of papers on program analysis that are precursors ofhigh performance in parallel environments. LCPC 2008 received 35 paper submissions.Each paper received at least three independent reviews; and then the papers and thereferee comments were discussed during a Program Committee meeting. The PC decided toaccept 18 papers as regular papers and 6 papers as short papers. The short papers …,*,2008,4
Good Guys vs. Bad Guys: Countering Cheating in Peer-to-Peer Authority Computations over Social Networks.,Mauro Sozio; Josiane Xavier Parreira; Tom Crecelius; Gerhard Weikum,ABSTRACT Eigenvector computations are an important building block for computingauthority; trust; and reputation scores in social networks and other graphs. In peer-to-peernetworks or other forms of decentralized settings (such as multi-agent platforms); this kind ofanalysis needs to be performed in a distributed manner and requires bilateral dataexchanges between peers. This gives rise to the problem that dishonest peers may cheat inorder to manipulate the computation's outcome. This paper presents a distributed algorithmfor countering the effects of such misbehavior; under the assumption that the fraction ofdishonest peers is bounded and that there is an unforgeable mechanism for peer identities;which can be implemented using security tools available. The algorithm is based on generalprinciples of replication and randomization and thus widely applicable to social network …,WebDB,2008,4
Future Digital Library Management Systems: System Architecture and Information Access,Yannis Ioannidis; Hans-Jörg Schek; Gerhard Weikum,*,*,2005,4
Rethinking the conference reviewing process,Michael J Franklin; Jennifer Widom; Anastassia Ailamaki; Philip A Bernstein; David DeWitt; Alon Halevy; Zachary Ives; Gerhard Weikum,In recent years the database research community has endeavored to expand the scope ofthe field and attract a larger and more varied base of participants. We have also long workedat “educating” academic tenure committees and research management about theimportance of our major conferences. We may now be seeing some unintended effects ofour success. There is a growing dissatisfaction with conference reviewing from all sides ofthe process. Many now perceive the process to be" broken". A number of factors can beidentified as precipitating the discontent:• The number of submitted papers has spikeddramatically in recent years (see Figure 1).,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,4
Web Caching,Gerhard Weikum; Erhard Rahm; Gottfried Vossen,Author: Weikum; Gerhard et al.; Genre: Book Chapter; Published in Print: 2003; Title: Web Caching.,*,2003,4
Fundamentals of Transactional Information Systems: Theory; Algorithms; and Practice of Concurrency Control and Recovery,Gerhard Weikum; Gottfried Vossen,The following list of errate has been compiled from numerous sources; including colleaguesand students who have provided us with a number of hints to items and points that areunclear; typos; and other stuff. We appreciate all this input and to try make use of it in thenext edition of the book. In the meantime; this list should help our readers in case questionsarise.,*,2001,4
Letter from the special issue editor,Gerhard Weikum,Author: Weikum; Gerhard; Genre: Journal Article; Published in Print:2001; Title: Letter from the Special Issue Editor.,IEEE Data (Base) Engineering Bulletin,2001,4
Zielorientiertes Performance-Engineering auf Basis von Message-orientierter Middleware,Achim Kraiß; Gerhard Weikum,*,Workshop zum Performance-Engineering in der Software-Entwicklung; Darmstadt,2000,4
Inter-and intra-transaction parallelism for combined OLTP/OLAP workloads,Christof Hasse; Gerhard Weikum,Abstract This paper presents the architecture and run-time mechanisms of an experimentalprototype system; PLENTY; that is specifically geared for combined OLTP/OLAP workloadswith update transactions and complex queries concurrently executing on the samedatabase. The system is able to parallelize both retrieval and update transactions at thelevel of precedence-graph scripts with nodes corresponding to SQL-like statements orinternal operator trees. Employing this form of infra-transaction parallelism in a multi-userenvironment reduces the lock duration and thus the potential for data contention; so thatOLTP and OLAP applications can be reconciled with good performance on the same shareddatabase. The implementation of the underlying concurrency control and recoverymechanisms is based on multi-level transactions. In addition to presenting the overall …,*,1997,4
Tutorial on parallel database systems,Gerhard Weikum,Parallel database technology has evolved to a point where commercial systems appear tobe fairly mature. Algorithms and run-time mechanisms for both speeding up query responsetime and scaling up transaction throughput are well understood. However; using a paralleldatabase system effectively under real application workloads entails a number of complexoptimization problems with respect to query processing policies; data placement; andresource management. These policy issues are still largely unexplored; from a theoretical aswell as practical perspective. This tutorial gives an overview of the state of the art in paralleldatabase technology; and points out important avenues of further research with emphasison data placement issues. Data placement for parallel systems poses a number of hardoptimization problems. Since database systems process enormous volumes of data (in …,International Conference on Database Theory,1995,4
Heuristic optimization of speedup and benefit/cost for parallel database scans on shared-memory multiprocessors,Michael Rys; Gerhard Weikum,Previous work on parallel database systems has paid little attention to the interaction ofasynchronous disk prefetching and processor parallelism. The authors investigate this issuefor scan operations on shared-memory multiprocessors. Two heuristic methods aredeveloped for the allocation of processors and memory to optimize either the speedup or thebenefit/cost ratio of database scan operations. The speedup optimization balances the dataproduction rate of the disks and the data consumption rate of the processors; aiming atoptimal speedup while ensuring that resources are not allocated unnecessarily. Thebenefit/cost optimization considers explicitly the resource consumption of a scan operationand aims to allocate processors and memory so that the ratio of the speedup attained to theoperation's resource-time product is maximized. Such an awareness of resource …,Parallel Processing Symposium; 1994. Proceedings.; Eighth International,1994,4
A Log-Structured History Data Access Method (LHAM).,Patrick E O'Neil; Gerhard Weikum,ABSTRACT There are numerous applications that require on–line access to a history ofbusiness events. Ideally; both historical and current data should be logically integrated intosome form of temporal database; also known as a multi–version database; historicaldatabase; or rollback database. The underlying access method should support the migrationof old record versions onto inexpensive write–once media; different forms of “time–travel”queries; and potentially high rates of update events. This paper introduces a new accessmethod for history data; called the Log–structured History data Access Method (LHAM). Thebasic principle of LHAM is to partition the data into successive components based on thetimestamps of the record versions; and to employ a rolling merge process for efficient datamigration between components. The concept of a rolling merge process was introduced …,HPTS,1993,4
The background of the DASDBS & COSMOS projects,H-J Schek; Marc H Scholl; Gerhard Weikum,Abstract This survey describes the conceptual framework behind the DASDBS andCOSMOS projects. COSMOS is the current research program of the database researchgroup at ETH Zurich. These activities are a natural follow-on of DASDBS; the DarmstadtDatabase System project at the Technical University of Darmstadt. While most emphasis inDASDBS was on a database kernel system; the project at ETH focuses on the cooperationbetween a database system and its environment. The environment consists of clients askingfor database service and other systems offering service to the database system. Theresearch objective is the exploration of the architecture of a COoperative System for theManagement of ObjectS (COSMOS). In short; we are on the way “from the kernel to thecosmos”.,Symposium on Mathematical Fundamentals of Database Systems,1991,4
Unterstützung des Büro-Ablage-Service durch ein Datenbankkernsystem,H-B Paul; A Söder; H-J Schek; Gerhard Weikum,Kurzfassung Eine der wichtigsten Komponenten in einem integrierten elektronischenBürosystem ist das zentrale Ablagesystem mit hoher Speicherkapazität. Es mußBürodokumente; die Text; Bilder; Graphik; Daten und Sprache enthalten; effizient speichernund wieder auffinden können. Bei den internationalen Normungsgremien ISO und ECMA isthierfür der Büro-Ablage-Service (FRS= Filing-and-Retrieval-Service) in der Diskussion. Wirschlagen für die Implementierung des FRS das Aufsetzen auf dem Speicher-System (CRS=Complex Record System) des Darmstädter Datenbanksystems (DASDBS) vor; das alsanwendungsspezifisch erweiterbarer Datenverwaltungskern für Standard-und Non-Standard-Anwendungen konzipiert ist. In diesem Papier werden der Entwurf geeigneterSpeicherungsstrukturen für FRS-Objekte und die Realisierung der FRS-Operationen …,*,1987,4
Continuous experience-aware language model,Subhabrata Mukherjee; Stephan Günnemann; Gerhard Weikum,Abstract Online review communities are dynamic as users join and leave; adopt newvocabulary; and adapt to evolving trends. Recent work has shown that recommendersystems benefit from explicit consideration of user experience. However; prior work assumesa fixed number of discrete experience levels; whereas in reality users gain experience andmature continuously over time. This paper presents a new model that captures thecontinuous evolution of user experience; and the resulting language model in reviews andother posts. Our model is unsupervised and combines principles of Geometric BrownianMotion; Brownian Motion; and Latent Dirichlet Allocation to trace a smooth temporalprogression of user experience and language model respectively. We develop practicalalgorithms for estimating the model parameters from data and for inference with our …,Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2016,3
Poly: Mining relational paraphrases from multilingual sentences,Adam Grycner; Gerhard Weikum,Abstract Language resources that systematically organize paraphrases for binary relationsare of great value for various NLP tasks and have recently been advanced in projects likePATTY; WiseNet and DEFIE. This paper presents a new method for building such a resourceand the resource itself; called POLY. Starting with a very large collection of multilingualsentences parsed into triples of phrases; our method clusters relational phrases usingprobabilistic measures. We judiciously leverage fine-grained semantic typing of relationalarguments for identifying synonymous phrases. The evaluation of POLY shows significantimprovements in precision and recall over the prior works on PATTY and DEFIE. An extrinsicuse case demonstrates the benefits of POLY for question answering.,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,2016,3
Efficient querying and learning in probabilistic and temporal databases,Maximilian Dylla,Probabilistische Datenbanken können große Mengen an ungewissen Informationenspeichern; anfragen und verwalten. Diese Doktorarbeit treibt den Stand der Technik indiesem Gebiet auf drei Arten vorran: 1. Ein abgeschlossenes und vollständigesDatenmodell für temporale; probabilistische Datenbanken wird präsentiert. Anfragenwerden mittels Deduktionsregeln gestellt; welche logische Formeln induzieren; die sowohlZeit als auch Ungewissheit erfassen. 2. Ein Methode zur Berechnung der k Anwortenhöchster Wahrscheinlichkeit wird entwickelt. Sie basiert auf logischen Formeln erster Stufe;die Mengen an Antwortkandidaten repräsentieren. Beschränkungen der Wahrscheinlichkeitdieser Formeln ermöglichen das Kürzen von Antworten mit niedriger Wahrscheinlichkeit. 3.Das Problem des Lernens von Tupelwahrscheinlichkeiten für das Aktualisieren und …,*,2014,3
Human computing games for knowledge acquisition,Sarath Kumar Kondreddi; Peter Triantafillou; Gerhard Weikum,Abstract Automatic information extraction techniques for knowledge acquisition are known toproduce noise; incomplete or incorrect facts from textual sources. Human computing offers anatural alternative to expand and complement the output of automated information extractionmethods; thereby enabling us to build high-quality knowledge bases. However; relyingsolely on human inputs for extraction can be prohibitively expensive in practice. Wedemonstrate human computing games for knowledge acquisition that employ humancomputing to overcome the limitations in automated fact acquisition methods. We provide acombined approach that tightly integrates automated extraction techniques with humancomputing for effective gathering of facts. The methods we provide gather facts in the form ofrelationships between entities. The games we demonstrate are specifically designed to …,Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,2013,3
OpinioNetIt: A structured and faceted knowledge-base of opinions,Rawia Awadallah; Maya Ramanath; Gerhard Weikum,We propose a demonstration of our system; OpinioNetIt; a structured; faceted; knowledge-base of opinions; and its use in political analysis. OpinioNetIt consists of information aboutpeople; topics and opinions in the form of triples; indicating the opinion of a person on atopic. Our focus is on acquiring opinions held by various stakeholders on politicallycontroversial topics. Our system can be used for various kinds of analyses such asgenerating heat maps showing political bias; identifying flip-flopping politicians; andidentifying dissenters; etc. In this demonstration proposal; we give a brief overview of oursystem and the proposed demonstration.,Data Mining Workshops (ICDMW); 2012 IEEE 12th International Conference on,2012,3
Finding images of rare and ambiguous entities,Bilyana Taneva; M Kacimi El Hassani; Gerhard Weikum,Abstract Despite much progress on entity-oriented Web search and automaticallyconstructed knowledge bases with millions of entities; it is still difficult to find images ofnamed entities like people or places. While images of famous entities are abundant on theInternet; they are much harder to retrieve for less popular entities such as notable computerscientists or regionally interesting churches. Querying the entity names in image searchengines yields large candidate lists; but they often have low precision and unsatisfactoryrecall. In this paper; we propose a principled model for finding images of rare or ambiguousnamed entities. We propose a set of efficient; light-weight algorithms for identifying entity-specific keyphrases from a given textual description of the entity; which we then use to scorecandidate images based on the matches of keyphrases in the underlying Web pages. Our …,*,2011,3
The bag-of-opinions method for review rating prediction from sparse text patterns,Georgiana Ifrim; Gerhard Weikum,*,Proceedings of Coling,2010,3
Analytical and Stochastic Modeling Techniques and Applications: 16th International Conference; ASMTA 2009; Madrid; Spain; June 9-12; 2009; Proceedings,Khalid Al-Begain; Dieter Fiems; Gábor Horváth,This book constitutes the refereed proceedings of the 16th International Conference onAnalytical and Stochastic Modeling Techniques and Applications; ASMTA 2009; held inMadrid; Spain; in June 2009 in conjunction with ECMS 2009; the 23nd EuropeanConference on Modeling and Simulation. The 27 revised full papers presented werecarefully reviewed and selected from 55 submissions. The papers are organized in topicalsections on telecommunication networks; wireless & mobile networks; simulation; quueingsystems & distributions; queueing & scheduling in telecommunication networks; modelchecking & process algebra; performance & reliability analysis of various systems.,*,2009,3
Fuzzy Logic and Applications: 8th International Workshop; WILF 2009 Palermo; Italy; June 9-12; 2009 Proceedings,Vito Di Gesù; Sankar Kumar Pal; Alfredo Petrosino,The 8th International Workshop on Fuzzy Logic and Applications (WILF 2009) held inPalermo (Italy); June 9-12; 2009; covered topics related to theoretical and experimentalareas of fuzzy sets and systems with emphasis on different applications. This eventrepresents the continuation of an established tradition of biannual interdisciplinary meetings.The previous editions of WILF were held; with an increasing number of participants; inNaples (1995); Bari (1997); Genoa (1999); Milan (2001); Naples (2003); Crema (2005) andCamogli (2007). Each event focused on distinct main thematic areas of fuzzy logic andrelated applications. WILF 2009 aimed to highlight connections and synergies of fuzzy setstheory with nonconventional computing (eg; neural networks; evolutionary computation;support vector machines; molecular computing; quantum computing) and cognitive …,*,2009,3
Graffiti: node labeling in heterogeneous networks,Ralitsa Angelova; Gjergji Kasneci; Fabian M Suchanek; Gerhard Weikum,Abstract We introduce a multi-label classification model and algorithm for labelingheterogeneous networks; where nodes belong to different types and different types havedifferent sets of classification labels. We present a graph-based approach which models themutual influence between nodes in the network as a random walk. When viewing classlabels as" colors"; the random surfer is" spraying" different node types with different colorpalettes; hence the name Graffiti. We demonstrate the performance gains of our method bycomparing it to three state-of-the-art techniques for graph-based classification.,Proceedings of the 18th international conference on World wide web,2009,3
SOFSEM 2009: Theory and Practice of Computer Science: 35th Conference on Current Trends in Theory and Practice of Computer Science; Špindleruv Mlýn; Czech...,Mogens Nielsen; Antonin Kucera; Peter Bro Miltersen; Catuscia Palamidessi; Petr Tuma; Frank Valencia,This book constitutes the refereed proceedings of the 35th Conference on Current Trends inTheory and Practice of Computer Science; SOFSEM 2009; held in Špindleruv Mlýn; CzechRepublic; in January 2009. The 49 revised full papers; presented together with 9 invitedcontributions; were carefully reviewed and selected from 132 submissions. SOFSEM 2009was organized around the following four tracks: Foundations of Computer Science; Theoryand Practice of Software Services; Game Theoretic Aspects of E-commerce; and Techniquesand Tools for Formal Verification.,*,2009,3
Times of Convergence. Technologies Across Learning Contexts: Third European Conference on Technology Enhanced Learning; EC-TEL 2008; Maastricht; The Net...,Pierre Dillenbourg; Marcus Specht,The European Conference on Technology-Enhanced Learning (EC-TEL 2008) was the thirdevent of a series that started in 2006. The two first editions were organized by Pro-Learn(http://www. prolearn-project. org/); a European Network of Excellence. In 2008; severalmembers of Kaleidoscope; the other European Network of Excellence (http://www. noe-kaleidoscope. org/pub/); joined as co-chair; committee members; reviewers and authors.These two networks are no longer funded; but our aim was to turn EC-TEL into a sustainableseries of high-quality events and thereby to contribute to the scientific landscape oftechnology-enhanced learning. A new network; named STELLAR; will be launched in 2009;with members from both existing networks as well as new members and will support thefuture editions of this conference. The scope of EC-TEL 2008 covered the different fields …,*,2008,3
Web information systems engineering–WISE 2008,James Bailey David Maier; Klaus-Dieter Schewe Bernhard Thalheim; Xiaoyang Sean Wang,WISE 2008 was held in Auckland; New Zealand; during September 1–3; at The AucklandUniversity of Technology City Campus Conference Centre. The aim of this conference wasto provide an international forum for researchers; professionals; and industrial practitionersto share their knowledge in the rapidly growing area of Web technologies; methodologies;and applications. Previous WISE conferences were held in Hong Kong; China (2000); Kyoto;Japan (2001); Singapore (2002); Rome; Italy (2003); Brisbane; Australia (2004); New York;USA (2005); Wuhan; China (2006) and Nancy; France (2007). The call for papers createdconsiderable interest. Around 110 paper submissions were received and the internationalProgram Committee selected 31 papers out of the 110 submissions (an acceptance rate of28.2%). Of these; 17 papers were chosen for standard presentation and the remaining 14 …,*,2008,3
Formal verification of web service interaction contracts,German Shegalov; Gerhard Weikum,Abstract Recovery is the last resort when other components exhibit bugs. It is therefore ofparamount importance that the correctness of the recovery protocols be formally verified.Recovery not only needs to cope with database failures but should handle and ideally maskmessage and process failures in clients and servers. Otherwise; when a reply message islost the application must be able to determine" manually" whether the interaction is to berepeated. This paper develops a statechart specification of a recovery framework thatgenerically guarantees exactly-once execution and applies model checking to prove itscorrectness.,Services Computing; 2008. SCC'08. IEEE International Conference on,2008,3
A pocket guide to web history,Klaus Berberich; Srikanta Bedathur; Gerhard Weikum,Abstract Web archives like the Internet Archive preserve the evolutionary history of largeportions of the Web. Access to them; however; is still via rather limited interfaces–a searchfunctionality is often missing or ignores the time axis. Time-travel search alleviates thisshortcoming by enriching keyword queries with a time-context of interest. In order to beeffective; time-travel queries require historical PageRank scores. In this paper; we addressthis requirement and propose rank synopses as a novel structure to compactly representand reconstruct historical PageRank scores. Rank synopses can reconstruct the PageRankscore of a web page as of any point during its lifetime; even in the absence of a snapshot ofthe Web as of that time. We further devise a normalization scheme for PageRank scores tomake them comparable across different graphs. Through a comprehensive evaluation …,International Symposium on String Processing and Information Retrieval,2007,3
P2P authority analysis for social communities,Josiane Xavier Parreira; Sebastian Michel; Matthias Bender; Tom Crecelius; Gerhard Weikum,Abstract PageRank-style authority analyses of Web graphs are of great importance for Webmining. Such authority analyses also apply to hot" Web 2.0" applications that exhibit anatural graph structure; such as social networks (eg; MySpace; Facebook) or taggingcommunities (eg; Flickr; Del. icio. us). Finding the most trustworthy or most importantauthorities in such a community is a pressing need; given the huge scale and also theanonymity of social networks. Computing global authority measures in a Peer-to-Peer (P2P)collaboration of autonomous peers is a hot research topic; in particular because of theincomplete local knowledge of the peers; which typically only know about (arbitrarilyoverlapping) sub-graphs of the complete graph. We demonstrate a self-organizing P2Pcollaboration that; based on the local sub-graphs; efficiently computes global authority …,Proceedings of the 33rd international conference on Very large data bases,2007,3
Advanced methods for query routing in peer-to-peer information retrieval,Matthias Bender,Eines der drängendsten Probleme in Peer-to-Peer-Netzwerken ist Query-Routing: daseffektive und effiziente Identifizieren solcher Peers; die qualitativ hochwertige lokaleErgebnisse zu einer gegebenen Anfrage liefern können. Die bisher bekannten Verfahrenaus dem Bereich der verteilten Informationssuche sowie der Metasuchmaschinen werdenden Besonderheiten von Peer-to-Peer-Netzwerken nicht gerecht. Die Hautbeiträge dieserArbeit teilen sich in folgende Schwerpunkte: 1. Query-Routing unter Berücksichtigung dergegenseitigen überlappung der Kollektionen verschiedener Peers; 2. Query-Routing unterBerücksichtigung der Korrelationen zwischen verschiedenen Termen; 3. VergleichendeEvaluierung verschiedener Methoden zum Query-Routing. Unsere Experimente bestätigendie Überlegenheit der in dieser Arbeit entwickelten Verfahren gegenüber den bisher …,*,2007,3
Semantic web; ontologies and databases,Y An; T Topaloglou; V Christophides; M Collard; C Gutierrez,*,VLDB Workshop; SWDB-ODBIS,2007,3
Information retrieval and data mining,Gerhard Weikum,Page 1. Information Retrieval and Data Mining Winter Semester 2005/06 Saarland University;Saarbrücken Prof. Dr. Gerhard Weikum weikum@mpi-inf.mpg.de http://www.mpi-inf.mpg.de/departments/d5/teaching/ws05_06/irdm/ IRDM WS 2005 1-1 Page 2. Organization • Lectures:Tuesday 14-16 and Thursday 14-16 in 45/001 Office hours Prof. Weikum: appointment bye-mail • Assignments / Tutoring Groups: Friday 9-11; 11-13; or 14-16 Monday 9-11; 11-13;or 13-15 Paper assignments given out in Tuesday lecture; to be solved until next TuesdayFirst paper assignment given out on Tuesday; Oct 25 First meetings of tutoring groups: Friday;Nov 4; and Monday; Nov 7 • Requirements for obtaining 9 credit points: • will be announcedin second week IRDM WS 2005 1-2 Page 3. Outline 1. Overview and System Architectures2. Basics from Probability Theory and Statistics (1) …,Computer Science Lecture at University of Saarland; Winter Term,2007,3
A user-interaction model for the european library portal,Julia Luxenburger; Eric van der Meulen; Gerhard Weikum,ABSTRACT Users are a far too often neglected variable in the design of information-seekingsystems. This also holds for digital libraries [1]. In this paper; we study navigational patternsof users within The European Library portal obtained from action logs covering a 4-monthperiod. We point out current bottlenecks in the portal interface design; as well as;opportunities for an enhancement of user search experience and user-tailored services.,Proceedings of the 10th DELOS Thematic Workshop on Personalized Access; Profile Management; and Context Awareness in Digital Libraries (PersDL),2007,3
EOS 2: unstoppable stateful PHP,German Shegalov; Gerhard Weikum,A growing number of businesses deliver mission-critical applications (stock trading;auctions; etc.) to their customers as Web Services. These applications compriseheterogeneous components distributed over multiple layers. They pose strong requirementsfor service and consistent data availability from both legal and business standpoints. Sincemany systems count many millions of lines of code; some bugs pass quality assuranceundetected which leads to unpredictable service outages at some point. Recovery intransactional systems guarantees: i) that an operation sequence declared as a transaction isexecuted atomically (either completely or not at all when interrupted by a failure) ii) and thatcompleted transactions persist all further failures. Atomicity and persistence do not suffice toguarantee correctness. It is the application that needs to handle timeouts and other …,Proceedings of the 32nd international conference on Very large data bases,2006,3
of Proceedings: Advances in Database Technology-EDBT 2006: 10th International Conference on Extending Database Technology,Ralf Schenkel; Martin Theobald,Abstract/Description: Relevance Feedback is an important way to enhance retrieval qualityby integrating relevance information provided by a user. In XML retrieval; feedback enginesusually generate an expanded query from the content of elements marked as relevant ornonrelevant. This approach that is inspired by text-based IR completely ignores thesemistructured nature of XML. This paper makes the important step from content-based tostructural feedback. It presents an integrated solution for expanding keyword queries withnew content; path; and document constraints. An extensible framework evaluates suchquery conditions with existing keyword-based XML search engines while allowing to easilyintegrate new dimensions of feedback. Extensive experiments with the established INEXbenchmark show the feasibility of our approach.,*,2006,3
Untersuchungen zur automatischen Klassifikation von Lamellengraphit mit Hilfe des Stützvektorverfahrens,Kathrin Roberts; Gerhard Weikum; Frank Mücklich,Kurzfassung Die unterschiedliche Graphitausbildung in Gusseisen ist wesentlich für diemechanischen Eigenschaften dieses Werkstoffes. Deshalb wurden in der Norm EN ISO 945:1994 sechs generelle Formen für die Graphitausbildung und darunter für denLamellengraphit fünf Anordnungsklassen definiert. Die subjektive Klassifikation letztererkann zu widersprüchlichen Ergebnissen führen und sollte durch ein objektives Klassifizierenmit Hilfe der Bildanalyse möglichst ersetzt werden. Vorgestellt wird der Einsatz desStützvektorverfahrens bei dem durch Berechnung einer separierenden Hyperebene mitmaximaler Bandbreite im m-dimensionalen Raum binär klassifiziert wird. Die Lage derHyperebene wird von Stützvektoren definiert; die durch Messung bildanalytischerKenngrößen an Trainingsbildern ermittelt werden. In dieser Untersuchung wurden zu …,Practical Metallography,2005,3
An information system for material microstructures,Kathrin Roberts; Frank Mucklich; Ralf Schenkel; Gerhard Weikum,This work presents an information system that supports a materialographic laboratory inclass material samples based on microstructure images. The system uses database andWeb technologies to manage its information and make it accessible to Internet users. Its coreis a class based on support vector machines; that provides an automatic diagnosis of thematerial class of a given sample. The classifier uses texture features from an underlyingimage analysis; the so-called Haralick parameters; and stereologic features such as fractaldimension; Euler parameter; etc. In addition to the class the system provides a sensitivityanalysis that allows the user to understand which features are most influential for certainclass decisions. The system is fully operational and can be used on the Web.,Scientific and Statistical Database Management; 2004. Proceedings. 16th International Conference on,2004,3
Transactional Iinformation Systems,Gerhard Weikum; Gottfried Vossen,*,*,2002,3
Taming the Tiger: How to Cope with Real Database Products in Transactional Federations for Internet Applications.,Ralf Schenkel; Gerhard Weikum,Abstract Data consistency in transactional federations is a key requirement of advanced E-service applications on the Internet; such as electronic auctions or real-estate purchase.Federated concurrency control needs to be aware of the fact that virtually all commercialdatabase products support sub-serializability isolation levels; such as Snapshot Isolation;and that applications make indeed use of such local options. This paper discusses theproblems that arise with regard to global serializability in such a setting; and proposessolutions. Protocols are developed that can guarantee global serializability over componentsystems that provide only weaker isolation levels. A full-fledged implementation is presentedthat makes use of OrbixOTS and runs on top of Oracle8i and O2 databases. Performancemeasurements with this prototype indicate the practical viability of the developed methods.,GI-Workshop Internet-Datenbanken,2000,3
Inter-and Intra-Transaction Parallelism in Database Systems,Christof Hasse; Gerhard Weikum,Abstract This paper presents an approach to improving database performance by combiningparallelism of multiple independent transactions and parallelism of multiple subtransactionswithin a transaction. An experimental prototype has been built that supports this combinationof inter-and intra-transaction parallelism based on the framework of multi-level transactionmanagement. A performance study for a foreign exchange banking application has beencarried out to obtain insights into the interaction of inter-and intra-transaction parallelism.Significant performance improvements could be gained when both update and retrievaltransactions were parallelized. 1 Introduction Database transaction processing differs fromother parallel applications in their multi-user characteristics. In online transaction processing[GR93]; also known as OLTP; many transactions are invoked from a large number of …,1Jth Speedup Workshop on Parallel and Vector Computing 7 Zurich 7 Switzerland,1993,3
Mentor-lite: Integrating light-weight workflow management systems within business environments (extended abstract). Available on the Web athttp://www-dbs. cs. uni...,Peter Muth; Jeanine Weissenfels; Michael Gillmann; Gerhard Weikum,Abstract Workjlow management systems support the eficient; largely au-tomated executionof business processes. Howevel; using a workflow management system typically requiresimplementing the application's controljlow exclusively by the workflow management system.This approach is powerful cf the control flow is specified and implemented from scratch; but ithas severe drawbacks if a workflow management system is to be integrated within environ-ments with existing solutions for implementing control flow. Usual-ly; the existing solutionsare too complex to be substituted by the workjlow management system at once. Hence; theworkjlow man-agement system must support an incremental integration; ie the reuse ofexisting implementations of controljlow as well as their in-cremental substitution. Extendingthe workjlow management system's functionality ac-cording to fiture application needs; eg …,*,*,3
Distilling task knowledge from how-to communities,Cuong Xuan Chu; Niket Tandon; Gerhard Weikum,Abstract Knowledge graphs have become a fundamental asset for search engines. A fairamount of user queries seek information on problem-solving tasks such as building a fenceor repairing a bicycle. However; knowledge graphs completely lack this kind of how-toknowledge. This paper presents a method for automatically constructing a formal knowledgebase on tasks and task-solving steps; by tapping the contents of online communities such asWikiHow. We employ Open-IE techniques to extract noisy candidates for tasks; steps and therequired tools and other items. For cleaning and properly organizing this data; we deviseembedding-based clustering techniques. The resulting knowledge base; HowToKB;includes a hierarchical taxonomy of disambiguated tasks; temporal orders of sub-tasks; andattributes for involved items. A comprehensive evaluation of HowToKB shows high …,Proceedings of the 26th International Conference on World Wide Web,2017,2
WebChild 2.0: fine-grained commonsense knowledge distillation,Niket Tandon; Gerard de Melo; Gerhard Weikum,Abstract Despite important progress in the area of intelligent systems; most such systems stilllack commonsense knowledge that appears crucial for enabling smarter; more human-likedecisions. In this paper; we present a system based on a series of algorithms to distill fine-grained disambiguated commonsense knowledge from massive amounts of text. OurWebChild 2.0 knowledge base is one of the largest commonsense knowledge basesavailable; describing over 2 million disambiguated concepts and activities; connected byover 18 million assertions.,Proceedings of ACL 2017; System Demonstrations,2017,2
Instant Espresso: Interactive Analysis of Relationships in Knowledge Graphs,Stephan Seufert; Patrick Ernst; Srikanta J Bedathur; Sarath Kumar Kondreddi; Klaus Berberich; Gerhard Weikum,Abstract We demonstrate InstantEspresso; a system to explain the relationship between twosets of entities in knowledge graphs. Instant-Espresso answers questions of the form. WhichEuropean politicians are related to politicians in the United States; and how? or How canone summarize the relationship between China and countries from the Middle East? Eachquestion is specified by two sets of query entities. These sets (eg European politicians orUnited States politicians) can be determined by an initial graph query over a knowledgegraph capturing relationships between real-world entities. Instant-Espresso analyzes the(indirect) relationships that connect entities from both sets and provides a user-friendlyexplanation of the answer in the form of concise subgraphs. These so-called relatednesscores correspond to important event complexes involving entities from the two sets. Our …,Proceedings of the 25th International Conference Companion on World Wide Web,2016,2
Data; Responsibly (Dagstuhl Seminar 16291),Serge Abiteboul; Gerome Miklau; Julia Stoyanovich; Gerhard Weikum,Abstract Big data technology promises to improve people's lives; accelerate scientificdiscovery and innovation; and bring about positive societal change. Yet; if not usedresponsibly; large-scale data analysis and data-driven algorithmic decision-making canincrease economic inequality; affirm systemic bias; and even destabilize global markets.While the potential benefits of data analysis techniques are well accepted; the importance ofusing them responsibly-that is; in accordance with ethical and moral norms; and with legaland policy considerations-is not yet part of the mainstream research agenda in computerscience. Dagstuhl Seminar" Data; Responsibly" brought together academic and industryresearchers from several areas of computer science; including a broad representation ofdata management; but also data mining; security/privacy; and computer networks; as well …,Dagstuhl Reports,2016,2
Semantic type classification of common words in biomedical noun phrases,Amy Siu; Gerhard Weikum,Abstract Complex noun phrases are pervasive in biomedical texts; but are largelyunderexplored in entity discovery and information extraction. Such expressions often containa mix of highly specific names (diseases; drugs; etc.) and common words such as“condition”;“degree”;“process”; etc. These words can have different semantic typesdepending on their context in noun phrases. In this paper; we address the task of classifyingthese common words onto fine-grained semantic types: for instance;“condition” can be typedas “symptom and finding” or “configuration and setting”. For information extraction tasks; it iscrucial to consider common nouns only when they really carry biomedical meaning; hencethe classifier must also detect the negative case when nouns are merely used in a generic;uninformative sense. Our solution harnesses a small number of labeled seeds and …,Proceedings of BioNLP 15,2015,2
On the SPOT: Question Answering over Temporally Enhanced Structured Data,Mohamed Yahya; Klaus Berberich; Maya Ramanath; Gerhard Weikum,ABSTRACT Natural-language question answering is a convenient way for humans todiscover relevant information in structured Web data such as knowledge bases or LinkedOpen Data sources. This paper focuses on data with a temporal dimension; and discussesthe problem of mapping natural-language questions into extended SPARQL queries overRDF-structured data. We specifically address the issue of disambiguating temporal phrasesin the question into temporal entities like dates and named events; and temporal predicates.For the situation where the data has only partial coverage of the time dimension but isaugmented with textual descriptions of entities and facts; we also discuss how to generatequeries that combine structured search with keyword conditions.,Proceedings of Workshop on Time-aware Information Access; TAIA2013. Dublin; Ireland,2013,2
High-Performance Reachability Query Processing under Index Size Restrictions,Stephan Seufert; Avishek Anand; Srikanta Bedathur; Gerhard Weikum,Abstract: In this paper; we propose a scalable and highly efficient index structure for thereachability problem over graphs. We build on the well-known node interval labelingscheme where the set of vertices reachable from a particular node is compactly encoded asa collection of node identifier ranges. We impose an explicit bound on the size of the indexand flexibly assign approximate reachability ranges to nodes of the graph such that thenumber of index probes to answer a query is minimized. The resulting tunable indexstructure generates a better range labeling if the space budget is increased; thus providing adirect control over the trade off between index size and the query processing performance.By using a fast recursive querying method in conjunction with our index structure; we showthat in practice; reachability queries can be answered in the order of microseconds on an …,arXiv preprint arXiv:1211.3375,2012,2
Predicting the Evolution of Taxonomy Restructuring in Collective Web Catalogues.,Natalia Prytkova; Marc Spaniol; Gerhard Weikum,ABSTRACT Collectively maintained Web catalogues organize links to interesting Web sitesinto topic hierarchies; based on community input and editorial decisions. These taxonomicsystems reflect the interests and diversity of ongoing societal discourses. Catalogues evolveby adding new topics; splitting topics; and other restructuring; in order to capture newlyemerging concepts of long-lasting interest. In this paper; we investigate these changes intaxonomies and develop models for predicting such structural changes. Our approachidentifies newly emerging latent concepts by analyzing news articles (or social media); bymeans of a temporal term relatedness graph. We predict the addition of new topics to thecatalogue based on statistical measures associated with the identified latent concepts.Experiments with a large news archive corpus demonstrate the high precision of our …,WebDB,2012,2
Graph-based methods for large-scale multilingual knowledge integration,Gerard de Melo,Da ein großer Teil unseres Wissens in textueller Form vorliegt; sind Informationssysteme inzunehmendem Maße auf Wissen über Wörter und den von ihnen repräsentierten Entitätenangewiesen. Gegenstand dieser Arbeit sind neue Methoden zur automatischen Erstellunggroßer multilingualer Wissensbanken; welche semantische Beziehungen zwischen Wörternbzw. Namen und Konzepten bzw. Entitäten formal erfassen. In drei Hauptbeiträgen werdenjeweils graphtheoretische bzw. statistische Verfahren zur Verknüpfung von Indizien ausmehreren Wissensquellen vorgestellt. Bei der lexikalischen Integration werden statistischeModelle zur Disambiguierung gebildet. Die Entitäten-Integration fasst semantischeEinheiten unter Auflösung von Konflikten zwischen Äquivalenz-undVerschiedenheitsinformationen zusammen. Diese werden schließlich bei der …,*,2010,2
Recent Advances in Parallel Virtual Machine and Message Passing Interface: 16th European PVM/MPI Users' Group Meeting; Espoo; Finland; September 7-10; 200...,Matti Ropo; Jan Westerholm; Jack Dongarra,This book constitutes the refereed proceedings of the 16th European PVM/MPI Users' GroupMeeting on Recent Advances in Parallel Virtual Machine and Message Passing Interface;EuroPVM/MPI 2009; held in Espoo; Finland; September 7-10; 2009. The 27 paperspresented were carefully reviewed and selected from 48 submissions. The volume alsoincludes 6 invited talks; one tutorial; 5 poster abstracts and 4 papers from the specialsession on current trends in numerical simulation for parallel engineering environments. Themain topics of the meeting were Message Passing Interface (MPI) performance issues invery large systems; MPI program verification and MPI on multi-core architectures.,*,2009,2
Implementation and Application of Automata: 14th International Conference; CIAA 2009; Sydney; Australia; July 14-17; 2009; Proceedings,Sebastian Maneth,The 14th International Conference on Implementation and Application of Automata (CIAA2009) was held in NICTA's Neville Roach Laboratory at the University of New South Wales;Sydney; Australia during July 14–17; 2009. This volume of Lecture Notes in ComputerScience contains the papers that were presented at CIAA 2009; as well as abstracts of theposters and short papers that were presented at the conference. The volume also includespapers or extended abstracts of the three invited talks presented by Gonzalo Navarro onImplementation and Application of Automata in String Processing; by Christoph Koch onApplications of Automata in XML Processing; and by Helmut Seidl on Program AnalysisThrough Finite Tree Automata. The 23 regular papers were selected from 42 submissionscovering various ﬁelds in the application; implementation; and theory of automata and …,*,2009,2
Developments in Language Theory: 13th International Conference; DLT 2009; Stuttgart; Germany; June 30--July 3; 2009; Proceedings,Volker Diekert; Dirk Nowotka,Since 1993 the conference Developments in Language Theory (DLT) has been held inEurope every odd year and; since 2002; outside Europe every even year. The 13thconference in this series was DLT 2009. It took place in Stuttgart fromJune30to July3.PreviousmeetingsoccurredinTurku (1993); Magdeburg (1995); Thessaloniki (1997); Aachen(1999); Vienna (2001); Kyoto (2002); Szeged (2003); Auckland (2004); Palermo (2005);Santa Barbara (2006); Turku (2007); and Kyoto (2008). The DLT conference has developedinto the main forum for language theory and related topics. This has also been re? ected inthe high quality of the 70 s-missions received in 2009. Most submissions were reviewed byfour Programme Committeemembersandtheirsub-referees.TheProgrammeCommitteeselected the best 35 papers for presentation during the …,*,2009,2
Distributed Applications and Interoperable Systems: 9th IFIP WG 6.1 International Conference; DAIS 2009; Lisbon; Portugal; June 9-12; 2009; Proceedings,Twittie Senivongse; Rui Oliveira,This book constitutes the refereed proceedings of the 9th IFIP WG 6.1 InternationalConference on Distributed Applications and Interoperable Systems; DAIS 2009; held inLisbon; Portugal; in June 2009. The DAIS conference was held as part of the federatedevent on Distributed Computing Techniques (DisCoTec); together with the 11th InternationalConference on Coordination Models and Languages (Coordination 2009) and the IFIP WG6.1 International Conference on Formal Techniques for Distributed Systems(FMOODS/FORTE 2009). The 12 revised full papers presented were carefully reviewed andselected from 32 submissions. The papers address service orientation; quality of service andservice contract; business processes; Web services; service components; algorithms andprotocols supporting dependability; fault tolerance; data replication; group communication …,*,2009,2
Towards Digital Optical Networks: COST Action 291 Final Report,Ioannis Tomkos; Maria Spyropoulou; Karin Ennser; Martin Köhn; Branko Mikac,COST–the acronym for European COoperation in Science and Technology–is the oldestand widest European intergovernmental network for cooperation in-search. Established bythe Ministerial Conference in November 1971; COST is presently used by the scientificcommunities of 35 European countries to coop-ate in common research projects supportedby national funds. The funds provided by COST–less than 1% of the total value of theprojects–support the COST cooperation networks (COST Actions) through which; with€ 30million per year; more than 30;000 European scientists are involved in-search having a totalvalue which exceeds€ 2 billion per year. This is the financial worth of the European addedvalue which COST achieves. A “bottom up approach”(the initiative of launching a COSTAction comes from the European scientists themselves);“à la carte participation”(only …,*,2009,2
Future Internet-FIS 2008: First Future Internet Symposium Vienna; Austria; September 28-30; 2008 Revised Selected Papers,John Domingue; Paolo Traverso,The First Future Internet Symposium was held during September 28–30; 2008 in Vienna;Austria. FIS 2008 provided a forum for leading researchersand pr-titioners to meet anddiscuss the wide-ranging scienti? c and technical issues related to the design of a newInternet. The sentiment shared in Vienna was that we are at the beginning of something veryexciting and challenging and that FIS 2008 has played a role in forming a community toaddress this. With overa billionusers; today's Internet is arguablythe most successful-manartifact ever created. The Internet's physical infrastructure; software; and content now play anintegralpart in the lives of everyoneon the planet; whether they interact with it directly or not.Now nearing its? fth decade; the Int-net has shown remarkable resilience and? exibility inthe face of ever-increasing numbers of users; data volume; and changing usage patterns …,*,2009,2
Algorithms and Models for the Web-Graph: 6th International Workshop; WAW 2009 Barcelona; Spain; February 12-13; 2009; Proceedings,Konstantin Avratchenkov; Debora Donato; Nelly Litvak,This book constitutes the refereed proceedings of the 6th International Workshop onAlgorithms and Models for the Web-Graph; WAW 2009; held in Barcelona; Spain; inFebruary 2009-co-located with WSDM 2009; the Second ACM International Conference onWeb Search and Data Mining. The 14 revised full papers presented were carefully reviewedand selected from numerous submissions for inclusion in the book. The papers address awide variety of topics related to the study of the Web-graph such as theoretical and empiricalanalysis of the Web graph and Web 2.0 graphs; random walks on the Web and Web 2.0graphs and their applications; and design and performance evaluation of the algorithms forsocial networks. The workshop papers have been naturally clustered in three topicalsections on graph models for complex networks; pagerank and Web graph; and social …,*,2009,2
High Performance Computing-HiPC 2008: 15th International Conference; Bangalore; India; December 17-20; 2008; Proceedings,Ponnuswamy Sadayappan; Manish Parashar; Ramamurthy Badrinath; Viktor K Prasanna,This book constitutes the refereed proceedings of the 15th International Conference on High-Performance Computing; HiPC 2008; held in Bangalore; India; in December 2008. The 46revised full papers presented together with the abstracts of 5 keynote talks were carefullyreviewed and selected from 317 submissions. The papers are organized in topical sectionson applications performance optimizazion; parallel algorithms and applications; schedulingand resource management; sensor networks; energy-aware computing; distributedalgorithms; communication networks as well as architecture.,*,2008,2
Formal Modeling and Analysis of Timed Systems: 6th International Conference; FORMATS 2008; Saint Malo; France; September 15-17; 2008; Proceedings,Franck Cassez; Claude Jard,This book constitutes the refereed proceedings of the 6th International Conference onFormal Modeling and Analysis of Timed Systems; FORMATS 2008; held in Saint Malo;France; September 2008. The 17 revised full papers presented together with 3 invited talkswere carefully reviewed and selected from 37 submissions. The papers are organized intopical sections on extensions of timed automata and semantics; timed games and logic;case studies; model-checking of probabilistic systems; verification and test; timed petri nets.,*,2008,2
Social recommendations at work,Tom Crecelius; Mouna Kacimi; Sebastian Michel; Thomas Neumann; Josiane X Parreira; Ralf Schenkel; Gerhard Weikum,Abstract Online communities have become popular for publishing and searching content;and also for connecting to other users. User-generated content includes; for example;personal blogs; bookmarks; and digital photos. Items can be annotated and rated bydifferent users; and users can connect to others that are usually friends and/or sharecommon interests.,Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,2008,2
Medical Imaging and Augmented Reality: 4th International Workshop Tokyo; Japan; August 1-2; 2008; Proceedings,Takeyoshi Dohi; Ichiro Sakuma; Hongen Liao,The 4th International Workshop on Medical Imaging and Augmented Reality; MIAR 2008;was held at the University of Tokyo; Tokyo; Japan during August 1–2; 2008. The goal ofMIAR 2008 was to bring together researchersin medical imaging and intervention to presentstate-of-the-art developments in this ever-growing research area. Rapid technical advancesin medical imaging; including its gr-ing application in drug/gene therapy andinvasive/interventional procedures; have attracted signi? cant interest in the closeintegration of research in the life sciences; medicine; physical sciences; and engineering.Current research is also motivated by the fact that medical imaging is moving increasinglyfrom a p-marily diagnostic modality towards a therapeutic and interventional aid; driven bythe streamlining of diagnostic and therapeutic processes for human diseases by means of …,*,2008,2
VizSEC 2007: Proceedings of the Workshop on Visualization for Computer Security,John R Goodall; Gregory Conti; Kwan-Liu Ma,Networked computers are ubiquitous; and are subject to attack; misuse; and abuse. Onemethod to counteracting this cyber threat is to provide security analysts with better tools todiscover patterns; detect anomalies; identify correlations; and communicate their findings.Visualization for computer security (VizSec) researchers and developers are doing just that.VizSec is about putting robust information visualization tools into the hands of humananalysts to take advantage of the power of the human perceptual and cognitive processes insolving computer security problems. This volume collects the papers presented at the 4thInternational Workshop on Computer Security-VizSec 2007.,*,2008,2
Detection of Intrusions and Malware; and Vulnerability Assessment.,D Zamboni,*,Proceedings of 5th International Conference; DIMVA.–Springer International Publishing,2008,2
Rewriting Techniques and Applications,Andrei Voronkov,This volume contains the papers presented at RTA 2008: 19th International Conference onRewriting Techniques and Applications held July 15–17th in Hagenberg; Austria andorganised by the Research Institute on Symbolic Computation. There were 57 submissions.Each submission was reviewed by at least four Programme Committee members. Thecommittee decided to accept 30 papers. The submission and Programme Committee workwas organised through the EasyChair system. I thank the Programme Committee membersfor their very efficient work. My special thanks to Temur Kutsia; Aart Middeldorp; RobertNieuwenhuis and Maribel Fernandez for their help and advice on many aspects of the RTAorganisation and traditions. I would also like to thank RTA General Chair Bruno Buchberger.,*,2008,2
Recent Advances in Intrusion Detection,Richard Lippmann; Andrew Clark,On behalf of the Program Committee; it is our pleasure to present the proceedings of the10th Symposium on Recent Advances in Intrusion Detection (RAID 2007); which took placein Queensland; Australia; September 5–7; 2007. As in every year since 1998; thesymposium brought together leading researchers and practitioners from academia;government; and industry to discuss intrusion detection research and practice. This year; theRAID Program Committee received 101 paper submissions from all over the world. Allsubmissions were carefully reviewed by at least three members of the Program Committeeand judged on the basis of scientific novelty; importance to the field; and technical quality.The final selection took place at the Program Committee meeting held in Oakland; USA; May22–23; 2007. Sixteen full papers and one short paper were selected for presentation and …,*,2008,2
Design alternatives for large-scale web search: Alexander was great; Aeneas a pioneer and Anakin has the force,S Michel; M Bender; P Triantafillou; G Weikum,*,*,2007,2
TopX–AdHoc and Feedback Tasks,Martin Theobald; Andreas Broschart; Ralf Schenkel; Silvana Solomon; Gerhard Weikum,TopX–AdHoc and Feedback Tasks Martin Theobald; Andreas Broschart; Ralf Schenkel; SilvanaSolomon; and Gerhard Weikum Max-Planck-Institut für Informatik Saarbrücken; Germanyhttp://www. mpi-inf. mpg. de/departments/d5/ {mtb; abrosch; schenkel; solomon; weikum}@mpi-inf. mpg. de Abstract. This paper describes the setup and results of our contribu- tions tothe INEX 2006 AdHoc and Feedback tasks. 1 System Overview TopX [10; 11] aims to bridgethe fields of database systems (DB) and informa- tion retrieval (IR). From a DB viewpoint; it providesan efficient algorithmic basis for top-k query processing over multidimensional datasets; rangingfrom structured data such as product catalogs (eg; bookstores; real estate; movies; etc.) to unstructuredtext documents (with keywords or stemmed terms defin- ing the feature space) and semistructuredXML data in between. From an IR viewpoint; TopX provides ranked retrieval based on a …,Center for Computer Science http://inex. is. informatik. uni-duisburg. de/2006,2006,2
事务信息系统: 并发控制与恢复的理论; 算法与实践,G WEIKUM; G VOSSEN,*,*,2006,2
IO-Top-k at TREC 2006: Terabyte Track,Holger Bast; Debapriyo Majumdar; Ralf Schenkel; Martin Theobald; Gerhard Weikum,Abstract This paper describes the setup and results of our contribution to the TREC 2006Terabyte Track. Our implementation was based on the algorithms proposed in [1]“IO-Top-k:Index-Access Optimized Top-K Query Processing; VLDB'06”; with a main focus on theefficiency track. 1.,Proceedings of the 15th Text REtrieval Conference (TREC 2006,2006,2
Evaluation and Comparison of the Service Architecture; P2P; and Grid Approaches for DLs,Maristella Agosti; Ludger Bischofs; Leonardo Candela; Donatella Castelli; Nicola Ferro; Wilhelm Hasselbring; N Moumoutzis; Heiko Schuldt; Gerhard Weikum; Manfred Wurz; Pavel Zezula,Abstract Second generation DLs have new requirements. Firstly; these requirements includethe way to make data and specialized DL applications available to a large set of users bymeans of well-defined services. Secondly; the interaction between independent data/serviceproviders (peers) within a network needs to be addressed. Finally; the computing andstorage resources that are available in large networks have to be made available in anefficient and effective way by means of grid technology. We provide a survey in order tohighlight whether and to what extent i.) service-oriented architectures; peer-to-peerinfrastructures; and grid infrastructures can contribute to satisfy the requirements of thesenew DLs. This survey contains a detailed summary of state-of-the-art in the threearchitectural paradigms and lists current approaches that aim to exploit these …,*,2006,2
06121 report: Break out session on guaranteed execution,Calton Pu; Jim Johnson; Rogério de Lemos; Andreas Reuter; David Taylor; Irfan Zakiuddin,Abstract The break out session discussed guaranteed properties during program execution.Using a workflow example application; we discussed several research topics that form partof the guaranteed properties; including declarative specifications; generation of workflowprogram; generation of invariant guards; automated failure analysis; automated repair; andautomated reconfiguration of workflow.,Dagstuhl Seminar Proceedings,2006,2
Automated retraining methods for document classification and their parameter tuning,Stefan Siersdorfer; Gerhard Weikum,Abstract This paper addresses the problem of semi-supervised classification on documentcollections using retraining (also called self-training). A possible application is focused Webcrawling which may start with very few; manually selected; training documents but can beenhanced by automatically adding initially unlabeled; positively classified Web pages forretraining. Such an approach is by itself not robust and faces tuning problems regardingparameters like the number of selected documents; the number of retraining iterations; andthe ratio of positive and negative classified samples used for retraining. The paper developsmethods for automatically tuning these parameters; based on predicting the leave-one-outerror for a re-trained classifier and avoiding that the classifier is diluted by selecting toomany or weak documents for retraining. Our experiments with three different datasets …,International Conference on Web Information Systems Engineering,2005,2
MINERVA: Collaborative P2P web search,M Bender; S Michel; P Triantafillou; G Weikum; C Zimmer,*,*,2005,2
Information Retrieval by Dimension Reduction-A Comparative Study,Josiane Xavier Parreira,In this work wc present a study of different techniques for semantic indexing by dimensionreduction; with special emphasis on the LSI technique. Dimension reduction is important inthe Information Retrieval (IR) context to enable fast retrieval and elimination of noisy data.LSI attempts to improve IR quality by deriving a latent semantic space with lowerdimensionality; based on the co-occurrence of the terms in the documents from thedocument collection. It is a heuristic method and although experiments show that the LSItechnique often improves the retrieval performance; there are deficiencies regardingmathematical models and rigorous theorems. Several variants of the LSI technique havebeen proposed; which differ in the function used for the mapping to the lower-dimensionalspace.,*,2003,2
Recent Advances in Parallel Virtual Machine and Message Passing Interface: 9th European PVM/MPI User's Group Meeting Linz; Austria; September 29-October 2;...,Dieter Kranzlmüller,This book constitutes the refereed proceedings of the 9th European PVM/MPI Users' GroupMeeting held in Linz; Austria in September/October 2002. The 50 revised full paperspresented together with abstracts of 11 invited contributions were carefully reviewed andselected. The papers are organized in topical sections on Corss Grid; Par Sim; applicationusing MPI and PVM; parallel algorithms using message passing; programming tools for MPIand PVM; implementations of MPI and PVM; extensions of MPI and PVM; and performanceanalysis and optimization.,*,2002,2
The Web in Ten Years: Challenges and Opportunities for Database Research.,Gerhard Weikum,Abstract In order to evolve into a dependable and ubiquitous information infrastructure; theWorld Wide Web needs comprehensive quality; performance; and availability guarantees forall kinds of E-services including search engines. To improve the search result quality ofsearch engines and to exploit the Web's potential as a world-wide knowledge base;intensive research efforts are required that center around the role of the XML documentstandard. By itself XML is merely syntax; but its momentum for providing semanticallymeaningful annotations and; to some extent; standardizing domain-specific ontologiesprovides opportunities toward the widely envisioned” Semantic Web”. At the same time; theongoing information explosion in terms of volume and diversity require support for relevanceranking and automatic classification in addition to ontology-based searching. This paper …,SEBD,2002,2
Middleware-Antwortzeitgarantien für e-Services,Achim Kraiß; Frank Schön; Gerhard Weikum; Uwe Deppisch,Zusammenfassung Message-orientierte Middleware (MoM) ist eine zunehmend eingesetzteBasistechnologie; um die Backend-Serversysteme eines Unternehmens in eine Vielzahlmoderner e-Services zu integrieren. Bestandteil der Quality-of-Service (QoS) dieser e-Ser-vices; zB im Online-und mobile Banking; muss die Gewährleistung akzeptableranwenderklassenspezifischer Antwortzeiten für e-Service-Transaktionen sein. Aus dieserForderung ergeben sich besondere Anforderungen an die Antwortzeiten von Aufträgen anden integrierten Backend-Serversystemen. Ein Antwortzeitziel könnte beispielsweise sein;dass bestimmte e-Service-Transaktionen eine vorgegebene mittlere Antwortzeit amBackend-Server nicht überschreiten dürfen oder dass 95 Prozent der Transaktionen einevorgegebene maximale Antwortzeit nicht überschreiten dürfen. In der Praxis bieten MoM …,*,2001,2
Automatische Übersetzung von Geschäftsprozessmodellen in ausführbare Workflows,David Christensen; Achim Kraiss; Anja Syri; Gerhard Weikum,Zusammenfassung Die Abkehr vom tayloiistischen Modell und die Zuwendung zurganzheitlichen Betrachtung von Geschäftsprozessen setzt sich zunehmend durch;unabhängig davon; ob die Prozesse innerhalb eines Unternehmens oder interaktiv mitKunden (B2C) oder anderen Unternehmen (B2B) ablaufen. Im Rahmen der Ist-Analyse der„praktizierten “Geschäftsprozesse erfolgt in der Regel eine Modellierung mit Hilfesogenannter Geschäfisprozess (GP)-Modellierungswerkzeuge Wie zB ARIS der Firma IDSScheer AG [IDSOO]. Ein in einem solchen Werkzeug beschriebenes GP-Modell dient derKommunikation zwischen den IT-Fachleuten; den Beratern und den eigentlichen Experten;den am Geschäftsprozess beteiligten Personen; und bietet damit eine Grundlage für dieOptimierung der Geschäftsprozesse.,*,2001,2
Quality of Service Guarantees for Multimedia Digital Libraries and Beyond,Gerhard Weikum,Servers for multimedia digital libraries have to manage huge amounts of data and posechallenging performance requirements. Most notably; for smooth playback of video andaudio on client machines the server has to guarantee continuous data streams with just-in-time delivery of the underlying data fragments. This problem is referred to in the multimediainformation and networking community as the need for guaranteed quality of service; in shortQoS; where the details of such guarantees depend on the data and playback specifications(eg; video formats) 1; 4; 8; 9]. Servers aim to maximize their throughput in terms of theconcurrently sustained number of continuous data streams; but at the same time have toensure that each active stream meets the QoS demands of the clients. Therefore servers(and potentially also network components) need to employ an admission control that …,Data Engineering,1999,2
On the ubiquity of information services and the absence of guaranteed service quality,Gerhard Weikum,We are witnessing the proliferation of the global information society with a sheer explosionof information services on a world-spanning network. This opens up unprecedentedopportunities for information discovery; virtual enterprises and cyberspace-basedcollaboration; and also more mundane things such as electronic commerce [1; 2]. Usingsuch services is; however; often a frustrating experience. Many information services(including Web search engines) deliver poor results-inconsistent; arbitrarily inaccurate; orcompletely irrelevant data-to their clients; break easily and exhibit long outages; or performso poorly that unacceptable response times ultimately render the offered service useless.The bottom line is that the quality of services is highly unpredictable; and service qualityguarantees are usually absent in today's fast-moving IT world. Contrast this situation with …,International Conference on Extending Database Technology,1998,2
Databases and workflow management: What is it all about?(panel),Andreas Reuter; Stefano Ceri; Jim Gray; Betty Salzberg; Gerhard Weikum,*,Proceedings of the 21th International Conference on Very Large Data Bases,1995,2
I/O-Parallelitaet und Fehlertoleranz in Disk-Arrays. Teil 2: Fehlertoleranz,Gerhard Weikum; Peter Zabback,*,Informatik Spektrum,1993,2
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,Sandy Behrens,We know them as shadow systems; workaround systems; and even feral systems. Operatingat the fringes of an organization; they covertly replicate the data and functionality of formallysanctioned systems. Because of their duplicative acts; they are often said to have negativeconsequences for their hosts: undermining official systems; 12 sapping valuable resourcesand corrupting organizational data and processes. 7 But not all shadow systems live up totheir bad reputations. Some shadow systems offer an effective and efficient way for users tocope with the deficiencies of formal systems. 1; 4 This article reports on an ethnographicinvestigation of a shadow system used in a higher educational institution; CentralQueensland University (CQU); whose findings challenge conventional views of theseorganizational outlaws. The study explored how the system was built; implemented; and …,Communications of the ACM,*,2
Multi-Cultural Interlinking of Web Taxonomies with ACROSS,Natalia Boldyrev; Marc Spaniol; Gerhard Weikum,Abstract The Web hosts a huge variety of multi-cultural taxonomies. They encompassproduct catalogs of e-commerce; general-purpose knowledge bases and numerous domain-specific category systems. The enormous heterogeneity of those sources is a challengingaspect when multiple taxonomies have to be interlinked. In this paper we introduceACROSS system to support the alignment of independently created Web taxonomies. Formapping categories across different taxonomies; ACROSS harnesses instance-levelfeatures as well as distant supervision from an intermediate source like multiple Wikipediaeditions. ACROSS includes a reasoning step; which is based on combinatorial optimization.In order to reduce the run time of the reasoning procedure without sacrificing the quality; westudy two models of user involvement. Our experiments with heterogeneous taxonomies …,The Journal of Web Science,2017,1
Query-driven on-the-fly knowledge base construction,Dat Ba Nguyen; Abdalghani Abujabal; Nam Khanh Tran; Martin Theobald; Gerhard Weikum,Abstract Today's openly available knowledge bases; such as DBpedia; Yago; Wikidata orFreebase; capture billions of facts about the world's entities. However; even the largestamong these (i) are still limited in up-to-date coverage of what happens in the real world;and (ii) miss out on many relevant predicates that precisely capture the wide variety ofrelationships among entities. To overcome both of these limitations; we propose a novelapproach to build on-the-fly knowledge bases in a query-driven manner. Our system; calledQKBfly; supports analysts and journalists as well as question answering on emerging topics;by dynamically acquiring relevant facts as timely and comprehensively as possible. QKBflyis based on a semantic-graph representation of sentences; by which we perform three key IEtasks; namely named-entity disambiguation; co-reference resolution and relation …,Proceedings of the VLDB Endowment,2017,1
Privacy through Solidarity: A User-Utility-Preserving Framework to Counter Profiling,Asia J Biega; Rishiraj Saha Roy; Gerhard Weikum,Abstract Online service providers gather vast amounts of data to build user profiles. Suchprofiles improve service quality through personalization; but may also intrude on userprivacy and incur discrimination risks. In this work; we propose a framework which leveragessolidarity in a large community to scramble user interaction histories. While this is beneficialfor anti-profiling; the potential downside is that individual user utility; in terms of the quality ofsearch results or recommendations; may severely degrade. To reconcile privacy and userutility and control their trade-off; we develop quantitative models for these dimensions andeffective strategies for assigning user interactions to Mediator Accounts. We demonstrate theviability of our framework by experiments in two different application areas (search andrecommender systems); using two large datasets.,Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,2017,1
Exploring Latent Semantic Factors to Find Useful Product Reviews,Subhabrata Mukherjee; Kashyap Popat; Gerhard Weikum,Abstract Online reviews provided by consumers are a valuable asset for e-Commerceplatforms; influencing potential consumers in making purchasing decisions. However; thesereviews are of varying quality; with the useful ones buried deep within a heap of non-informative reviews. In this work; we attempt to automatically identify review quality in termsof its helpfulness to the end consumers. In contrast to previous works in this domainexploiting a variety of syntactic and community-level features; we delve deep into thesemantics of reviews as to what makes them useful; providing interpretable explanation forthe same. We identify a set of consistency and semantic factors; all from the text; ratings; andtimestamps of user-generated reviews; making our approach generalizable across allcommunities and domains. We explore review semantics in terms of several latent factors …,*,2017,1
Fides: Towards a platform for responsible data science,Julia Stoyanovich; Bill Howe; Serge Abiteboul; Gerome Miklau; Arnaud Sahuguet; Gerhard Weikum,Issues of responsible data analysis and use are coming to the forefront of the discourse indata science research and practice; with most significant efforts to date on the part of thedata mining; machine learning; and security and privacy communities. In these fields; theresearch has been focused on analyzing the fairness; accountability and transparency (FAT)properties of specific algorithms and their outputs. Although these issues are most apparentin the social sciences where fairness is interpreted in terms of the distribution of resourcesacross protected groups; management of bias in source data affects a variety of fields.Consider climate change studies that require representative data from geographicallydiverse regions; or supply chain analyses that require data that represents the diversity ofproducts and customers. Any domain that involves sparse or sampled data has exposure …,SSDBM'17-29th International Conference on Scientific and Statistical Database Management,2017,1
What computers should know; shouldn't know; and shouldn't believe,Gerhard Weikum,Abstract Automatically constructed knowledge bases (KB's) are a powerful asset for search;analytics; recommendations and data integration; with intensive use at big industrial stake-holders. Examples are the knowledge graphs for search engines (eg; Google; Bing; Baidu)and social networks (eg; Facebook); as well as domain-specific KB's (eg; Bloomberg;Walmart). These achievements are rooted in academic research and community projects.The largest general-purpose KB's with publicly accessible contents are BabelNet; DBpedia;Wikidata; and Yago. They contain millions of entities; organized in hundreds to hundredthousands of semantic classes; and billions of relational facts on entities. These and otherknowledge and data resources are interlinked at the entity level; forming the Web of LinkedOpen Data.,Proceedings of the 26th International Conference on World Wide Web Companion,2017,1
SESAME: European Statistics Explored via Semantic Alignment onto Wikipedia,Natalia Boldyrev; Marc Spaniol; Jannik Strötgen; Gerhard Weikum,Abstract Authorities such as the European Commission have recognized the need to offer aunified access to the data gathered by a wide variety of providers; such as the EuropeanStatistical Organization (Eurostat) or the European Environment Agency. Its EU Open DataPortal serves as a gateway to numerical data; statistical reports; and visualization tools.While making the data available to the users from all member states and concentratingefforts on bridging the language gap; the portal still focuses on a primarily statisticalperspective. That is; numerical data are explained with general terms; only. However; therelated events; people; or organizations``causing''or being``affected''by the statisticalobservation remain concealed to the user. In order to make statistical data betterunderstandable; we present the SESAME system (Statistics Explored via Semantic …,Proceedings of the 26th International Conference on World Wide Web Companion,2017,1
J-REED: Joint Relation Extraction and Entity Disambiguation,Dat Ba Nguyen; Martin Theobald; Gerhard Weikum,ABSTRACT Information extraction (IE) from text sources can either be performed as Model-based IE (ie; by using a pre-specified domain of target entities and relations) or as Open IE(ie; with no particular assumptions about the target domain). While Model-based IE haslimited coverage; Open IE merely yields triples of surface phrases which are usually notdisambiguated into a canonical set of entities and relations. This paper presents J-REED: ajoint approach for entity disambiguation and relation extraction that is based on probabilisticgraphical models. J-REED merges ideas from both Model-based and Open IE by mappingsurface names to a background knowledge base; and by making surface relations as crispas possible.,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management; CIKM 2017; Singapore; November 06-10; 2017,2017,1
ESPRESSO: Explaining Relationships between Entity Sets,Stephan Seufert; Klaus Berberich; Srikanta J Bedathur; Sarath Kumar Kondreddi; Patrick Ernst; Gerhard Weikum,Abstract Analyzing and explaining relationships between entities in a knowledge graph is afundamental problem with many applications. Prior work has been limited to extracting themost informative subgraph connecting two entities of interest. This paper extends andgeneralizes the state of the art by considering the relationships between two sets of entitiesgiven at query time. Our method; coined ESPRESSO; explains the connection betweenthese sets in terms of a small number of relatedness cores: dense sub-graphs that havestrong relations with both query sets. The intuition for this model is that the cores correspondto key events in which entities from both sets play a major role. For example; to explain therelationships between US politicians and European politicians; our method identifies eventslike the PRISM scandal and the Syrian Civil War as relatedness cores. Computing cores …,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,2016,1
Editorial,Markus Krötzsch; Gerhard Weikum,Autor: Krötzsch; Markus et al.; Genre: Zeitschriftenartikel;Im Druck veröffentlicht: 2016; Titel: Editorial.,Journal of Web Semantics,2016,1
Know2Look: Commonsense Knowledge for Visual Search,Sreyasi Nag Chowdhury; Niket Tandon; Gerhard Weikum,Abstract With the rise in popularity of social media; images accompanied by contextual textform a huge section of the web. However; search and retrieval of documents are still largelydependent on solely textual cues. Although visual cues have started to gain focus; theimperfection in object/scene detection do not lead to significantly improved results. Wehypothesize that the use of background commonsense knowledge on query terms cansignificantly aid in retrieval of documents with associated images. To this end we deploythree different modalities-text; visual cues; and commonsense knowledge pertaining to thequery-as a recipe for efficient search and retrieval.,Proceedings of the 5th Workshop on Automated Knowledge Base Construction,2016,1
Senti-LSSVM: Sentiment-oriented multi-relation extraction with latent structural SVM,Lizhen Qu; Yi Zhang; Rui Wang; Lili Jiang; Rainer Gemulla; Gerhard Weikum,Abstract Extracting instances of sentiment-oriented relations from user-generated webdocuments is important for online marketing analysis. Unlike previous work; we formulatethis extraction task as a structured prediction problem and design the correspondinginference as an integer linear program. Our latent structural SVM based model can learnfrom training corpora that do not contain explicit annotations of sentiment-bearingexpressions; and it can simultaneously recognize instances of both binary (polarity) andternary (comparative) relations with regard to entity mentions of interest. The empiricalevaluation shows that our approach significantly outperforms state-of-the-art systems acrossdomains (cameras and movies) and across genres (reviews and forum posts). The goldstandard corpus that we built will also be a valuable resource for the community.,Transactions of the Association for Computational Linguistics,2014,1
Matrix factorization over max-times algebra for data mining,Sanjar Karaev; Pauli Miettinen; Gerhard Weikum,Abstract Decomposing a given matrix into two factor matrices is a frequently used techniquein data mining for uncovering underlying latent patterns in the data. Unlike in puremathematics; the emphasis is put on obtaining results that are interpretable; rather thannecessarily having a small reconstruction error. One approach to increase interpretability isto pose constraints on the factors. For example they might be restricted to the same type asthe original matrix. Among many different ways one can define matrix multiplication; thestandard and Boolean cases are the most thoroughly studied. In this work we introducematrix multiplication over max-times algebra; which is the set of nonnegative real numbersendowed with standard multiplication; but with addition being replaced by the maximizationoperation. The main objective of the thesis is to develop efficient factorization algorithms …,*,2013,1
Yali: a crowdsourcing plug-in for NERD,Yafang Wang; Lili Jiang; Johannes Hoffart; Gerhard Weikum,Abstract We demonstrate the YaLi browser plug-in which discovers named entities in Webpages and provides background knowledge about them. The plug-in is implemented withtwo purposes. From a user perspective; it enriches the browsing experience with entities;helping users with their information needs. From the research perspective; we aim toimprove the methods that are used for named entity recognition and disambiguation (NERD)by leveraging the plug-in as an implicit crowdsourcing platform. YaLi tracks the system'serrors and the users' corrections; and also gathers implicit training data for improving NERDaccuracy.,Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval,2013,1
HIGGINS: knowledge acquisition meets the crowds,Sarath Kumar Kondreddi; Peter Triantafillou; Gerhard Weikum,Abstract We present HIGGINS; a system for Knowledge Acquisition (KA); placing emphasison its architecture. The distinguishing characteristic and novelty of HIGGINS lies in itsblending of two engines: an automated Information Extraction (IE) engine; aided by semanticresources and statistics; and a game-based Human Computing (HC) engine. We focus onKA from web pages and text sources and; in particular; on deriving relationships betweenentities. As a running application we utilize movie narratives; from which we wish to deriverelationships among movie characters.,Proceedings of the 22nd International Conference on World Wide Web,2013,1
Knowledge discovery on incompatibility of medical concepts,Adam Grycner; Patrick Ernst; Amy Siu; Gerhard Weikum,Abstract This work proposes a method for automatically discovering incompatible medicalconcepts in text corpora. The approach is distantly supervised based on a seed set ofincompatible concept pairs like symptoms or conditions that rule each other out. Twoconcepts are considered incompatible if their definitions match a template; and contain anantonym pair derived from WordNet; VerbOcean; or a hand-crafted lexicon. Our methodcreates templates from dependency parse trees of definitional texts; using seed pairs. Thetemplates are applied to a text corpus; and the resulting candidate pairs are categorized andranked by statistical measures. Since experiments show that the results face semanticambiguity problems; we further cluster the results into different categories. We applied thisapproach to the concepts in Unified Medical Language System; Human Phenotype …,International Conference on Intelligent Text Processing and Computational Linguistics,2013,1
D-Hive: Data Bees Pollinating RDF; Text; and Time,Srikanta Bedathur; Klaus Berberich; Ioannis Patlakas; Peter Triantafillou; Gerhard Weikum,ABSTRACT Although the problem of integrating IR and DB solutions is considered “old”; theincreasing importance of big data analytics and its formidable demands for both enrichedfunctionality and scalable performance creates the need to revisit the problem itself and tosee possible solutions from a new perspective. Our goal is to develop a system that willmake large corpora aware of entities and relationships (ER); addressing the challenges insearching and analyzing ER patterns in web data and social media. We put forward D-Hive;a system facilitating analytics over RDF-style (SPO) triples augmented with text and(validity/transaction) time capable of addressing the functionality and scalabilityrequirements which current solutions cannot meet. We consider various alternatives for thedata modeling; storage; indexing; and query processing engines of D-Hive paying …,Sixth Biennial Conference on Innovative Data Systems Research,2013,1
Boolean Matrix Factorization with missing values,Prashant Yadava; Pauli Miettinen; Gerhard Weikum,Abstract Is it possible to meaningfully analyze the structure of a Boolean matrix for which99% data is missing? Real-life data sets usually contain a high percentage of missingvalues which hamper structure estimation from the data and the difficulty only increaseswhen the missing values dominate the known elements in the data set. There are good real-valued factorization methods for such scenarios; but there exist another class of data—Boolean data; which demand a different handling strategy than their real-valued counterpart.There are many application which find logical representation only via Boolean matrices;where real-valued factorization methods do not provide correct and intuitive solutions.Currently; there exists no method which can factorize a Boolean matrix containing apercentage of missing values usually associated with non-trivial real-world data set. In …,Universitat des Saarlandes Max-Planck-Institut fiir In formatik AG5,2012,1
Efficient entity disambiguation via similarity hashing,Dat Ba Nguyen; Martin Theobald; Gerhard Weikum,Abstract The task of Named Entity Disambiguation (NED); which maps mentions ofambiguous names in natural language onto a set of known entities; has been an importantissue in many areas including machine translation and information extraction. Working witha huge amount of data (eg more than three million entities in Yago); some parts in an NEDsystem which estimate the probability of a mention matching an entity; the similarity betweena mention and an entity and the coherence among entity candidates for all mentionstogether might become bottlenecks. Thus; it is challenging for an interactive NED system toreach not only high accuracy but also efficiency. This thesis presents an efficient way ofdisambiguating named entities by similarity hashing. Our framework is integrated with AIDAwhich is an on-line tool for entity detection and disambiguation developed at Max-Planck …,*,2012,1
DEANNA: Natural Language Questions for the Web of Data,Mohamed Yahya; Klaus Berberich; Shady Elbassiousni; Maya Ramanath; Volker Tresp; Gerhard Weikum,In 1950; after a decade of stardom in American films; she starred in the Italian film Stromboli;which led to a love affair with director Roberto Rossellini while they were both alreadymarried. The affair and then marriage with Rossellini created a scandal that forced her toremain in Europe until 1956; when she made a successful Hollywood return in Anastasia; forwhich she won her second Academy Award; as well as the forgiveness of her fans. Many ofher personal and film documents can be seen in the Wesleyan University CinemaArchives.[4],*,2012,1
Semantic Search: from Names and Phrases to Entities and Relations.,Gerhard Weikum,ABSTRACT Web search is traditionally limited to keyword queries. In the era of Big Data andthe Web of Linked Data; one would expect that schema-free search over both text andstructured key-value pairs becomes more semantic; Systems should; for example; identifyentities in queries and return crisp answers referring to facts; other entities and relationships.Some of these desired advances are happening now; the Google Knowledge Graph;Microsoft Entity Cube; and IBM's Watson technology for question answering are examples.However; all these systems still have major limitations when dealing with ambiguous namesand phrases that need to be mapped to entities and relations in the underlying data andknowledge bases. This talk discusses recent and ongoing research on disambiguatingnames and phrases. The presented methods can boost the quality and extent of semantic …,VLDS,2012,1
Scalable Distributed Time-Travel Text Search,Thaer Samar; Klaus Berberich; Gerhard Weikum,Abstract Web archives play an important role in preserving born-digital contents; archivingdata is important for future generations; researchers; historians; and for the public. Time-travel text search addresses the limited access to web archives by extending regular textsearch with time-travel functionality. Time-travel text search combines Boolean queries (eg;mpi AND saarland) and keyword queries (eg; mpi saarland) with a time-point (eg;2011/02/01) or time-interval of interest (eg;[2010/01/01; 2010/12/31]. Only documents thatmatch the query and whose valid-time interval overlaps with the given query time-intervalshould be retrieved in response to the query. Time-travel text search has to be highlyscalable to cope with huge size of web archives. Hadoop and HBase; as open-sourceimplementations of Google's MapReduce and BigTable; have recently become popular …,*,2011,1
Transactions on High-Performance Embedded Architectures and Compilers IV,Per Stenström,Volume Editor Per Stenström Chalmers University of Technology Department of Computer Scienceand Engineering 412 96 Gothenburg; Sweden E-mail: per. stenstrom@ chalmers. se ISSN0302-9743 (LNCS) e-ISSN 1611-3349 (LNCS) ISSN 1864-306X (THIPEAC) e-ISSN 1864-3078(THIPEAC) ISBN 978-3-642-24567-1 e-ISBN 978-3-642-24568-8 DOI 10.1007/978-3-642-24568-8 Springer Heidelberg Dordrecht London New York Library of Congress Control Number: 2011940725CR Subject Classification (1998): B. 2; C. 1; D. 3.4; B. 5; C. 2; D. 4 © Springer-Verlag Berlin Heidelberg2011 This work is subject to copyright. All rights are reserved; whether the whole or part of thematerial is concerned; specifically the rights of translation; reprinting; re-use of illustrations;recitation; broadcasting; reproduction on microfilms or in any other way; and storage in data …… Editor-in-Chief's Message It is my pleasure to introduce you to the fourth volume of …,*,2011,1
Deriving a web-scale common sense fact knowledge base,Niket Tandon; Gerhard Weikum; Gerard de Melo; Martin Theobald,Abstract The fact that birds have feathers and ice is cold seems trivially true. Yet; mostmachine-readable sources of knowledge either lack such common sense facts entirely orhave only limited coverage. Prior work on automated knowledge base construction haslargely focused on relations between named entities and on taxonomic knowledge; whiledisregarding common sense properties. Extracting such structured data from text ischallenging; especially due to the scarcity of explicitly expressed knowledge. Even whenrelying on large document collections; patternbased information extraction approachestypically discover insufficient amounts of information. This thesis investigates harvestingmassive amounts of common sense knowledge using the textual knowledge of the entireWeb; yet staying away from the massive engineering efforts in procuring such a large …,*,2011,1
Extraction of Temporal Facts and Events from Wikipedia,Erdal Kuzey; Gerhard Weikum; Martin Theobald,Abstract In recent years; the great success of Wikipedia and the progress in informationextraction techniques led to automatic construction of large scale knowledge bases whichhave Subject-Predicate-Object style facts extracted from both semi-structured and naturallanguage text of Wikipedia articles. Those knowledge bases consist of millions of entities;relations about them and their semantic types. Unfortunately; most of the current knowledgebases focus on static facts and ignore their temporal dimension; although; the vast majorityof facts are evolving with time and are valid during a particular time period. In this thesis; weintroduce a complete information extraction framework which harvests temporal facts andevents from semi-structured and free text of Wikipedia articles to enrich a temporal ontology(T-YAGO). Furthermore; this thesis discusses methods for introducing a temporal …,Master's thesis; Saarland University,2011,1
RDF-stores und rdf-query-engines,Thomas Neumann; Gerhard Weikum,Zusammenfassung RDF ist das Datenmodell der Semantic-Web-und Linked-Data-Initiativen; das zunehmend Verbreitung findet. Es zeichnet sich durch feinkörnigeStrukturierungsmöglichkeiten in Form binärer Relationen und durch flexible Typisierungohne die Notwendigkeit eines präskriptiven Schemas aus. Aus diesen Gründen sind dieeffiziente Speicherung und Anfrageauswertung auf RDF-Datenkollektionen schwierigeForschungsthemen. Dieser Artikel gibt einen Überblick über verschiedene Alternativen zurSpeicherung von RDF-Daten und diskutiert kurz die Probleme; die sich für die Indexierungsowie Anfrageoptimmierung und-ausführung ergeben.,Datenbank-Spektrum,2011,1
Learning Soft Inference Rules in Large and Uncertain Knowledge Bases,Christina Teflioudi; Martin Theobald; Gerhard Weikum,Abstract Recent progress in information extraction has enabled us to create large semanticknowledge bases with millions of RDF facts extracted from the Web. Nevertheless; theresulting knowledge bases are still incomplete or might contain inconsistencies; eitherbecause of the heuristic nature of the extraction process; or due to the varying reliability ofthe Web sources from which they were collected. One possible way of resolving both issuesis to reinforce the knowledge base with deductive power by appending first-order logicalinference rules; which help to describe and to further constrain the domain with which theontology deals. In our work; we investigate learning these rules directly from the data usingInductive Logic Programming (ILP); a well known technique; which lies in the intersection ofmachine learning and logic. Although powerful; ILP is inherently expensive as there is a …,*,2011,1
Automatic Domain Model Creation Using Pattern-Based Fact Extraction,Christopher Thomas; Pankaj Mehra; Wenbo Wang; Amit P Sheth; Gerhard Weikum; Victor Chan,Abstract This paper describes a minimally guided approach to automatic domain modelcreation. The first step is to carve an area of interest out of the Wikipedia hierarchy based ona simple query or other starting point. The second step is to connect the concepts in thisdomain hierarchy with named relationships. A starting point is provided by Linked OpenData; such as DBPedia. Based on these community-generated facts we train a pattern-based fact-extraction algorithm to augment a domain hierarchy with previously unknownrelationship occurrences. Pattern vectors are learned that represent occurrences ofrelationships between concepts. The process described can be fully automated and thenumber of relationships that can be learned grows as the community adds more information.Unlike approaches that are aimed at finding single; highly indicative patterns; we use the …,*,2011,1
Efficiently identifying interesting time points in text archives,Vinay Setty; Gerhard Weikum; Srikanta Bedathur; Klaus Berberich,Abstract Large scale text archives are increasingly becoming available on the Web.Exploring their evolving contents along both text and temporal dimensions enables us torealize their full potential. Standard keyword queries facilitate exploration along the textdimension only. Recently proposed time-travel keyword queries enable query processingalong both dimensions; but require the user to be aware of the exact time point of interest.This may be impractical if the user does not know the history of the query within thecollection or is not familiar with the topic. In this work; our aim is to efficiently identifyinteresting time points in Web archives with an assumption that we receive a result list for agiven query in standard relevance-order from an existing retrieval system. We consider twoforms of Web archives:(i) one where documents have a publication time-stamp and never …,Master's thesis; Universität des Saarlandes; FR Informatik,2010,1
Harvesting Knowledge from Web Data and Text,Hady Lauw; Ralf Schenkel; Fabian Suchanek; Martin Theobald; Gerhard Weikum,The Web bears the potential of being the world's greatest encyclopedic source; but we arefar from fully ex-ploiting this potential. Valuable scientific and cultural content is interspersedwith a huge amount of noisy; low-quality; unstructured text and media. The proliferation ofknowledge-sharing communities like Wikipedia and the advances in automated informationextraction from Web pages give rise to an unprecedented opportunity: Can wesystematically harvest facts from the Web and compile them into a comprehensive machine-readable knowledge base? Such a knowledge base would contain not only the world'sentities; but also their semantic properties; and their relationships with each other. Imagine a“Structured Wikipedia” that has the same scale and richness as Wikipedia itself; but offers aprecise and concise representation of knowledge; eg; in the RDF format. This would …,CIKM,2010,1
Peer-to-peer web search: euphoria; achievements; disillusionment; and future opportunities,Gerhard Weikum,Abstract The peer-to-peer (P2P) computing paradigm has been very successful like filesharing in Internet-wide communities (eg; Gnutella; BitTorrent) or IP telephony (eg; Skype).P2P systems promise perfect scalability from few peers to many millions; and resilience tofailures; dynamic variability; and even misbehaving peers with egoistic or even maliciousbehavior. None of these salient properties requires any global planning; administration; orcontrol; so P2P systems are completely self-organizing. Web search seems to be a perfectmatch for P2P architectures. The Web has naturally distributed data; spread across theentire Internet; as opposed to artifically hosting all content by a centralized search engine.For user-provided contents in Web 2.0 communities; consideration of the content ownership;the autonomy of users; and the individualized control of privacy would also suggest …,*,2010,1
Artificial Intelligence in Medicine: 12th Conference on Artificial Intelligence in Medicine in Europe; AIME 2009; Verona; Italy; July 18-22; 2009; Proceedings,Carlo Combi; Yuval Shahar; Ameen Abu-Hanna,The European Society for Artiﬁcial Intelligence in Medicine (AIME) was established in 1986following a very successful workshop held in Pavia; Italy; the year before. The principal aimsof AIME are to foster fundamental and applied research in the application of artiﬁcialintelligence (AI) techniques to medical care and medical research; and to provide a forum atbiennial conferences for discussing any progress made. For this reason the main activity ofthe society was the organization of a series of biennial conferences; held in Marseilles;France (1987); London; UK (1989); Maastricht; The Netherlands (1991); Munich; Germany(1993); Pavia; Italy (1995); Grenoble; France (1997); Aalborg; Denmark (1999); Cascais;Portugal (2001); Protaras; Cyprus (2003); Aberdeen; UK (2005); and Amsterdam; TheNetherlands (2007). This volume contains the proceedings of AIME 2009; the 12th …,*,2009,1
Evolutionary Computation in Combinatorial Optimization: 9th European Conference; EvoCOP 2009; Tübingen; Germany; April 15-17; 2009; Proceedings,Carlos Cotta; Peter I Cowling,This book constitutes the refereed proceedings of the 9th European Conference onEvolutionary Computation in Combinatorial Optimization; EvoCOP 2009; held in Tübingen;Germany; in April 2009. The 21 revised full papers presented were carefully reviewed andselected from 53 submissions. The papers present the latest research and discuss currentdevelopments and applications in metaheuristics-a paradigm to effectively solve difficultcombinatorial optimization problems appearing in various industrial; economical; andscientific domains. Prominent examples of metaheuristics are evolutionary algorithms;simulated annealing; tabu search; scatter search; memetic algorithms; variableneighborhood search; iterated local search; greedy randomized adaptive searchprocedures; estimation of distribution algorithms and ant colony optimization.,*,2009,1
Advances in Web Mining and Web Usage Analysis: 9th International Workshop on Knowledge Discovery on the Web; WebKDD 2007; and 1st International Worksho...,Haizheng Zhang; Myra Spiliopoulou; Bamshad Mobasher; C Lee Giles; Andrew McCallum; Olfa Nasraoui; Jaideep Srivastava; John Yen,Thisyear'svolumeofAdvancesinWebMiningandWebU…thepostworkshopproceedingsofajointevent; the9thInternationalWorkshopon KnowledgeDiscovery from the Web (WEBKDD 2007) and the First SNA-KDD Workshop on SocialNetwork Analysis (SNA-KDD 2007). The joint workshop on Web Mining and Social NetworkAnalysis took place at the ACM SIGKDD International Conference on Knowledge Discoveryand Data Mining (KDD). It attracted 23 submissions; of which 14 were accepted forpresentation at the workshop. Eight of them have been extended for inclusion in this volume.WEBKDD is one of the most traditional workshops of the ACM SIGKDDinternationalconference; under the auspices of which it has been organizedsince 1999. Thestrong interest for knowledge discovery in the Web; fostered not least by WEBKDD itself …,*,2009,1
Bidirectional transformations: A cross-discipline perspective; grace meeting notes; state of the art; and outlook,D Hutchison; T Kanade; J Kittler; J Kleinberg; F Mattern; J Mitchell; M Naor; O Nierstrasz; C Pandu Rangan; B Steffen,*,Theory and Practice of Model Transformations; Second International Conference; ICMT,2009,1
Index partitioning strategies for peer-to-peer web archival,Avishek Anand; Gerhard Weikum; Srikanta Bedathur; Christos Tryfonopoulos,Abstract The World Wide Web has become a key source of knowledge pertaining to almostevery walk of life. The goal is to build a scalable peer-to-peer framework for web archivaland to further support time-travel search over it. We provide an initial design with crawling;persistent storage and indexing and also analyze the partitioning strategies for historicalanalysis of data. Peer-to-peer (p2p) systems are a nice fit here but they suffer from churn andcommunication overhead and hence require controlled replication for availability and loadbalancing. The core of the contribution is of index organization by temporally partitioning thetime-travel index lists for supporting efficient time-travel search. We also analyze thepartitioning strategies in terms of improving replication to improve availability while stillkeeping the overall blowup if the index in check. We present various heuristic approaches …,*,2009,1
Peer-to-Peer Web Search,Gerhard Weikum,P/FDM [5–7] integrated a functional data model with the logic programming language Prologfor general-purpose computation. The data model can be seen as an Entity-Relationshipdiagram with sub-types; much like a UML Class Diagram. The idea was for the user to beable to define a computation over objects in the diagram; instead of just using it as a schemadesign aid. Later versions of P/FDM included a graphic interface [2; 4] to build queries inDAPLEX syntax by clicking on the diagram and filling in values from menus.,*,2009,1
Exploiting Temporal References in Text Retrieval,Emine Irem Arıkan; Gerhard Weikum; Srikanta Bedathur; Klaus Berberich,Abstract Temporal expressions; such as between 1994 and 2000; are frequent across manykinds of documents. In state-of-art search engines the documents are retrieved based ontheir text relevancy to the given query. Text retrieval; though; treats temporal expressions ascommon terms; thus ignoring their inherent semantics. For queries with a strong temporalcomponent; such as US president 1999; this is problematic; since documents can not bereliably matched to the query. This work introduces a new aspect to the document retrievaland introduces temporal relevancy. The query will be processed not only taking intoconsideration the similarity between the query text and the textual content of the documentbut also the similarity between the temporal content of the document and temporal part of thequery if given. The introduction of the temporal aspect brings up some new problems to …,*,2009,1
Constraint handling rules: current research topics,Tom Schrijvers; Thom Frühwirth,The ConstraintHandling Rules (CHR) languagecameto life morethan 15 years ago.Sincethen; ithasbecomeamajordeclarativespeci? cationandimplemen-tion language forconstraint-based algorithms and applications. In recent years; the? ve Workshops onConstraint Handling Rules have spurred the exchange of ideas within the CHR community;which has led to increased international collaboration; new theoretical results and optimizedimplementations. The aim of this volume of Lecture Notes in Ariti? cial Intelligence was toattract high-quality research papers on these recent advances in CHR. The 8 papersinthis issuewereselectedfrom11submissionsafterc…subsequent revisions. Each paper was reviewd by three reviewers. The accepted papersrepresent some of the research teams on CHR around the world. It is not by accident that thecurrently most active research group is featured here with three articles. We also would …,*,2008,1
HCI and Usability for Education and Work: 4th Symposium of the Workgroup Human-Computer Interaction and Usability Engineering of the Austrian Computer Socie...,Andreas Holzinger,The Workgroup Human–Computer Interaction & Usability Engineering (HCI&UE) of theAustrian Computer Society (OCG) serves as a platform for interdisciplinary-change; researchand development. While human–computer interaction (HCI) tra-tionally brings togetherpsychologists and computer scientists; usability engineering (UE) is a software engineeringdiscipline and ensures the appropriate implementation of applications. Our 2008 topic wasHuman–Computer Interaction for Education and Work (HCI4EDU); culminating in the 4thannual Usability Symposium USAB 2008 held during November 20–21; 2008 in Graz;Austria (http://usab-symposium. tugraz. at). As with the field of Human–Computer Interactionin Medicine and Health Care (HCI4MED); which was our annual topic in 2007; technologicalperformance also increases exponentially in the area of education and work. Learners …,*,2008,1
Sequences and Their Applications-SETA 2008: 5th International Conference Lexington; KY; USA; September 14-18; 2008; Proceedings,Solomon W Golomb,This volume contains the refereed proceedings papers of the Fifth International ConferenceonSequencesandtheirApplicati…(SETA2008); heldinLexington; Kentucky (USA); September 14-18; 2008. The conferenceSETA is well est-lished in the mathematics and computer science community. Topics of"SETA" include-Randomness of sequences-Correlation (periodic and aperiodic types) andcombinatorial aspects of-quences (di? erence sets)-Sequences with applications in codingtheory and cryptography-Sequences over? nite? elds/rings/function? elds-Linear andnonlinear feedback shift register sequences-Sequences for radar distance ranging;synchronization; identi? cation; and hardware testing-Sequences for wirelesscommunication-Pseudorandom sequence generators-Correlation and transformation ofBoolean functions-Multidimensional sequences and their correlation properties-Linear …,*,2008,1
Efficiently handling dynamics in distributed link based authority analysis,Josiane Xavier Parreira; Sebastian Michel; Gerhard Weikum,Abstract Link based authority analysis is an important tool for ranking resources in socialnetworks and other graphs. Previous work have presented J^X_P; a decentralized algorithmfor computing PageRank scores. The algorithm is designed to work in distributed systems;such as peer-to-peer (P2P) networks. However; the dynamics of the P2P networks; one if itsmain characteristics; is currently not handled by the algorithm. This paper shows how toadapt J^X_P to work under network churn. First; we present a distributed algorithm thatestimates the number of distinct documents in the network; which is needed in the localcomputation of the PageRank scores. We then present a method that enables each peer todetect other peers leave and to update its view of the network. We show that the number ofstored items in the network can be efficiently estimated; with little overhead on the …,International Conference on Web Information Systems Engineering,2008,1
Euro-Par 2008 Parallel Processing: 14th International Euro-Par Conference; Las Palmas de Gran Canaria; Spain; August 26-29; 2008; Proceedings,Emilio Luque; Tomas Margalef; Domingo Benítez,This book constitutes the refereed proceedings of the 14th International Conference onParallel Computing; Euro-Par 2008; held in Las Palmas de Gran Canaria; Spain; in August2008. The 86 revised papers presented were carefully reviewed and selected from 264submissions. The papers are organized in topical sections on support tools andenvironments; performance prediction and evaluation; scheduling and load balancing; highperformance architectures and compilers; parallel and distributed databases; grid andcluster computing; peer-to-peer computing; distributed systems and algorithms; parallel anddistributed programming; parallel numerical algorithms; distributed and high-performancemultimedia; theory and algorithms for parallel computation; and high performance networks.,*,2008,1
Formal verification of a transactional interaction contract,German Shegalov; Gerhard Weikum,A transactional information system guarantees as part of the ACID contract that multipleoperations pertaining to a transaction are executed atomically and that if completed theireffect is durable. However; message losses resulting from client and server crashes are notpart of the transaction. Unlike failures occurring in the midst of a transaction (leading to itsabort); the loss of a reply to the final commit message poses a problem of determining thetransaction outcome that cannot be solved generically using state-of-the-art OLTP systems.This paper develops a formal specification of a generic transactional interaction contract thatis part of a broader Interaction Contracts framework guaranteeing the exactly-onceexecution in general multi-tier applications. The formal specification is designed and verifiedusing the statechart language and model checking in the reactive system design tool …,Services-Part I; 2008. IEEE Congress on,2008,1
Information Theoretic Security,Reihaneh Safavi-Naini,ICITS 2008; the Third International Conference on Information Theoretic Security; was heldin Calgary; Alberta; Canada; during August 10–13; 2008; at the University of Calgary. Thisseries of conferences was started with the 2005 IEEE Information Theory Workshop onTheory and Practice in Information-Theoretic Security (ITW 2005; Japan); held on AwajiIsland; Japan; October 16–19; 2005. The conference series aims at bringing focus tosecurity research when there is no unproven computational assumption on the adversary.This is the framework proposed by Claude Shannon in his seminal paper formalizingmodern unclassified research on cryptography. Over the last few decades; Shannon'sapproach to formalizing security has been used in various other areas includingauthentication; secure communication; key exchange; multiparty computation and …,*,2008,1
Data Warehousing and Knowledge Discovery,David Hutchison; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum; Il-Yeol Song; Johann Eder; Tho Manh Nguyen,*,*,2008,1
Formal Methods and Software Engineering: 10th International Conference on Formal Engineering Methods; ICFEM 2008; Kitakyushu-City; Japan; October 27-31; 20...,David Hutchison; Keijiro Araki; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Shaoying Liu; Tom Maibaum; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,1
Advances in Databases and Information Systems,Paolo Atzeni; Albertas Caplinskas; Hannu Jaakkola,This volume contains the best papers presented at the 12th East-European Conference onAdvances in Databases and Information Systems (ADBIS 2008) held during September 5–9;2008; in Pori; Finland. The series of ADBIS conferences is the successor of the annualinternational workshops with the same title that during 1993-1996 were organized in Russiaby the Moscow ACM SIGMOD Chapter. ADBIS 2008 continues the series of ADBISconferences held in St. Petersburg; Russia (1997); Poznan; Poland (1998); Maribor;Slovenia (1999); Prague; Czech Republic (2000); Vilnius; Lithuania (2001); Bratislava;Slovakia (2002); Dresden; Germany (2003); Budapest; Hungary (2004); Tallinn; Estonia(2005); Thessaloniki; Greece (2006); and Varna; Bulgaria (2007). The conferences areinitiated and supervised by an international Steering Committee chaired by professor …,*,2008,1
Design and Implementation of an Automatic Semantic Annotation Service,Alina Kopp,Abstract Administration and legal boundaries; coordination problems with respect to dataanalysis; information delivery and resource management pose numerous problems in theintegration of information systems for the European risk and crisis management. To addressthese issues; the European Commission has made “Improving risk management” to one ofits strategic objectives of the Information Society Technology (IST) research program. Theintegrated project ORCHESTRA is one of the projects initiated in this area. The overall goalof ORCHESTRA is the design and implementation of an open; service-oriented softwarearchitecture as a contribution to overcome the interoperability problems in the domain ofmulti-risk management. The Annotation Service specified in the context of the ORCHESTRAArchitecture automatically generates specific meta-information from structured and …,*,2007,1
TopX-Efficient and Versatile Top-k Query Process-ing for Text; Semistructured; and Structured Data.,Martin Theobald; Ralf Schenkel; Gerhard Weikum,Abstract: This paper presents a comprehensive overview of the TopX search engine; anextensive framework for unified indexing and querying large collections of unstructured;semistructured; and structured data. Residing at the very synapse of database (DB)engineering and information retrieval (IR); it integrates efficient scheduling algorithms for top-k-style ranked retrieval with powerful scoring models; as well as dynamic and self-throttlingquery expansion facilities.,BTW,2007,1
Efficient search and approximate information filtering in a distributed peer-to-peer environment of digital libraries,Christian Zimmer; Christos Tryfonopoulos; Gerhard Weikum,Abstract We present a new architecture for efficient search and approximate informationfiltering in a distributed Peer-to-Peer (P2P) environment of Digital Libraries. TheMinervaLight search system uses P2P techniques over a structured overlay network todistribute and maintain a directory of peer statistics. Based on the same directory; the MAPSinformation filtering system provides an approximate publish/subscribe functionality bymonitoring the most promising digital libraries for publishing appropriate documentsregarding a continuous query. In this paper; we discuss our system architecture thatcombines searching and information filtering abilities. We show the system components ofMinervaLight and explain the different facets of an approximate pub/sub system forsubscriptions that is high scalable; efficient; and notifies the subscribers about the most …,*,2007,1
Report on the Second International Workshop on Self-Managing Database Systems (SMDB 2007).,Anastassia Ailamaki; Surajit Chaudhuri; Sam Lightstone; Guy M Lohman; Patrick Martin; Kenneth Salem; Gerhard Weikum,Information management systems are growing rapidly in scale and complexity; while skilleddatabase administrators are becoming rarer and more expensive. Increasingly; the total costof ownership of information management systems is dominated by the cost of people; ratherthan hardware or software costs. This economic dynamic dictates that information systems ofthe future be more automated and simpler to use; with most administration tasks transparentto the user.,IEEE Data Eng. Bull.,2007,1
Verteilte Annotation von Dokumenten-Architekturentwurf und Implementierung eines Frameworks auf J2EE-Basis,Frank Fuchs; Gerhard Weikum,Durch die hohe Anzahl von Dokumenten (die deutsche Wikipedia-Seite allein mit über390.000 Artikeln) und der damit verbundenen Rechenlast bedarf es einer Lastenverteilung.Da die oben genannten Werkzeuge als typische Einzelplatzlösungen konzipiert und nichtfür das Verwenden im Netz vorgesehen sind; wird ein Framework benötigt; das dieseUnzulänglichkeiten beseitigt.,Bachelorarbeit; Universität des Saarlandes,2006,1
Foundations of automated database tuning (tutorial),Surajit Chaudhuri; Gerhard Weikum; Ling Liu; Andreas Reuter; Kyu-Young Whang; Jianjun Zhang,Abstract Our society is more dependent on information systems than ever before. However;managing the information systems infrastructure in a cost-effective manner is a growingchallenge. The total cost of ownership (TCO) of information technology is increasinglydominated by people costs. In fact; mistakes in operations and administration of informationsystems are the single most reasons for system outage and unacceptable performance. Forinformation systems to provide value to their customers; we must reduce the complexityassociated with their deployment and usage.,Untitled Event,2006,1
A comparative study of pub/sub methods in structured p2p networks,Sebastian Parkitny; Gerhard Weikum; Matthias Bender; Sebastian Michel; Thorsten Herfet,Abstract Publish/Subscribe systems have become an established model to provide contentdelivery from publishers to subscribers. Many approaches based on top of a P2P networkhave been proposed and evaluated; but typically each approach is evaluated on its own. Weidentify two approaches to implement Publish/Subscribe based on a structured P2P networkand provide a mathematical analysis of their complexity. Furthermore; we compare the twoapproaches for several choices of system parameters and associate the outcomes to certainusage scenarios. Thus; we can provide evidence of which of these approaches is suitablefor certain scenarios. Finally; we design and implement a discrete event simulator andpresent results of experimental measurements of both approaches. viii,Universität des Saarlandes; Tech. Rep,2006,1
Overlap-aware global df estimation in distributed information retrieval systems,Matthias Bender; Sebastian Michel; Gerhard Weikum; Peter Triantafilou,Zusammenfassung Peer-to-Peer (P2P) search engines and other forms of distributedinformation retrieval (IR) are gaining momentum. Unlike in centralized IR; it is difficult andexpensive to compute statistical measures about the entire document collection as it iswidely distributed across many computers in a highly dynamic network. On the other hand;such network-wide statistics; most notably; global document frequencies of the individualterms; would be highly beneficial for ranking global search results that are compiled fromdifferent peers. This paper develops an efficient and scalable method for estimating globaldocument frequencies in a large-scale; highly dynamic P2P network with autonomouspeers. The main difficulty that is addressed in this paper is that the local collections ofdifferent peers may arbitrarily overlap; as many peers may choose to gather popular …,*,2006,1
Verteilte Annotation von Dokumenten–Entwurf und Implementierung eines modularen Annotationsservers auf Basis von NLP-Technologien,D Fuchs; Gerhard Weikum,Wie die meisten großen Ideen; kann Tim Berners-Lee's Idee vom semantischen Web1einfach beschrieben werden: wenn Rechner nicht nur Informationen gewinnen können;sondern auch in der Lage sind; die im Web vorhandenen Daten zu verstehen; werden wirein neues Web und neue Arten von intelligenten Web-Anwendungen haben. In der nahenZukunft werden Rechner jedoch nicht so intelligent sein; um selbständig verstehen zukönnen; was für Informationen Menschen ins Web gestellt haben. Andererseits; wenn wirneben den vom Menschen verstandenen Informationen zusätzliche vom Rechnerverstandenen Informationen aufstellen; werden Rechner intelligenter. 2 Um die Vision dessemantischen Webs zu verwirklichen; werden mehrere Bausteine benötigt. 3 Beispielsweisewerden standardisierte Sprachen gebraucht; um die semantischen Daten zu beschreiben …,Universität des Saarlandes,2006,1
IQN routing: Integrating quality and novelty in P2P querying and ranking,M Bender; S Michel; P Triantafillou; G Weikum,*,*,2006,1
Das MINERVA-Projekt: Datenbankselektion für Peer-to-Peer-Websuche,Matthias Bender; Sebastian Michel; Gerhard Weikum; Christian Zimmer,Zusammenfassung In diesem Artikel wird MINERVA präsentiert; eine prototypischeImplementierung einer verteilten Suchmaschine basierend auf einer Peer-to-Peer (P2P)-Architektur. MINERVA setzt auf die in der P2P-Welt verbreitete Technik verteilter Hash-Tabellen auf und benutzt diese zum Aufbau eines verteilten Verzeichnisses. Peers inunserem Ansatz entsprechen völlig autonomen Benutzern mit ihren lokalen Suchm"oglichkeiten; die bereit sind; ihr lokales Wissen und ihre lokalen Suchmöglichkeiten imRahmen einer Kollaboration zur Verfügung zu stellen. Wir formalisieren unsereSystemarchitektur und beschreiben das zentrale Problem einer effizienten Suche nachvielversprechenden Peers für eine konkrete Anfrage innerhalb des Verbundes. Wir greifendabei auf existierende Methoden zurück and passen diese an unseren Systemkontext an …,Informatik-Forschung und Entwicklung,2005,1
Conceptual Modeling-ER 2004: 23rd International Conference on Conceptual Modeling; Shanghai; China; November 8-12; 2004. Proceedings,Paolo Atzeni; Wesley Chu; Hongjun Lu; Shuigeng Zhou; Tok Wang Ling,The 23rd International Conference on Conceptual Modeling (ER 2004) was held inShanghai; China; November 8–12; 2004. Conceptual modeling is a fundamental techniqueused in analysis and design as a real-world abstraction and as the basis for communicationbetween technology experts and their clients and users. It has become a fundamentalmechanism for understanding and representing organizations; including new e-worlds; andthe information systems that support them. The International Conference on ConceptualModeling provides a major forum for presenting and discussing current research andapplications in which conceptual modeling is the major emphasis. Since the first edition in1979; the ER conference has evolved into the most prestigious one in the areas ofconceptual modeling research and applications. Its purpose is to identify challenging …,*,2005,1
P2P Web Search with MINERVA: How do you want to search tomorrow?,Sebastian Michel; Matthias Bender; Peter Triantafillou; Gerhard Weikum; Christian Zimmer,Abstract. MINERVA1 is a novel approach towards P2P Web search that connects an a-prioriunlimited number of peers; each of which maintains a personal local database and a localsearch facility. Each peer posts a small amount of metadata to a physically distributeddirectory layered on top of a DHT-based overlay network that is used to efficiently selectpromising peers from across the peer population that can best locally execute a query. Thispaper proposes a live demonstration of MINERVA; showcasing the full information lifecycle:crawling web pages; disseminating metadata to a distributed directory; and executingqueries online. We additionally invite all visitors to instantly join the network by executing asmall piece of software.,Untitled Event,2005,1
Report on the 2004 SIGMOD Conference.,Gerhard Weikum; Patrick Valduriez,The 2004 ACM SIGMOD International Conference on Management of Data; held in in Parisin the week of June 13-18; was the first SIGMOD ever held outside of America. It had chosena place that is rich in tradition but also rich in new departures; one of the focal points of theage of enlightenment and the place of the French revolution in 1789. At SIGMOD 2004 norevolution happened; the 10-minute power outage that struck some of the presentations wasmerely caused by a strike of the electricity utility workers-“unmouvement social” in French.However; the technical program of SIGMOD did indicate that the database researchcommunity is well on its way into a broader agenda with new challenges arising frombusiness intelligence and scientific information management as well as network-enableddata-rich applications. The conference was located at La Maison de la Chimie; which was …,SIGMOD Record,2004,1
The Atomic Manifesto: a Story in Four Quarks,Cliff Jones; David Lomet; Alexander Romanovsky; Gerhard Weikum; Alan Fekete; Marie-Claude Gaudel; Henry F Korth; Rogerio de Lemos; Eliot Moss; Ravi Rajwar; Krithi Ramamritham; Brian Randell; Luis Rodrigues,Abstract This report summarizes the viewpoints and insights gathered in the DagstuhlSeminar on Atomicity in System Design and Execution; which was attended by 32 peoplefrom four different scientific communities: database and transaction processing systems; faulttolerance and dependable systems; formal methods for system design and correctnessreasoning; and hardware architecture and programming languages. Each communitypresents its position in interpreting the notion of atomicity and the existing state of the art;and each community identifies scientific challenges that should be addressed in future work.In addition; the report discusses common themes across communities and strategic researchproblems that require multiple communities to team up for a viable solution. The generaltheme of how to specify; implement; compose; and reason about extended and relaxed …,Dagstuhl Seminar Proceedings,2004,1
Detection of intrusions and malware and vulnerability assessment,Ulrich Flegel; M Meier,On behalf of the Program Committee; it is our pleasure to present to you the proceedings ofthe 9 th GI International Conference on Detection of Intrusions and Malware & VulnerabilityAssessment (DIMVA). Each year; DIMVA brings together international experts fromacademia; industry; and government to present and discuss novel security research. DIMVAis organized by the Special Interest Group Security–Intrusion Detection and Response(SIDAR) of the German Informatics Society (GI). The DIMVA 2012 Program Committeereceived 44 submissions from a diverse set of countries. All submissions were carefullyreviewed by Program Committee members and external experts according to the criteria ofscientific novelty; technical quality; and practical impact. The final selection took place at theProgram Committee meeting held on April 5; 2012; at the University of Bonn. Ten full …,Proc. of the GI Special Interest Group SIDAR Workshop (DIMVA),2004,1
Advances in Database Technology-EDBT 2000: 7th International Conference on Extending Database Technology Konstanz; Germany; March 27-31; 2000 Proceedi...,Carlo Zaniolo; Peter C Lockemann; Marc H Scholl; Torsten Grust,EDBT 2000 is the seventh conference in a series dedicated to the advancement of databasetechnology. This year's conference special theme;\Connect Millions of Users and DataSources;" underscores the importance of databases for the information age that is dawningwith the new millennium. The importance-rives not just from the observation that theinformation age essentially rests on theconvergenceofcommunications; computing;andstorage. Equallyimportant; many of the concepts and techniques underlying the successof databasesystems have independent meaning and impact for today's distributedinformation s-tems. The papers in the volume should also be seen in this light. The EDBT2000 conference program includes 30 research papers selected by the program committeeout of 187 submissions; covering advances in research; development; and applications of …,*,2003,1
Mit HEART zu Middleware-basierten Antwortzeitgarantien für E-Services,Achim Kraiß; Frank Schön; Gerhard Weikum; Uwe Deppisch,Zusammenfassung Message-orientierte Middleware (MoM) ist eine weit eingesetzteBasistechnologie; um die Backend-Serversysteme eines Unternehmens in eine Vielzahlmoderner e-Services zu integrieren. Bestandteil der Qualityof-Service (QoS) dieser e-Services; zB im Online-und Mobile-Banking; muss die Gewährleistung akzeptableranwenderklassenspezifischer Antwortzeiten für e-Service-Transaktionen sein. Aus dieserForderung ergeben sich besondere Anforderungen an die Antwortzeiten von Aufträgen anden integrierten Backend-Serversystemen. Ein Antwortzeitziel könnte beispielsweise sein;dass bestimmte e-Service-Transaktionen eine vorgegebene mittlere Antwortzeit amBackend-Server nicht überschreiten dürfen oder dass 95 Prozent der Transaktionen einevorgegebene maximale Antwortzeit nicht überschreiten dürfen. In der Praxis bieten …,Datenbank-Spektrum,2002,1
Computing and Combinatorics: 6th Annual International Conference; COCOON 2000; Sydney; Australia; July 26-28; 2000 Proceedings,Dingzhu Du,The papers in this volume were selected for presentation at the 6th Annual InternationalComputing and Combinatorics Conference (COCOON2000); in Sydney; Australia from July26-28; 2000. The topics cover many areas in t-oretical computer science and combinatorialoptimization. There were 81 high quality papers submitted to COCOON2000. Each paperwas reviewed by at least three program committee members; and the 44 papers wereselected. It is expected that most of them will appear in a more complete form in scienticjournals. In addition to the selected papers; the volume also contains the papers from twoinvited keynote speeches by Christos Papad-itriou and Richard Brent. This year the HaoWang Award was given to honor the paper judged by the programcommittee to have thegreatest merit. The recipient is\Approximating Uniform TriangularMeshes in Polygons" …,*,2000,1
Data Placement In Bubba,Gerhard Weikum,Abstract This paper; which came out of the Bubba project at MCC; was the first to addressthe physical database design problem for parallel database servers; with particular focus onthe partitioning and allocation of (relational) data across multiple disks or processing nodes.These issues are key to good performance tuning. To this end; the paper introduced thefundamental notion of data heat as a measure for the disk access load attributed to a dataunit or collection of units; and the notion of temperature to normalize heat by the consumedspace. Based on these metrics; the paper developed an elegant framework and heuristicalgorithms for choosing which data should be placed on which disk so as to balance the diskload; and which data should be cached in memory so as to minimize the overall disk load. Ihad the great opportunity of spending a postdoc year in the Bubba group at MCC where I …,ACM SIGMOD Digital Review,2000,1
Configuring and performance modeling of multimedia servers with mixed workload,G Nerjes; Y Rombogiannakis; P Muth; M Paterakis; P Triantafillou; G Weikum,*,*,1997,1
The database research group at ETH Zurich,Moira C Norrie; Stephen M Blott; Hans-Jörg Schek; Gerhard Weikum,Increasingly; we are becoming a data-driven society with massive information requirementsand evermore numerous on-line data sources. The research activities of the DatabaseGroup at ETH are centred on the investigation of architectures and techniques for exploringand managing the data COSMOS with its proliferation and diversity of data; and with itsinherent heterogeneity. Our key aim is to provide a spectrum of data connectivity wherebydata sources and application systems may cooperate at various levels of interoperability andintegration. Multilevel interoperability allows application systems to cooperate withapplication systems; database systems to coopera. te with dat; a. base systems; and storageservices to cooperate with storage services. To meet this aim; we require Cooperative ObjectSer-vice Management for Open Systems (COSMOS). The overall goal of COSMOS is to …,ACM SIGMOD Record,1994,1
Cost/performance control in SNOWBALL distributed file manager,Radek Vingralek; Yuri Breitbart; Gerhard Weikum,Abstract Networks of workstations are an emerging architectural paradigm forhighperformance parallel and distributed systems. Exploiting networks of workstations formassive data management poses exciting challenges. We consider here the problem ofmanagement of record-structured les in such an environment. The le records are accessedby a dynamically growing set of clients based on a search key. To scale up the throughput ofclient accesses with approximately constant response time; the les and thus also theiraccess load are dynamically redistributed across a growing set of workstations. Theredistribution method is capable of an explicit control of system cost/performance. Namely;the system maintains its cost/performance at a prescribed constant level for a wide spectrumof workloads as con rmed by experimental simulation results. Consequently; the system is …,Fifteenth Database Conference DataSem,*,1
On Measuring Bias in Online Information,Evaggelia Pitoura; Panayiotis Tsaparas; Giorgos Flouris; Irini Fundulaki; Panagiotis Papadakos; Serge Abiteboul; Gerhard Weikum,Abstract Bias in online information has recently become a pressing issue; with searchengines; social networks and recommendation services being accused of exhibiting someform of bias. In this vision paper; we make the case for a systematic approach towardsmeasuring bias. To this end; we discuss formal measures for quantifying the various types ofbias; we outline the system components necessary for realizing them; and we highlight therelated research challenges and open problems.,ACM SIGMOD Record,2018,*
VISIR: Visual and Semantic Image Label Refinement,Sreyasi Nag Chowdhury; Niket Tandon; Hakan Ferhatosmanoglu; Gerhard Weikum,Abstract The social media explosion has populated the Internet with a wealth of images.There are two existing paradigms for image retrieval: 1) content-based image retrieval (BIR);which has traditionally used visual features for similarity search (eg; SIFT features); and 2)tag-based image retrieval (TBIR); which has relied on user tagging (eg; Flickr tags). CBIRnow gains semantic expressiveness by advances in deep-learning-based detection of visuallabels. TBIR benefits from query-and-click logs to automatically infer more informative labels.However; learning-based tagging still yields noisy labels and is restricted to concreteobjects; missing out on generalizations and abstractions. Click-based tagging is limited toterms that appear in the textual context of an image or in queries that lead to a click. Thispaper addresses the above limitations by semantically refining and expanding the labels …,Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining,2018,*
Spec-QP: Speculative Query Planning for Joins over Knowledge Graphs,Madhulika Mohanty; Maya Ramanath; Mohamed Yahya; Gerhard Weikum,Abstract: Organisations store huge amounts of data from multiple heterogeneous sources inthe form of Knowledge Graphs (KGs). One of the ways to query these KGs is to use SPARQLqueries over a database engine. Since SPARQL follows exact match semantics; the queriesmay return too few or no results. Recent works have proposed query relaxation where thequery engine judiciously replaces a query predicate with similar predicates using weightedrelaxation rules mined from the KG. The space of possible relaxations is potentially too largeto fully explore and users are typically interested in only top-k results; so such query enginesuse top-k algorithms for query processing. However; they may still process all therelaxations; many of whose answers do not contribute towards top-k answers. This leads tocomputation overheads and delayed response times. We propose Spec-QP; a query …,arXiv preprint arXiv:1711.07581,2017,*
Privacy of Hidden Profiles: Utility-Preserving Profile Removal in Online Forums,Sedigheh Eslami; Asia J Biega; Rishiraj Saha Roy; Gerhard Weikum,Abstract Users who wish to leave an online forum often do not have the freedom to erasetheir data completely from the service providers'(SP) system. The primary reason behind thisis that analytics on such user data form a core component of many online providers'business models. On the other hand; if the profiles reside in the SP's system in anunchanged form; major privacy violations may occur if the infrastructure is compromised; orthe SP is acquired by another organization. In this work; we investigate an alternativesolution to standard profile removal; where posts of different users are split and merged intosynthetic mediator profiles. The goal of our framework is to preserve the SP's data miningutility as far as possible; while minimizing users' privacy risks. We present severalmechanisms of assigning user posts to such mediator accounts and show the …,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,2017,*
Learning to Un-Rank: Quantifying Search Exposure for Users in Online Communities,Asia J Biega; Azin Ghazimatin; Hakan Ferhatosmanoglu; Krishna P Gummadi; Gerhard Weikum,Abstract Search engines in online communities such as Twitter or Facebook not only returnmatching posts; but also provide links to the profiles of the authors. Thus; when a userappears in the top-k results for a sensitive keyword query; she becomes widely exposed in asensitive context. The effects of such exposure can result in a serious privacy violation;ranging from embarrassment all the way to becoming a victim of organizationaldiscrimination. In this paper; we propose the first model for quantifying search exposure onthe service provider side; casting it into a reverse k-nearest-neighbor problem. Moreover;since a single user can be exposed by a large number of queries; we also devise a learning-to-rank method for identifying the most critical queries and thus making the warnings user-friendly. We develop efficient algorithms; and present experiments with a large number of …,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,2017,*
Us and Them: Adversarial Politics on Twitter,Anna Guimaraes; Liqiang Wang; Gerhard Weikum,Abstract—Social-media debates on longitudinal political topics often take the form ofadversarial discussions: highly polarized user posts; favoring one of two opposing parties;over an extended time period. Recent prominent cases are the US Presidential campaignand the UK Brexit referendum. This paper approaches such discussions as a multi-faceteddata space; and applies data mining to identify interesting patterns and factors of influence.Specifically; we study how topics are addressed by different parties; their factual and “post-factual” undertones; and the role of highly active “power users” on either side of thediscussion.,2017 IEEE International Conference on Data Mining Workshops (ICDMW),2017,*
Completeness-Aware Rule Learning from Knowledge Graphs,Thomas Pellissier Tanon; Daria Stepanova; Simon Razniewski; Paramita Mirza; Gerhard Weikum,Abstract Knowledge graphs (KGs) are huge collections of primarily encyclopedic facts. Theyare widely used in entity recognition; structured search; question answering; and otherimportant tasks. Rule mining is commonly applied to discover patterns in KGs. However;unlike in traditional association rule mining; KGs provide a setting with a high degree ofincompleteness; which may result in the wrong estimation of the quality of mined rules;leading to erroneous beliefs such as all artists have won an award; or hockey players do nothave children. In this paper we propose to use (in-) completeness meta-information to betterassess the quality of rules learned from incomplete KGs. We introduce completeness-awarescoring functions for relational association rules. Moreover; we show how one can obtain (in-) completeness meta-data by learning rules about numerical patterns of KG edge counts …,International Semantic Web Conference,2017,*
KnowNER: Incremental Multilingual Knowledge in Named Entity Recognition,Dominic Seyler; Tatiana Dembelova; Luciano Del Corro; Johannes Hoffart; Gerhard Weikum,Abstract: KnowNER is a multilingual Named Entity Recognition (NER) system that leveragesdifferent degrees of external knowledge. A novel modular framework divides the knowledgeinto four categories according to the depth of knowledge they convey. Each categoryconsists of a set of features automatically generated from different information sources (suchas a knowledge-base; a list of names or document-specific semantic annotations) and isused to train a conditional random field (CRF). Since those information sources are usuallymultilingual; KnowNER can be easily trained for a wide range of languages. In this paper;we show that the incorporation of deeper knowledge systematically boosts accuracy andcompare KnowNER with state-of-the-art NER approaches across three languages (ie;English; German and Spanish) performing amongst state-of-the art systems in all of them …,arXiv preprint arXiv:1709.03544,2017,*
Towards Nonmonotonic Relational Learning from Knowledge Graphs,Francesca A Lisi; Gerhard Weikum,Abstract. Recent advances in information extraction have led to the socalled knowledgegraphs (KGs); ie; huge collections of relational factual knowledge. Since KGs areautomatically constructed; they are inherently incomplete; thus naturally treated under theOpen World Assumption (OWA). Rule mining techniques have been exploited to support thecrucial task of KG completion. However; these techniques can mine Horn rules; which areinsufficiently expressive to capture exceptions; and might thus make incorrect predictions onmissing links. Recently; a rule-based method for filling in this gap was proposed which;however; applies to a flattened representation of a KG with only unary facts. In this work wemake the first steps towards extending this approach to KGs in their original relational form;and provide preliminary evaluation results on real-world KGs; which demonstrate the …,Inductive Logic Programming: 26th International Conference; ILP 2016; London; UK; September 4-6; 2016; Revised Selected Papers,2017,*
Correlation by Compression,Kailash Budhathoki; Jilles Vreeken,Abstract Discovering correlated variables is one of the core problems in data analysis. Manymeasures for correlation have been proposed; yet it is surprisingly ill-defined in general.That is; most; if not all; measures make very strong assumptions on the data distribution ortype of dependency they can detect. In this work; we provide a general theory on correlation;without making any such assumptions. Simply put; we propose correlation by compression.To this end; we propose two correlation measures based on solid information theoreticfoundations; ie Kolmogorov complexity. The proposed correlation measures possessinteresting properties desirable for any sensible correlation measure. However; Kolmogorovcomplexity is not computable; and hence we propose practical and computableinstantiations based on the Minimum Description Length (MDL) principle. In practice; we …,*,2017,*
Continuous Experience-aware Language Model for Recommender Systems using Brownian Motion,Subhabrata Mukherjee; Stephan Guennemann; Gerhard Weikum,Abstract: Online review communities are dynamic as users join and leave; adopt newvocabulary; and adapt to evolving trends. Recent work has shown that recommendersystems benefit from explicit consideration of user experience. However; prior work assumesa fixed number of discrete experience levels; whereas in reality users gain experience andmature continuously over time. This paper presents a new model that captures thecontinuous evolution of user experience; and the resulting language model in reviews andother posts. Our model is unsupervised and combines principles of Geometric BrownianMotion; Brownian Motion; and Latent Dirichlet Allocation to trace a smooth temporalprogression of user experience and language model respectively. We develop practicalalgorithms for estimating the model parameters from data and for inference with our …,arXiv preprint arXiv:1705.02669,2017,*
Credible Review Detection with Limited Information using Consistency Analysis,Subhabrata Mukherjee; Sourav Dutta; Gerhard Weikum,Abstract: Online reviews provide viewpoints on the strengths and shortcomings ofproducts/services; influencing potential customers' purchasing decisions. However; theproliferation of non-credible reviews--either fake (promoting/demoting an item); incompetent(involving irrelevant aspects); or biased--entails the problem of identifying credible reviews.Prior works involve classifiers harnessing rich information about items/users--which mightnot be readily available in several domains--that provide only limited interpretability as towhy a review is deemed non-credible. This paper presents a novel approach to address theabove issues. We utilize latent topic models leveraging review texts; item ratings; andtimestamps to derive consistency features without relying on item/user histories; unavailablefor" long-tail" items/users. We develop models; for computing review credibility scores to …,arXiv preprint arXiv:1705.02668,2017,*
Leveraging Joint Interactions for Credibility Analysis in News Communities with Continuous Conditional Random Field,Subhabrata Mukherjee; Gerhard Weikum,Abstract: Media seems to have become more partisan; often providing a biased coverage ofnews catering to the interest of specific groups. It is therefore essential to identify credibleinformation content that provides an objective narrative of an event. News communities suchas digg; reddit; or newstrust offer recommendations; reviews; quality ratings; and furtherinsights on journalistic works. However; there is a complex interaction between differentfactors in such online communities: fairness and style of reporting; language clarity andobjectivity; topical perspectives (like political viewpoint); expertise and bias of communitymembers; and more. This paper presents a model to systematically analyze the differentinteractions in a news community between users; news; and sources. We develop aprobabilistic graphical model that leverages this joint interaction to identify 1) highly …,arXiv preprint arXiv:1705.02667,2017,*
Item Recommendation with Evolving User Preferences and Experience,Subhabrata Mukherjee; Hemank Lamba; Gerhard Weikum,Abstract: Current recommender systems exploit user and item similarities by collaborativefiltering. Some advanced methods also consider the temporal evolution of item ratings as aglobal background process. However; all prior methods disregard the individual evolution ofa user's experience level and how this is expressed in the user's writing in a reviewcommunity. In this paper; we model the joint evolution of user experience; interest in specificitem facets; writing style; and rating behavior. This way we can generate individualrecommendations that take into account the user's maturity level (eg; recommending artmovies rather than blockbusters for a cinematography expert). As only item ratings andreview texts are observables; we capture the user's experience and interests in a latentmodel learned from her reviews; vocabulary and writing style. We develop a generative …,arXiv preprint arXiv:1705.02519,2017,*
Efficiency-aware Answering of Compositional Questions using Answer Type Prediction,David Ziegler; Abdalghani Abujabal; Rishiraj Saha Roy; Gerhard Weikum,Abstract This paper investigates the problem of answering compositional factoid questionsover knowledge bases (KB) under efficiency constraints. The method; called TIPI;(i)decomposes compositional questions;(ii) predicts answer types for individual sub-questions;(iii) reasons over the compatibility of joint types; and finally;(iv) formulatescompositional SPARQL queries respecting type constraints. TIPI's answer type predictor istrained using distant supervision; and exploits lexical; syntactic and embedding-basedfeatures to compute context-and hierarchy-aware candidate answer types for an inputquestion. Experiments on a recent benchmark show that TIPI results in state-of-the-artperformance under the real-world assumption that only a single SPARQL query can beexecuted over the KB; and substantial reduction in the number of queries in the more …,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),2017,*
QUINT: Interpretable Question Answering over Knowledge Bases,Abdalghani Abujabal; Rishiraj Saha Roy; Mohamed Yahya; Gerhard Weikum,Abstract We present QUINT; a live system for question answering over knowledge bases.QUINT automatically learns role-aligned utterance-query templates from user questionspaired with their answers. When QUINT answers a question; it visualizes the completederivation sequence from the natural language utterance to the final answer. The derivationprovides an explanation of how the syntactic structure of the question was used to derive thestructure of a SPARQL query; and how the phrases in the question were used to instantiatedifferent parts of the query. When an answer seems unsatisfactory; the derivation providesvaluable insights towards reformulating the question.,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,2017,*
Answer Type Prediction for Question Answering over Knowledge Bases,David Ziegler; Abdalghani Abujabal; Rishiraj Saha Roy; Gerhard Weikum,Author: Ziegler; David et al.; Genre: Thesis; Published in Print: 2017; Title: AnswerType Prediction for Question Answering over Knowledge Bases.,*,2017,*
An Approach to Nonmonotonic Relational Learning from Knowledge Graphs,Hai Dang Tran; Daria Stepanova; Gerhard Weikum,Author: Tran; Hai Dang et al.; Genre: Thesis; Published in Print: 2017; Title: An Approachto Nonmonotonic Relational Learning from Knowledge Graphs.,*,2017,*
Utility-preserving Profile Removal in Online Forums,Sedigheh Eslami; Gerhard Weikum; Rishiraj Saha Roy,Author: Eslami; Sedigheh et al.; Genre: Thesis; Published in Print: 2017;Title: Utility-preserving Profile Removal in Online Forums.,*,2017,*
Personalized Item Recommendation with Continuous Experience Evolution of Users using Brownian Motion,Subhabrata Mukherjee; Stephan Guennemann; Gerhard Weikum,Abstract Online review communities are dynamic as users join and leave; adopt newvocabulary; and adapt to evolving trends. Recent work has shown that recommendersystems benefit from explicit consideration of user experience. However; prior work assumesa fixed number of discrete experience levels; whereas in reality users gain experience andmature continuously over time. This paper presents a new model that captures thecontinuous evolution of user experience; and the resulting language model in reviews andother posts. Our model is unsupervised and combines principles of Geometric BrownianMotion; Brownian Motion; and Latent Dirichlet Allocation to trace a smooth temporalprogression of user experience and language model respectively. We develop practicalalgorithms for estimating the model parameters from data and for inference with our …,arXiv preprint arXiv:1705.02669,2017,*
People on Media: Jointly Identifying Credible News and Trustworthy Citizen Journalists in Online Communities,Subhabrata Mukherjee; Gerhard Weikum,Zusammenfassung Media seems to have become more partisan; often providing a biasedcoverage of news catering to the interest of specific groups. It is therefore essential to identifycredible information content that provides an objective narrative of an event. Newscommunities such as digg; reddit; or newstrust offer recommendations; reviews; qualityratings; and further insights on journalistic works. However; there is a complex interactionbetween different factors in such online communities: fairness and style of reporting;language clarity and objectivity; topical perspectives (like political viewpoint); expertise andbias of community members; and more. This paper presents a model to systematicallyanalyze the different interactions in a news community between users; news; and sources.We develop a probabilistic graphical model that leverages this joint interaction to identify …,arXiv preprint arXiv:1705.02667,2017,*
Time in Newspaper: A Large-Scale Analysis of Temporal Expressions in News Corpora,Lukas Lange; Jannik Strötgen; Gerhard Weikum,Author: Lange; Lukas et al.; Genre: Thesis; Published in Print: 2017; Title: Time in Newspaper:A Large-Scale Analysis of Temporal Expressions<br/>in News Corpora.,*,2017,*
Mining How-to Task Knowledge From Online Communities,Cuong Xuan Chu; Gerhard Weikum; Niket Tandon; Jilles Vreeken,Abstract Nowadays; knowledge graphs have become a fundamental asset for searchengines which need background commonsense knowledge for natural interactions. A fairamount of user queries seek information on problem-solving tasks such as painting a wall orrepairing a bicycle. While projects like ConceptNet and Webchild have successfullycompiled large amounts of knowledge on properties of objects in our daily life; there is still abig gap regarding knowledge on everyday activities; especially problem-solving tasks (how-to knowledge). Recent efforts to automatically compile commonsense have one or more thefollowing weaknesses:(i) they ignore activity commonsense;(ii) they operate at a smallscale;(iii) their outputs are not semantically organized;(iv) they are domain-specific (egcooking scripts or movie scripts). All of them lack how-to knowledge.,*,2016,*
Towards Nonmonotonic Relational Learning from Knowledge Graphs,Hai Dang Tran; Daria Stepanova; Mohamed H Gad-Elrab; Francesca A Lisi; Gerhard Weikum,Abstract Recent advances in information extraction have led to the so-called knowledgegraphs (KGs); ie; huge collections of relational factual knowledge. Since KGs areautomatically constructed; they are inherently incomplete; thus naturally treated under theOpen World Assumption (OWA). Rule mining techniques have been exploited to support thecrucial task of KG completion. However; these techniques can mine Horn rules; which areinsufficiently expressive to capture exceptions; and might thus make incorrect predictions onmissing links. Recently; a rule-based method for filling in this gap was proposed which;however; applies to a flattened representation of a KG with only unary facts. In this work wemake the first steps towards extending this approach to KGs in their original relational form;and provide preliminary evaluation results on real-world KGs; which demonstrate the …,International Conference on Inductive Logic Programming,2016,*
Boolean Tensor Decomposition based on the Walk’n’Merge Algorithm,Helge Dombrowski; Pauli Miettinen; Gerhard Weikum,Abstract Tensor decomposition is a long standing data mining technique with numerousapplications in different fields. It has been used in image processing; signal processing;computer vision; social network analysis and many more. It'sa useful tool for understandinghigh dimensional data; like classic matrix factorization (eg SVD) for two dimensional data.This work focuses on the decomposition of 3-way Boolean tensors; and explores newalgorithms based on the Walk'n'Merge algorithm by Miettinen and Erdős. The main focus ison the by the original paper introduced FiberGraph; which is a data structure of the fibrestructure of a tensor. The original Paper explored the FiberGraph using random walks. In thiswork; it is explored using several different clustering; walking and partitioning strategies.These were evaluated on a new implementation,*,2016,*
Editorial| Web Semantics: Science; Services and Agents on the World Wide Web-Volumes 37–38,Markus Krötzsch; Gerhard Weikum,*,*,2016,*
Die Abteilung Datenbanken und Informationssysteme am Max-Planck-Institut für Informatik,Gerhard Weikum,Das Leitthema und die strategische Mission der Abteilung ist die maschinelle Erschließ{}ungumfassender Wissensbasen aus Internetquellen; Texten und sozialen Medien. EinComputer; der zum Beispiel den kompletten Inhalt von Wikipedia in einer formalen Wissensrepräsentationkennt und für Sprachverstehen; tiefe Textanalyse und logisches Schließ{}en nutzen kann; wärein der Lage; semantisch präzise Antworten auf komplexe Fragen zu geben; und könnte womöglichsogar die Abiturprüfung in bestimmten Fächern bestehen. Das Langzeitziel wäre; denTuring-Test zu bestehen: Kann sich ein Computer in einer Konversation mit einem menschlichenPartner so verhalten; dass der Gesprächspartner nicht unterscheiden kann; ob er mit einem Menschenoder einer Maschine kommuniziert … Die Abteilung verfolgt dieses Ziel seit 2005 und warVorreiter bei der automatischen Konstruktion formaler Wissensbasen – ein Trend; der …,Datenbank-Spektrum,2016,*
Index problems for game automata,D Hutchison; T Kanad; J Kittler; JM Kleinberg; F Mattern; JC Mitchell; M Naor; C Pandu Rangan; B Steffen; D Terzopoulos; D Tygar; G Weikum; Michał Skrzypczak,*,*,2016,*
Collapse for unambiguous automata,D Hutchison; T Kanad; J Kittler; JM Kleinberg; F Mattern; JC Mitchell; M Naor; CP Rangan; B Steffen; D Terzopoulos; D Tygar; G Weikum; Michał Skrzypczak,*,*,2016,*
Separation for ωB-and ωS-regular Languages,D Hutchison; T Kanade; J Kittler; JM Kleinberg; F Mattern; JC Mitchell; M Naor; C Pandu Rangan; B Steffen; D Terzopoulos; D Tygar; G Weikum; Michał Skrzypczak,*,*,2016,*
When a Thin Language Is Definable in WMSO,D Hutchison; T Kanad; J Kittler; JM Kleinberg; F Mattern; JC Mitchell; M Naor; C Pandu Rangan; B Steffen; D Terzopoulos; D Tygar; G Weikum; Michał Skrzypczak,*,*,2016,*
When a Büchi language is definable in WMSO,D Hutchison; T Kanad; J Kittler; JM Kleinberg; F Mattern; JC Mitchell; M Naor; C Pandu Rangan; B Steffen; D Terzopoulos; D Tygar; G Weikum; Michał Skrzypczak,*,*,2016,*
Basic notions,D Hutchison; T Kanad; J Kittler; JM Kleinberg; F Mattern; JC Mitchell; M Naor; C Pandu Rangan; B Steffen; D Terzopoulos; D Tygar; G Weikum; Michał Skrzypczak,*,*,2016,*
Uniformization on thin trees,D Hutchison; T Kanade; J Kittler; JM Kleinberg; F Mattern; JC Mitchell; M Naor; C Pandu Rangan; B Steffen; D Terzopoulos; D Tygar; G Weikum; Michał Skrzypczak,*,*,2016,*
Descriptive complexity of MSO+ U,D Hutchison; T Kanade; J Kittler; JM Kleinberg; F Mattern; JC Mitchell; M Naor; C Pandu Rangan; B Steffen; D Terzopoulos; D Tygar; G Weikum; Michał Skrzypczak,*,*,2016,*
Undecidability of MSO+ U,D Hutchison; T Kanade; J Kittler; JM Kleinberg; F Mattern; JC Mitchell; M Naor; C Pandu Rangan; B Steffen; D Terzopoulos; D Tygar; G Weikum; Michał Skrzypczak,*,*,2016,*
Recognition by thin algebras,D Hutchison; T Kanad; J Kittler; JM Kleinberg; F Mattern; JC Mitchell; M Naor; C Pandu Rangan; B Steffen; D Terzopoulos; D Tygar; G Weikum; Michał Skrzypczak,*,*,2016,*
Skim: Alternative Candidate Selections for Slim through Sketching,Magnus Halbe; Jilles Vreeken; Gerhard Weikum,Author: Halbe; Magnus et al.; Genre: Thesis; Published in Print: 2016; Title: Skim:Alternative Candidate Selections for Slim through Sketching.,*,2016,*
Squish: Efficiently Summarising Sequences with Rich and Interleaving Patterns,Apratim Bhattacharyya; Jilles Vreeken; Gerhard Weikum,Author: Bhattacharyya; Apratim et al.; Genre: Thesis; Published in Print: 2016; Title: Squish:Efficiently Summarising Sequences with Rich and Interleaving Patterns.,*,2016,*
Towards Summarising Large Transaction Databases,Manan Gandhi; Jilles Vreeken; Gerhard Weikum,Author: Gandhi; Manan et al.; Genre: Thesis; Published in Print: 2016; Title:Towards Summarising Large Transaction Databases.,*,2016,*
An Approach for Ontological Pattern-based Summarization,Kathrin Grosse; Jilles Vreeken; Gerhard Weikum,Author: Grosse; Kathrin et al.; Genre: Thesis; Published in Print: 2016; Title:An Approach for Ontological Pattern-based Summarization.,*,2016,*
Fast Computation of Highest Correlated Segments in Multivariate Time-Series,Amir Hossein Baradaranshahroudi; Jilles Vreeken; Gerhard Weikum,Author: Baradaranshahroudi; Amir Hossein et al.; Genre: Thesis; Published in Print: 2016; Title:Fast Computation of Highest Correlated Segments in Multivariate Time-Series.,*,2016,*
Summarising and Recommending with Skipisodes,Margarita Salyaeva; Jilles Vreeken; Gerhard Weikum,Author: Salyaeva; Margarita et al.; Genre: Thesis; Published in Print: 2016;Title: Summarising and Recommending with Skipisodes.,*,2016,*
Spaghetti: Finding Storylines in Large Collections of Documents,Beata Anna Wójciak; Jilles Vreeken; Gerhard Weikum,Author: Wójciak; Beata Anna et al.; Genre: Thesis; Published in Print: 2016; Title:Spaghetti: Finding Storylines in Large Collections of Documents.,*,2016,*
An Incremental Approach to Distilling Named Events from News Streams,Daniel Spanier; Gerhard Weikum; Vinay Setty,Autor: Spanier; Daniel et al.; Genre: Hochschulschrift; Im Druck veröffentlicht: 2016; Titel: AnIncremental Approach to Distilling Named Events from News Streams.,*,2016,*
Diversified Social Media Retrieval for News Stories,Hang Zhang; Günther Neumann; Gerhard Weikum; Vinay Setty,Autor: Zhang; Hang et al.; Genre: Hochschulschrift; Im Druck veröffentlicht: 2016;Titel: Diversified Social Media Retrieval for News Stories.,*,2016,*
Interactive Boolean Matrix Factorization,Nelson Mukuze; Pauli Miettinen; Gerhard Weikum,Autor: Mukuze; Nelson et al.; Genre: Hochschulschrift; Im Druck veröffentlicht:2016; Titel: Interactive Boolean Matrix Factorization.,*,2016,*
Maximum Entropy Models for Redescription Mining,Janis Kalofolias; Pauli Miettinen; Gerhard Weikum,Autor: Kalofolias; Janis et al.; Genre: Hochschulschrift; Im Druck veröffentlicht: 2016;Titel: Maximum Entropy Models for Redescription Mining.,*,2016,*
Recognizing Visual Activities,Ali Shah; Gerhard Weikum; Klaus Berberich,Autor: Shah; Ali et al.; Genre: Hochschulschrift; Im Druckveröffentlicht: 2016; Titel: Recognizing Visual Activities.,*,2016,*
On Some Problems of Rounding Rank,Stefan Neumann; Pauli Miettinen; Rainer Gemulla; Gerhard Weikum,Abstract This thesis is devoted to the study of the rounding rank problem: Given a binarymatrix and a real number; the rounding threshold; we want to find the real-valued matrix oflowest rank; that after rounding according to the given threshold results in the given inputmatrix. We call this rank the rounding rank.,*,2015,*
Knowledge Bases for Web Content Analytics,Johannes Hoffart; Nicoleta Preda; Fabian M Suchanek; Gerhard Weikum,The proliferation of knowledge-sharing communities such as Wikipedia and the progress inscalable information extraction from Web and text sources has enabled the automaticconstruction of very large knowledge bases (KBs). Recent endeavors of this kind includeacademic research projects such as DBpedia; KnowItAll; Probase; ReadTheWeb; andYAGO; as well as industrial ones such as Freebase; the Google Knowledge Graph;Amazon's Evi; Microsoft's Satori; and related efforts at Bloomberg; Walmart; and others.These projects provide automatically constructed KBs of facts about named entities; theirsemantic classes; and their mutual relationships. They usually contain millions of entitiesand hundreds of millions of facts about them. Such world knowledge in turn enablescognitive applications and knowledge-centric services like disambiguating …,Proceedings of the 24th International Conference on World Wide Web,2015,*
On-topic Cover Stories from News Archives,Christian Schulte; Bilyana Taneva; Gerhard Weikum,Abstract While Web or newspaper archives store large amounts of articles; they also containa lot of near-duplicate information. Examples include articles about the same eventpublished by multiple news agencies or articles about evolving events that lead to copies ofparagraphs to provide background information. To support journalists; who attempt to readall information on a given topic at once; we propose an approach that; given a topic and atext collection; extracts a set of articles with broad coverage of the topic and minimumamount of duplicates. We start by extracting articles related to the input topic and detectingduplicate paragraphs. We keep only one instance from each group of duplicates by using aweighted quadratic optimization problem. It finds the best position for all paragraphs; suchthat some articles consist mainly of distinct paragraphs and others consist mainly of …,European Conference on Information Retrieval,2015,*
Mining European Statistics for Social Events,Mena Srinivasamurthy; Gerhard Weikum; Marc Spaniol,Autor: Srinivasamurthy; Mena et al.; Genre: Hochschulschrift; Im Druck veröffentlicht:2015; Titel: Mining European Statistics for Social Events.,*,2015,*
Mitigation of Privacy Risk for Search Queries,Agim Kopali; Gerhard Weikum; Pauli Miettinen,Autor: Kopali; Agim et al.; Genre: Hochschulschrift; Im Druck veröffentlicht:2015; Titel: Mitigation of Privacy Risk for Search Queries.,*,2015,*
K-Shortest Paths with Overlap Constraints,Kaustuv Chakrabarti; Gerhard Weikum; Vinay Setty,Autor: Chakrabarti; Kaustuv et al.; Genre: Hochschulschrift; Im Druck veröffentlicht:2015; Titel: K-Shortest Paths with Overlap Constraints.,*,2015,*
Information Theoretic Supervised Feature Selection for Continuous Data,Panagiotis Mandros; Gerhard Weikum; Jilles Vreeken,Author: Mandros; Panagiotis et al.; Genre: Thesis; Published in Print: 2015; Title:Information Theoretic Supervised Feature Selection for Continuous Data.,*,2015,*
Question Generation from Knowledge Graphs,Dominic Seyler; Klaus Berberich; Gerhard Weikum,Abstract In this thesis we present a novel approach for generating natural languagequestions; using factual information from a knowledge graph and automatically assessingtheir difficulty. Our work elicits a further utilization of the knowledge captured in knowledgegraphs that could find applications in research; education and leisure. In general; coming upwith question manually can be a resource consuming endeavor. An automatic approach cantherefore provide an alternative that substantially reduces the required effort. Establishedmethods for question generation have used document corpora as their main source ofinformation. However; the utilization of knowledge graphs for this purpose has received farless attention. Furthermore; to the best of our knowledge; no previous work has examinedquestion difficulty in the context of question generation. We specify a framework for a …,*,2015,*
AIDArabic+ Named Entity Disambiguation for Arabic Text,Mohamed Gad-Elrab; Gerhard Weikum; Mohamed Amir Yosef; Klaus Berberich,Abstract Named Entity Disambiguation (NED) is the problem of mapping mentions ofambiguous names in a natural language text onto canonical entities such as people orplaces; registered in a knowledge base. Recent advances in this field enable semanticallyunderstanding content in different types of text. While the problem had been extensivelystudied for the English text; the support for other languages and; in particular; Arabic is still inits infancy. In addition; Arabic web content (eg in the social media) has been exponentiallyincreasing over the last few years. Therefore; we see a great potential for endeavors thatsupport entity-level analytics of these data. AIDArabic is the first work in the direction of usingevidences from both English and Arabic Wikipedia to allow disambiguation of Arabic contentto an automatically generated knowledge base from Wikipedia. The contributions of this …,*,2015,*
EDRAK: Entity-Centric Data Resource for Arabic Knowledge,Mohamed H Gad-Elrab; Mohamed Amir Yosef; Gerhard Weikum,Abstract Online Arabic content is growing very rapidly; with unmatched growth in Arabicstructured resources. Systems that perform standard Natural Language Processing (NLP)tasks such as Named Entity Disambiguation (NED) struggle to deliver decent quality due tothe lack of rich Arabic entity repositories. In this paper; we introduce EDRAK; anautomatically generated comprehensive Arabic entity-centric resource. EDRAK containsmore than two million entities together with their Arabic names and contextual keyphrases.Manual evaluation confirmed the quality of the generated data. We are making EDRAKpublicly available as a valuable resource to help advance research in Arabic NLP and IRtasks such as dictionary-based Named-Entity Recognition; entity classification; and entitysummarization.,Proceedings of the Second Workshop on Arabic Natural Language Processing,2015,*
Methods for the Alignment of Multi-Cultural Knowledge Taxonomies,Natalia Prytkova; Marc Spaniol; Gerhard Weikum,Toggle navigation. HAL: HAL; HALSHS; TEL; MédiHAL; Liste des portails; AURéHAL; API;Documentation. Episciences.org; Sciencesconf.org; Support. Connexion: Connexion; Créerun compte; Mot de passe oublié ? Login oublié ? fr; en. Accueil; Dépôt; Consultation: Lesderniers dépôts; Par type de publication; Par discipline; Par année de publication; Parstructure de recherche; Les portails de l'archive; Les collections. Recherche; Documentation:Tutoriels; Compte et profil: Pourquoi créer un compte et un profil dans HAL; Créer son compteet son profil dans HAL; Modifier son compte ou son profil dans HAL; Modifier son mot depasse; Login ou mot de passe oublié; Les droits associés au profil. Déposer: Avant decommencer; Les types de publication acceptés …,24th World Wide Web Conference (www2015),2015,*
Robust Type Classification of Out of Knowledge Base Entities,Darya Dedik; Gerhard Weikum; Marc Spaniol,Author: Dedik; Darya et al.; Genre: Thesis; Published in Print: 2015; Title: RobustType Classification of Out of Knowledge Base Entities.,*,2015,*
Part-Whole Commonsense Knowledge Harvesting from the Web,Charles Darwis Hariman; Gerhard Weikum,Author: Hariman; Charles Darwis et al.; Genre: Thesis; Published in Print: 2015; Title:Part-Whole Commonsense Knowledge Harvesting from the Web.,*,2015,*
Reasoning Web. Reasoning and the Web in the Big Data Era: 10th International Summer School 2014; Athens; Greece; September 8-13; 2014. Proceedings,Manolis Koubarakis; Giorgos Stamou; Giorgos Stoilos; Ian Horrocks; Phokion Kolaitis; Georg Lausen; Gerhard Weikum,This volume contains the lecture notes of the 10th Reasoning Web Summer School 2014;held in Athens; Greece; in September 2014. In 2014; the lecture program of the ReasoningWeb introduces students to recent advances in big data aspects of semantic web and linkeddata; and the fundamentals of reasoning techniques that can be used to tackle big dataapplications.,*,2014,*
Reasoning Web,Manolis Koubarakis; Giorgos B Stamou; Giorgos Stoilos; Ian Horrocks; Phokion G Kolaitis; Georg Lausen; Gerhard Weikum,Herausgeber: Koubarakis; Manolis et al.; Genre: Konferenzband;Im Druck veröffentlicht: 2014; Titel: Reasoning Web.,10th Reasoning Web Summer School,2014,*
Big Text: von Sprache zu Wissen.,Gerhard Weikum,Abstract: Nachrichten; soziale Medien; Webseiten und Dokumente in Unternehmenbeinhalten eine Fülle wertvoller Informationen in textueller oder gesprochener Form. Einentscheidender Schritt; um diese Form von Big Data für Analytik und Entscheidungsfindungzu nutzen; ist die Identifikation von Entitäten wie Personen; Organisationen oder Produktensowie von Beziehungen zwischen Entitäten. Dieser Schritt ermöglicht auch das Verknüpfenvon strukturierten und unstrukturierten Daten für Big-Data-Anwendungen. Allerdings stehtdieser Schritt vor dem herausfordernden Problem; dass Namen von Entitäten undrelationale Phrasen oft in hohem Maße mehrdeutig sind. Man benötigt eine skalierbare;hochwertige Lösung für die Disambiguierung von Namen in Texten und heterogenenTabellen. Dieser Vortrag diskutiert den Stand der Kunst; Anwendungen und offene …,GI-Jahrestagung,2014,*
Combining information extraction and human computing for crowdsourced knowledge acquisition,S Kumar Kondredi; P Triantafillou; G Weikum,*,*,2014,*
Modeling and Simulation of Soft Tissue Deformation,David Hutchison; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum; Hiroyuki Yoshida; Simon Warfield; Michael W Vannier; Yuping Duan; Weimin Huang; Huibin Chang; Wenyu Chen; Kyaw Kyar Toe; Jiayin Zhou; Tao Yang; Jiang Liu; Soo Kng Teo; Chi Wan Lim; Yi Su; Chee Kong Chui; Stephen Chang,A stable and accurate deformable model to simulate the deformation of soft tissues is achallenging area of research. This paper describes a soft tissue simulation method that candeform multiple organs synchronously and interact with virtual surgical instrumentsaccurately. The model we used in our method is a multi-organ system by point masses andsprings. The organs that anatomically connect to each other are jointed together by highstiffness springs. Here we propose a volume preserved mass-spring model for simulation ofsoft organ deformation. It does not rely on any direct constraint on the volume oftetrahedrons; but rather two constraints on the length of springs and the third constraint onthe direction of springs. To provide reliable interaction between the soft tissues andkinematic instruments we incorporate the position-based attachment to accurately move …,*,2013,*
Image Analysis and Recognition: 10th International Conference; ICIAR; Aveiro; Portugal; June 26-28; 2013; Proceedings,Mohamed Kamel; Aurélio Campilho,This book constitutes the thoroughly refereed proceedings of the 10th InternationalConference on Image Analysis and Recognition; ICIAR 2013; held in Póvoa do Varzim;Portugal; in June 2013; The 92 revised full papers presented were carefully reviewed andselected from 177 submissions. The papers are organized in topical sections on biometrics:behavioral; biometrics: physiological; classification and regression; object recognition;image processing and analysis: representations and models; compression; enhancement;feature detection and segmentation; 3D image analysis; tracking; medical imaging: imagesegmentation; image registration; image analysis; coronary image analysis; retinal imageanalysis; computer aided diagnosis; brain image analysis; cell image analysis; RGB-Dcamera applications; methods of moments; applications.,*,2013,*
Transactions on Petri Nets and Other Models of Concurrency VII,Kurt Jensen; Wil MP Van der Aalst; Gianfranco Balbo; Maciej Koutny; Karsten Wolf,These Transactions publish archival papers in the broad area of Petri nets and other modelsof concurrency; ranging from theoretical work to tool support and industrial applications.ToPNoC issues are published as LNCS volumes; and hence are widely distributed andindexed. This Journal has its own Editorial Board which selects papers based on a rigoroustwo-stage refereeing process. ToPNoC contains:-Revised versions of a selection of the bestpapers from workshops and tutorials at the annual Petri net conferences-Specialsections/issues within particular subareas (similar to those published in the Advances inPetri Nets series)-Other papers invited for publication in ToPNoC-Papers submitted directlyto ToPNoC by their authors The 7th volume of ToPNoC contains revised material from the5th International Summer School “Advanced Course on Petri Nets”; held in September …,*,2013,*
Faculty of Natural Sciences and Technology I Department of Computer Science,Razvan Belet,ABSTRACT With the emergence of big data; inducting regression trees on very large datasets became a common data mining task. Even though centralized algorithms for computingensembles of Classification/Regression trees are a well studied machine learning/datamining problem; their distributed versions still raise scalability; efficiency and accuracyissues. Most state of the art tree learning algorithms require data to reside in memory on asingle machine. Adopting this approach for trees on big data is not feasible as the limitedresources provided by only one machine lead to scalability problems. While more scalableimplementations of tree learning algorithms have been proposed; they typically requirespecialized parallel computing architectures rendering those algorithms complex and error-prone.,*,2013,*
EIN SUBSYSTEM ZUR STABILEN SPEICHERUNG VERSIONENBEHAFTETER,U Deppisch; V Obermeit; HB Paul; HJ Schek; M Scholl; G Weikum,KURZFASSUNG Das hier beschriebene Speichersystem unseres DarmstädterDatenbankkernsystems kann als Erweiterung der Dateiverwaltung von Betriebssystemenum Grundfunktionen eines Datenbanksystems betrachtet werden: Es dient der stabilenSpeicherung von strukturierten Datensätzen unter Einschluß einer Transaktionsverwaltungund optionaler Versionierung. Zur Strukturierung werden ausschließlich Relationen mitatomaren oder relationenwertigen Attributen zugelassen. Zur Manipulation erlauben wir imwesentlichen eine geschachtelte Projektion und mengenorientierte Änderungsoperationen.Eine solche Schnittstelle relativ weit unten in der Systemarchitektur erlaubt es; gezielt aufeinzelne Daten und gleichzeitig effizient auf Datenmengen zuzugreifen. Dementsprechendist die direkt darunterliegende Pufferschnittstelle (seiten-) mengenorientiert angelegt. Für …,Datenbank-Systeme für Büro; Technik und Wissenschaft: GI-Fachtagung; Karlsruhe; 20.–22. März 1985 Proceedings,2013,*
Automatische Übersetzung von,David Christensen; Achim Kraiß; Anja Syri; Gerhard Weikum,Schon dieser erste Überblick macht deutlich; wie wichtig es ist; Übersetzer bereitzustellen;die eine möglichst automatische Transformation der GP-Modelle in eine für das WfMSverständliche Spezifikation unterstützen. Ziemlich schnell wird allerdings auch klar; welcheProbleme bei der Transformation auftauchen. Beide Modellwelten unterscheiden sichinsbesondere in Hinblick auf „Inhalt und Zweck “(JBS97). Zum einen adressieren dieWerkzeuge unterschiedliche Benutzerkreise. Dies zeigt sich direkt in den angebotenenModellkonstrukten und deren Benennung und Semantik. Während das GP-Modell eineumgangssprachliche Spezifikation der Arbeitsschritte und des Ablaufes zulässt; müssen dieim Wf-Modell berücksichtigten Komponenten sehr viel formaler und detaillierter spezifiziertwerden; um eine spätere Automatisierung der Wf-Instanzen zu ermöglichen.,Datenbanksysteme in Büro; Technik und Wissenschaft: 9. GI-Fachtagung Oldenburg; 7.-9. März 2001,2013,*
Leveraging Independence and Locality for Random Forests in a Distributed Environment,Razvan Belet; Gerhard Weikum; Ralf Schenkel,Abstract With the emergence of big data; inducting regression trees on very large data setsbecame a common data mining task. Even though centralized algorithms for computingensembles of Classification/Regression trees are a well studied machine learning/datamining problem; their distributed versions still raise scalability; efficiency and accuracyissues. Most state of the art tree learning algorithms require data to reside in memory on asingle machine. Adopting this approach for trees on big data is not feasible as the limitedresources provided by only one machine lead to scalability problems. While more scalableimplementations of tree learning algorithms have been proposed; they typically requirespecialized parallel computing architectures rendering those algorithms complex and error-prone. In this thesis we will introduce two approaches to computing ensembles of …,*,2013,*
Analyzing and Creating Top-k Entity Rankings,Evica Ilieva; Sebastian Michel; Gerhard Weikum,Autor: Ilieva; Evica et al.; Genre: Hochschulschrift; Im Druck veröffentlicht:2013; Titel: Analyzing and Creating Top-k Entity Rankings.,*,2013,*
Design and Evaluation of an IR-Benchmark for SPARQL Fulltext Queries,Arunav Mishra; Martin Theobald; Gerhard Weikum,Abstract In this thesis; we design a new IR-benchmark that aims to bridge the prevailing gapbetween traditional keyword-based retrieval techniques and semantic web-based retrievaltechniques. We present a unique; entity-centric data collection; coined Wikipedia-LOD; thataims to combine the benefits of both text-oriented and structured retrieval settings. Thiscollection combines RDF data from DBpedia and YAGO2 structured Knowledge Bases(KBs); and textual data from the contents Wikipedia articles into XML-ified documents; calledthe Wiki-XML documents; corresponding to every Wikipedia entity. To evaluate such acollection; we introduce a new query format; called SPARQL-fulltext (SPARQL-FT) queries.We design the SPARQL-FT query format by extending the W3C standard SPARQL withadditional FTContains operator that constraints an entity by a set of keywords …,*,2013,*
The 3 rd Temporal Web Analytics Workshop (TempWeb),Ricardo Baeza-Yates; Julien Masanès; Marc Spaniol; Omar Alonso; Ralitsa Angelova; Srikanta Bedathur; Andras A Benczur; Klaus Berberich; Roi Blanco; Philipp Cimiano; Renata Galante; Adam Jatowt; Scott Kirkpatrick; Frank McCown; Michael Nelson; Kjetil Nørvåg; Nikos Ntarmos; Rodrygo Luis Teodoro Santos; Philippe Rigaux; Thomas Risse; Torsten Suel; Masashi Toyoda; Gerhard Weikum,Supporters: European Union 7th Framework IST programme STREP (contract no. 258105),*,2013,*
Data Discovery,Gerhard Weikum,Discovery of documents; data sources; facts; and opinions is at the very heart of digitalinformation and knowledge services. Being able to search; discover; compile; and analyserelevant information for a user's specific tasks is of utmost importance in science (eg;computational life sciences; digital humanities; etc.); business (eg; market and mediaanalytics; customer relationship management; etc.); and society at large (eg; consumerinformation; traffic logistics; health discussions; etc.). Traditionally; information discovery isbased on search engines; which in turn have mostly focused on finding documents ofvarious kinds: publications; Web pages; news articles; etc. Search engine technology hasbeen developed for both Internet/Web search and; with somewhat different requirements;enterprise search within companies and intranets of organisations. Digital library services …,Data Science Journal,2013,*
D-hive: Data bees pollinating RDF and text,S Bedathur; K Berberich; Y Patlakas; P Triantafillou; G Weikum,*,*,2013,*
HIGGINS: where knowledge acquisition meets the crowds,S Kumar Kondreddi; Peter Triantafillou; Gerhard Weikum,We present HIGGINS; an engine for high quality Knowl-edge Acquisition (KA); placingspecial emphasis on its ar-chitecture. The distinguishing characteristic and novelty ofHIGGINS lies in its special blending of two engines: An automated Information Extraction (IE)engine; aided by semantic resources; and a game-based; Human Computing engine (HC).We focus on KA from web data and text sources and; in particular; on deriving relationshipsbetween enti-ties. As a running application we utilise movie narratives; using which we wishto derive relationships among movie characters.,*,2013,*
Design and Implementation of a Multidimensional Query Interface for Large-Scale Data Stored in HBase,Behrang Zeini; Marc Spaniol; Gerhard Weikum,Abstract Comparing and evaluating the development of web data based on a time line isnotoriously difficult. Despite this it is of increasing practical importance. In this thesis wepresent a newly developed software to aid timeline analysis. We discuss a novel approachto easily search large amounts of data. Furthermore it is our goal to rise the analysis of large-scale data from a URL-driven search to the entity level. To process such a search the userinterface for large-scale data search needs to be multidimensional. It also needs to presentsearch queries for the entity; type; timeframe; etc. of any data sought after. The data will berepresented in an HBase data base to enable us easy access and short average accesstimes.,*,2012,*
Redundancy Control in Web Archives,Bibek Paudel; Klaus Berberich; Avishek Anand; Gerhard Weikum,Abstract Large scale text collections like web archives evolve over time. However; theaddition of new documents does not always add novel content; but also introduces contentsthat are copied; enriched; or recompiled from already existing documents. Thus; suchcollections are characterized by a lot of redundant content. Redundant documents wastestorage space; make content analysis difficult and decrease the quality of search results.Although existing duplicate detection systems are able to identify content shared acrossdocuments; they are not enough for all user requirements and application scenarios. Wewould like to give users control over how redundancy is defined. In this work; we propose asolution to systematically remove such documents from the collection whose content issufficiently covered by other documents that; in addition; meet other user-specified …,*,2012,*
Towards an Architecture for Open-Domain Information Extraction: Integrated Extraction; Clustering; and Reasoning with Patterns,Artem Boldyrev; Martin Theobald; Gerhard Weikum,Abstract In this thesis; we present an integrated approach for automatic knowledgeextraction from unstructured sources. We combine occurrences extraction; pattern clustering;and probabilistic reasoning in a single pipeline. The occurrence extraction step subsumesstatistical pattern mining; which is widely used in information extraction. Moreover; in thisstep; a pattern lifting and an entity disambiguation are performed. The pattern lifting replacessome predefining parts of speech by their word classes. For pattern clustering we use matrixfactorization and facts from a knowledge base such as YAGO. The distributed stochasticgradient descent algorithm for matrix factorization enables large scale information extraction.We use the output of the matrix factorization to find a high quality mapping from patterns torelations. In the probabilistic reasoning step; we introduce new rules as follows. A pattern …,*,2012,*
Methods and Models for Web Archive Crawling,Dimitar Denev; Gerhard Weikum,Zusammenfassung Web archives offer a rich and plentiful source of information toresearchers; analysts; and legal experts. For this purpose; they gather Web sites as the siteschange over time. In order to keep up to high standards of data quality; Web archives haveto collect all versions of the Web sites. Due to limited resuources and technical constraintsthis is not possible. Therefore; Web archives consist of versions archived at various timepoints without guarantee for mutual consistency. This thesis presents a model for assessingthe data quality in Web archives as well as a family of crawling strategies yielding high-quality captures. We distinguish between single-visit crawling strategies for exploratory andvisit-revisit crawling strategies for evidentiary purposes. Single-visit strategies downloadevery page exactly once aiming for an``undistorted''capture of the ever-changing Web …,*,2012,*
Modelling and Evaluation of Co-Evolution in Collective Web Memories,Natalia Prytkova; Gerhard Weikum; Marc Spaniol,Abstract The constantly evolving Web reflects the evolution of society in the cyberspace.Projects like the Open Directory Project (dmoz. org) can be understood as a collectivememory of society on the Web. The main assumption is that such collective Web memoriesevolve when a certain cognition level about a concept has been exceeded. In the scope ofour work we analyse the New York Times archive for concept detection. There are severalapproaches to the concept modelling. We introduce an alternative model for concepts; whichdoes not make any additional assumptions about types of contained entities or the numberof entities in the corpus. Moreover; the proposed distributed concept computation algorithmenables the large scale archive analysis. We also introduce a model of cognition level andexplain how it can be employed to predict changes in the category system of DMOZ.,*,2011,*
Keynote: for a few triples more,Gerhard Weikum,Abstract The Web of Linked Data contains about 25 billion RDF triples and almost half abillion links across data sources; it is becoming a great asset for semantic applications.Linked Data comprises large general-purpose knowledge bases like DBpedia; Yago; andFreebase; as well as many reference collections in a wide variety of areas; spanningsciences; culture; entertainment; and more. Notwithstanding the great potential of LinkedData; this talk argues that there are significant limitations that need to be overcome forfurther progress. These limitations regard data scope and; especially; data quality. The talkdiscuss these issues and approaches to extending and enriching Linked Data; in order toimprove its scope; quality; interpretability; cross-linking; and usefulness.,Proceedings of the 10th international conference on The semantic web-Volume Part II,2011,*
Relations and Kleene algebras in computer science,Rudolf Berghammer; Ali Mohamed Jaoua; Bernhard Möller,*,*,2011,*
Guest Editors' Introduction to the Special Section on the 26th International Conference on Data Engineering,Shahram Ghandeharizadeh; Jayant R Haritsa; Gerhard Weikum,THE 26th International Conference on Data Engineering; ICDE 2010; was held in LongBeach; California; during 1-6 March 2010. A program committee of 230 members evaluatedthe 523 research manuscripts submitted to the research track of ICDE; producing anoutstanding technical program consisting of 69 full and 41 short research papers. Thesepapers covered diverse topics ranging from data clouds to social networks and location-based services. The technical program also included industrial sessions; panels; demos;and tutorials. There were three thought-provoking keynote addresses; by Richard Winterand Pekka Kostamaa on Large Scale Data Warehousing: Trends and Observations; JefferyNaughton on Lessons from the First 50 Years; Speculations for the Next 40; and DonaldKossmann on How New Is the Cloud?,IEEE Transactions on Knowledge and Data Engineering,2011,*
Context-Aware Timeline for Entity Exploration,Anh Tuan Tran; Gerhard Weikum; Nicoleta Preda; Shady Elbassuoni; Martin Theobald,Abstract With millions of articles in multiple languages; Wikipedia has become the de-factosource of reference on the Internet today. Each article on Wikipedia contains encyclopedicinformation about various topics (people; events; inventions; etc.) and implicitly representsan entity. Extracting the most important facts about such entity will help users to find desiredinformation more quickly and effectively. However; this task is challenging due to theincomplete and noisy nature of Wikipedia articles. This calls for a mechanism to detect andsummarize the most important information about an entity on Wikipedia. This thesisproposes and implements CATE (Context-Aware Timeline for Entity Exploration); aframework that utilizes Wikipedia to summarize and visualize the important aspects ofentities in a timeline fashion. Such a system will help users to draw quickly an informative …,*,2011,*
Distributed Analytics over Web Archives,Yagiz Kargin; Srikanta Bedathur; Avishek Anand; Gerhard Weikum,Abstract Evolving content of the Web is being accumulated over time into Web archivalcollections. This creates the need for time travel search to explore the dynamics of thecontent. Text analytics has also a key role in exploring interesting information in textcollections. Moreover; frequent phrase mining; a special case of text analytics; is animportant analytical task that is motivated by the need of knowledge on frequent phrases invarious areas of computer science; such as information retrieval and machine translationetc. However; time travel search and frequent phrase mining have to be conducted onincreasingly large-scale data. Distributed approaches such as MapReduce; which is mainlydesigned to work on vast amount of text; can be utilized in this case. We address twoseparate problems in this thesis. The first problem is that time travel inverted index; which …,*,2011,*
Proceedings of the 20th ACM Conference on Information and Knowledge Management,Craig Macdonald; Iadh Ounis; Ian Ruthven,CIKM 2011 took place in Glasgow; Scotland; UK; 24th-28th October 2011. Glasgow isScotland's largest city and one of the most visited cities in Europe. A cosmopolitanmetropolis; Glasgow is a culturally rich; vibrant night time city with a long history at theforefront of socio-economic and political change in Scotland and the UK; offering everythingone would expect from a great British city but with a Scottish flare. Since 1992; the ACMConference on Information and Knowledge Management (CIKM) has successfully broughttogether leading researchers and developers from the database; information retrieval; andknowledge management communities. The purpose of the conference is to identifychallenging problems facing the development of future knowledge and information systems;and to shape future research directions through the publication of high quality; applied …,*,2011,*
of Proceedings: The Semantic Web-ISWC 2011: 10th International Semantic Web Conference,Andreas Schwarte; Peter Haase; Katja Hose; Ralf Schenkel; Michael Schmidt,Document title: FedX: Optimization Techniques for Federated Query Processing on Linked DataAuthors: Schwarte; Andreas; Haase; Peter; Hose; Katja; Schenkel; Ralf; Schmidt; MichaelDocument type: Conference-Paper Language: English Audience: Experts Only Title of Series:Lecture Notes in Computer Science External Publication Status: published Title of Proceedings:The Semantic Web - ISWC 2011 : 10th International Semantic Web Conference Place ofConference/Meeting: Bonn; Germany Full Name(s) of Editor(s) of Proceedings: Aroyo; Lora; Welty;Chris; Alani; Harith; Taylor; Jamie; Bernstein; Abraham; Kagal; Lalana; Noy; Natasha; Blomqvist;Eva Place of Publication: Berlin Volume (in Journal): 7031 Last Change of the Resource(YYYY-MM-DD): 2012-03-22 (Start) Date of Conference/Meeting (YYYY-MM-DD): 2011-10-23Date of Publication (YYYY-MM-DD): 2011 End Date of Conference/Meeting (YYYY …,*,2011,*
of Proceedings: Enabling Real-Time Business Intelligence: 4th International Workshop; BIRTE 2010,Gerhard Weikum; Srikanta Bedathur; Ralf Schenkel,Abstract/Description: Knowledge bases about entities and their relationships are a greatasset for business intelligence. Major advances in information extraction and theproliferation of knowledge-sharing communities like Wikipedia have enabled ways for thelargely automated construction of rich knowledge bases. Such knowledge about entity-oriented facts can greatly improve the output quality and possibly also efficiency ofprocessing business-relevant documents and event logs. This holds for information withinthe enterprise as well as in Web communities such as blogs. However; no knowledge basewill ever be fully complete and real-world knowledge is continuously changing: new factssupersede old facts; knowledge grows in various dimensions; and completely new classes;relation types; or knowledge structures will arise. This leads to a number of difficult …,*,2011,*
Semantic Knowledge Bases from Web Sources,Fabian Suchanek; Martin Theobald; Gerhard Weikum; Hady Lauw; Ralf Schenkel,The Web bears the potential of being the world's greatest encyclopedic source; but we arefar from fully ex-ploiting this potential. Valuable scientific and cultural content is interspersedwith a huge amount of noisy; low-quality; unstructured text and media. The proliferation ofknowledge-sharing communities like Wikipedia and the advances in automated informationextraction from Web pages give rise to an unprecedented opportunity: Can wesystematically harvest facts from the Web and compile them into a comprehensive machine-readable knowledge base? Such a knowledge base would contain not only the world'sentities; but also their semantic properties; and their relationships with each other. Imagine a“Structured Wikipedia” that has the same scale and richness as Wikipedia itself; but offers aprecise and concise representation of knowledge; eg; in the RDF format. This would …,IJCAI,2011,*
of Proceedings: Reasoning Web: Semantic Technologies for the Web of Data; 7th International Summer School 2011,Katja Hose; Ralf Schenkel; Martin Theobald; Gerhard Weikum,*,*,2011,*
of Proceedings: The Semanic Web: Research and Applications: 8th Extended Semantic Web Conference; ESWC 2011,Andreas Schwarte; Peter Haase; Katja Hose; Ralf Schenkel; Michael Schmidt,Abstract/Description: Driven by the success of the Linked Open Data initiative today'sSemantic Web is best characterized as a Web of interlinked datasets. Hand in hand with thisstructure new challenges to query processing are arising. Especially queries for which morethan one data source can contribute results require advanced optimization and evaluationapproaches; the major challenge lying in the nature of distribution: Heterogenous datasources have to be integrated into a federation to globally appear as a single repository. Onthe query level; though; techniques have to be developed to meet the requirements ofefficient query computation in the distributed setting. We present FedX; a project whichextends the Sesame Framework with a federation layer that enables ef-ficient queryprocessing on distributed Linked Open Data sources. We discuss key insights to its …,*,2011,*
26th International Conference on Data Engineering· ICDE 2010,Man Lung Yiu; Yimin Lin; Kyriakos Mouratidis,Shortest path search in transportation networks is unarguably one of the most importantonline search services nowadays (eg; Google Maps; MapQuest; etc); with applicationsspanning logistics; spatial optimization; or everyday driving decisions. Often times; theowner of the road network data (eg; a transport authority) provides its database to third-partyquery services; which are responsible for answering shortest path queries posed by theirclients. The issue arising here is that a query service might be returning sub-optimal pathseither purposely (in order to serve its own purposes like computational savings orcommercial reasons) or because it has been compromised by Internet attackers who falsifythe results. Therefore; for the above applications to succeed; it is essential that eachreported path is accompanied by a proof; which allows clients to verify the path's …,Proceedings of the 26th International Conference on Data Engineering,2010,*
Accelerating Rule-Based Reasoning in Disk-Resident RDF Knowledge Bases,Mohamed Yahya; Martin Theobald; Gerhard Weikum,Abstract Collections of tens of millions of automatically extracted facts represented using thesubject-predicate-object RDF model are available for several domains. As big as thesecollections are; they are unable to capture all information about a domain; simply becausethe sources from which they were extracted are incomplete. This can be tackled by creatingknowledge bases where facts are enforced with rules showing how new facts can begenerated from existing ones and constraints which must hold in the relevant domain.Querying such knowledge bases is expensive for two main reasons. First; data is diskresident; which makes access to it slow. Secondly; rule definitions can be recursive; whichrequires special query evaluation techniques and renders traditional cost-based queryoptimization and join-ordering techniques less effective. This thesis presents the …,*,2010,*
Evolutionary Computation; Machine Learning and Data Mining in Bioinformatics: 8th European Conference; EvoBIO 2010; Istanbul; Turkey; April 7-9; 2010; Proceedi...,Clara Pizzuti; Marylyn D Ritchie; Mario Giacobini,The field of bioinformatics has two main objectives: the creation and maintenance ofbiological databases; and the discovery of knowledge from life sciences data in order tounravel the mysteries of biological function; leading to new drugs and therapies for humandisease. Life sciences data come in the form of biological sequences; structures; pathways;or literature. One major aspect of discovering biological knowledge is to search; predict; ormodel specific information in a given dataset in order to generate new interestingknowledge. Computer science methods such as evolutionary computation; machinelearning; and data mining all have a great deal to offer the field of bioinformatics. The goal ofthe 8th European Conference on Evolutionary Computation; Machine Learning; and DataMining in Bioinformatics (EvoBIO 2010) was to bring together experts in these fields in …,*,2010,*
Knowledge on the web: robust and scalable harvesting of entity-relationship facts,Gerhard Weikum,Abstract The proliferation of knowledge-sharing communities like Wikipedia and theadvances in automatic information extraction from semistructured and textual Web data haveenabled the construction of very large knowledge bases. These knowledge collectionscontain facts about many millions of entities and relationships between them; and can beconveniently represented in the RDF data model. Prominent examples are DBpedia; YAGO;Freebase; Trueknowledge; and others. These structured knowledge collections can beviewed as “Semantic Wikipedia Databases”; and they can answer many advancedquestions by SPARQL-like query languages and appropriate ranking models. In addition;the knowledge bases can boost the semantic capabilities and precision of entity-orientedWeb search; and they are enablers for value-added knowledge services and applications …,International Conference on Database Systems for Advanced Applications,2010,*
Fast Distributed Replication in Modern Networks,Faraz Makari Manshadi; Gerhard Weikum; Mauro Sozio,Autor: Makari Manshadi; Faraz et al.; Genre: Hochschulschrift; Im Druck veröffentlicht:2010; Titel: Fast Distributed Replication in Modern Networks.,*,2010,*
Implementation and evaluation of an efficient; distributed replication algorithm in a real network,Luis de la Garza; Gerhard Weikum; Mauro Sozio,Autor: de la Garza; Luis et al.; Genre: Hochschulschrift; Im Druck veröffentlicht: 2010; Titel:Implementation and evaluation of an efficient; distributed replication algorithm in a real network.,*,2010,*
Efficiently Identifying Interesting Time-Points in Text Archive Search,Vinay Setty; Gerhard Weikum; Srikanta Bedathur,Autor: Setty; Vinay et al.; Genre: Hochschulschrift; Im Druck veröffentlicht: 2010; Titel:Efficiently Identifying Interesting Time-Points in Text Archive Search.,*,2010,*
of Proceedings: Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),Lizhen Qu; Georgiana Ifrim; Gerhard Weikum,Document title: The Bag-of-Opinions Method for Review Rating Prediction from Sparse TextPatterns Authors: Qu; Lizhen; Ifrim; Georgiana; Weikum; Gerhard Document type:Conference-Paper Language: English Audience: Experts Only Title of Series: ACL AnthologyExternal Publication Status: published Title of Proceedings: Proceedings of the 23rd InternationalConference on Computational Linguistics (Coling 2010) Place of Conference/Meeting: Beijing;China Full Name(s) of Editor(s) of Proceedings: Huang; Chu-Ren; Jurafsky; Dan Place ofPublication: Beijing; China Last Change of the Resource (YYYY-MM-DD): 2011-03-03 (Start)Date of Conference/Meeting (YYYY-MM-DD): 2011-03-01 Date of Publication (YYYY-MM-DD):2010 End Date of Conference/Meeting (YYYY-MM-DD): 2011-03-01 Intended Educational Use:No Publisher: Tsinghua University Press Communicated by: Gerhard Weikum Affiliations …,*,2010,*
Query Evaluation with Asymmetric Web Services,Nicoleta Preda; Fabian Suchanek; Wenjun Yuan; Gerhard Weikum,Motivation. Recent projects such as DBpedia [3]; YAGO [28; 18]; freebase. com;wolframalpha. com; trueknowledge. com; KnowItAll [4]; and others have successfullyconstructed semantic knowledge bases of large scale. Factual knowledge is typicallyrepresented in RDF; the W3C standard for Semantic-Web contents. RDF data can be seenas a graph whose nodes are entities (eg; persons; companies; movies; locations) andwhose edges are relationships (eg; bornOnDate; isCEOof; actedIn). These knowledgebases can be queried using the W3C-endorsed SPARQL [33] language. Yet; a knowledgebase about entities can never be fully complete or always up to date. With the ANGIE system[23]; we have shown that Web services can step in to fill this gap. Web services lendthemselves to the extension of knowledge bases; because they deliver structured data …,*,2010,*
of Proceedings: Database Systems for Advanced Applications: 15th International Conference; DASFAA 2010.-Pt. I,Gerhard Weikum,*,*,2010,*
Transactions on Computational Science IV,Marina L Gavrilova CJ Kenneth Tan; Edward David Moreno,Computational science; an emerging and increasingly vital field; is now widely recognizedas an integral part of scientific and technical investigations; affecting researchers andpractitioners in areas ranging from aerospace and automotive research to biochemistry;electronics; geosciences; mathematics; and physics. Computer systems research and theexploitation of applied research naturally complement each other. The increased complexityof many challenges in computational science demands the use of supercomputing; parallelprocessing; sophisticated algorithms; and advanced system software and architecture. It istherefore invaluable to have input by systems research experts in applied computationalscience research. Transactions on Computational Science focuses on original high-qualityresearch in the realm of computational science in parallel and distributed environments …,*,2009,*
Bridging the Terminology Gap in Web Archive Search,Klaus Berberich Srikanta Bedathur Mauro Sozio; Gerhard Weikum,ABSTRACT Web archives play an important role in preserving our cultural heritage for futuregenerations. When searching them; a serious problem arises from the fact that terminologyevolves constantly. Since today's users formulate queries using current terminology; old butrelevant documents are often not retrieved. The query saint petersburg museum; forinstance; does not retrieve documents from the 1970s about museums in Leningrad (theformer name of Saint Petersburg). We address this problem by determining queryreformulations that paraphrase the user's information need using terminology prevalent inthe past. A measure of across-time semantic similarity that assesses the degree ofrelatedness between two terms when used at different times is proposed. Using thismeasure as a crucial building block; we propose a novel query reformulation technique …,Max-Planck Institute for Informatics Saarbrucken; Germany; Twelfth InternationalWorkshop on theWeb and Databases (WebDB 2009),2009,*
Language-model-based ranking in entity-relation graphs,Shady Elbassuoni; Maya Ramanath; Gerhard Weikum,Abstract We propose a language-model-based ranking approach for SPARQL-like querieson entity-relationship graphs. Our ranking model supports exact matching; approximatestructure matching; and approximate matching with text predicates. We show theeffectiveness of our model through examples.,Proceedings of the First International Workshop on Keyword Search on Structured Data,2009,*
Rewriting Techniques and Applications: 20th International Conference; RTA 2009; Brasília; Brazil; June 29-July 1; 2009 Proceedings,Ralf Treinen,This volume contains the papers of the 20th International Conference on RewritingTechniques and Applications (RTA 2009); which was held from June 29 to July 1; 2009; inBrasılia; Brazil as part of the 5th International Conference on Rewriting; Deduction; andProgramming (RDP 2009) together with the International Conference on Typed LambdaCalculi and Applications (TLCA 2009); the International School on Rewriting (ISR 2009); the4th Workshop on Logical and Semantic Frameworks with Applications (LSFA 2009); the 10thInternational Workshop on Rule-Based Programming (RULE 2009); the 8th InternationalWorkshop on Functional and (Constraint) Logic Programming (WFLP 2009); the 9thInternational Workshop on Reduction Strategies in Rewriting and Programming (WRS2009); and the annual meeting of the IFIP Working Group 1.6 on term rewriting. RTA is the …,*,2009,*
Advances in Information Security and Assurance: Third International Conference and Workshops; ISA 2009; Seoul; Korea; June 25-27; 2009. Proceedings,Hsiao-Hwa Chen; Mohammed Atiquzzaman; Changhoon Lee; Sang-Soo Yeo,Welcome to the Third International Conference on Information Security and Ass-ance (ISA2009). ISA 2009 was the most comprehensive conference focused on the various aspects ofadvances in information security and assurance. The concept of security and assurance isemerging rapidly as an exciting new paradigm to provide reliable and safe life services. Ourconference provides a chance for academic and industry professionals to discuss recentprogress in the area of communication and networking including modeling; simulation andnovel applications associated with the utilization and acceptance of computing devices andsystems. ISA 2009 was a succ-sor of the First International Workshop on InformationAssurance in Networks (IAN 2007; Jeju-island; Korea; December; 2007); and the SecondInternational Conference on Information Security and Assurance (ISA 2008; Busan; Korea …,*,2009,*
Advances in Self-Organizing Maps: 7th International Workshop; WSOM 2009; St. Augustine; Florida; June 8-10; 2009. Proceedings,José C Príncipe; Risto Miikkulainen,th These proceedings contain refereed papers presented at the 7 WSOM held at the CasaMonica Hotel; St. Augustine; Florida; June 8–10; 2009. We designed the wo-shop to serveas a regular forum for researchers in academia and industry who are interested in theexciting field of self-organizing maps (SOM). The program includes excellent examples ofthe use of SOM in many areas of social sciences; economics; computational biology;engineering; time series analysis; data visualization and c-puter science as well a vibrant setof theoretical papers that keep pushing the envelope of the original SOM. Our deepappreciation is extended to Teuvo Kohonen and Ping Li for the plenary talks and AmauryLendasse for the organization of the special sessions. Our sincere thanks go to the membersof the Technical Committee and other reviewers for their excellent and timely reviews …,*,2009,*
Trustworthy Software Development Processes: International Conference on Software Process; ICSP 2009 Vancouver; Canada; May 16-17; 2009 Proceedings,Qing Wang; Vahid Garousi; Raymond Madachy; Dietmar Pfahl,This volume contains papers presented at the International Conference on Software Process(ICSP 2009) held in Vancouver; Canada; during May 16-17; 2009. ICSP 2009 was the thirdconference of the ICSP series; continuing the software process workshops from 25 yearsago. The theme of ICSP 2009 was “Processes to Develop Trustworthy Software.” Softwaredevelopment takes place in a dynamic context of frequently changing technologies andlimited resources. Teams worldwide are under increasing pressure to deliver trustworthysoftware products more quickly and with higher levels of quality. At the same time; globalcompetition is forcing software development organizations to cut costs by rationalizingprocesses; outsourcing part or all of their activities; re-ing existing software in new ormodified applications and evolving existing systems to meet new needs; while still …,*,2009,*
NETWORKING 2009: 8th International IFIP-TC 6 Networking Conference; Aachen; Germany; May 11-15; 2009; Proceedings,Luigi Fratta; Henning Schulzrinne; Yutaka Takahashi; Otto Spaniol,“What a di? erence a year makes–52 little weeks” This variation of the? rst line from DinahWashington's famous song; which originally reads;“What a di? erence a day makes-24 littlehours;” brings it to the point: Accordingtoallexperts; thepress; andmostpeople'simpressionwearetoday in a serious economic recession. Less than one year ago; wepractically lived on the “island of the blessed”(namely; at Networking 2008 that was held onthe island of Singapore); or in the famous country where “milk and honey? ow”(or “wherewine and liquor? ow”). This convenient situation has changed abruptly within less than 52weeks. It looks like the same kind of problems has emerged in all areas–and the“Networking” area has; of course; been a? ected; too. Looking into the 2009 proceedings;however; you will immediately notice that the manuscripts are largely una? ected by any …,*,2009,*
Hybrid Systems: Computation and Control: 12th International Conference; HSCC 2009; San Francisco; CA; USA; April 13-15; 2009; Proceedings,Rupak Majumdar; Paulo Tabuada,This book constitutes the refereed proceedings of the 12th International Conference onHybrid Systems: Computation and Control; HSCC 2009; held in San Francisco; CA; USA; inApril 2009. The 30 revised full papers and 10 revised short papers presented were carefullyreviewed and selected from numerous submissions for inclusion in the book. The papersfocus on research in embedded reactive systems involving the interplay betweensymbolic/discrete and continuous dynamical behaviors and feature the latest developmentsof applications and theoretical advancements in the analysis; design; control; optimization;and implementation of hybrid systems.,*,2009,*
Topics in Cryptology-CT-RSA 2009: The Cryptographers' Track at the RSA Conference 2009; San Francisco; CA; USA; April 20-24; 2009; Proceedings,Marc Fischlin,This book constitutes the refereed proceedings of the Cryptographers' Track at the RSAConference 2009; CT-RSA 2009; held in San Francisco; CA; USA in April 2009. The 31revised full papers presented were carefully reviewed and selected from 93 submissions.The papers are organized in topical sections on identity-based encryption; protocol analysis;two-party protocols; more than signatures; collisions for hash functions; cryptanalysis;alternative encryption; privacy and anonymity; efficiency improvements; multi-partyprotocols; security of encryption schemes as well as countermeasures and faults.,*,2009,*
Coupling knowledge bases and web services for active knowledge,Nicoleta Preda; Fabian M Suchanek; Gjergji Kasneci; Thomas Neumann; Gerhard Weikum,Abstract We present ANGIE; a system that can answer user queries by combiningknowledge from a local database with knowledge retrieved from Web services. If a userposes a query that cannot be answered by the local database alone; ANGIE calls theappropriate Web services to retrieve the missing information. In ANGIE; Web services act asdynamic components of the knowledge base that deliver knowledge on demand. To theuser; this is fully transparent; the dynamically acquired knowledge is presented as if it werestored in the local knowledge base. We have developed a RDF based model for declarativedefinition of functions embedded in the local knowledge base. The results of available Webservices are cast into RDF subgraphs. Parameter bindings are automatically constructed byANGIE; services are invoked; and the semi-structured information returned by the …,*,2009,*
Information Security and Cryptoloy-ICISC 2008: 11th International Conference; Seoul; Korea; December 3-5; 2008; Revised Selected Papers,Pil Joong Lee; Jung Hee Cheon,This book constitutes the refereed proceedings of the 11th International Conference onInformation Security and Cryptology; ICISC 2008; held in Seoul; Korea; during December 3-5; 2008. The 26 revised full papers presented have gone through two rounds of reviewingand improvement and were carefully selected from 131 submissions. The papers areorganized in topical sections on public key encryption; key management and secret sharing;privacy and digital rights; digital signature and voting; side channel attack; hash and mac;primitives and foundations; as well as block and stream ciphers.,*,2009,*
Practical Aspects of Declarative Languages: 11th International Symposium; PADL 2009; Savannah; GA; USA; January 19-20; 2009; Proceedings,Andy Gill; Terrance Swift,Declarative languages have long promised the ability to rapidly create easily maintainablesoftware for complex applications. The International Symposium of Practical Aspects ofDeclarative Languages (PADL) provides a yearly-rum for presenting results on theprinciples the implementations and especially the applications of declarative languages.The PADL symposium held January 19–20; 2009 in Savannah; Georgia was the 11th in thisseries. This year 48 papers were submitted from authors in 17 countries. The P-gramCommittee performed outstandingly to ensure that each of these papers submitted to PADL2009 was thoroughly reviewed by at least three referees in a short period of time. Theresulting symposium presented a microcosm of how the current generation of declarativelanguages are being used to address real applications; along with on-going work on the …,*,2009,*
Advanced Information Systems Engineering: 21st International Conference; CAiSE 2009; Amsterdam; the Netherlands; June 8-12; 2009. Proceedings,David Hutchison; Doug Tygar; Bernhard Steffen; Roel Wieringa; Jaap Gordijn; Pascal Eck; Gerhard Weikum; Moshe Y Vardi; Madhu Sudan; Friedemann Mattern; C Pandu Rangan; Oscar Nierstrasz; Moni Naor; John C Mitchell; Demetri Terzopoulos; Jon M Kleinberg; Josef Kittler; Takeo Kanade,*,*,2009,*
Dataspace: The Final Frontier: 26th British National Conference on Databases; BNCOD 26; Birmingham; UK; July 7-9; 2009. Proceedings,David Hutchison; Moni Naor; Alan P Sexton; Gerhard Weikum; Moshe Y Vardi; Doug Tygar; Demetri Terzopoulos; Madhu Sudan; Bernhard Steffen; Oscar Nierstrasz; John C Mitchell; Friedemann Mattern; Jon M Kleinberg; Josef Kittler; Takeo Kanade; C Pandu Rangan,*,*,2009,*
Information Security and Privacy: 14th Australasian Conference; ACISP 2009 Brisbane; Australia; July 1-3; 2009 Proceedings,David Hutchison; Josef Kittler; Moshe Y Vardi; Doug Tygar; Demetri Terzopoulos; Madhu Sudan; Bernhard Steffen; C Pandu Rangan; Oscar Nierstrasz; Moni Naor; Friedemann Mattern; Takeo Kanade; Jon M Kleinberg; John C Mitchell; Gerhard Weikum; Juan González Nieto; Colin Boyd,*,*,2009,*
Computer Aided Verification: 21st International Conference; CAV 2009; Grenoble; France; June 26-July 2; 2009. Proceedings,David Hutchison; Ahmed Bouajjani; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Oded Maler; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar,*,*,2009,*
Bioinformatics and Computational Biology,Sanguthevar Rajasekaran,This volume presents the proceedings of the First International Conference onBioinformatics and Computational Biology (BICoB 2009). This conference was supported bythe International Society for Computers and Applications (ISCA) and Springer.Computational techniques have already enabled unprecedented advances in modernbiology and medicine. This continues to be a vibrant research area with broadening ofcomputational techniques and new emerging challenges. The Bioinformatics andComputational Biology (BICoB) conference has the goal of promoting the advancement ofcomputing techniques and their application to life sciences. The topics of interest include(and are not limited to):,BICoB 2009 (2009: New Orleans; La.),2009,*
Logical Foundations of Computer Science: International Symposium; LFCS 2009; Deerfield Beach; FL; USA; January 3-6; 2009. Proceedings,David Hutchison; Sergei Artemov; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Anil Nerode; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar,*,*,2009,*
Detection of intrusions and malware; and vulnerability assessment: 6th international conference; DIMVA 2009; Como; Italy; July 9-10; 2009: proceedings,D Bruschi; Ulrich Flegel,Attenzione: i dati modificati non sono ancora stati salvati. Per confermare inserimenti ocancellazioni di voci è necessario confermare con il tasto SALVA/INSERISCI in fondo alla paginaIRIS logo nascondi/visualizza icone a destra nascondi/visualizza menu in alto. Aiuto. Login …,LECTURE NOTES IN COMPUTER SCIENCE,2009,*
Automata; Languages and Programming: 36th International Colloquium; ICALP 2009; Rhodes; Greece; July 5-12; 2009; Proceedings,David Hutchison; C Pandu Rangan; Bernhard Steffen; Moshe Y Vardi; Madhu Sudan; Friedemann Mattern; Sotiris Nikoletseas; Susanne Albers; Takeo Kanade; Demetri Terzopoulos; Josef Kittler; Wolfgang Thomas; Jon M Kleinberg; Yossi Matias; Oscar Nierstrasz; John C Mitchell; Moni Naor; Doug Tygar; Gerhard Weikum; Alberto Marchetti-Spaccamela,*,*,2009,*
Experimental Algorithms,Jan Vahrenhold,This volume contains the papers presented at the 8th International Symposium onExperimental Algorithms (SEA 2009). The symposium was held at the TechnischeUniversität Dortmund; Germany; during June 4–6; 2009. The main theme of the SEA seriesis the role of experimentation and of algorithm engineering techniques in the design andevaluation of algorithms and data structures. Contributions are supported by experimentalevaluation; methodological issues in the design and interpretation of experiments; the use of(meta-) heuristics; or application-driven case studies that deepen the understanding of aproblem's complexity. For each symposium; papers are solicited from all areas of algorithmicengineering research. Previous meetings; under the name of “Workshop on ExperimentalAlgorithms”(WEA); were held in Riga (Latvia; 2001); Ascona (Switzerland; 2003); Angra …,Lecture Notes in Computer Science,2009,*
Epistemological Aspects of Computer Simulation in the Social Sciences: Second International Workshop; EPOS 2006; Brescia; Italy; October 5-6; 2006; Revised Sel...,David Hutchison; Randy Goebel; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Jörg Siekmann; Flaminio Squazzoni; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos,*,*,2009,*
Case-Based Reasoning Research and Development: 8th International Conference on Case-Based Reasoning; ICCBR 2009 Seattle; WA; USA; July 20-23; 2009 Pro...,David Hutchison; Randy Goebel; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; Lorraine McGinty; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Jörg Siekmann; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos,*,*,2009,*
Time-aware Link Prediction in Evolving Social Networks,Tomasz Tylenda; Gerhard Weikum; Srikanta Bedathur,Autor: Tylenda; Tomasz et al.; Genre: Hochschulschrift; Im Druck veröffentlicht: 2009;Titel: Time-aware Link Prediction in Evolving Social Networks.,*,2009,*
A Useful Resource for Defect Prediction Models,Roxana Ragneala; Andreas Zeller; Gerhard Weikum,Zusammenfassung Predicting likely software defects in the future is valuable for projectmanagers when planning resource allocation for software testing. But building predictionmodels using only code metrics may not be suffice for accurate results. In this work; weinvestigate the value of code history metrics that can be collected from the project's versionarchives for the purpose of defect prediction. Our results suggest that prediction models builtusing code history metrics outperform those using traditional code metrics only.,*,2009,*
Scalable phrase mining for ad-hoc text analytics,Srikanta Bedathur; Klaus Berberich; Jens Dittrich; Nikos Mamoulis; Gerhard Weikum,Abstract Large text corpora with news; customer mail and reports; or Web 2.0 contributionsoffer a great potential for enhancing business-intelligence applications. We propose aframework for performing text analytics on such data in a versatile; efficient; and scalablemanner. While much of the prior literature has emphasized mining keywords or tags in blogsor social-tagging communities; we emphasize the analysis of interesting phrases. Theseinclude named entities; important quotations; market slogans; and other multi-word phrasesthat are prominent in a dynamically derived ad-hoc subset of the corpus; eg; being frequentin the subset but relatively infrequent in the overall corpus. The ad-hoc subset may bederived by means of a keyword query against the corpus; or by focusing on a particular timeperiod. We investigate alternative definitions of phrase interestingness; based on the …,*,2009,*
Atomicity,Gerhard Weikum,1. Software non-determinism: A software system is non-deterministic if; when re-executed; itresults in a different execution path than a prior execution. Non-determinism can arise when;for example; paths are determined by relative processor speed or the sequence of externalevents. Such software bugs have been called ''Heisenbugs''(hard failures being ''Bohrbugs'').2. Soft hardware failures: Hardware can also suffer from ''Heisenbugs.''For example; atransient hardware failure may be triggered by an environmental cause; such as a cosmicray changing a memory bit; etc. 3. Operator failures: Systems occasionally require operatorintervention. Operators; being human; make mistakes. An operator is unlikely to make thesame mistake at the same point in a subsequent execution.,*,2009,*
Multilevel Transactions and Object-Model Transactions,Gerhard Weikum,Advances in high throughput sequencing and ''omics''technologies and the resultingexponential growth in the amount of macromolecular sequence; structure; gene expressionmeasurements; have unleashed a transformation of biology from a data-poor science into anincreasingly data-rich science. Despite these advances; biology today; much like physicswas before Newton and Leibnitz; has remained a largely descriptive science. Machinelearning [6] currently offers some of the most cost-effective tools for building predictivemodels from biological data; eg; for annotating new genomic sequences; for predictingmacromolecular function; for identifying functionally important sites in proteins; for identifyinggenetic markers of diseases; and for discovering the networks of genetic interactions thatorchestrate important biological processes [3]. Advances in machine learning eg …,*,2009,*
Multi-Level Recovery and the ARIES Algorithm,Gerhard Weikum,Advances in high throughput sequencing and ''omics''technologies and the resultingexponential growth in the amount of macromolecular sequence; structure; gene expressionmeasurements; have unleashed a transformation of biology from a data-poor science into anincreasingly data-rich science. Despite these advances; biology today; much like physicswas before Newton and Leibnitz; has remained a largely descriptive science. Machinelearning [6] currently offers some of the most cost-effective tools for building predictivemodels from biological data; eg; for annotating new genomic sequences; for predictingmacromolecular function; for identifying functionally important sites in proteins; for identifyinggenetic markers of diseases; and for discovering the networks of genetic interactions thatorchestrate important biological processes [3]. Advances in machine learning eg …,*,2009,*
Efficient phrase matching and proximity-based ranking for XML full-text search.,Yuliya Akkuzhyna; Gerhard Weikum; Martin Theobald,Abstract For a significant amount of queries posed to search engines; the proximity of thequeried terms is no less important then their frequency in the document. This work aims toadapt TopX; a full-text top-k search engine over semi-structured data developed at the Max-Planck Institute for Informatics; to proximity-aware indexing and retrieval. The existing indexstructure was extended and implemented for phrase and proximity search with regard tosemi-structured data. The integration of proximity search into top-k algorithm can potentiallybreak monotonic score aggregation which is necessary for early pruning of resultcandidates. The challenge of this work is to keep the top-k style processing of the queries; ieuse the benefit of early termination. The changes in behavior of ranking after proximity-aware scoring was studied and experimentally evaluated. For evaluation we use the …,*,2009,*
Quality in Phrase Mining,Alekh Jindal; Jens Dittrich; Gerhard Weikum,Abstract Phrase snippets of large text corpora like news articles or web search results offergreat insight and analytical value. While much of the prior work is focussed on efficientstorage and retrieval of all candidate phrases; little emphasis has been laid on the quality ofthe result set. In this thesis; we define phrases of interest and propose a framework formining and post-processing interesting phrases. We focus on the quality of phrases anddevelop techniques to mine minimal-length maximal-informative sequences of words. Thetechniques developed are streamed into a post-processing pipeline and include exact andapproximate match-based merging; incomplete phrase detection with filtering; and heuristics-based phrase classification. The strategies aim to prune the candidate set of phrases downto the ones being meaningful and having rich content. We characterize the phrases with …,*,2009,*
“Catch me if you can”: visual Analysis of Coherence Defects in Web Archiving,Gerhard Weikum; Dimitar Denev; Arturas Mazeika; Marc Spaniol,The World Wide Web is a continuously evolving network of contents (eg Web pages;images; sound files; etc.) and an interconnecting link structure. Hence; an archivist maynever be sure if the contents collected so far are still consistent with those contents sheneeds to retrieve next. Therefore; questions arise about detecting; measuring them and–finally–understanding coherence defects. To this end; visualization strategies are beingpresented that might be applied on different level of granularities: working with (in the idealcase) properly set last-modified timestamps; based on metadata extracted from the crawlerin accelerated crawl-revisit pairs; or from the Internet Archive's WARC files. In order to helpthe archivist in understanding the nature of these defects; this paper investigates means forvisualizing change behavior and archive coherence.,The 9th International Web Archiving Workshop (IWAW 2009),2009,*
Database Tuning using Trade-off Elimination,Surajit Chaudhuri; Gerhard Weikum,realized using such logical structures. For example; in tree based data acquisition protocols;a collection tree is built that is rooted at the data collection center such as the sink node [8].The dissemination of the data requests from the participating nodes and collection of datafrom the sensor nodes are accomplished using this tree. A cluster based data acquisitionmechanism has been proposed in [3]. As shown in Fig. 1; nodes are organized into a fixednumber of clusters; and nodes within each cluster dynamically elect a cluster head. The dataacquisition is carried out in two phases. In the first phase; cluster heads collect data fromtheir cluster nodes. In the second phase; cluster heads send collected data to the nodes thathave subscribed to the data. The cluster heads are re-elected to balance energyconsumption among the nodes in the cluster. Zhang et al.[13] have proposed an adaptive …,*,2009,*
Simulated Evolution and Learning: 7th International Conference; SEAL 2008; Melbourne; Australia; December 7-10; 2008; Proceedings,Xiaodong Li; Michael Kirley; Mengjie Zhang; Vic Ciesielski; Zbigniew Michalewicz; Tim Hendtlass; Kalyanmoy Deb; KC Tan; Jürgen Branke,This LNCS volume contains the papers presented at SEAL 2008; the 7th Int-nationalConference on Simulated Evolutionand Learning; held December 7–10; 2008; inMelbourne; Australia. SEAL is a prestigious international conference series in evolutionarycomputation and learning. This biennial event was? rst held in Seoul; Korea; in 1996; andthen in Canberra; Australia (1998); Nagoya; Japan (2000); Singapore (2002); Busan; Korea(2004); and Hefei; China (2006). SEAL 2008 received 140 paper submissions from morethan 30 countries. After a rigorous peer-review process involving at least 3 reviews for eachpaper (ie; over 420 reviews in total); the best 65 papers were selected to be presented at theconference and included in this volume; resulting in an acceptance rate of about 46%. Thepapers included in this volume cover a wide range of topics in simulated evolution and …,*,2008,*
Principles of Distributed Systems: 12th International Conference; OPODIS 2008; Luxor; Egypt; December 15-18; 2008. Proceedings,Theodore P Baker; Alain Bui; Sébastien Tixeuil,This volume contains the 30 regular papers; the 11 short papers and the abstracts of twoinvited keynotes that were presented at the 12th International Conference on Principles ofDistributed Systems (OPODIS) held during December 15–18; 2008 in Luxor; Egypt. OPODISis a yearly selective international forum for researchers and practitioners in design anddevelopment of distributed systems. This year; we received 102 submissions from 28countries. Each submission was carefully reviewed by three to six Program Committeemembers with the help of external reviewers; with 30 regular papers and 11 short papersbeing selected. The overall quality of submissions was excellent and there were manypapers that had to be rejected because of organization constraints yet deserved to bepublished. The two invited keynotes dealt with hot topics in distributed systems:“The Next …,*,2008,*
Languages and Compilers for Parallel Computing: 21th International Workshop; LCPC 2008; Edmonton; Canada; July 31-August 2; 2008; Revised Selected Papers,José Nelson Amaral,In 2008 the Workshop on Languages and Compilers for Parallel Computing left the USA tocelebrate its 21st anninversary in Edmonton; Alberta; Canada. Following its long-established tradition; the workshop focused on topics at the frontierofresearchanddevelopmentinlangua…; optimizingcompilers; appli-tions; and programming models for high-performancecomputing. While LCPC continues to focus on parallel computing; the 2008 edition includedthe pres-tation of papers on program analysis that are precursors of high performance inparallel environments. LCPC 2008 received 35 paper submissions. Eachpaper received atleast three independent reviews; and then the papers and the referee comments were d-cussed during a Program Committee meeting. The PC decided to accept 18 papers asregular papers and 6 papers as short papers. The short papers appear at the end of this …,*,2008,*
Advances in Blended Learning: Second Workshop on Blended Learning; WBL 2008; Jinhua; China; August 20-22; 2008; Revised Selected Papers,Elvis Wai Chung Leung; Fu Lee Wang; Lanfang Miao; Jianmin Zhao; Jifeng He,The Second Workshop of Blended Learning (WBL 2008); as part of the 7th Inter-tionalConference on Web-Based Learning (ICWL 2008); was held in Zhejiang N-mal University;Jinhua; Zhejiang; China during August 20–22; 2008. WBL 2008 provided an internationalforum for the dissemination of original results in the design; implementation; and evaluationof blended learning systems and related areas. In particular; the aim of WBL 2008 was tobring together researchers from academia as well as commercial developers from industryto explore ideas; exchange and share experiences; and further build the blended learningresearch network. The inspirations and new ideas were expected to emerge from intensivediscussions during formal sessions and social activities. The main focus of WBL 2008 wason the most critical areas of blended learning; namely;'e-Learning Platforms and Tools;'' …,*,2008,*
Stabilization; Safety; and Security of Distributed Systems: 10th International Symposium; SSS 2008; Detroit; MI; USA; November 21-23; 2008. Proceedings,Sandeep Kulkarni; Andre Schiper,This volume contains the proceedings of the 10th International Symposium on Stabilization;Safety; and Security of Distributed Systems (SSS); held November 21–23; 2008 in Detroit;Michigan USA. SSS started as the Workshop on Self-Stabilizing Systems (WSS); whichwas? rst held at Austin in 1989. From the second WSS in Las Vegas in 1995; the-rum washeld biennially; at Santa Barbara (1997); Austin (1999); Lisbon (2001); San Francisco(2003) and Barcelona (2005). The title of the forum changed to the Symposium on Self-Stabilizing Systems (SSS) in 2003. Since 2005; SSS was run annually; and in 2006 (Dallas)the scope of the conference was extended to cover all safety and security-related aspects ofself-* systems. This extension followed the demand for self-stabilization in various areas ofdistributed c-puting including peer-to-peer networks; wireless sensor networks; mobile ad …,*,2008,*
Computer Vision-ECCV 2008: 10th European Conference on Computer Vision; Marseille; France; October 12-18; 2008; Proceedings,David Forsyth; Philip Torr; Andrew Zisserman,Welcome to the 2008EuropeanConference onComputer Vision. These proce-ings are theresult of a great deal of hard work by many people. To produce them; a total of 871 paperswere reviewed. Forty were selected for oral pres-tation and 203 were selected for posterpresentation; yielding acceptance rates of 4.6% for oral; 23.3% for poster; and 27.9% in total.Weappliedthreeprinciples. First; sincewehadastronggroupofAreaChairs; the? nal decisionsto accept or reject a paper rested with the Area Chair; who wouldbeinformedbyreviewsandcouldactonl…Chair. Second; we felt that authors were entitled to a summary that explained how the AreaChair reached a decision for a paper. Third; we were very careful to avoid con? icts ofinterest. Each paper was assigned to an Area Chair by the Program Chairs; and each AreaChair received a pool of about 25 papers. The Area Chairs then identi? ed and …,*,2008,*
Advances in Conceptual Modeling-Challenges and Opportunities: ER 2008 Workshops CMLSA; ECDM; FP-UML; M2AS; RIGiM; SeCoGIS; WISM; Barcelona; Spain;...,Il-Yeol Song; Mario Piattini; Yi-Ping Phoebe Chen; Sven Hartmann; Fabio Grandi; Andreas L Opdahl; Fernando Ferri; Patrizia Grifoni; Maria Chiara Caschera; Colette Rolland; Carson Woo; Camille Salinesi; Christophe Claramunt; Flavius Frasincar; Geert-Jan Houben; Philippe Thiran,This book constitutes the refereed joint proceedings of seven international workshops heldin conjunction with the 27th International Conference on Conceptual Modeling; ER 2008; inBarcelona; Spain; in October 2008. The 42 revised full papers presented were carefullyreviewed and selected from 108 submissions. Topics addressed by the workshops areconceptual modeling for life sciences applications (CMLSA 2008); evolution and change indata management (ECDM 2008); foundations and practices of UML (FP-UML 2008);modeling mobile applications and services (M2AS 2008); requirements; intentions andgoals in conceptual modeling (RIGiM 2008); semantic and conceptual issues in geographicinformation systems (SeCoGIS 2008); and Web information systems modeling (WISM 2008).,*,2008,*
Provable Security: Second International Conference; ProvSec 2008; Shanghai; China; October 30-November 1; 2008. Proceedings,Joonsang Baek,Thesecondinternationalconferenceonprova…; ProvSec2008; washeld in Shanghai; China;during October 30th-November 1st; 2008. The conference wassponsoredbyShanghaiJiaoTongUniver…(SJTU) incooperationwiththe Chinese Association for Cryptologic Research (CACR) andthe Natural Science Foundation of China (NSFC). The aim of ProvSec is to provide aplatform for researchers; scholars; and practitionerstoexchangeideasandextendkn…; whichis an important research area in cryptography. The? rst ProvSec was held in Wollongong;Australia; in 2007. This year; the conference received 79 papers and the program committeeselected 25 papers during eight weeks' thorough reviewing process. The authors oftheselected papersarefrom12 di? erentcountries: Australia; Belgium; China; Estonia; France;Germany; India; Japan; Norway; Singapore; the UK; and the USA. We are grateful to the …,*,2008,*
Advanced Concepts for Intelligent Vision Systems: 10th International Conference; ACIVS 2008; Juan-les-Pins; France; October 20-24; 2008. Proceedings,Salah Bourennane; Wilfried Philips; Dan Popescu; Paul Scheunders,This book constitutes the refereed proceedings of the 10th International Conference onAdvanced Concepts for Intelligent Vision Systems; ACIVS 2008; held in Juan-les-Pins;France; in October 2008. The 33 revised full papers and 69 posters presented were carefullyreviewed and selected from 179 submissions. The papers are organized in topical sectionson image and video coding; systems and applications; video processing; filtering andrestoration; segmentation and feature extraction; tracking; scene understanding andcomputer vision; medical imaging; and biometrics and surveillance.,*,2008,*
Computer Security-ESORICS 2008: 13th European Symposium on Research in Computer Security; Málaga; Spain; October 6-8; 2008. Proceedings,Sushil Jajodia,These proceedings contain the papers selected for presentation at the 13th EuropeanSymposium on Research in Computer Security––ESORICS 2008––held October 6–8; 2008in Torremolinos (Malaga); Spain; and hosted by the University of Malaga; C-puter ScienceDepartment. ESORICS has become the European research event in computer security. Thesymposium started in 1990 and has been organized on alternate years in different Europeancountries. From 2002 it has taken place yearly. It attracts an international audience from boththe academic and industrial communities. In response to the call for papers; 168 paperswere submitted to the symposium. These papers were evaluated on the basis of theirsignificance; novelty; and technical quality. Each paper was reviewed by at least threemembers of the Program Comm-tee. The Program Committee meeting was held …,*,2008,*
Theorem Proving in Higher Order Logics: 21st International Conference; TPHOLs 2008; Montreal; Canada; August 18-21; 2008; Proceedings,Otmane Ait Mohamed; César Munoz; Sofiène Tahar,This book constitutes the refereed proceedings of the 21st International Conference onTheorem Proving in Higher Order Logics; TPHOLs 2008; held in Montreal; Canada; inAugust 2008. The 17 revised full papers presented together with 1 proof pearl (concise andelegant presentations of interesting examples); 5 tool presentations; and 2 invited paperswere carefully reviewed and selected from 40 submissions. The papers cover all aspects oftheorem proving in higher order logics as well as related topics in theorem proving andverification such as formal semantics of specification; modeling; and programminglanguages; specification and verification of hardware and software; formalisation ofmathematical theories; advances in theorem prover technology; as well as industrialapplication of theorem provers.,*,2008,*
Engineering Interactive Systems 2008: Second Conference on Human-Centered Software Engineering; HCSE 2008 and 7th International Workshop on Task Models...,Fabio Paternò,Engineering Interactive Systems (EIS) 2008 was an international event combining the 2ndworking conference on Human-Centred Software Engineering (HCSE 2008) and the 7thInternational Workshop on TAsk MOdels and DIAgrams (TAMODIA 2008). HCSE is aworking conference that brings together researchers and practitioners-terested instrengthening the scientific foundations of user interface design and examining therelationship between software engineering and human-computer interaction and how tostrengthen user-centred design as an essential part of so-ware engineering processes. As aworking conference; substantial time is devoted to the open and lively discussion of papers.TAMODIA is an international workshop on models; such as task models and visualrepresentations in Human-Computer Interaction (one of the most widely used notations in …,*,2008,*
Cooperative Design; Visualization; and Engineering: 5th International Conference; CDVE 2008 Calvià; Mallorca; Spain; September 21-25; 2008 Proceedings,Yuhua Luo,This book constitutes the refereed proceedings of the 5th International Conference onCooperative Design; Visualization; and Engineering; CDVE 2008; held in Calvià; Mallorca;Spain; in September 2008. The 45 revised full papers presented were carefully reviewedand selected from numerous submissions. The papers cover all current issues incooperative design; visualization; and engineering; ranging from theoretical andmethodological topics to various systems and frameworks to applications in a variety offields. The papers are organized in topical segments on cooperative design; cooperativevisualization; cooperative engineering; cooperative applications; as well as basic theories;methods and technologies that support CDVE.,*,2008,*
Graph Transformations: 4th International Conference; ICGT 2008; Leicester; United Kingdom; September 7-13; 2008; Proceedings,Hartmut Ehrig; Reiko Heckel; Grzegorz Rozenberg; Gabriele Taentzer,This book constitutes the refereed proceedings of the 4th International Conference on GraphTransformations; ICGT 2008; held in Leicester; UK; in September 2008. The 27 revised fullpapers presented together with 5 tutorial and workshop papers and 3 invited lectures werecarefully selected from 57 submissions. All current aspects in graph drawing are addressedincluding hypergraphs and termgraph rewriting; applications of graph transformation;execution of graph transformations; compositional systems; validation and verification; graphlanguages and special transformation concepts; as well as patterns and modeltransformations. In addition the volume contains 17 short papers of the ICGT 2008 DoctoralSymposium.,*,2008,*
Haptic and Audio Interaction Design: Third International Workshop; HAID 2008 Jyväskylä; Finland; September 15-16; 2008 Proceedings,Antti Pirhonen; Stephen Brewster,Bringing Them Under the Same Roof The Haptic and Audio Interaction Design workshopseries is now in its third year. These workshops have already demonstrated a clear need fora venue in which-searchers and practitioners in these areas gather together under the sameroof. Three years have also shown clear developments in the approaches taken–with thebenefits of combining haptics and audio shown practically and conceptually in this year's-pers. In other words; it seems that when there is interaction between audio and hapticresearchers; they really learn from each other and multimodal approaches emerge. Thereare many good reasons for using haptics and audio together. There are the practical needsin application development. Mobile devices are an obvious example–while the device issmall in size and is used on the move; interaction cannot rely solely on visual display. On …,*,2008,*
P2P Information Retrieval and Filtering with MAPS,Christian Zimmer; Johannes Heinz; Christos Tryfonopoulos; Gerhard Weikum,In this demonstration paper we present MAPS; a novel system that combines approximateinformation retrieval and filtering functionality in a peer-to-peer setting. In MAPS; a user isable to submit one-time and continuous queries; and receive matching resources andnotifications from selected information sources. The selection of these sources in theretrieval case is based on well-known resource selection techniques for peer-to-peer queryrouting; while in the filtering case a combination of resource selection and novel behaviorprediction techniques using time-series analysis of publisher statistics is used. Theintegration of the two functionalities is done in a seamless way utilizing the same machinery:a conceptually global; but physically distributed directory of statistics about informationsources based on distributed hash tables.,Peer-to-Peer Computing; 2008. P2P'08. Eighth International Conference on,2008,*
Implementation and Application of Functional Languages: 19th International Workshop; IFL 2007; Freiburg; Germany; September 27-29; 2007 Revised Selected Pap...,Olaf Chitil,This book constitutes the thoroughly refereed post-proceedings of the 19th InternationalWorkshop on Implementation and Applications of Functional Languages; IFL 2007; held inFreiburg; Germany in September 2007. The 15 revised full papers presented went throughtwo rounds of reviewing and improvement and were selected from 33 submissions. Thepapers address all current theoretical and methodological issues on functional and function-based languages such as type checking; contract checking; compilation; parallelism;development and debugging; data structures; parsing as well as various performancerelated concepts.,*,2008,*
Hybrid Systems: Computation and Control: 11th International Workshop; HSCC 2008; St. Louis; MO; USA; April 22-24; 2008; Proceedings,Magnus Egerstedt; Bud Mishra,This volume contains the proceedings ofthe 11th Workshop on Hybrid Systems:Computation and Control (HSCC 2008) held in St. Louis; Missouriduring April 22–24; 2008.The annual workshop on hybrid systems focuses on researchin-bedded; reactivesystemsinvolvingtheinterplaybetwe…/switchingand continuous dynamical behaviors. HSCC attracts academic as well asindustrial researchers to exchange information on the latest developments of applicationsand theoretical advancements in the design; analysis; control; optimization; andimplementation of hybrid systems; with particular attention to embedded and networkedcontrol systems. New for this year was that HSCC was part of the inaugural CPSWEEK(Cyber-Physical Systems Week)–a co-located cluster of three conferences: HSCC; RTAS(Real-Time and Embedded Technology and Applications Sym-sium); and IPSN …,*,2008,*
Data and Applications Security XXII: 22nd Annual IFIP WG 11.3 Working Conference on Data and Applications Security London; UK; July 13-16; 2008; Proceedings,Vijay Atluri,This volume contains the papers presented at the 22nd Annual IFIP WG 11.3 WorkingConference on Data and Applications Security (DBSEC) held in L-don; UK; July 13–16;2008. This year's working conference continued its tra-tion of being a forum fordisseminating original research results and practical experiences in data and applicationssecurity. This year we had an excellent program that consists of 9 research paper s-sionswith 22 high-quality research papers; which were selected from a total of 56 submissionsafter a rigorous reviewing process by the Program Committee members and externalreviewers. These sessions included such topics as access control; privacy; auditing; systemssecurity and data security in advanced app-cation domains. In addition; theprogramincluded a keynote address; an invited talk and a panel session. The success of …,*,2008,*
Changing Television Environments: 6th European Conference; EuroITV 2008; Salzburg; Austria; July 3-4; 2008; Proceedings,Manfred Tscheligi; Marianna Obrist; Arthur Lugmayr,Since the time when interactive television emerged as a medium for the home environment;it has been permanently evolving. Changing requirements and user behavior; eg; thedemand for being mobile and have access to information and entertainment anywhere andany time; are challenging interactive TV. New kinds of interactive services have to beconceived for the increasing mobile; ubiquitous requirements of the diﬀerent user groups. Inthese changing environments; a better understanding of emerging contexts and theirimplications is essential. This gave birth to the idea for the theme of the EuroITV 2008Conference:“Changing Television Environments.” EuroITV 2008; the 6th edition of theEuropean Conference on Interactive Television; was organized and hosted by the HCI andUsability Unit; ICT&S Center; University of Salzburg; Austria. The EuroITV Conference …,*,2008,*
Applications of Evolutionary Computing: EvoWorkshops 2008: EvoCOMNET; EvoFIN; EvoHOT; EvoIASP; EvoMUSART; EvoNUM; EvoSTOC; and EvoTransLog,Mario Giacobini; Anthony Brabazon; Stefano Cagnoni; Gianni A Di Caro; Rolf Drechsler; Aniko Ekart; Anna I Esparcia-Alcazar; Muddassar Farooq; Andreas Fink; Jon McCormack; Michael O'Neill; Juan Romero; Franz Rothlauf; Giovanni Squillero; Sima Uyar; Shengxiang Yang,Evolutionary computation (EC) techniques are e? cient; nature-inspired pl-ning andoptimization methods based on the principles of natural evolution and genetics. Due to theire? ciency and simple underlying principles; these me-ods can be used in the context ofproblem solving; optimization; and machine learning. A large and continuously increasingnumber of researchers and prof-sionals make use of EC techniques in various applicationdomains. This volume presents a careful selection of relevant EC examples combined with athorough examination of the techniques used in EC. The papers in the volume illustrate thecurrent state of the art in the application of EC and should help and-spire researchers andprofessionals to develop e? cient EC methods for design and problem solving. All papers inthis book were presented during EvoWorkshops 2008; which consisted of a range of …,*,2008,*
Incremental Relevance Feedback for TopX,Osama Sammodi,Abstract TopX is a highly efficient and effective search engine for ranked retrieval of XMLand plain text data. However; for some difficult queries; the results provided by TopX are notyet completely satisfying. Towards the solution of this problem; an extensible framework hasbeen proposed that incorporates feedback from the user to generate a better; expandedquery. In this thesis; we integrate the feedback framework with the TopX search engine. Weextend the existing browser-based interface of TopX to support explicit relevance feedback;reevaluate the query when new feedback is available. We also modify the existing TopXsearch engine to support incremental expansion of queries; ie; if a query that is alreadyevaluated is expanded (based on feedback); the evaluation of the expanded query shouldreuse the partial results from the evaluation of the original query.,*,2008,*
Lower Bounds for Merging on the Hypercube,Michael Kerber; Benjamin Doerr; Spyros Angelopoulos; Josiane Xavier Parreira; Sebastian Michel; Gerhard Weikum; Thoralf Klein; Ulf Brefeld; Tobias Scheffer; Chien-Chung Huang; Telikepalli Kavitha; Dimitrios Michail; Meghana Nasre; Matthias Bender; Sebastian Michel; Gerhard Weikum; Hans de Nivelle; Bodo Rosenhahn; Uwe Kersting; Katie Powell; Reinhard Klette; Gisela Klette; Hans-Peter Seidel,@inproceedings{Rub:1994:LBM:188380.188407; author = {R\"{u}b; Christine}; title = {LowerBounds for Merging on the Hypercube}; booktitle = {Proceedings of the Second Italian Conferenceon Algorithms and Complexity}; series = {CIAC '94}; year = {1994}; isbn = {3-540-57811-0}; location ={Rome; Italy}; pages = {213--222}; numpages = {10}; url = {http://dl.acm.org/citation.cfm?id=188380.188407}; acmid = {188407}; publisher = {Springer-Verlag New York; Inc.}; address ={Secaucus; NJ; USA}; } @inproceedings{Kerber:2009:CRR:1691106.1691121; author = {Kerber;Michael}; title = {On the Complexity of Reliable Root Approximation}; booktitle = {Proceedingsof the 11th International Workshop on Computer Algebra in Scientific Computing}; series = {CASC'09}; year = {2009}; isbn = {978-3-642-04102-0}; location = {Kobe; Japan}; pages = {155--167};numpages = {13}; url = {http://dx.doi.org/10.1007/978-3-642 …,Proceedings of the Second Italian Conference on Algorithms and Complexity,2008,*
Information Theoretic Security: Third International Conference; ICITS 2008; Calgary; Canada; August 10-13; 2008. Proceedings,David Hutchison; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Reihaneh Safavi-Naini; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
Data Management in Grid and Peer-to-Peer Systems: First International Conference; Globe 2008; Turin; Italy; September 3; 2008. Proceedings,David Hutchison; Abdelkader Hameurlain; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
Advances in Cryptology-ASIACRYPT 2008: 14th International Conference on the Theory and Application of Cryptology and Information Security; Melbourne; Australi...,David Hutchison; Jon M Kleinberg; Gerhard Weikum; Moshe Y Vardi; Doug Tygar; Demetri Terzopoulos; Madhu Sudan; Josef Kittler; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Josef Pieprzyk; Bernhard Steffen; Takeo Kanade,*,*,2008,*
Model Driven Engineering Languages and Systems: 11th International Conference; MoDELS 2008; Toulouse; France; September 28-October 3; 2008. Proceedings,David Hutchison; Jean-Michel Bruel; Krzysztof Czarnecki; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; Ileana Ober; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Axel Uhl; Moshe Y Vardi; Markus Völter; Gerhard Weikum,*,*,2008,*
Web Information Systems Engineering-WISE 2008: 9th International Conference; Auckland; New Zealand; September 1-3; 2008. Proceedings,David Hutchison; James Bailey; Takeo Kanade; Josef Kittler; Jon M Kleinberg; David Maier; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Klaus-Dieter Schewe; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Bernhard Thalheim; Doug Tygar; Moshe Y Vardi; Xiaoyang Sean Wang; Gerhard Weikum,*,*,2008,*
Smart Card Research and Advanced Applications,David Hutchison; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum; Gilles Grimaud; François-Xavier Standaert,*,*,2008,*
Unconventional Computing: 7th International Conference; UC 2008 Vienna; Austria; August 25-28; 2008. Proceedings,David Hutchison; Cristian S Calude; José Félix Costa; Rudolf Freund; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; Marion Oswald; C Pandu Rangan; Grzegorz Rozenberg; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
Times of Convergence. Technologies Across Learning Contexts: Third European Conference on Technology Enhanced Learning; EC-TEL 2008; Maastricht; the Net...,David Hutchison; Pierre Dillenbourg; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Marcus Specht; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
Intelligence and security informatics: European conference; EuroISI 2008; Esbjerg; Denmark; December 3-5; 2008; proceedings,EuroISI,This book constitutes the refereed proceedings of the First European Conference onIntelligence and Security Informatics; EuroISI 2008; held in Esbjerg; Denmark; in December2008. The 23 revised full papers and 2 revised poster papers presented were carefullyreviewed and selected from 48 submissions. The papers are organized in topical sectionson criminal and social network analysis; intelligence analysis and knowledge discovery;Web-based intelligence monitoring and analysis; privacy protection; access control; anddigital rights management; malware and intrusion detection; as well as surveillance andcrisis management.,*,2008,*
Sequences and Their Applications-SETA 2008: 5th International Conference Lexington; KY; USA; September 14-18; 2008 Proceedings,David Hutchison; Solomon W Golomb; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Matthew G Parker; Alexander Pott; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum; Arne Winterhof,*,*,2008,*
Mathematics of Program Construction: 9th International Conference; MPC 2008; Marseille; France; July 15-18; 2008. Proceedings,David Hutchison; Philippe Audebaud; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Christine Paulin-Mohring; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
Algebraic Biology: Third International Conference; AB 2008; Castle of Hagenberg; Austria; July 31-August 2; 2008 Proceedings,Tatsuya Akutsu; Morihiro Hayashida; Takeyuki Tamura; Katsuhisa Horimoto; Georg Regensburger; Markus Rosenkranz; Hiroshi Yoshida,*,*,2008,*
Transactions on Rough Sets VIII,David Hutchison; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; James F Peters; Andrzej Skowron; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
Advances in web-based learning; ICWL 2008: 7th International Conference; Jinhua; China; August 20-22; 2008; proceedings,Fu Lee WANG; LI Frederick; MIAO Lanfang; ZHAO Jianmin,WANG; FL; LI; F; MIAO; L & ZHAO; J (eds) 2008; Advances in web-based learning; ICWL2008: 7th International Conference; Jinhua; China; August 20-22; 2008; proceedings. Lecturenotes in computer science; vol. 5145; Springer; Berlin … WANG; FL; LI; F.; MIAO; L.; &ZHAO; J. (Eds.) (2008). Advances in web-based learning; ICWL 2008: 7th InternationalConference; Jinhua; China; August 20-22; 2008; proceedings. (Lecture notes in computerscience; Vol. 5145). Berlin: Springer … WANG FL; (ed.); LI F; (ed.); MIAO L; (ed.); ZHAO J;(ed.). Advances in web-based learning; ICWL 2008: 7th International Conference; Jinhua;China; August 20-22; 2008; proceedings. Berlin: Springer; 2008. (Lecture notes in computerscience) … WANG; Fu Lee (Editor); LI; Frederick (Editor); MIAO; Lanfang (Editor); ZHAO; Jianmin(Editor) / Advances in web-based learning; ICWL 2008 : 7th International Conference …,*,2008,*
Advances in Computation and Intelligence: Third International Symposium; ISICA 2008 Wuhan; China; December 19-21; 2008 Proceedings,Lishan Kang; Zhihua Cai; Xuesong Yan; Yong Liu,*,*,2008,*
Provenance and Annotation of Data and Processes: Second International Provenance and Annotation Workshop; IPAW 2008; Salt Lake City; UT; USA; June 17-18;...,David Hutchison; Juliana Freire; Takeo Kanade; Josef Kittler; Jon M Kleinberg; David Koop; Friedemann Mattern; John C Mitchell; Luc Moreau; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
Evolvable Systems: From Biology to Hardware: 8th International Conference; ICES 2008; Prague; Czech Republic; September 21-24; 2008. Proceedings,David Hutchison; Pauline C Haddow; Gregory S Hornby; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Lukáš Sekanina; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
Automata; Languages and Programming,David Hutchison; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum; Luca Aceto; Ivan Damgård; Leslie Ann Goldberg; Magnús M Halldórsson; Anna Ingólfsdóttir; Igor Walukiewicz,*,*,2008,*
Advances in Image and Video Technology: Third Pacific Rim Symposium; PSIVT 2009; Tokyo; Japan; January 13-16; 2009. Proceedings,David Hutchison; Madhu Sudan; Demetri Terzopoulos; Stephen Lin; Friedemann Mattern; Moshe Y Vardi; Fay Huang; Gerhard Weikum; John C Mitchell; Toshikazu Wada; Bernhard Steffen; C Pandu Rangan; Doug Tygar; Oscar Nierstrasz; Moni Naor; Takeo Kanade; Josef Kittler; Jon M Kleinberg,*,*,2008,*
Abstract State Machines ‚B and Z ‚First International Conference ‚ABZ 2008 ‚London ‚UK ‚September 16− 18 ‚2008. Proceedings,Paul Boca,*,ABZ,2008,*
Focused Access to XML Documents: 6th International Workshop of the Initiative for the Evaluation of XML Retrieval; INEX 2007 Dagstuhl Castle; Germany; Decembe...,David Hutchison; Norbert Fuhr; Jaap Kamps; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Mounia Lalmas; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Andrew Trotman; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
Business Process Management: 6th International Conference; BPM 2008; Milan; Italy; September 2-4; 2008. Proceedings,David Hutchison; Marlon Dumas; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Manfred Reichert; Ming-Chien Shan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
Advanced Intelligent Computing Theories and Applications. With Aspects of Theoretical and Methodological Issues: 4th International Conference on Intelligent Com...,David Hutchison; De-Shuang Huang; Kang-Hyun Jo; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Daniel S Levine; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum; Donald C Wunsch,*,*,2008,*
08111 Abstracts Collection--Ranked XML Querying,Sihem Amer-Yahia; Divesh Srivastava; Gerhard Weikum,Abstract From 09.03. to 14.03. 08; the Dagstuhl Seminar 08111``Ranked XML Querying''washeld in the International Conference and Research Center (IBFI); Schloss Dagstuhl. Duringthe seminar; several participants presented their current research; and ongoing work andopen problems were discussed. Abstracts of the presentations given during the seminar aswell as abstracts of seminar results and ideas are put together in this paper. The first sectiondescribes the seminar topics and goals in general. Links to extended abstracts or full papersare provided; if available.,Dagstuhl Seminar Proceedings,2008,*
Artificial Immune Systems: 7th International Conference; ICARIS 2008; Phuket; Thailand; August 10-13; 2008. Proceedings,Johnny Kelsey; Brian Henderson; Rob Seymour; Andy Hone; Peter J Bentley; Doheon Lee; Sungwon Jung,*,*,2008,*
Personalization of Search on Structured Data,Minko Dudev; Gerhard Weikum; Julia Luxenburger; Andreas Zeller,Abstract Recent research on semantic search methods has given rise to a number of largegraph-based knowledge repositories which could form the basis of semantic searchengines; fully capable of finding and ranking facts. Similar to traditional Web search; theranking provided may prove to be satisfactory on average; however just as in other types ofknowledge bases; personalization could be highly beneficial in ensuring that the resultordering is truly relevant to the specific user. While a significant amount of work has beendone in the areas of personalization on the Web; XML search and databases; this thesis forthe first time addresses the issue of personalizing query results in the specific setting ofgraph-based knowledge bases. The two main issues in personalization are explored indetail: i) the construction of a user profile including a spreading activation methodology …,*,2008,*
Personalizing XML Full Text Search in PIMENTO,Irini Fundulaki; Sihem Amer-Yahia; Lakshmanan Laks,Abstract In PIMENTO we advocate a novel approach to XML search that leverages userinformation to return more relevant query answers. This approach is based on formalizing{em user profiles} in terms of {em scoping rules} which are used to rewrite an input query;and of {em ordering rules} which are combined with query scoring to customize the rankingof query answers to specific users.,Dagstuhl Seminar Proceedings,2008,*
Articulated Motion and Deformable Objects: 5th International Conference; AMDO 2008; Port D'Andratx; Mallorca; Spain; July 9-11; 2008. Proceedings,David Hutchison; Robert B Fisher; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Francisco J Perales; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
The Future of DB & IR,Gerhard Weikum; Gjergji Kasneci; Maya Ramanath; Fabian Suchanek,ABSTRACT This article advocates that database-systems (DB) and information-retrieval (IR)methods be integrated to address the needs of important applications that emerge with theongoing explosion and diversification of digital information. A grand challenge that a jointDB&IR endeavor could aim for is to automatically build and maintain a comprehensiveknowledge base that encompasses all important facts from encyclopedic sources and thescientific literature. It should represent facts in terms of typed entities and relations; and allowexpressive queries that return ranked results with high precision in an efficient and scalablemanner. The article presents various approaches and the roles of DB and IR methodstowards this ambitious objective.,*,2008,*
Cellular Automata: 8th International Conference on Cellular Aotomata for Research and Industry; ACRI 2008; Yokohama; Japan; September 23-26; 2008. Proceedin...,David Hutchison; Stefania Bandini; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Toshihiko Komatsuzaki; Friedemann Mattern; John C Mitchell; Shin Morishita; Moni Naor; Oscar Nierstrasz; Katsuhiro Nishinari; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Hiroshi Umeo; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
08111 Report--Ranked XML Querying,Sihem Amer-Yahia; Djoerd Hiemstra; Thomas Roelleke; Divesh Srivastava; Gerhard Weikum,Abstract This paper is based on a five-day workshop on" Ranked XML Querying" that tookplace in Schloss Dagstuhl in Germany in March 2008 and was attended by 27 people fromthree different research communities: database systems (DB); information retrieval (IR); andWeb. The seminar title was interpreted in an IR-style" andish" sense (it covered also subsetsof {Ranking; XML; Querying}; with larger sets being favored) rather than the DB-style strictlyconjunctive manner. So in essence; the seminar really addressed the integration of DB andIR technologies with Web 2.0 being an important target area.,Dagstuhl Seminar Proceedings,2008,*
Secure Data Management: 5th VLDB Workshop; SDM 2008; Auckland; New Zealand; August 24; 2008. Proceedings,David Hutchison; Willem Jonker; Takeo Kanade; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Milan Petković; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
Other Workshop Reports-DB&IR Integration: Report on the Dagstuhl Seminar" Ranked XML Querying",Sihem Amer-Yahia; Djoerd Hiemstra; Thomas Roelleke; Divesh Srivastava; Gerhard Weikum,*,SIGIR Forum,2008,*
Deliverable D2. 1 Defining the Main Components of the SAPIR Software; their Functionality; and their Interfaces January 2008,Walter Allasia; Michal Batko; Mouna Kacimi; Yosi Mass; Fausto Rabitti; Michal Shmueli-Scheuer; Gerhard Weikum,*,*,2008,*
Computers Helping People with Special Needs: 11th International Conference; ICCHP 2008; Linz; Austria; July 9-11; 2008. Proceedings,David Hutchison; Takeo Kanade; Arthur Karshmer; Josef Kittler; Joachim Klaus; Jon M Kleinberg; Friedemann Mattern; Klaus Miesenberger; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum; Wolfgang Zagler,*,*,2008,*
Computer Performance Engineering (5 conf.),Nigel Thomas; Carlos Juiz,This volume of LNCS contains papers presented at the 5th European PerformanceEngineering Workshop held in Palma de Mallorca during September 24–25; 2008. Theworkshop is truly international; the European part of the title refers only to its location. Paperswere submitted from Asia; North and South America; although the majority were fromEurope. In all; 39 papers were submitted from which 16 were chosen following peer review.Every member of the Programme Committee was responsible for reviewing at least fivepapers each in a little over two weeks; a degree of dedication without which the workshopcould not be a success. The selection criteria were harsh and by necessity several excellentpapers were rejected. However; the result is a programme of the highest quality. Theaccepted papers reflect the diversity of modern performance engineering. There were a …,*,2008,*
of Proceedings: Proceedings of the First International Conference on Global Interoperability for Language Resources,Gerard de Melo; Gerhard Weikum,Abstract/Description: WordNet is a lexical database describing English words and theirsenses. We propose a method for automatically producing similar resources for newlanguages by taking advantage of the original WordNet in conjunction with translationdictionaries. A small set of training mappings is used to learn a model for predictingassociations between terms and senses. The associations are represented using a variety ofscores that take into account structural properties as well as semantic relatedness andcorpus frequency information. For evaluation; we created a German-language wordnet; andthe data indicate a significantly better coverage and higher precision than previousheuristics. The resulting resources provide not only valuable information for monolingualNLP tasks but also enable a high degree of cross-lingual interoperability.,*,2008,*
of Proceedings: Research and Advanced Technology for Digital Libraries: 12th European Conference; ECDL 2008,Paraskevi Raftopoulou; Euripides GM Petrakis; Christos Tryfonopoulos; Gerhard Weikum,Document title: Information Retrieval and Filtering over Self-Organising Digital Libraries Authors:Raftopoulou; Paraskevi; Petrakis; Euripides GM; Tryfonopoulos; Christos; Weikum; GerhardDocument type: Conference-Paper Language: English Audience: Experts Only Title of Series:Lecture Notes in Computer Science External Publication Status: published Title of Proceedings:Research and Advanced Technology for Digital Libraries : 12th European Conference; ECDL 2008Place of Conference/Meeting: Aarhus; Denmark Full Name(s) of Editor(s) of Proceedings:Christensen-Dalsgaard; Birte Castelli; Donatella; Jurik; Bolette Ammitzbøll; Lippincott; Joan Placeof Publication: Berlin Volume (in Journal): 5173 Last Change of the Resource (YYYY-MM-DD):2009-03-26 (Start) Date of Conference/Meeting (YYYY-MM-DD): 2008-09-14 Date of Publication(YYYY-MM-DD): 2008 End Date of Conference/Meeting (YYYY-MM …,*,2008,*
Ant Colony Optimization and Swarm Intelligence: 6th International Conference; ANTS 2008; Brussels; Belgium; September 22-24; 2008. Proceedings,Rami N Khushaba; Ahmed Al-Ani; Akram AlSukker; Adel Al-Jumaily; Marco Dorigo; Mauro Birattari; Christian Blum; Maurice Clerc; Thomas Stützle; Alan FT Winfield,*,*,2008,*
Hybrid Learning and Education: First International Conference; ICHL 2008 Hong Kong; China; August 13-15; 2008 Proceedings,David Hutchison; Takeo Kanade; John C Mitchell; C Pandu Rangan; Oscar Nierstrasz; Jon M Kleinberg; Joseph Fong; Bernhard Steffen; Reggie Kwan; Friedemann Mattern; Moni Naor; Josef Kittler; Madhu Sudan; Doug Tygar; Moshe Y Vardi; Demetri Terzopoulos; Fu Lee Wang; Gerhard Weikum,*,*,2008,*
of Proceedings: Web Information Systems Engineering–WISE 2008: 9th International Conference,Thomas Neumann; Matthias Bender; Sebastian Michel; Ralf Schenkel; Peter Triantafillou; Gerhard Weikum,Abstract/Description: Top-k query processing is a fundamental building block for efficientranking in a large number of applications. Efficiency is a central issue; especially fordistributed settings; when the data is spread across different nodes in a network. This paperintroduces novel optimization methods for top-k aggregation queries in such distributedenvironments that can be applied to all algorithms that fall into the frameworks of the priorTPUT and KLEE methods. The optimizations address 1) hierarchically grouping input listsinto top-k operator trees and optimizing the tree structure; and 2) computing data-adaptivescan depths for different input sources. The paper presents comprehensive experiments withtwo different real-life datasets; using the ns-2 network simulator for a packet-level simulationof a large Internet-style network.,*,2008,*
Efficient XML Query Processing and Full-Text Search,Mohammed AbuJarour,Abstract XML query processing is an essential building block in ranked XML informationretrieval. To rank documents according to a user's query; we need to maintain pre-computedinverted index lists about the data set we consider. The way we handle these lists is the keyto evaluate XML queries efficiently. In this work; we introduce a new approach to arrangesuch index lists about XML documents in a compact form by removing redundant piecesfrom the index as much as possible. Our approach optimizes IO throughput and henceoptimizes the overall throughput. The main focus of our work is to optimize storagerequirements for this kind of ranked retrieval.,*,2007,*
TOIS reviewers January 2006 through May 2007,Gary Marchionini; Ahmed Abbasi; Eugene Agichtein; Khurshid Ahmad; Azzah Al-Maskari; Gianni Amati; Sihem Amer Yahia; Shlomo Argamon; Daniel Ashbrook; Paolo Atzeni; Michela Bacchin; Godmar Back; Antonio Badia; Andras Banczur; Bettina Berendt; Elisa Bertino; B Bhagyavati; Suresh Bhavnani; Devdutta Bhosale; David Bodoff; Paolo Boldi; Johan Bollen; Angela Bonifati; Pia Borlund; Jit Bose; Athman Bouguettaya; Michael Brinkmeier; Peter Brown; Peter Brusilovsky; Peter Bruza; Christopher Burges; Robin Burke; Ben Carterette; Arthur Cater; Kuiyu Chang; Hsin Hsi Chen; Zheng Chen; James Cheney; Pu Jen Cheng; Roger Chiang; Byron Choi; Tat Seng Chua; Charlie Clarke; Paul Clough; Mariano Consens; Gordon Cormack; Nick Craswell; Fabio Crestani; Carolyn Crouch; Silviu Petru Cucerzan; Hang Cui; Sally Jo Cunningham; Edward Cutrell; Pablo De La Fuente; Arjen De Vries; Anne Diekema; Sandor Dominich; Shyamala Doraisamy; Mark Dunlop; Georges Dupret; Miles Efron; Jeremy Ellman; Peter Enser; Gunes Erkan; Laura Fochtmann; Anders Fongen; Nigel Ford; Martin Franz; Xin Fu; Paolo Garza; Susan Gauch; Pierre Geneves; Henry Gladney; Melanie Gnasa; Andrew Goldberg; Marcos Goncalves; Cyril Goutte; David Grossman; Dennis Groth; Jacek Gwizdka; Stephanie Haas; Sanda Harabagiu; Donna Harman; Andreas Henrich; Djoerd Hiemstra; Lee Hollaar; Chun Nan Hsu; Fei Huang; Zan Huang; Mike Huhns; Carlos Hurtado; Keisuke Innoue; Panagiotis Ipeirotis; Bernard Jansen; Wang Jianqiang; Rong Jin; Marko Junkkari; Patrick Juola; Vinay Kakade; Jaap Kamps; In Ho Kang; Damianos Karakos; Vangelis Karkaletsis; Martin Kaszkiel; Siddharth Kaza; Jaana Kekäläinen; Diane Kelly; Benny Kimelfeld; Alek Kolcz; Joseph Konstan; Kui Lam Kwok; Abhimanyu Lad; Alberto Laender; Mounia Lalmas; Leah Larkey; Ray Larson; Nabil Layaida; Zhang Le; Dik Lun Lee; Dongwon Lee; Jochen Leidner; Gina Levow; Hang Li; Xin Li; Chin Yew Lin; Jimmy Lin; Tie Yan Liu; Zehua Liu; David Losada; Jie Lu; Yiming Ma; Inderjeet Mani; Murali Mani; Ioana Manolescu; Catherine Marshall; Mercedes Martinez; Yosi Mass; Paul McNamee; Sean McNee; Brahim Medjahed; Lokman Meho; Donald Metzler; Rada Mihalcea; Ruslan Mitkov; Bamshad Mobasher; Marina Mongiello; Ani Nenkova; Frank Neven; Dorbin Ng; Wilfred Ng,Marchionini; Gary; Abbasi; Ahmed; Agichtein; Eugene; Ahmad; Khurshid; Al-Maskari; Azzah;Amati; Gianni; Yahia; Sihem Amer; Argamon; Shlomo; Ashbrook; Daniel; Atzeni; Paolo;Bacchin; Michela; Back; Godmar; Badia; Antonio; Banczur; Andras; Berendt; Bettina; Bertino;Elisa; Bhagyavati; B.; Bhavnani; Suresh; Bhosale; Devdutta; Bodoff; David; Boldi; Paolo;Bollen; Johan; Bonifati; Angela; Borlund; Pia; Bose; Jit; Bouguettaya; Athman; Brinkmeier;Michael; Brown; Peter; Brusilovsky; Peter; Bruza; Peter; Burges; Christopher; Burke; Robin;Carterette; Ben; Cater; Arthur; Chang; Kuiyu; Chen; Hsin Hsi; Chen; Zheng; Cheney; James;Cheng; Pu Jen; Chiang; Roger; Choi; Byron; Chua; Tat Seng; Clarke; Charlie; Clough; Paul;Consens; Mariano; Cormack; Gordon; Craswell; Nick; Crestani; Fabio; Crouch … In: ACMTransactions on Information Systems; Vol. 25; No. 4; 15; 01.10.2007.,ACM Transactions on Information Systems,2007,*
Harvesting and organizing knowledge from the web,Gerhard Weikum,Abstract Information organization and search on the Web is gaining structure and contextawareness and more semantic flavor; for example; in the forms of faceted search; verticalsearch; entity search; and Deep-Web search. I envision another big leap forward byautomatically harvesting and organizing knowledge from the Web; represented in terms ofexplicit entities and relations as well as ontological concepts. This will be made possible bythe confluence of three strong trends: 1) rich Semantic-Web-style knowledge repositorieslike ontologies and taxonomies; 2) large-scale information extraction from high-quality textsources such as Wikipedia; and 3) social tagging in the spirit of Web 2.0. I refer to the threedirections as Semantic Web; Statistical Web; and Social Web (at the risk of someoversimplification); and I briefly characterize each of them.,East European Conference on Advances in Databases and Information Systems,2007,*
Relevance Feedback using Query Logs,Gaurav Pandey; Gerhard Weikum; Julia Luxenburger,Abstract A search engine retrieves the documents based on the query submitted to it.However; incorporation of user modelling; by the inclusion of past information (like theprevious queries submitted and the titles of the documents clicked) is expected to increasethe accuracy of the search results. Especially; in the case of short term history; such historyinformation is highly related with the current user query and can help in explaining the userinformation needs in a batter way. In order to do the same; we develop and experiment withsome “history incorporation and term reweighting” techniques that incorporate the userhistory along with the current query. These techniques expand the current query byincluding terms from the history queries and the document titles; and reweight the terms. Theresults confirm that the incorporation of history along with the current query is …,*,2007,*
Journal on data semantics VIII,David Hutchison; Takeo Kanade; Josef Kittler,*,*,2007,*
On the Spectral Retrieval and Efficient Top-k Query Processing,Debapriyo Majumdar; Gerhard Weikum; Holger Bast,Autor: Majumdar; Debapriyo et al.; Genre: Hochschulschrift; Im Druck veröffentlicht: 2007;Titel: On the Spectral Retrieval and Efficient Top-k Query Processing.,*,2007,*
Retrieval Model Enhancement by Implicit Feedback from Query Logs,Gaurav Pandey; Gerhard Weikum; Hannah Bast,Autor: Pandey; Gaurav et al.; Genre: Hochschulschrift; Im Druck veröffentlicht: 2007; Titel:Retrieval Model Enhancement by Implicit Feedback from Query Logs.,*,2007,*
Efficient Large-Scale Clustering of Spelling Variants; with Applications to Error-Tolerant Text Search,Marjan Celikik; Gerhard Weikum; Holger Bast,Zusammenfassung In this thesis; the following spelling variants clustering problem isconsidered: Given a list of distinct words; called lexicon; compute (possibly overlapping)clusters of words which are spelling variants of each other. We are looking for algorithmsthat are both efficient and accurate. Accuracy is measured with respect to human judgment;eg; a cluster is 100 accurate if it contains all true spelling variants of the unique correct wordit contains and no other words; as judged by a human. We have sifted the large body ofliterature on approximate string searching and spelling correction problem for itsapplicability to our problem. We have combined various ideas from previous approaches totwo new algorithms; with two distinctly different trade-offs between efficiency and accuracy.We have analyzed both algorithms and tested them experimentally on a variety of test …,*,2007,*
Efficient Semantic Annotation of the English Wikipedia,Alexandru Chitea; Hannah Bast; Gerhard Weikum,Autor: Chitea; Alexandru et al.; Genre: Hochschulschrift; Im Druck veröffentlicht: 2007;Titel: Efficient Semantic Annotation of the English Wikipedia.,*,2007,*
of Book: Dynamics of Search Engines: An Introduction,Sergey Chernov; Pavel Serdyukov; Matthias Bender; Sebastian Michel; Gerhard Weikum; Christian Zimmer,Document title: Database Selection and Result Merging in P2P Web Search Authors: Chernov;Sergey; Serdyukov; Pavel; Bender; Matthias; Michel; Sebastian; Weikum; Gerhard; Zimmer;Christian Document type: InBook Language: English Publisher: The Icfai University PressIntended Educational Use: No External Publication Status: published Audience: ExpertsOnly Title of Book: Dynamics of Search Engines: An Introduction Date of Publication(YYYY-MM-DD): 2007 Place of Publication: Hyderabad; India Last Change of the Resource(YYYY-MM-DD): 2008-03-19 Full Name of Book-Editor(s): Ravi Kumar Jain BandamuthaCommunicated by: Gerhard Weikum Affiliations: MPI für Informatik/Databases and InformationSystems Group Identifiers: ISBN:81-314-0679-2,*,2007,*
of Proceedings: Preproceedings of INEX 2007,Andreas Broschart; Ralf Schenkel; Martin Theobald; Gerhard Weikum,Abstract/Description: This paper describes the setup and results of the Max-Planck-Institut f{\" u} r Informatik's contributions for the INEX 2007 AdHoc Track task. The runs wereproduced with TopX; a search engine for ranked retrieval of XML data that supports aprobabilistic scoring model for full-text content conditions and tag-term combinations; pathconditions as exact or relaxable constraints; and ontology-based relaxation of terms and tagnames.,*,2007,*
of Proceedings: String Processing and Information Retrieval: 14th International Symposium; SPIRE 2007,Ralf Schenkel; Andreas Broschart; Seungwon Hwang; Martin Theobald; Gerhard Weikum,Abstract/Description: In addition to purely occurrence-based relevance models; termproximity has been frequently used to enhance retrieval quality of keyword-oriented retrievalsystems. While there have been approaches on effective scoring functions that incorporateproximity; there has not been much work on algorithms or access methods for their efficientevaluation. This paper presents an efficient evaluation framework including a proximityscoring function integrated within a top-k query engine for text retrieval. We proposeprecomputed and materialized index structures that boost performance. The increasedretrieval effectiveness and efficiency of our framework are demonstrated through extensiveexperiments on a very large text benchmark collection. In combination with static indexpruning for the proximity lists; our algorithm achieves an improvement of two orders of …,*,2007,*
of Proceedings: Digital Libraries: Research and Development; First International DELOS Conference,Christian Zimmer; Christos Tryfonopoulos; Gerhard Weikum,Abstract/Description: We present a new architecture for efficient search and approximateinformation filtering in a distributed {P} eer-to-{P} eer ({P2P}) environment of Digital Libraries.The {M} inerva {L} ight search system uses {P2P} techniques over a structured overlaynetwork to distribute and maintain a directory of peer statistics. Based on the same directory;the {MAPS} information filtering system provides an approximate publish/subscribefunctionality by monitoring the most promising digital libraries for publishing appropriatedocuments regarding a continuous query. In this paper; we discuss our system architecturethat combines searching and information filtering abilities. We show the system componentsof {M} inerva {L} ight and explain the different facets of an approximate pub/sub system forsubscriptions that is high scalable; efficient; and notifies the subscribers about the most …,*,2007,*
Standing On the Shoulders of Peers: Caching in Peer-to-Peer Information Retrieval,Christian Zimmer; Srikanta Bedathur; Gerhard Weikum,A number of research prototypes of P2P-based Web search and information retrievalengines have been developed recently. These systems allow users to autonomously collectand index a subsets of the Web and share it with the rest of the community over a P2Pnetwork. This network of collections is queried via a set of keywords; similar to modernsearch engines. In order to be effective; a P2P search engine has to address the followingtwo conflicting requirements while keeping the computational costs at each involved peerlow:(i) provide high quality results in terms of precision and recall; and (ii) support scalabilitysuch that the number of participating peers is unlimited while handling large volumes ofdata. A few recent research prototypes have successfully overcome many of the issues inP2P search systems [1–4]. Despite these advances; large-scale deployment of P2P …,Fifth International Workshop on Databases; Information Systems and Peer-to-Peer Computing,2007,*
of Proceedings: Databases; Information Systems; and Peer-to-Peer Computing: International Workshops; DBISP2P 2005/2006,Sergey Chernov; Pavel Serdyukov; Matthias Bender; Sebastian Michel; Gerhard Weikum; Christian Zimmer,Abstract/Description: Intelligent Web search engines are extremely popular now. Currently;only the commercial centralized search engines like Google can process terabytes of Webdata. Alternative search engines fulfilling collaborative Web search on a voluntary basis areusually based on a blooming Peer-to-Peer (P2P) technology. In this paper; we investigatethe effectiveness of different database selection and result merging methods in the scope ofP2P Web search engine Minerva. We adapt existing measures for database selection andresults merging; all directly derived from popular document ranking measures; to addressthe specific issues of P2P Web search. We propose the general approach to both tasksbased on the combination of pseudo-relevance feedback methods. From experiments withTREC Web data; we observe that the pseudo-relevance feedback information from the …,*,2007,*
of Proceedings: Advances in Databases and Information Systems: 11th East European Conference; ADBIS 2007,Gerhard Weikum,Abstract/Description: Information organization and search on the {W} eb is gaining structureand context awareness and more semantic flavor; for example; in the forms of facetedsearch; vertical search; entity search; and {D} eep-{W} eb search. I envision another big leapforward by automatically harvesting and organizing knowledge from the {W} eb; representedin terms of explicit entities and relations as well as ontological concepts. This will be madepossible by the confluence of three strong trends: 1) rich {S} emantic-{W} eb-style knowledgerepositories like ontologies and taxonomies; 2) large-scale information extraction from high-quality text sources such as {W} ikipedia; and 3) social tagging in the spirit of {W} eb 2.0. Irefer to the three directions as {S} emantic {W} eb;{S} tatistical {W} eb; and {S} ocial {W} eb(at the risk of some oversimplification); and I briefly characterize each of them.,*,2007,*
of Proceedings: PersDL 2007: 10th DELOS Thematic Workshop on Personalized Access; Profile Management; and Context Awareness in Digital Libraries,Julia Luxenburger; Eric van der Meulen; Gerhard Weikum,*,*,2007,*
of Book: Dynamics of Search Engines: An Introduction,Jens Graupmann; Michael Biwer; Christian Zimmer; Patrick Zimmer; Matthias Bender; Martin Theobald; Gerhard Weikum,*,*,2007,*
Database Transactions-an Overview,Gottfried Vossen; Gerhard Weikum,*,*,2007,*
of Proceedings: Research and Advanced Technology for Digital Libraries: 11th European Conference; ECDL 2007,Christian Zimmer; Christos Tryfonopoulos; Gerhard Weikum,Abstract/Description: We present Minerva {DL}; a digital library architecture that supportsapproximate information retrieval and filtering functionality under a single unifyingframework. The architecture of {M} inerva {DL} is based on the peer-to-peer search engine{M} inerva; and is able to handle huge amounts of data provided by digital libraries in adistributed and self-organizing way. The two-tier architecture and the use of the distributedhash table as the routing substrate provides an infrastructure for creating large networks ofdigital libraries with minimal administration costs. We discuss the main components of thisarchitecture; present the protocols that regulate node interactions; and experimentallyevaluate our approach. This work has been partly supported by the {DELOS}{N} etwork of{E} xcellence and the {EU}{I} ntegrated {P} roject {AEOLUS}.,*,2007,*
A Time Machine for Text Search,Klaus Berberich Srikanta Bedathur Thomas Neumann; Gerhard Weikum,*,*,2007,*
Unstoppable stateful PHP web services,German Shegalov; Gerhard Weikum; Klaus Berberich,Abstract This paper presents the architecture and implementation of the EOS 2 failure-masking framework for composite Web Services. EOS 2 is based on the recently proposednotion of interaction contracts (IC); and provides exactly-once execution semantics forgeneral; arbitrarily distributed Web Services in the presence of message losses andcomponent crashes without requiring explicit coding effort by the application programmer.The EOS 2 implementation masks failures by adding a recovery layer to popular Webtechnology products:(i) the server-side script language PHP run on Apache Web server; and(ii) Internet browsers like IE to deliver recovery guarantees to the end-user.,International Conference on Web Information Systems Engineering,2006,*
Efficient and Decentralized PageRank Approximation in a P2P Network,Josiane Xavier Parreira; Debora Donato; Sebastian Michel; Gerhard Weikum,Page 1 …,*,2006,*
The database research group at the Max-Planck Institute for Informatics,Gerhard Weikum,Abstract The Max-Planck Institute for Informatics (MPI-INF) is one of 80 institutes of the Max-Planck Society; Germany's premier scientific organization for foundational research withnumerous Nobel prizes in natural sciences and medicine. MPI-INF hosts about 150researchers (including graduate students) and comprises 5 research groups on algorithmsand complexity; programming logics; computational biology and applied algorithmics;computer graphics; and databases and information systems (DBIS). This report gives anoverview of the DBIS group's mission and ongoing research.,ACM SIGMOD Record,2006,*
A Study for the Design of Personalization Functions in TEL,Maristella Agosti; Giorgio Maria Di Nunzio; Yannis Ioannidis; Julia Luxenburger; Gerhard Weikum,Background The European Library 12 is a Web portal which offers access to differentresources of 45 national libraries of Europe. This portal has its origin in the TEL projectwhose aim was to investigate the feasibility of establishing a new service to give access tothe combined resources of the national libraries of Europe. The European Library service isaimed at informed citizens world-wide (both professional and non-professional) who want apowerful and simple way of finding library materials. Moreover; it is expected to attractresearchers as there is a vast virtual collection of material from all disciplines. It offersanyone with an interest a simple route to access European cultural resources.,Network of Excellence on Digital Libraries,2006,*
Personalized Query Routing in Peer-to-Peer Federations of Digital Libraries,Matthias Bender; Norbert Fuhr; Yannis Ioannidis; Donald Kossmann; Hans-Jörg Scheck; Gerhard Weikum; Pavel Zezula; Christian Zimmer,The peer-to-peer (P2P) paradigm [SW05] is a promising approach for coping withdynamically evolving federations of loosely coupled digital libraries where user agents withpowerful personalized tools may participate besides the libraries as peers; too. Theadvantages include scalability; failure robustness; and reduced vulnerability to informationmanipulation or attacks. The participating peers are autonomous and collaborate on behalfof user requests at their discretion. A user peer applies its local profile to pose appropriatequeries to the most suitable library peers for the given information demand. All-dominant isthe decision about which libraries should receive a query. In the literature; the decision isknown as query routing or database selection or resource selection. In the area ofdistributed IR and metasearch engines [LC03; NF03; MYL02]; database selection has …,Network of Excellence on Digital Libraries,2006,*
202 Michel; S. 30 Milani; A. 3 98 249 249 301 Miranda; H. Monod; M.,F Morabito; M Bender; R Beraldi; V Bezençon; D Calvanese; I Carreras; I Chlamtac; A Corsaro; G Cortese; P Cudré-Mauroux; F Davide; G De Giacomo; F De Pellegrini; V Dohnal; P Eugster; P Felber; R Guerraoui; I Gupta; F Heine; C Kiraly; S Leggio; D Lembo; M Lenzerini; V Martins; D Novak; E Pacitti; T Pitoura; V Quema; L Rodrigues; S Scipioni; S Sivasubramanian; M Szymaniak; P Triantafillou; S Tucci Piergiovanni; P Valduriez; M van Steen; A Virgillito; G Weikum; P Zezula; C Zimmer,Global Data Management 359 R. Baldoni et al.(Eds.) IOS Press; 2006 © 2006 The authors. Allrights reserved. Aberer; K. Aekaterinidis; I. Akbarinia; R. Altherr; P. Baehni; S. Bender; M.Beraldi; R. Bezençon; V. Calvanese; D. Carreras; I. Chlamtac; I. Corsaro; A. Cortese; G.Cudré-Mauroux; P. Davide; F. De Giacomo; G. De Pellegrini; F. Dohnal; V. Eugster; P.Felber; P. Guerraoui; R. Gupta; I. Heine; F. Kiraly; C. Leggio; S. Lembo; D. Lenzerini; M.Martins; V. Author Index 202 Michel; S. 30 Milani; A. 3 98 249 249 301 Miranda; H. Monod;M. Morabito; F. Novak; D. 53 249 Ntarmos; N. Pacitti; E. 177 146 146 Pierre; G. Pitoura; T.Quema; V …,Global Data Management,2006,*
of Proceedings: Advances in XML Information Retrieval and Evaluation; 4th International Workshop of the Initiative for the Evaluation of XML Retrieval; INEX 2005,Ralf Schenkel; Martin Theobald,*,*,2006,*
''To Infinity and Beyond'': P2P Web Search with Minerva and Minerva∞,Matthias Bender; Sebastian Michel; Peter Triantafillou; Gerhard Weikum; Christian Zimmer; Roberto Baldoni; Giovanni Cortese; Fabrizio Davide; Angelo Melpignano,Abstract Peer-to-peer (P2P) computing is an intriguing paradigm for Web search for severalreasons: 1) the computational resources of a huge computer network can facilitate richermathematical and linguistic models for ranked retrieval; 2) the network provides acollaborative infrastructure where recommendations of many users and the communitybehavior can be leveraged for better search result quality; and 3) the decentralizedarchitecture of a P2P search engine is a great alternative to the de-facto monopoly of the fewlarge-scale commercial search services with the potential risk of information bias or evencensorship. The challenges of implementing this visionary approach lie in coping with thehuge scale and high dynamics of P2P networks. This paper discusses the architecturaldesign space for a scalable P2P Web search engine and presents two specific …,*,2006,*
Query-driven Term Correlations for Advanced P2P Query Routing,Thomas Mangel; Gerhard Weikum,Autor: Mangel; Thomas et al.; Genre: Hochschulschrift; Im Druck veröffentlicht: 2006; Titel:Query-driven Term Correlations for Advanced P2P Query Routing.,*,2006,*
Yago-a core of semantic knowledge,Gjergji Kasneci; Fabian Suchanek; Gerhard Weikum,Abstract We present YAGO; a light-weight and extensible ontology with high coverage andquality. YAGO builds on entities and relations and currently contains roughly 900;000entities and 5;000;000 facts. This includes the Is-A hierarchy as well as non-taxonomicrelations between entities (such as relation {hasWonPrize}). The facts have beenautomatically extracted from the unification of Wikipedia and WordNet; using a carefullydesigned combination of rule-based and heuristic methods described in this paper. Theresulting knowledge base is a major step beyond WordNet: in quality by adding knowledgeabout individuals like persons; organizations; products; etc. with their semantic relationships--and in quantity by increasing the number of facts by more than an order of magnitude. Ourempirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based …,*,2006,*
MAPS: Approximate Publish/Subscribe Functionality in Peer-to-Peer Networks,Christos Tryfonopoulos; Manolis Koubarakis; GR73100 Chania; Klaus Berberich; Gerhard Weikum; Christian Zimmer,ABSTRACT Information filtering has been a research issue for years. The information needrepresented by subscriptions results in notifications about published documents or events.The combination of the publish/subscribe scenario with the peerto-peer (P2P) approach ofautonomous peers makes high demands on the scalability and the efficiency of such a givenhighly distributed network. However; we are sure that a peer is not interested in gettingnotified about all published documents or events but rather in the most interesting ones. Inthis paper; we present our approach of an approximate publish/subscribe search system forcontinuous queries that is high scalable; efficient; and notifies the subscribers about themost interesting publications. Our work builds up a distributed global directory containinginformation about the peer behavior. The directory is leveraged to combine the current …,*,2006,*
Time-series Rule Discovery on Gene Expression Data,Isabell Schu; Gerhard Weikum; Hans-Peter Lenhof,Abstract Gene expression data capture which genes are activated or inhibited at a particularpoint in time. The mechanism of gene expression depends on transcription factors that workas promoters or enhancers of the gene expression. However; the control mechanism ofgene expression is partly unknown. We aim at finding dependencies between genes of theform if gene A is active then gene B becomes active or inactive within a certain time with thegoal to constitute new and important biological information about gene expression. To reachthis goal we model the dependencies; described above; as association rules. We extend themost efficient algorithm for finding association rules; the A-priori algorithm [2]; to discoverrules with a certain time offset from the given gene expression data. We have to handle theproblem of finding an immense number of rules and false positive rules; ie rules that are …,*,2006,*
of Proceedings: PKDD 2006: 10th European Conference on Principles and Practice of Knowledge Discovery in Databases,Georgiana Ifrim; Gerhard Weikum,Abstract/Description: We present a generative model based approach for transductivelearning for text classification. Our approach combines three methodological ingredients:learning from background corpora; latent variable models for decomposing the topic-wordspace into topic-concept and concept-word spaces; and explicit knowledge models (light-weight ontologies; thesauri; eg WordNet) with named concepts for populating latentvariables. The combination has synergies that can boost the combined performance. Thispaper presents the theoretical model and extensive experimental results on three datacollections. Our experiments show improved classification results over state-of-the-artclassification techniques such as the Spectral Graph Transducer and Transductive SupportVector Machines; particularly for the case of sparse training.,*,2006,*
06121 Abstracts Collection--Atomicity: A Unifying Concept in Computer Science,Gerhard Weikum; Clifford B Jones; David Lomet; Alexander Romanovsky,Abstract From 19.03. 06 to 24.03. 06; the Dagstuhl Seminar 06121``Atomicity: A UnifyingConcept in Computer Science''was held in the International Conference and ResearchCenter (IBFI); Schloss Dagstuhl. During the seminar; several participants presented theircurrent research; and ongoing work and open problems were discussed. Abstracts of thepresentations given during the seminar as well as abstracts of seminar results and ideas areput together in this paper. The first section describes the seminar topics and goals ingeneral. Links to extended abstracts or full papers are provided; if available.,Dagstuhl Seminar Proceedings,2006,*
of Proceedings: Third International Workshop on Databases; Information Systems and Peer-to-Peer Computing (DBISP2P 2005),Sergey Chernov; Pavel Serdyukov; Matthias Bender; Sebastian Michel; Gerhard Weikum; Christian Zimmer,*,*,2006,*
3.2." To Infinity and Beyond": P2P Web Search with Minerva and Minerva,M Bender; S Michel; P Triantafillou; G Weikum; C Zimmer,*,EMERGING COMMUNICATION,2006,*
A Reproducible Benchmark for P2P Retrieval,Thomas Neumann Matthias Bender Sebastian Michel; Gerhard Weikum,ABSTRACT With the growing popularity of information retrieval (IR) in distributed systemsand in particular P2P Web search; a huge number of protocols and prototypes have beenintroduced in the literature. However; nearly every paper considers a different benchmark forits experimental evaluation; rendering their mutual comparison and the quantification ofperformance improvements an impossible task. We present a standardized; generalpurpose benchmark for P2P IR systems that finally makes this possible. We start bypresenting a detailed requirement analysis for such a standardized benchmark frameworkthat allows for reproducible and comparable experimental setups without sacrificingflexibility to suit different system models. We further suggest Wikipedia as a publicly-available and all-purpose document corpus and finally introduce a simple but yet flexible …,*,2006,*
Information Search in Peer-to-Peer Systems.,Gerhard Weikum,ABSTRACT The peer-to-peer (P2P) computing paradigm has been very successful in theproliferation of global applications like file sharing in Internet-wide communities (eg;Gnutella; BitTorrent) or IP telephony (eg; Skype). P2P systems should be completelydecentralized and should work without any centralized components that could becomebottlenecks in terms of performance; availability; or vulnerability to attacks. They should bescalable without any limitations; by being able to grow from a few nodes to many millions ofcomputers. They emphasize the autonomy of the underlying computers and should toleratefrequent node failures; high dynamics in terms of rapidly changing data and loadcharacteristics; and high churn by allowing nodes to join and leave the network without priornotice. They should even be robust to misbehaving peers which may span egoistic …,COMAD,2006,*
Global document frequency estimation in peer-to-peer web search,S Michel; M Bender; P Triantafillou; G Weikum,*,*,2006,*
of Proceedings: Web Information Systems-WISE 2006; 7th International Conference on Web Information Systems Engineering,German Shegalov; Gerhard Weikum; Klaus Berberich,*,*,2006,*
06121 Executive Summary--Atomicity: A Unifying Concept in Computer Science,Gerhard Weikum; Clifford B Jones; David Lomet; Alexander Romanovsky,Abstract This seminar was based on and continued the interaction of different computer-science communities that was begun in an earlier Dagstuhl seminar in April 2004. Bothseminars have aimed at a deeper understanding of the fundamental concept of atomicactions and their roles in system design; execution; modeling; and correctness reasoning;and at fostering collaboration; synergies; and a unified perspective across largely separatedresearch communities. Each of the two seminar brought together about 30 researchers andindustrial practitioners from the four areas of database and transaction processing systems;fault tolerance and dependable systems; formal methods; and to smaller extent; hardwarearchitecture and programming languages. The interpretations and roles of the atomicityconcept (s) vary substantially across these communities. For example; the emphasis in …,Dagstuhl Seminar Proceedings,2006,*
Foreword,Dan Suciu; Gerhard Weikum,This issue consists of extended versions of research papers selected from those presented atSIGMOD and PODS 2004; which took place in Paris; France; on June 14–17. SIGMOD had aprogram consisting of 69 papers; and PODS had a program consisting of 30 papersrespectively. Continuing a tradition established several years ago; ACM TODS in collaborationwith the two program chairs of SIGMOD and PODS have invited some of the highest ranked researchpapers in the two conferences for submission to ACM TODS. The authors of the selected paperswere asked to significantly extend their work to include substantially new material; and eachsubmission went through a thorough review process. We are pleased to present in this issueof ACM TODS the seven papers invited from SIGMOD and PODS 2004. Gerhard Weikum wasassisted in selecting the papers from SIGMOD by the Sigmod Best Paper Award …,ACM Transactions on Database Systems (TODS),2005,*
It is a pleasure and a privilege to write this first letter to you; the SIGMOD membership; in my newly-elected role of Chair. Mary Fernández; Yannis Ioannidis and I hav...,Rakesh Agrawal; Phil Bernstein; Peter Buneman; David DeWitt; Hector Garcia-Molina; Jim Gray; Masaru Kitsuregawa; Jiawei Han; Alberto Laender; Tamer Ozsu; Krithi Ramamritham; Hans Schek; Rick Snodgrass; Gerhard Weikum,One of the first actions of the new Executive Committee was to invite several distinguishedmembers of the community to serve on the SIGMOD Advisory Board; and I'm happy toannounce that we now have a wonderful resource renewed. We will benefit greatly fromtheir insight; institutional memory; and breadth of experience. The new board is:,SIGMOD Record,2005,*
Information Security and Privacy: 10th Australasian Conference; ACISP 2005; Brisbane; Australia; July 4-6; 2005; Proceedings,Colin Boyd; Juan M González Nieto,The 2005 Australasian Conference on Information Security and Privacy was the tenth in theannual series that started in 1996. Over the years ACISP has grown from a relatively smallconference with a large proportion of papers coming from Australia into a truly internationalconference with an established reputation. ACISP 2005 was held at Queensland Universityof Technology in Brisbane; during July 4–6; 2005. This year there were 185 papersubmissions and from these 45 papers were accepted. Accepted papers came from 13countries; with the largest proportions coming from Australia (12); China (8) and Japan (6).India and Korea both contributed 2 papers and one came from Singapore. There were also11 papers from European countries and 3 from North America. We would like to extend oursincere thanks to all authors who submitted papers to ACISP 2005. The contributed …,*,2005,*
TopX: Efficient Top-k Query Processing for Text; Semistructured; and Structured Data,Martin Theobald; Gerhard Weikum; Norbert Fuhr,Zusammenfassung TopX is a top-$ k $ retrieval engine for text and XML data. UnlikeBoolean engines; it stops query processing as soon as it can safely determine the $ k $ top-ranked result objects according to a monotonous score aggregation function with respect toa multidimensional query. The main contributions of the thesis unfold into four main points;confirmed by previous publications at international conferences or workshops:\begin{itemize}\item Top-$ k $ query processing with probabilistic guarantees.\item Index-accessoptimized top-$ k $ query processing.\item Dynamic and self-tuning; incremental queryexpansion for top-$ k $ query processing.\item Efficient support for ranked XML retrieval andfull-text search.\end {itemize} Our experiments demonstrate the viability and improvedefficiency of our approach compared to existing related work for a broad variety of …,*,2005,*
The Atomic Manifesto,Gerhard Weikum,Zusammenfassung This paper is a manifesto for future research on" atomicity" in its manyguises and is based on a five-day workshop on" Atomicity in System Design and Execution"that took place in Schloss Dagstuhl in Germany in April 2004.,Journal of Universal Computer Science,2005,*
of Proceedings: Knowledge discovery in databases: PKDD 2005: 9th European Conference on Principles and Practice of Knowledge Discovery in Databases,Dimitrios Mavroeidis; George Tsatsaronis; Michalis Vazirgiannis; Martin Theobald; Gerhard Weikum,Abstract/Description: The introduction of hierarchical thesauri (HT) that contain significantsemantic information; has led researchers to investigate their potential for improvingperformance of the text classification task; extending the traditional “bag of words”representation; incorporating syntactic and semantic relationships among words. In thispaper we address this problem by proposing a Word Sense Disambiguation (WSD)approach based on the intuition that word proximity in the document implies proximity also inthe HT graph. We argue that the high precision exhibited by our WSD algorithm in varioushumanly-disambiguated benchmark datasets; is appropriate for the classification task.Moreover; we define a semantic kernel; based on the general concept of GVSM kernels; thatcaptures the semantic relations contained in the hierarchical thesaurus. Finally; we …,*,2005,*
of Proceedings: On the Move to Meaningful Internet Systems 2005: CoopIS; DOA; and ODBASE: OTM Confederated International Conferences; CoopIS; DOA; and O...,Odysseas Papapetrou; Sebastian Michel; Matthias Bender; Gerhard Weikum,*,*,2005,*
Towards Peer-to-Peer Web Search,Gerhard Weikum; Holger Bast; Geoffrey Canright; David Hales; Christian Schindelhauer; Peter Triantafillou,The peer-to-peer (P2P) computing paradigm is an intriguing alternative to Google-stylesearch engines for querying and ranking Web content. In a network with many thousands ormillions of peers the storage and access load requirements per peer are much lighter thanfor a centralized Google-like server farm; thus more powerful techniques from informationretrieval; statistical learning; computational linguistics; and ontological reasoning can beemployed on each peer's local search engine for boosting the quality of search results [1; 2;10–12; 26]. In addition; peers can dynamically collaborate on advanced and particularlydifficult queries. Moroever; a peer-to-peer setting is ideally suited to capture local userbehavior; like query logs and click streams; and disseminate and aggregate this informationin the network; at the discretion of the corresponding user; in order to incorporate richer …,Untitled Event,2005,*
of Proceedings: Web information systems engineering-WISE 2005: 6th International Conference on Web Information Systems Engineering,Stefan Siersdorfer; Gerhard Weikum,*,*,2005,*
Short Paper Session 2-Recommendation and Web Information Extraction-Automated Retraining Methods for Document Classification and Their Parameter Tuning,Stefan Siersdorfer; Gerhard Weikum,*,Lecture Notes in Computer Science,2005,*
of Proceedings: Middleware 2005: ACM; IFIP; USENIX 6th International Middleware Conference,Sebastian Michel; Peter Triantafillou; Gerhard Weikum,*,*,2005,*
Cooperative Information Systems (CoopIS) 2005 International Conference-Heterogeneity-On the Usage of Global Document Occurrences in Peer-to-Peer Informatio...,Odysseas Papapetrou; Sebastian Michel; Matthias Bender; Gerhard Weikum,*,Lecture Notes in Computer Science,2005,*
of Proceedings: Advances in information retrieval: 27th European Conference on IR Research; ECIR 2005,Stefan Siersdorfer; Gerhard Weikum,*,*,2005,*
Informations-und Wissensmanagement im Jahr 2025: BTW allez oder BTW passeé.,Gerhard Weikum; Gottfried Vossen; Frank Leymann; Peter C Lockemann; Wolffried Stucky,Abstract: Datenbanksysteme sind eine ausgereifte Technologie. Sie bilden das Rückgratvieler moderner IT-Lösungen; und ihr hoher Impact-Faktor steht außer Frage. Auf derwissenschaftlichen Seite kann man sich aber dementsprechend fragen; wie spannend undergiebig das Gebiet noch sein mag. Ist und bleibt es attraktiv für jungeNachwuchswissenschaftler; oder werden die talentiertesten Köpfe in der Informatik eher vonGrand-Challenge-Themen wie Quanten-Computing; beweisbar sicherer Kryptographie oderSpieltheorie fürs Internet angezogen? Entwickelt sich das Datenbankgebiet zu einer reinenInfrastruktur für spannende Anwendungen etwa in der Bioinformatik-als Service für dieNaturwissenschaften nützlich; aber selbst ohne Sex-Appeal und wirklicheHerausforderungen? Ist Datenbanktechnologie primär durch Anwendungen getrieben …,BTW,2005,*
of Proceedings: European Conference on Complex Systems (ECCS'05) Workshop on Peer-to-peer Data Management in the Complex Systems Perspective,Gerhard Weikum; Holger Bast; Geoffrey Canright; David Hales; Christian Schindelhauer; Peter Triantafillou,Abstract/Description: The peer-to-peer computing paradigm is an intriguing alternative toGoogle-style search engines for querying and ranking Web content. In a network with manythousands or millions of peers the storage and access load requirements per peer are muchlighter than for a centralized Google-like server farm; thus more powerful techniques frominformation retrieval; statistical learning; computational linguistics; and ontological reasoningcan be employed on each peer's local search engine for boosting the quality of searchresults. In addition; peers can dynamically collaborate on advanced and particularly difficultqueries. Moroever; a peer-to-peer setting is ideally suited to capture local user behavior; likequery logs and click streams; and disseminate and aggregate this information in thenetwork; at the discretion of the corresponding user; in order to incorporate richer …,*,2005,*
Digital library information-technology infrastructures,Γιάννης Ιωαννίδης; David Maier; Serge Abiteboul; Gerhard Weikum; Hans Schek; Fausto Rabitti; Craig Knoblock; Alon Halevy; Edward A Fox; Susan Davidson; Peter Buneman; Yannis Ioannidis,This paper charts a research agenda on systems-oriented issues in digital libraries. Itfocuses on the most central and generic system issues; including system architecture; user-level functionality; and the overall operational environment. With respect to user-levelfunctionality; in particular; it abstracts the overall information lifecycle in digital libraries tofive major stages and identifies key research problems that require solution in each stage.Finally; it recommends an explicit set of activities that would help achieve the research goalsoutlined and identifies several dimensions along which progress of the digital library fieldcan be evaluated.,*,2005,*
of Proceedings: Peer-to-peer; grid; and service-orientation in digital library architectures: 6th Thematic Workshop of the EU Network of Excellence DELOS,Matthias Bender; Sebastian Michel; Christian Zimmer; Gerhard Weikum,Abstract/Description: We consider the problem of collaborative search across a largenumber of digital libraries and query routing strategies in a peer-to-peer (P2P) environment.Both digital libraries and users are equally viewed as peers and; thus; as part of the P2Pnetwork. Our system provides a versatile platform for a scalable search engine combininglocal index structures of autonomous peers with a global directory based on a distributedhash table (DHT) as an overlay network.,*,2005,*
Approximate top-k query algorithms,S Michel; P Triantafillou; G Weikum,*,*,2005,*
Analyse der Evolution von P2P Systeme WS 2004/05 Ausarbeitung,Gerhard Weikum,Die P2P-Systeme gibt es schon seit langer Zeit in verschiedenen Anwendungsszenarien-von Mediendaten Austausch bis verteilter Speicherung von Information. Um einetheoretische Analyse eines P2P System durchzuführen; müssen wir zunächst wissen; wasman unter einem gut funktionierenden P2P System versteht. Unter einem idealen P2PSystem; versteht man ein System; das kontinuierlich lange Zeit funktionieren kann; das eineffizientes Suchprotokoll implementiert-und somit schnelle und zuverlässige Ergebnisseliefert-das das Verbinden und Trennen von Knoten zulässt und seine Netzabdeckung inkurzer Zeit noch mal richtig setzen kann. Die meisten P2P Systeme bieten volleFunktionalität; wenn das Verbinden von Knoten sequentiell funktioniert. Aber wie können wirein gleichzeitiges Verbinden von Knoten unterstützen? Wenn einige Knoten das Netz …,*,2004,*
Types for Proofs and Programs: International Workshop; TYPES 2003; Torino; Italy; April 30-May 4; 2003; Revised Selected Papers,Stefano Berardi; Mario Coppo; Ferruccio Damiani,These proceedings contain a selection of refereed papers presented at or related to the 3rdAnnual Workshop of the Types Working Group (Computer-Assisted Reasoning Based onType Theory; EU IST project 29001); which was held during April 30 to May 4; 2003; in VillaGualino; Turin; Italy. The workshop was attended by about 100 researchers. Out of 37submitted papers; 25 were selected after a refereeing process. The final choices were madeby the editors. Two previous workshops of the Types Working Group under EU IST project29001 were held in 2000 in Durham; UK; and in 2002 in Berg en Dal (close to Nijmegen);The Netherlands. These workshops followed a series of meetings organized in the period1993–2002 within previous Types projects (ESPRIT BRA 6435 and ESPRIT Working Group21900). The proceedings of these earlier workshops were also published in the LNCS …,*,2004,*
of Proceedings: Conceptual modeling; ER 2004: 23rd International Conference on Conceptual Modeling,Gerhard Weikum; Jens Graupmann; Ralf Schenkel; Martin Theobald,Abstract/Description: The envisioned Semantic Web aims to provide richly annotated andexplicitly structured Web pages in XML; RDF; or description logics; based upon underlyingontologies and thesauri. Ideally; this should enable a wealth of query processing andsemantic reasoning capabilities using XQuery and logical inference engines. However; webelieve that the diversity and uncertainty of terminologies and schema-like annotations willmake precise querying on a Web scale extremely elusive if not hopeless; and the sameargument holds for large-scale dynamic federations of Deep Web sources. Therefore;ontology-based reasoning and querying needs to be enhanced by statistical means; leadingto relevanceranked lists as query results. This paper presents steps towards such a"statistically semantic" Web and outlines technical challenges. We discuss how statistically …,*,2004,*
SIGMOD 2004: proceedings of the ACM SIGMOD International Conference on Management of Data,Gerhard Weikum; Arnd Christian Koenig; Stefan Dessloch,Herausgeber: Weikum; Gerhard et al.; Genre: Konferenzband; Im Druckveröffentlicht: 2004; Titel: SIGMOD 2004 : proceedings of the ACM SIGMODInternational Conference on Management of Data.,Untitled Event,2004,*
of Proceedings: Proceedings 2004 VLDB Conference: The 30th International Conference on Very Large Databases (VLDB),Martin Theobald; Gerhard Weikum; Ralf Schenkel,Abstract/Description: Top-k queries based on ranking elements of multidimensional datasetsare a fundamental building block for many kinds of information discovery. The best knowngeneral-purpose algo-rithm for evaluating top-k queries is Fagin's threshold algorithm (TA).Since the user's goal behind top-k queries is to identify one or a few relevant and novel dataitems; it is intriguing to use approximative variants of TA to reduce run-time costs. This paperintroduces a family of approximative top-k algorithms based on probabilistic arguments.When scanning index lists of the underlying multidimensional data space in descendingorder of local scores; various forms of convolution and derived bounds are employed topredict when it is safe; with high probability; to drop candidate items and to prune the indexscans. The precision and the efficiency of the developed methods are experimentally …,*,2004,*
of Proceedings: Proceedings of the SIGIR Workshop on Peer-to-Peer Information Retrieval: 27th Annual International ACM SIGIR Conference; SIGIR 2004 P2PIR W...,Matthias Bender; Sebastian Michel; Christian Zimmer; Gerhard Weikum,Abstract/Description: We consider the problem of collaborative Web search and queryrouting strategies in a peer-to-peer (P2P) environment. In our architecture every peer has afull-fledged search engine with a (thematically focused) crawler and a local index whosecontents may be tailored to the user's specific interest profile. Peers are autonomous andpost meta-information about their bookmarks and index lists to a global directory; which isefficiently implemented in a decentralized manner using Chord-style distributed hash tables.A query posed by one peer is first evaluated locally; if the result is unsatisfactory the query isforwarded to selected peers. These peers are chosen based on a benefit/cost measurewhere benefit reflects the thematic similarity of peers' interest profiles; derived frombookmarks; and cost captures estimated peer load and response time. The meta …,*,2004,*
of Proceedings: Web information systems; WISE 2004: 5th International Conference on Web Information Systems Engineering,Julia Luxenburger; Gerhard Weikum,Abstract/Description: The ongoing explosion of web information calls for more intelligent andpersonalied methods towards better search result quality for advanced queries. Query logand click streams obtained from web browsers or search engines can contribute to betterquality by exploiting the collaborative recommendations that are implicitly embedded in thisinformation. This paper presents a new method that incorporates the notion of query nodesinto PageRank model and integrates the implicite relevance feedback given by click streamsinto the automated process of authority analysis. This approach generalizes the well-knownrandom-surfer model into a random-expert model that mimics the behavior of an expert userin an extended session consisting of queries; query refinements; and result-navigationsteps. The enhanced PageRank scores; coined QRank scores; can be computed offline; …,*,2004,*
The Lowell Database Research Self Assessment,Stan Zdonik; Jennifer Widom; Gerhard Weikum; Jeff Ullman; Rick Snodgrass; Mike Stonebraker; Avi Silberschatz; Timos Sellis; Hans Schek; Jeff Naughton; David Maier; Serge Abiteboul; Rakesh Agrawal; Phil Bernstein; Mike Carey; Stefano Ceri; Bruce Croft; David DeWitt; Mike Franklin; Hector Garcia Molina; Dieter Gawlick; Jim Gray; Laura Haas; Alon Halevy; Joe Hellerstein; Yannis Ioannidis; Martin Kersten; Michael Pazzani; Mike Lesk,Abstract A group of senior database researchers gathers every few years to assess the stateof database research and to point out problem areas that deserve additional focus. Thisreport summarizes the discussion and conclusions of the sixth ad-hoc meeting held May 4-6;2003 in Lowell; Mass. It observes that information management continues to be a criticalcomponent of most complex software systems. It recommends that database researchersincrease focus on: integration of text; data; code; and streams; fusion of information fromheterogeneous data sources; reasoning about uncertain data; unsupervised data mining forinteresting correlations; information privacy; and self-adaptation and repair.,*,2003,*
Report on the 10th Conference on Database Systems for Business; Technology; and the Web (BTW 2003),Gerhard Weikum; Harald Schöning; Erhard Rahm,The 10th Conference on Database Systems for Business; Technology; and the Web (BTW2003 for short) took place in February 26-28; 2003 at Leipzig; a city in East Germany of greatcultural heritage: it is the city of the late work of Johann Sebastian Bach; the location ofAuerbachs Keller made famous by Goethe's Faust; the place where Napoleon was defeatedin the Battle of Nations in 1813; and the origin of the German reunion starting with theMonday Demonstrations in 1989. The BTW 2003 conference was hosted by the University ofLeipzig. The bi-annual BTW conference is the major forum of the database communities inGermany; Austria; and Switzerland; and brought together more than 270 scientists andpractitioners from this central European region and beyond. The program included 3keynotes; 14 long and 17 short papers in the scientific track that were selected by the …,SIGMOD Record,2003,*
Datenbanksysteme für Business; Technologie und Web (BTW): 10. GI-Fachtagung,Gerhard Weikum; Harald Schöning; Erhard Rahm,Editor: Weikum; Gerhard et al.; Genre: Proceedings; Published in Print: 2003; Title:Datenbanksysteme für Business; Technologie und Web (BTW) : 10. GI-Fachtagung.,BTW 2003,2003,*
Intelligent search on XML data-Preface,HM Blanken; T Grabs; HJ Schek; R Schenkel; G Weikum,*,INTELLIGENT SEARCH ON XML DATA,2003,*
of Book: Intelligent Search on XML Data,Ralf Schenkel; Anja Theobald; Gerhard Weikum,Document title: Ontology-Enabled XML Search Authors: Schenkel; Ralf; Theobald; Anja; Weikum;Gerhard Editors: Blanken; Henk; Grabs; Torsten; Schek; Hans-Jörg; Schenkel; Ralf; Weikum;Gerhard Document type: InBook Language: English Publisher: Springer Intended EducationalUse: No Volume: 2818 Review Status: not specified External Publication Status: publishedAudience: Experts Only Title of Book: Intelligent Search on XML Data Date of Publication(YYYY-MM-DD): 2003 Place of Publication: Berlin; Germany Last Change of the Resource(YYYY-MM-DD): 2004-07-05 Full Name of Book-Editor(s): Blanken; Henk; Grabs; Torsten; Schek;Hans-Jörg; Schenkel; Ralf; Weikum; Gerhard Communicated by: Gerhard Weikum Affiliations:MPI für Informatik/Databases and Information Systems Group Identifiers: ISBN:3-540-40768-5,*,2003,*
HIP: Intelligente Suche nach Fachinformationen für das Handwerk.,Sergej Sizov; Klaus Meier; Gerhard Weikum,Abstract: Angesichts des exponentiell wachsenden Informationsangebots im World WideWeb hat sich die Suche nach relevanten Ressourcen und Datenquellen mit der Zeit zueinem eigenständigen Problem entwickelt. Allgemeine Web-Suchmaschinen verwenden fürdie Erstellung der Rangliste der Treffer Autoritätswerte; die durch Linkanalyseverfahren aufrepräsentativen Web-Ausschnitten bestimmt werden (ggf. kombiniert mit textbasierterDokument-Query Ahnlichkeit). Diese Vorgehensweise hat sich bewährt für Massen-Anfragen wie'Madonna Tour'; scheitert jedoch oft bei sehr spezifischen fachlichen Anfragenmit insgesamt kleinem Recall (bekannt als Nadelim-Heuhaufen Problem). Darüber hinausbleiben zahlreiche'Hidden Web'-Informationsquellen (zB die Datenbanken derInformationsportale) für konventionelle Crawler nicht zugänglich.,GI Jahrestagung (2),2003,*
of Proceedings: Proceedings of the 8th International Conference on Extending Database Technology,Arnd Christian König; Gerhard Weikum,Abstract/Description: Maintaining statistics on multidimensional data distributions is crucialfor predicting the run-time and result size of queries and data analysis tasks with acceptableaccuracy. To this end a plethora of techniques have been proposed for maintaining acompact data" synopsis" on a single table; ranging from variants of histograms to methodsbased on wavelets and other transforms. However; the fundamental question of how toreconcile the synopses for large information sources with many tables has been largelyunexplored. This paper develops a general framework for reconciling the synopses on manytables; which may come from different information sources. It shows how to compute theoptimal combination of synopses for a given workload and a limited amount of availablememory. The practicality of the approach and the accuracy of the proposed heuristics are …,*,2002,*
of Proceedings: 28th International Conference on Very Large Databases (VLDB 2002),Gerhard Weikum; Axel Mönkeberg; Christof Hasse; Peter Zabback,Document title: Self-tuning Database Technology and Information Services: from Wishful Thinkingto Viable Engineering Authors: Weikum; Gerhard; Mönkeberg; Axel; Hasse; Christof; Zabback;Peter Document type: Conference-Paper Language: English Audience: Experts Only ExternalPublication Status: published Title of Proceedings: 28th International Conference on Very LargeDatabases (VLDB 2002) Place of Conference/Meeting: Hongkong; China Place of Publication:San Francisco; USA Last Change of the Resource (YYYY-MM-DD): 2005-03-31 (Start) Date ofConference/Meeting (YYYY-MM-DD): 2002-08-20 Date of Publication (YYYY-MM-DD): 2002End Date of Conference/Meeting (YYYY-MM-DD): 2002-08-23 Intended Educational Use: NoPublisher: Morgan Kaufmann Communicated by: Gerhard Weikum Affiliations: MPI fürInformatik/Databases and Information Systems Group Identifiers:,*,2002,*
SPECIAL SECTION: 16TH INTERNATIONAL CONFERENCE ON DATA ENGINEERING-Guest Editorial Introduction to the Special Section,DB Lomet; G Weikum,*,IEEE Transactions on Knowledge and Data Engineering,2001,*
Guest editorial introduction to the special section on the 16th international conference on data engineering,David B.  Lomet; Gerhard Weikum,HE 16th International Conference on Data Engineering (ICDE 2000) was held in San Diego;California; from 29 February through 3 March 2000. As the first major database conferenceof the new millenium; it marked the threshold of a new database era with a proliferation ofexciting data-intensive; network-centric applications; and increasing penetration of databasetechnology into the software infrastructure of cyberspace. The conference technical programreflected this; covering topics ranging from data mining and knowledge discovery to XML; e-commerce; and mobile computing; while also capturing advances in traditional and stillcritical engine-technology. The 287 submissions for the ICDE technical program required usto be very selective; but also resulted in an outstanding program. We accepted 41 researchpapers. In addition; 24 submissions were accepted for poster sessions. The technical …,IEEE Transactions on Knowledge and Data Engineering,2001,*
of Proceedings: Proceedings of 27th International Conference on Very Large Data Bases (VLDB 2001),Shalom Tsur; Serge Abiteboul; Rakesh Agrawal; Umeshwar Dayal; Johannes Klein; Gerhard Weikum,Document title: Are Web Services the Next Revolution in e-Commerce? (Panel) Authors: Tsur;Shalom; Abiteboul; Serge; Agrawal; Rakesh; Dayal; Umeshwar; Klein; Johannes; Weikum;Gerhard Document type: Conference-Paper Language: English Audience: Experts Only Titleof Series: Lecture Notes in Computer Science External Publication Status: published Title ofProceedings: Proceedings of 27th International Conference on Very Large Data Bases (VLDB2001) Place of Conference/Meeting: Roma; Italy Full Name(s) of Editor(s) of Proceedings: Apers;Peter MG; Atzeni; Paolo; Ceri; Stefano; Paraboschi; Stefano; Ramamohanarao; Kotagiri;Snodgrass; Richard T. Place of Publication: San Francisco; USA Volume (in Journal): . Last …,*,2001,*
Reminiscences on Influential Papers,Kenneth A Ross,My personal interest in database estimation and approximation problems did not start until I readthis paper a few years after it was written. I believe there are two reasons why it had such a significantinfluence on my work: its contents and its elegance. It took a comprehensive look at all the basicassumptions that database optimizers used at the time (and some still do) and studied theirimpact; showing that they generate estimates that are extreme while reality is often betterbehaved. Several other papers had approached similar subjects; but they either addressed specialcases or were rather informal and empirical. I felt that this paper cleared the field by giving consiseanswers to several critical problems; and set the stage for much of the subsequent work in theestimation/approximation area. In addition to its technical merits from a databaseperspective; however; I was also attracted to its really elegant mathematics. This paper …,ACM SIGMOD Record,2000,*
TerraServer: A Spatial Data Warehouse,Gerhard Weikum,Abstract The paper presents the application design; system architecture; and operationalprocedures of the TerraServer web site. This server manages satellite images of the earthand their relationships to maps and other geographical information in a SQL database andmakes them Internet-accessible through a middle-tier web application server. Practicalexperiences illustrate advantages and disadvantages of the design decisions. I find thecomprehensive discussion of the TerraServer application extremely insightful in terms ofhow to build and operate a very large; data-intensive; Internet-accessible application withnon-standard forms of data. Most notably; the section on the data cleansing and loadingprocess contains important lessons that everybody in our community should be made awareof.,ACM SIGMOD Digital Review,2000,*
XTRACT: A System for Extracting Document Type Descriptors from XML Documents,Gerhard Weikum,Abstract The paper describes the architecture of XTRACT; a system for inferring an accurate;meaningful; near optimal DTD schema for a repository of XML documents. The paperpresents some very interesting ideas on an important and challenging subject. The XTRACTsystem executes three steps: 1. Generalization (finding patterns in the input sequences andreplacing them with regular expressions to generate general candidate DTDs) 2. Factoring(factoring candidate DTDs using adaptions of algorithms for the optimization of Booleanfunctions) 3. applying MDL principle (applying the Minimum Description Length principle tofind the near optimal DTD among the candidates). The authors provide experimental resultsin comparison with DDbE (Data Description by Example generated by IBM alphaworks (R))The paper's key contribution lies in applying the MDL principle for defining an information …,ACM SIGMOD Digital Review,2000,*
Reminiscences on Influential Papers,Christos Faloutsos; A Levy; P O'Neil; E Simon; D Srivastava; V Vianu; G Weikum,I continue to invite unsolicited contributions to this column. (I haven't received any so far; butthe previous issue has only been out a month or so at the time of writing.) See http://www.acm.org/sigmod/record/author.html for submission guidelines … Christos Faloutsos; Carnegie MellonUniversity; christos@cs.cmu.edu … [Manfred Schroeder. Fractals; Chaos; Power Laws: Minutesfrom an Infinite Paradise. WH Freeman and Company; 1991.] … "What was the single most influentialwork for your research?" There is a handful of truly in- fluential papers: the R-tree; RAID; the AssociationRules; each started a revolution. However; I find myself citing repeatedly this masterpiecebook. Beyond George Kingsley Zipf and his famous 'law'; and beyond the milestone book byMandelbrot on fractals; Schroeder's book explains how self-similarity and power laws appearin countless phenomena; it shows how to measure the fractal dimensions; and; it gives a …,SIGMOD RECORD,2000,*
of Proceedings: Proceedings of 26th International Conference on Very Large Data Bases (VLDB 2000),Surajit Chaudhuri; Gerhard Weikum,Abstract/Description: Database technology is one of the cornerstones for the newmillennium's IT landscape. However; database systems as a unit of code packaging anddeployment are at a crossroad: commercial systems have been adding features for a longtime and have now reached complexity that makes them a difficult choice; in terms of their"gain/pain ratio"; as a central platform for value-added information services such as ERP or e-commerce. It is critical that database systems be easy to manage.,*,2000,*
of Proceedings: 5th IFCIS International Conference on Cooperative Information Systems (CoopIS 2000),Ralf Schenkel; Gerhard Weikum,Abstract/Description: This paper reconsiders the problem of transactional federations; morespecifically the concurrency control issue; with particular consideration of componentsystems that provide only snapshot isolation; which is the default setting in Oracle andwidely used in practice. The paper derives criteria and practical protocols for guaranteeingglobal serializability at the federation level. The paper generalizes the well-known ticketmethod and develops novel federation-level graph testing methods to incorporate sub-serializability component systems like Oracle. These contributions are embedded in apractical project that built a CORBA-based federated database architecture suitable formodern Internet-or Intranet-based applications such as electronic commerce. This prototypesystem; which includes a federated transaction manager coined Trafic (Transactional …,*,2000,*
of Proceedings: GI-Workshop" Internet-Datenbanken",Ralf Schenkel; Gerhard Weikum,Abstract/Description: Data consistency in transactional federations is a key requirement ofadvanced E-service applications on the Internet; such as electronic auctions or real-estatepurchase. Federated concurrency control needs to be aware of the fact that virtually allcommercial database products support sub-serializability isolation levels; such as SnapshotIsolation; and that applications make indeed use of such local options. This paper discussesthe problems that arise with regard to global serializability in such a setting; and proposessolutions. Protocols are developed that can guarantee global serializability over componentsystems that provide only weaker isolation levels. A full-fledged implementation is presentedthat makes use of OrbixOTS and runs on top of Oracle8i and O2 databases. Performancemeasurements with this prototype indicate the practical viability of the developed methods.,*,2000,*
Atomicity versus Anonymity: Distributed Transactions for Electronic Commerce,Gerhard Weikum,Zusammenfassung Payment protocols for electronic commerce have traditionally beenstudied mostly by the cryptography; security; and distributed computing communities.However; as pointed out by this invited paper; the database-style notion of atomicity is ofcrucial importance for such protocols as well and has been neglected so far. In fact; onecould argue that keeping distributed data consistent in the presence of server failures is anabsolutely critical concern in electronic commerce; whereas perfect protection againsttampering; albeit highly desirable; is not the most pressing issue given that a decent suite ofsecurity measures is already in place. In fact; being billed for some goods that one has neverreceived or accidentally receiving some ordered goods twice may turn out to be astroublesome as a stolen credit card number. Using atomic transactions for the distributed …,ACM SIGMOD Digital Review,1999,*
ELIZABETH J. O'NEIL AND PATRICK E. O'NEIL,GERHARD WEIKUM,*,Journal of the ACM.,1999,*
Intelligente Suche nach Ätzrezepturen für Werkstoffe (Kurzbeitrag),Christian Mähler; Frank Mücklich; Gerhard Weikum,Zusammenfassung Die Arbeit behandelt ein Problem der Ähnlichkeitssuche im Bereich derWerkstoffwissenschaften. In der Materialographie; einem wichtigen Teilgebiet derWerkstoffwissenschaften; benötigt man zur Untersuchung des sog. Gefüges einerWerkstoffprobe Rezepturen zur gezielten Präparation; um bestimmte Gefügeeigenschaftenunter dem Mikroskop zu kontrastieren. Diese sog. Ätzrezepturen hängen von derchemischen Zusammensetzung des Werkstoffs; seiner Behandlung und auch demspezifischen Ziel der Kontrastierung selbst ab. In Zusammenarbeit zweier Lehrstühle derInformatik und der Werkstoffwissenschaften wurde ein Internet-fähiges Informationssystemfür Ätzrezepturen entwickelt; das in der Arbeit vorgestellt wird. Das System unterstütztinsbesondere die Ähnlichkeitssuche nach annähernd passenden Ätzrezepturen; da man …,*,1999,*
of Proceedings: Proceedings of the CEUR Workshop on Enterprise-wide and Cross-enterprise Workflow Management,Michael Gillmann; Jeanine Weißenfels; Gerhard Weikum; Achim Kraiss,Document title: Performance Assessment and Configuration of Enterprise-Wide WorkflowManagement Systems (extended abstract) Authors: Gillmann; Michael; Weißenfels; Jeanine;Weikum; Gerhard; Kraiss; Achim Document type: Conference-Paper Language: English Audience:Experts Only External Publication Status: published Title of Proceedings: Proceedings of theCEUR Workshop on Enterprise-wide and Cross-enterprise Workflow Management Place ofConference/Meeting: Paderborn; Germany Full Name(s) of Editor(s) of Proceedings: Dadam;Peter; Reichert; Manfred Place of Publication: Aachen; Germany Volume (in Journal): 24 LastChange of the Resource (YYYY-MM-DD): 2006-04-10 (Start) Date of Conference/Meeting(YYYY-MM-DD): 1999-10-06 Date of Publication (YYYY-MM-DD): 1999 End Date ofConference/Meeting (YYYY-MM-DD): 1999-10-06 Intended Educational Use: No Publisher …,*,1999,*
of Proceedings: Proceedings of the 8th International Workshop on Foundations of Models and Languages for Data and Objects-Transactions and Database Dynami...,Ralf Schenkel; Gerhard Weikum; Norbert Weißenberg; Xuequn Wu,*,*,1999,*
of Proceedings: Proceedings der 8. GI-Fachtagung für Datenbanksysteme in Büro; Technik und Wissenschaft (BTW 99),Gerhard Weikum,Abstract/Description: The impressive advances in global networking and informationtechnology provide great opportunities for all kinds of ubiquitous information services;ranging from digital libraries and information discovery to virtual-enterprise workflows andelectronic commerce. However; many of these services too often exhibit rather poor qualityand are thus unsuitable for mission-critical applications. In this paper I would like toencourage more intensive research efforts towards service quality guarantees; the ultimategoal being the ability to construct and deploy truly dependable systems with provablecorrectness; continuous availability; and predictable performance. The paper aims to sort outsome of the issues towards these elusive goals; mainly by discussing a case study onworkflow management.(only the first paragraph),*,1999,*
of Proceedings: Transactions and Database Dynamics-Proceedings of the Eight International Workshop on Foundations of Models and Languages for Data and Obj...,Ralf Schenkel; Gerhard Weikum; Norbert Weißenberg; Xuequn Wu,*,*,1999,*
of Proceedings: Proceedings of 25th International Conference on Very Large Data Bases (VLDB 99),Arnd Christian König; Gerhard Weikum,Abstract/Description: This paper aims to improve the accuracy of query result-sizeestimations in query optimizers by leveraging the dynamic feedback obtained fromobservations on the executed query workload. To this end; an approximate" synopsis" ofdata-value distributions is devised that combines histograms with parametric curve fitting;leading to a specific class of linear splines. The approach reconciles the benefits ofhistograms; simplicity and versatility; with those of parametric techniques especially theadaptivity to statistically biased and dynamically evolving query workloads. The paperpresents efficient algorithms for constructing the linear-spline synopsis for data-valuedistributions from a moving window of the most recent observations on (the most critical)query executions. The approach is worked out in full detail for capturing frequency as well …,*,1999,*
Scheduling strategies for mixed workloads in multimedia information systems,G Nerjes; Y Robogianakis; P Muth; M Paterakis; P Triantafillou; G Weikum,*,*,1998,*
Parallel and Distributed Information Systems,Jeffrey F Naughton; Gerhard Weikum,Abstract. A warehouse is a data repository containing integrated information for efficientquerying and analysis. Maintaining the consistency of warehouse data is challenging;especially if the data sources are autonomous and views of the data at the warehouse spanmultiple sources. Transactions containing multiple updates at one or more sources; eg;batch updates; complicate the consistency problem. In this paper we identity and discussthree fundamental transaction processing scenarios for data warehousing. We define fourlevels of consistency for warehouse data and present a new family of algorithms; the Strobefamily; that maintain consistency as the warehouse is updated; under the variouswarehousing scenarios. All of the algorithms are incremental and can handle a continuousand overlapping stream of updates from the sources. Our implementation shows that the …,*,1998,*
New and Forgotten Dreams in Database Research,Surajit Chaudhuri; R Agrawal; K Dittrich; A Reuter; A Silberschatz; G Weikum,In last year's ICDE panel in New Orleans [l]; we examined the question of whether databaseresearch is able to provide leadership to database industries. There was a consensus thatwith the maturing of the field; we should now focus on new areas where we can leverage offour rich experience in database research. The question of what problem to work on next hasalways been a difficult one to answer and we suspect that it will not get easier. Even then; itwill be rewarding to examine the question of how the successful and not so successfulthreads of research came into being and what caught our fancy and why. Specifically; wewill seek the perspective of the panelists on the following questions: l How much progressdid we make in the new ideas which excited our field 5 years and 2 years ago respectively? lHave we seen any initiatives in our community in the last year or two that appears like a …,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON DATA ENGINEERING,1997,*
Stochastic Service Guarantees for Continuous Data on Multi-Zone Disks,Peter Muth; Guido Nerjes; Gerhard Weikum,Abstract Continuous data types like video and audio require the real-time delivery of datafragments from a server's disks to the client at which the data is displayed. This paperdevelops a stochastic model for analyzing the rate at which data fragments arrive too late atthe client and thus cause display “glitches”. The model is based on deriving the Laplace-Stieltjes transform of the service time distribution for batched disk service under a multi-userload of concurrently served continuous-data streams; and applying Chernoff bounds to thetail of the service time distribution and the resulting distribution of the glitch rate per stream.The results from the model provide the basis for configuring a server and exerting anadmission control such that the admitted streams suffer no more than a specified (small) rateof glitches with a specified (very high) probability. The model considers variable display …,*,1997,*
Dagstuhl Workshop Performance Enhancement in Object Bases (Leistungssteigerung in Objektbanken) April 1–4; 1996,U Dayal; A Kemper; G Moerkotte; G Weikum,Method Optimization A difficult problem in building an OBMS is to deal with methods; ie; thebehavioral component of data. Despite its relevance; there has been only slow progress inthis research area. So far; methods have been considered as black boxes by queryoptimizers. Even worse; it is usually difficult even to attach precise costs to methods. As aresult; they have been typically treated as unoptimizable and unpredictable components of aquery. Run Time System Optimization Typical design decisions concern buffer management;object representation; organization of client/server communication; concurrency control;recovery; prefetching objects and/or pages; etc. The evaluation of existing strategies for runtime optimization; and the development of new strategies; is the thrust of research within thistopic.,*,1996,*
Data partitioning and load balancing in parallel disk,Peter Scheuermann; Gerhard Weikum; Peter Zabback,*,*,1996,*
Fachgespräch 4: Prozessentwurf und Workflow-Management,H Österle; H-J Schek; F Fritz; S Jablonski; D Karagiannis; A Kotz Dittrich; C Stern; G Thalmeier; G Weikum,Zusammenfassung Workflow-Management-Systeme werden eingesetzt für dieAutomatisierung bestehender betrieblicher Abläufe; sie führen aber auch zurReorganisation dieser Abläufe. Ziel dieses Fachgesprächs ist es; aufzuzeigen; was unterdem Thema Business-Process-Redesign verstanden wird; welche Methoden man einsetzenkann; um betriebliche Abläufe zu analysieren und effizienter zu gestalten; was heutigeWorkflow-Management-Systeme leisten; wo sie eingesetzt werden und wasforschungsmässig getan wird; um den Stand der Technik zu verbessern.,*,1995,*
Data partitioning and load balancing in parallel storage systems,Gerhard Weikum,Summary form only given; as follows. Parallel storage systems such as disk arrays or diskfarms provide opportunities for exploiting I/O parallelism in two possible ways: viainterrequest parallelism and via intrarequest parallelism. We argue for software-controlledstorage systems in which each disk can be accessed individually and data partitioning; aswell as data allocation; is completely under the control of the file system. We discuss themain issues in performance tuning of such systems-striping and load balancing-and showtheir relationship to response time and throughput. We outline the main components of anintelligent file system that performs striping on a file-specific basis by taking into account therequirements of the applications and performs load balancing by judicious file allocation anddynamic redistributions of the data when access patterns change. Our system uses …,Mass Storage Systems; 1994.'Towards Distributed Storage and Data Management Systems.'First International Symposium. Proceedings.; Thirteenth IEEE Symposium on,1994,*
Data partitioning and load balancing in parallel disk systems,Peter Zabback; Gerhard Weikum; Peter I Scheuermann,Abstract Parallel disk systems such as disk arrays provide opportunities for exploiting I/Oparallelism in two possible ways; namely via inter-request and intra-request parallelism. Inthis paper we argue for software-controlled disk arrays in which each disk can be accessedin-dividually and data partitioning as well as data allocation is completely under the controlof the file system. We discuss the main issues in performance tuning of such systems;namely striping and load balancing; and show their relationship to response time andthroughput.,ETH; Eidgenössische Technische Hochschule Zürich; Departement Informatik; Institut für Informationssysteme,1994,*
The COMFORT prototype: a step towards automated database performance tuning,Axel Moenkeberg; Peter Zabback; Christof Hasse; Gerhard Weikum,COMFORT stands for “Gs? mfortable Perffnmmce liming”. The long-term goal that we pursuein the COMFORT project is to automate; to the largest possible extent; the performancetuning of database systems. Tuning of database systems depends critically on the expertiseand experience of system administrators and other human tuning experts which areresponsible for the setting of system parameters. The purpxe of such system parameters; or“tuning knobs”; is to adapt the system to the speeific characteristics of a given workload. Withawider useof OLTP and other multi-user databaseapplications; on the one hand; and a lackof qualified tuning experts; on the other hand; there is a strong need for simplifying the trickyjob of human administrators and ideally automating at least some critical tuning decisions.Our approach is to derive appropriate tuning heuristics; or “rules of thumb”; from …,ACM SIGMOD Record,1993,*
Data Engineering,Gerhard Weikum; Andrew Deacon; Werner Schaad; Hans Schek; Rajeev Rastogi; Sharad Mehrotra; Henry K Korth; Abraham Silberschatz,The Bulletin of the Technical Committee on Data Engineering is published quarterly and isdistributed to all TC members. Its scope includes the design; implementation; modelling;theory and application of database systems and their technology. Letters; conferenceinformation; and news should be sent to the Editor-in-Chief. Papers for each issue aresolicited by and should be sent to the Associate Editor responsible for the issue. Opinionsexpressed in contributions are those of the authors and do not necessarily re ect thepositions of the TC on Data Engineering; the IEEE Computer Society; or the authors'organizations. Membership in the TC on Data Engineering is open to all current members ofthe IEEE Computer Society who are interested in database systems.,Urbana,1993,*
Bulletin of the Technical Committee on,Gerhard Weikum; Arnd Christian; Achim Kraiss; Markus Sinnwell; Surajit Chaudhuri; Eric Christensen; Goetz Graefe; Vivek Narasayya; Michael Zwilling,Membership in the TC on Data Engineering (http: www. is open to all current members of theIEEE Computer Society who are interested in database systems. The web page for the DataEngineering Bulletin is http://www. research. microsoft. com/research/db/debull. The webpage for the TC on Data Engineering is http://www. ccs. neu. edu/groups/IEEE/tcde/index.html.,Urbana,1993,*
Automatic Tuning of Data Placement and Load Balancing in Disk Arrays,Gerhard Weikum; Peter Zabback; P Scheuermann,Weikum; G.; Zabback; P.; & Scheuermann; P. (1993). Automatic Tuning of Data Placement andLoad Balancing in Disk Arrays. Database Systems for Next‑Generation Applications‑ Principlesand Practice; Advanced Database Research and Development Series; 291-301 … AutomaticTuning of Data Placement and Load Balancing in Disk Arrays. / Weikum; Gerhard; Zabback;Peter; Scheuermann; P … In: Database Systems for Next‑Generation Applications‑ Principlesand Practice; Advanced Database Research and Development Series; 1993; p. 291-301 …Weikum; G; Zabback; P & Scheuermann; P 1993; 'Automatic Tuning of Data Placement and LoadBalancing in Disk Arrays' Database Systems for Next‑Generation Applications‑ Principles andPractice; Advanced Database Research and Development Series; pp. 291-301 … WeikumG; Zabback P; Scheuermann P. Automatic Tuning of Data Placement and Load …,Database Systems for Next‑Generation Applications‑Principles and Practice; Advanced Database Research and Development Series,1993,*
Datenpartitionierung zur Optimierung der I/O-Parallelität in Non-Standard-Anwendungen,Peter Zabback; Gerhard Weikum,Zusammenfassung Disk–Arrays sind ein vielversprechender Ansatz zur Überwindung derviel zitierten “I/O–Krise”. Der Einsatz einer großen Anzahl kleiner Plattenlaufwerke birgt einhohes Potential zur Parallelisierung von I/O–Aufträgen und trägt damit entscheidend zurReduzierung der Antwortzeit einzelner I/O–Aufträge bei. Der Schlüssel zum erfolgreichenEinsatz dieser Technologie für Non–Standard–Datenbanksysteme liegt in derPartitionierung und Verteilung der Daten über die Platten des Disk–Arrays. In diesemBeitrag stellen wir eine Methode zur Partitionierung von Dateien vor; die nicht nur auf dieMinimierung der Antwortzeit abzielt; sondern auch die Einhaltung vonDurchsatzanforderungen der Anwendungen berücksichtigt. Dabei werden die Größen derPartitionen für jede Datei individuell bestimmt. In einer umfassenden Performance …,*,1993,*
The COMFORT Project* Project Synopsis (Work in Progress) Gerhard Weikum; Christof Hasse; Axel Moenkeberg; Michael Rys; Peter Zabback ETH Zurich Departme...,Gerhard Weikum,*,Proceedings of the... International Conference on Parallel and Distributed Information Systems,1993,*
Towards a unified theory of concurrency control and recovery,Haiyan Ye; Gerhard Weikum; Hans-Jörg Schek,Abstract The theory of transaction management is based on two different and independentcriteria for the correct execution of transactions. The first criterion; serializability; ensurescorrect execution of parallel transactions under the assumption that no failures occur. Thesecond criterion; strictness; ensures correct recovery in case of failures.,ETH; Eidgenössische Technische Hochschule Zürich; Departement Informatik; Institut für Informationssysteme,1992,*
Automatic tuning of data placement and load balancing in disk arrays,Peter Zabback; Gerhard Weikum; Peter I Scheuermann,Abstract Large arrays of small disks are providing an attractive approach for highperformance I/O-systems. They allow for low-cost; reliable storage and can achieve higherthroughput compared to large disks. However; in order to make effective use of thecommercially available architectures; it is necessary to develop intelligent software tools thatallow automatic tun-ing of the disk arrays to varying workloads. In this paper we describe anintegrated set of algorithms and the implementation of a file manager for automatic filepartitioning and allocation and for load balancing on disk arrays. Our approach consists ofmodular building blocks that can be invoked independently of each other; thus; algorithmsfor file allocation and disk load balancing can be used regardless of whether striping isemployed or not. Our heuristic method for file partitioning aims to determine the optimal …,ETH; Eidgenössische Technische Hochschule Zürich; Departement Informatik; Institut für Informationssysteme 175,1992,*
Departement Informatik Institut für Informationssysteme,Gerhard Weikum; Christof Hasse; Multi-Level Transaction,Abstract Multi-level transactions are a variant of open nested transactions in which thesubtransactions correspond to operations at different levels of a layered system architecture.The point of multilevel transactions is that the semantics of high-level operations can beexploited in order to increase concurrency. As a consequence; undoing a transactionrequires compensation of completed subtransactions. In addition; multi-level recoverymethods have to take into account that high-level operations are not necessarily atomic ifmultiple pages are updated in a single subtransaction. This paper presents algorithms formulti-level transaction management that are implemented in the database kernel systemDASDBS. In particular; it is shown that multi-level recovery can be implemented in anefficient way. We discuss performance measurements; using a synthetic benchmark for …,*,1991,*
Multi-level transaction management for complex objects,Christof Hasse; Gerhard Weikum,Abstract Multi-leveltransactions are a variant of open nested transactions in which thesubtransactions correspondto operations at differentlevels ofalayered system architecture.The point ofmultilevel transactions is that the semantics of high-level operations can beexploited in order to increase concurrency. As a consequence; undoing a transactionrequires compensation of completed subtransactions. In addition; multi-level recoverymethods have to take into account that high-level operations are not necessarily atomic ifmultiple pages are updated in a single subtransaction. This paper presents algorithms formulti-leveltransaction management that are implemented in the database kernel systemDASDBS. In particular; it is shown that multi-level recovery can be implemented in anefficient way. We discuss performance measurements; using a synthetic benchmark for …,ETH; Eidgenössische Technische Hochschule Zürich; Departement Informatik; Institut für Computersysteme,1991,*
Konflikt-gesteuerte Lastkontrolle in Datenbanksystemen basierend auf der Analyse von Transaktionsprogrammen,Axel Mönkeberg; Gerhard Weikum,Zusammenfassung Der Artikel stellt einen Algorithmus zur Vermeidung von dateninduzierterÜberlast in Datenbanksystemen mit Zwei-Phasen-Sperrprotokoll vor. Der Algorithmus paßtden Parallelitätsgrad des Systems dynamisch der jeweiligen Lastsituation an und ermöglichtdadurch sogar Leistungssteigerungen gegenüber dem bestmöglichen statischenParallelitätsgrad. Informationen über das Referenzverhalten der Transaktionslast werdenvon dem Verfahren bei seinen Entscheidungen mitberücksichtigt. Die möglichenAuswirkungen; die eine genauere Schätzung des Referenzverhaltens der Transaktionen aufden Durchsatz eines Systems haben kann; werden aufgezeigt. Eine Methode zurAbschätzung des Referenzverhaltens von Transaktionsprogrammen wird vorgestellt.,*,1991,*
IE M Шш,Axel Moenkeberg; Gerhard Weikum,Abstract This paper deals with load control for the avoidance of data-contention thrashingcaused by excessive lock conflicts. We present a conflict-driven approach to automatic loadcontrol. Various definitions of conflict rate are investigated as to whether they are suitable asa control metric. We provide evidence that there exists at least one suitable metric and asingle value; called critical conflict rate; that indicates data-contention thrashing regardlessof the number or types of transactions in the system. Based on this observation; an algorithmis developed that admits new transactions and/or cancels running transactions dependingon the current conflict rate. The algorithm and its various substrategies for transactionadmission and transaction cancellation are evaluated under several sorts of overloadsituations.,*,1990,*
ЕТМГ,Gerhard Weikum; Peter Zabback; Peter Scheuermann,Abstract Large arrays of small disks are being considered as a promising approach to highperformance I/O architectures. In this paper we deal with the problem of data placement insuch a disk array. The prevalent approach is to decluster large files across a number of disksso as to minimize the access time to a file and balance the I/O load across the disks. Thedata placement problem entails determining the number of disks and the set of disks acrosswhich a file is declustered. Unlike previous work; this paper does not assume that all filesare allocated at the same time but rather considers dynamic file creations and expansions.This makes the placement problem considerably harder because each placement decisionhas to take into account the current allocation state and the access frequencies of the disksand the existing files. As a result; both file creation and expansion may involve partial …,*,1990,*
ETH,Gerhard Weikum; Christof Hasse; Axel Mönkeberg; Peter Zabback,Abstract This paper is an Initial outline of the COMFORT project. The overall objective ofCOMFORT Is to automate tuning decisions for transaction processing in database systems.The basic idea is to determine most tuning parameters and run-time strategies on the basisof a compile-time analysis of the transaction programs and statistical information about thedata and the workload. The issues that are particularly addressed within this framework are:1) workload-customized concurrency control and recovery protocols that exploit semanticsyet do not incur much overhead; 2) exploiting Intra-transaction parallelism in nestedtransactions; 3) intelligent data allocation and migration to optimize data access In a storagehierarchy consisting of large memory; disk-arrays; and optical disks; and 4) intelligentresource management to coordinate the policies for load control; CPU scheduling; and …,*,1990,*
From the KERNEL to COSMOS,Marc H Scholl; Gerhard Weikum; Hans-Jörg Schek,Abstract This report describes COSMOS; the research program of the database researchgroup at ETH Zurich. These activities are a natural follow-on of DASDBS; the DarmstadtDatabase System project at the Technical University of Darmstadt. While most emphasis inDASDBS was on a database kernel system; the project at ETH focuses on the coopera-tionbetween a database system and its environment. The environment consists of clients askingfor database service and other systems offering servicė to the database system. Theresearch objective is the exploration of the architecture of a COoperative System for theManagement of ObjectS (COSMOS). In short; we are on the way" from the kernel to theCOSmos".,ETH; Eidgenössische Technische Hochschule Zürich; Departement Informatik; Institut für Informationssysteme,1990,*
ZL1 rich,Hans J Schek; H-Bernhard Paul; Marc H Scholl; Gerhard Weikum,Abstract This paper is a retrospective on the Darmstadt database system project; also knownas DASDBS. The project aimed at providing data management support for advancedapplications such as geoscientific information systems and office automation. Similar to thedichoto-my of RSS and RDS in System R; we pursued a layered architectural approach: Astorage management kernel serves as the lowest common denominator of the requirementsof the various application classes; and a family of applicatio/2—01'ientedfrontends providessemantically richer functions on top of the kernel.,*,1989,*
Mentor-lite: Integrating Light-Weight Workflow Management Systems within Business Environments,Peter Muth; Jeanine Weissenfels; Michael Gillmann; Gerhard Weikum,Workflow management systems support an efficient; largely automated execution ofbusiness processes. If a workflow management system is used to implement a computersupported business process from scratch; all the functionality of the workflow managementsystem can be fully exploited. In particular; the control flow will be completely specified in theworkflow management system's specification language; and will be completely controlled bythe workflow engine at execution time. In this case; external applications only implementactivities of a workflow and can be integrated by means of a simple invocation interface. Thisis what almost all workflow management systems aim at. Consequently; the workflowmanagement coalition (WfMC) proposes an architecture specifically tailored to this scenario[WFMC]. However; only in rare cases there is an opportunity to computerize a business …,Business Environments; Int'l Conf. on Data Engineering (ICDE,*,*
Time; Space; Context; and Many Languages,Johannes Hoffart; Fabian M Suchanek; Klaus Berberich; Edwin Lewis-Kelham; Gerard de Melo; Gerhard Weikum,ABSTRACT We present YAGO2; an extension of the YAGO knowledge base with a focus ontemporal and spatial knowledge. It is automatically built from Wikipedia; GeoNames; andWord-Net; and contains nearly 10 million entities and events; as well as 80 million factsrepresenting general world knowledge. An enhanced data representation introduces timeand location as first-class citizens. The wealth of spatio-temporal information in YAGO canbe explored either graphically or through a special time-and space-aware query language.,*,*,*
The JXP Method for Robust PageRank Approximation in a Peer-to-Peer Web Search Network⋆,Josiane Xavier Parreira; Carlos Castillo; Debora Donato; Sebastian Michel; Gerhard Weikum,*,*,*,*
Approche cognitive pour la désambiguïsation d’entités nommées dans les articles d’actualité,Nivo RANDRIAMBOLOLONA; Yvon ANDRIANAHARISON; Gerhard WEIKUM; RANDIMBINDRAINIBE Falimanana,Résumé:-Cet article propose une approche cognitive basée sur l'utilisation d'une grammaireformelle pour la désambiguïsation d'entités nommées dans les articles d'actualité. Utiliséeconjointement avec un modèle de régression logistique; son objectif principal est d'éviterl'exclusion d'entités émergentes qui sont difficilement reconnaissables par voie empiriquesans pour autant désavantager les entités préétablies. Ce projet est né dans le contexte duprojet originel AIDA [1] qui a été réalisé au laboratoire duMPIIen Allemagne et pour cetteraison on lui a attribué l'appellation AIDA-for-News.,*,*,*
Technical Report Completeness-aware Rule Learning from Knowledge Graphs,Thomas Pellissier Tanona; Daria Stepanovaa; Simon Razniewskib; Paramita Mirzaa; Gerhard Weikuma,Abstract. Knowledge graphs (KGs) are huge collections of primarily encyclopedic facts. Theyare widely used in entity recognition; structured search; question answering; and otherimportant tasks. Rule mining is commonly applied to discover patterns in KGs. However;unlike in traditional association rule mining; KGs provide a setting with a high degree ofincompleteness; which may result in the wrong estimation of the quality of mined rules;leading to erroneous beliefs such as all artists have won an award; or hockey players do nothave children. In this paper we propose to use (in-) completeness meta-information to betterassess the quality of rules learned from incomplete KGs. We introduce completenessawarescoring functions for relational association rules. Moreover; we show how one can obtain (in-) completeness meta-data by learning rules about numerical patterns of KG edge counts …,*,*,*
General Chair: Per-ke (Paul) Larson; Microsoft; USA Program Co-Chairs: David Lomet; Microsoft; USA Gerhard Weikum; University of Saarland; Germany Panel Pro...,Pamela Drew; Yannis Papakonstantinou; Chaitan Baru,Page 1. xviii Organizing Committee General Chair: Per- ke (Paul) Larson; Microsoft; USA ProgramCo-Chairs: David Lomet; Microsoft; USA Gerhard Weikum; University of Saarland; Germany PanelProgram Chair: Mike Carey; IBM Almaden; USA Tutorial Program Chair: Praveen Seshadri; CornellUniversity; USA Industrial Program Co-Chairs: Anil Nori; Asera; Inc.; USA Pamela Drew; Boeing;USA Demo/Exhibits Chair: Ling Liu; Georgia Tech.; USA Publicity Chair: Qiang Zhu; Universityof Michigan; USA Financial Chair: Roger Barga; Microsoft; USA Local Arrangements: YannisPapakonstantinou; University of California at San Diego; USA Chaitan Baru; San Diego Supercomp.Center; USA Publication Chair: Vijay Kumar; University of Missouri - Kansas City; USA SteeringCommittee Chair: Erich Neuhold; GMD-IPSI; Germany,*,*,*
Technical Report: Exception-enriched Rule Learning from Knowledge Graphs,Mohamed Gad-Elraba; Daria Stepanovaa; Jacopo Urbanib; Gerhard Weikuma,Abstract. Advances in information extraction have enabled the automatic construction oflarge knowledge graphs (KGs) like DBpedia; Freebase; YAGO and Wikidata. These KGs areinevitably bound to be incomplete. To fill in the gaps; data correlations in the KG can beanalyzed to infer Horn rules and to predict new facts. However; Horn rules do not take intoaccount possible exceptions; so that predicting facts via such rules introduces errors. Toovercome this problem; we present a method for effective revision of learned Horn rules byadding exceptions (ie; negated atoms) into their bodies. This way errors are largely reduced.We apply our method to discover rules with exceptions from real-world KGs. Ourexperimental results demonstrate the effectiveness of the developed method and theimprovements in accuracy for KG completion by rule-based fact prediction.,*,*,*
Chair’s Message,Serge Abiteboul; Mike Carey; David Maier; Moshe Y Vardi; Gerhard Weikum; Hans-Joerg Schek; Ricardo Baeza-Yates; Sophie Cluet; Jim Gray; Masaru Kitsuregawa; Gail Mitchell; Beng Chin Ooi,Greetings. Since I last wrote to you in this forum; I attended the meeting of the ACM SIGGoverning Board (SGB) in Chicago; and we (the EC) have been discussing ways to mount amembership drive. We've been observing the enormous effort involved in organizingSIGMOD 2006; and assisting when we can. Meantime; Beng Chin Ooi has been assemblingan impressive PC for SIGMOD 2007. An important development is the new SIGMOD awardscommittee structure. We adopted the structure of the ACM awards committee; with fivemembers who serve staggered terms (to ensure continuity as members rotate off thecommittee). I am very grateful to the following members of our community for agreeing to beon this important committee:,*,*,*
Improving Collection Selection with Overlap Awareness in P2P Search Engines,Gerhard Weikum; Matthias Bender; Christian Zimmer,Page 1. Sebastian Michel Improving Collection Selection with Overlap Awareness in P2PSearch Engines Max-Planck-Institut Informatik University of Patras NetCInS Lab ImprovingCollection Selection with Overlap Awareness in P2P Search Engines Sebastian Michel PeterTriantafillou Gerhard Weikum Matthias Bender Christian Zimmer Max-Planck-Institut Informatikand University of Patras Page 2. ACM SIGIR 2005; Salvador; Brazil 2 Sebastian MichelImproving Collection Selection with Overlap Awareness in P2P Search EnginesMax-Planck-Institut Informatik University of Patras NetCInS Lab Overview • Motivation •Structured Peer-to-Peer Systems • Design Fundamentals • Query Routing • How to estimate“Novelty” • The iterative Query Routing Algorithm • Experimental Evaluation • Conclusion •Future Work Page 3. ACM SIGIR 2005; Salvador; Brazil 3 …,*,*,*
FC Gärtner,H Hojjat; H Nakhost; M Sirjani; B Henderson-Sellers; M Serour; T McBride; C Gonzalez-Perez; L Dagher; P Gorm Larsen; CB Jones; D Lomet; A Romanovsky; G Weikum,H. Hojjat; H. Nakhost; M. Sirjani: Integrating Module Checking and Deduction in a Formal Prooffor the Perlman Spanning Tree Protocol (STP) In the IEEE 802.1D standard for the Media AccessControl layer (MAC layer) bridges; there is an STP (Spanning Tree Protocol) definition; basedon the algorithm that was proposed by Radia Perlman. In this paper; we give a formal proof forcorrectness … B. Henderson-Sellers; M. Serour; T. McBride; C. Gonzalez-Perez; L.Dagher: Process Construction and Customization Adopting the most appropriate methodologyfor particular software developments remains a challenge for all industrial IT organizations. Previousattempts to promote a single approach as useful for … CB Jones; D. Lomet; A.Romanovsky; G. Weikum: The Atomic Manifesto This paper is a manifesto for future researchon "atomicity" in its many guises and is based on a five-day workshop on "Atomicity in …,*,*,*
page 1,Michael Rys; Gerhard Weikum,Abstract Previous work on parallel database systems has paid little attention to theinteraction of asynchronous disk prefetching and processor parallelism. This paperinvestigates this issue for scan operations on shared? memory multiprocessors. Twoheuristic methods are developed for the allocation of processors and memory to optimizeeither the speedup or the benefit/cost ratio of database scan operations. The speedupoptimization balances the data production rate of the disks and the data consumption rate ofthe processors; aiming at optimal speedup while ensuring that resources are not allocatedunnecessarily. The benefit/cost optimization considers explicitly the resource consumption ofa scan operation and aims to allocate processors and memory so that the ratio of thespeedup attained to the operation? s resource? time product is maximized. Such an …,*,*,*
Core Database Technology,Rodney Topor; U Griffith; Australia Serge Abiteboul; France INRIA-Futurs; Philippe Rigaux; U Orsay; France Daniel Keim; Agnes Voisard; Guido Moerkotte; Timos Sellis; Greece Michalis Vazirgianis; Greece S Sudarshan; Yoshifumi Masunaga; U Ochanomizu; Japan Wojtek Cellary; U Economics; Poznan Poland; Vassilis J Tsotras; Francois Llirbat; Bernd Amann; CNAM Paris; France Andreas Heuer; Alfons Kemper; Tiziana Catarci; Sang-Wook Kim; Roger Weber; Rivka Ladin; Andreas Geppert; France Jayant Haritsa; Shojiro Nishio; U Osaka; Japan Dik Lee; Bernhard Mitschang; Calton Pu; ETH Gustavo Alonso; Switzerland Laura Haas; Gerhard Weikum; U Saarland; Germany Oded Shmueli; Erhard Rahm; Soumen Chakrabarti; NTU Yannis Vassiliou; KAIST Kyu-Young Whang; Korea Donald Kossman; Klaus Dittrich; Hans Schek; Elisa Bertino; Jim Gray; Frank Leymann; Rudi Munz SAP; Patrick Valduriez; Anand Deshpande; Masaru Kitsuregawa; Darrell Long; Harald Schoening; Phil Bernstein,*,*,*,*
H. Mauser; E. Thurner,FC Gärtner; A Abraham; C Grosan; A Benso; M Rebaudengo; M Sonza Reorda; CB Jones; D Lomet; A Romanovsky; G Weikum,H. Mauser; E. Thurner: Electronic Throttle Control _ A Dependability Case Study The so-calledElectronic Throttle Control unit was a big step towards reducing important parameters like fuelconsumption or exhaust emmission. Due to its safety-criticality; a dependability study was initiatedby the manufacturer Siemens … FC Gärtner: Transformational Approaches to the Specificationand Verification of Fault-Tolerant Systems: Formal Background and Classification Proving thata program suits its specification and thus can be called … A. Abraham; C. Grosan: AutomaticProgramming Methodologies for Electronic Hardware Fault Monitoring This paper presents threevariants of Genetic Programming (GP) approaches for intelligent online performance monitoringof electronic circuits and systems. Reliability modeling of electronic circuits can be best performedby the stressor … A. Benso; M. Rebaudengo; M. Sonza Reorda: Fault Injection for …,*,*,*
Program Vice-Chairs,Jeff Naughton; Sunita Sarawagi; Hank Korth; Arnie Rosenthal; Jeff Ullman; Hans Schek; Phil Bernstein; Donald Kossmann; Stavros Christodoulakis; Theo Haerder; Beng Chin Ooi; HV Jagadish; Gerhard Weikum,Page 1. xix Program Vice-Chairs Jeff Naughton; University of Wisconsin; USA Sunita Sarawagi;IBM Almaden; USA Hank Korth; Lucent - Bell Labs; USA Arnie Rosenthal; Mitre; USA Jeff Ullman;Stanford University; USA Hans Schek; ETH Zurich; Switzerland Phil Bernstein; Microsoft; USADonald Kossmann; University of Passau; Germany Stavros Christodoulakis; University of Crete;Greece Theo Haerder; University of Kaiserslautern; Germany Beng Chin Ooi; National Universityof Singapore; Singapore HV Jagadish; University of Illinois at Urbana-Champaign; USA AwardCommittee Members Hank Korth Donald Kossmann Arnie Rosenthal Gerhard Weikum,*,*,*
VLDB Endowment Board of Trustees,Gerhard Weikum; Laura M Haas; Paolo Atzeni; Michael J Franklin; Amr El Abbadi; Gustavo Alonso; Peter MG Apers; Elisa Bertino; Peter Buneman; Johann Christoph Freytag; HV Jagadish; Christian S Jensen; Donald Kossmann; David Lomet; Renée J Miller; Shojiro Nishio; Beng Chin Ooi; Meral Ozsoyoglu; Krithi Ramamritham; Raghu Ramakrishnan; Stanley B Zdonik,The VLDB Endowment is a non-profit foundation whose objective is to promote scientific andeducational activities in the area of large-scale data; information; and knowledgemanagement. The Endowment serves as the steering committee for the VLDB conferenceseries. The Endowment also sponsors various scholarly activities. It has established aprogram that supports summer schools; tutorials; and other training activities of this kind; incountries that could otherwise not afford the expenses for such events. The Endowment isalso the main sponsor of the biennial Conference on Innovative Data Systems Research(CIDR); and it runs the VLDB Journal; one of the most successful journals in the databasearea. On various activities; the Endowment closely cooperates with ACM SIGMOD. TheVLDB Endowment has a board of 21 elected trustees; who are the legal guardians of the …,*,*,*
Srikanta Bedathur Max-Planck Institute für Informatik Campus E1 4; Saarbrücken; Germany Klaus Berberich Max-Planck Institute für Informatik,Jens Dittrich; Nikos Mamoulis; Gerhard Weikum,Abstract Large text corpora with news; customer mail and reports; or Web 2.0 contributionsoffer a great potential for enhancing business-intelligence applications. We propose aframework for performing text analytics on such data in a versatile; efficient; and scalablemanner. While much of the prior literature has emphasized mining keywords or tags in blogsor social-tagging communities; we emphasize the analysis of interesting phrases. Theseinclude named entities; important quotations; market slogans; and other multi-word phrasesthat are prominent in a dynamically derived ad-hoc subset of the corpus; eg; being frequentin the subset but relatively infrequent in the overall corpus. The ad-hoc subset may bederived by means of a keyword query against the corpus; or by focusing on a particular timeperiod. We investigate alternative definitions of phrase interestingness; based on the …,*,*,*
NAGA: Uncoiling the Web,Gjergji Kasneci; Fabian M Suchanek; Maya Ramanath; Gerhard Weikum,*,*,*,*
Exploiting Replication in Peer-to-Peer Search Over Distributed Digital Libraries,Christian Zimmer; Srikanta Bedathur; Christos Tryfonopoulos; Gerhard Weikum,Abstract. Existing peer-to-peer (P2P) networks suffer from dynamics: Both high churn withjoining and leaving of peers with unknown rates and without notification; as well as highdata dynamics with adding of new and disappearing of older data. We develop replicationstrategies from the existing P2P search engine Minerva where each participating peer ordigital library manages its own local document collection. A distributed directory on top of aChord-DHT stores per-term summaries of all peers. Given this scenario of a conceptuallyglobal but physically distributed directory; we design different replication strategies that canbe integrated into the query execution process. In this paper; we explore algorithms:,*,*,*
Authors’ Addresses Klaus Berberich Max-Planck-Institut für Informatik Campus E1 4 66123 Saarbrücken,Srikanta Bedathur; Omar Alonso; Gerhard Weikum,Abstract This work addresses information needs that have a temporal dimension conveyedby a temporal expression in the user's query. Temporal expressions such as “in the 1990s”are frequent; easily extractable; but not leveraged by existing retrieval models. Onechallenge when dealing with them is their inherent uncertainty. It is often unclear whichexact time interval a temporal expression refers to. We integrate temporal expressions into alanguage modeling approach; thus making them first-class citizens of the retrieval modeland considering their inherent uncertainty. Experiments on the New York Times AnnotatedCorpus using Amazon Mechanical Turk to collect queries and obtain relevanceassessments demonstrate that our approach yields substantial improvements in retrievaleffectiveness.,*,*,*
Database Selection and Result Merging in P2P Web Search,Matthias Bender; Sergey Chernov; Sebastian Michel; Pavel Serdyukov; Gerhard Weikum; Christian Zimmer,Abstract. Intelligent Web search engines are extremely popular now. Currently; only thecommercial centralized search engines like Google can process terabytes of Web data.Alternative search engines fulfilling collaborative Web search on a voluntary basis areusually based on a blooming Peer-to-Peer (P2P) technology. In this paper; we investigatethe effectiveness of different database selection and result merging methods in the scope ofP2P Web search engine Minerva. We adapt existing measures for database selection andresults merging; all directly derived from popular document ranking measures; to addressthe specific issues of P2P Web search. We propose the general approach to both tasksbased on combination of pseudo-relevance feedback methods. From experiments withTREC Web data; we observe that the pseudo-relevance feedback information from the …,*,*,*
JW Coleman; CB Jones,CB Jones; D Lomet; A Romanovsky; G Weikum; J Kienzle; J Burton; E Börger,CB Jones; D. Lomet; A. Romanovsky; G. Weikum: The Atomic Manifesto This paper is a manifestofor future research on "atomicity" in its many guises and is based on a five-day workshop on"Atomicity in System Design and Execution" that took place in Schloss Dagstuhl in Germanyin April 2004 … J. Kienzle: On Atomicity and Software Development This paper shows howthe concept of atomicity can ease the development of concurrent software. It illustrates by meansof a case study how atomicity is used to reduce the complexity of concurrency by presentingsimplified models or views of the … J. Burton; CB Jones: Investigating Atomicity and ObservabilityUsing the fiction of atomicity as a design abstraction and then refining atomicity as we developan implementation is widely used in areas of concurrent computing such as database systemsand transaction processing. In each of these and similar … E. Börger: The Origins and …,*,*,*
YAGO: ALargeOntologyfromWikipediaandWordNet,Fabian M Suchanek; Gjergji Kasneci; Gerhard Weikum,*,*,*,*
Deliverable D5. 2 Local search engine for complex queries Sep 2008,Michal Shmueli-Scheuer; Yosi Mass; Benjamin Sznajder; Fabrizio Falchi; Gerhard Weikum,*,*,*,*
E. Börger,E Börger; A Prinz; CB Jones; D Lomet; A Romanovsky; G Weikum; S Kell; JR Abrial,E. Börger: The Origins and the Development of the ASM Method for High Level System Designand Analysis The research belonging to the Abstract State Machines approach to system designand analysis is surveyed and documented in an annotated ASM bibliography. The survey coversthe period from 1984; when the idea for the concept of ASMs (under the name … E.Börger; A. Prinz: Quo Vadis Abstract State Machines … CB Jones; D. Lomet; A.Romanovsky; G. Weikum: The Atomic Manifesto This paper is a manifesto for future researchon "atomicity" in its many guises and is based on a five-day workshop on "Atomicity in SystemDesign and Execution" that took place in Schloss Dagstuhl in Germany in April 2004 …S. Kell: A Survey of Practical Software Adaptation Techniques Software adaptation techniquesappear in many disparate areas of research literature; and under many guises. This …,*,*,*
On Demand Creation of Focused Domain Models using Top-down and Bottom-up Information Extraction,Christopher Thomas; Pankaj Mehra; Amit Sheth; Wenbo Wang; Gerhard Weikum,ABSTRACT We present a hybrid method for automated on-demand creation of conceptualmodels of domain-specific knowledge. Models are thereby created using a two-step processof domain definition and domain description. Domain Definition creates a conceptual basewhereas in the Domain Description relationships are added to the conceptual model using apattern-based relational-targeting Information Extraction algorithm that deploys a novelrelational pertinence measure to disambiguate semantically overlapping types ofrelationships. The two-step process has the advantage over traditional approaches toontology learning that it provides conceptual grounding through a top-down extraction andover information extraction that the extraction operates on a conceptual level so that conceptintegrity and reference is guaranteed. At the core of the Information extraction algorithm is …,*,*,*
Technical Report TR-MPI-Clouds08,Michael Backes; Marek Hamerlik; Alessandro Linari; Matteo Maffei; Christos Tryfonopoulos; Gerhard Weikum,Abstract This paper presents Clouds; a peer-to-peer protocol that guarantees bothanonymity and censorship resistance in semantic overlay networks. The design of such aprotocol needs to meet a number of challenging goals: allowing for the exchange ofencrypted messages without assuming previously shared secrets; avoiding centralisedinfrastructures; like trusted servers or gateways; and guaranteeing efficiency withoutestablishing direct connections between peers. Anonymity is achieved by cloaking theidentity of protocol participants behind groups of semantically close peers. Censorshipresistance is guaranteed by a cryptographic protocol securing the anonymouscommunication between the querying peer and the resource provider. Although we groundour technique on semantic overlay networks to exploit their retrieval capabilities; our …,*,*,*
Deliverable D4. 2< Design of the Multimedia Indexing Technique> January 2008,Mouna Kacimi; Matteo Mordacchini; Fausto Rabitti; Gerhard Weikum,*,*,*,*
J. Kienzle,CB Jones; D Lomet; A Romanovsky; G Weikum; V Podobnik; A Petric; G Jezic; A Navarro; C Fetzer; P Felber,J. Kienzle: On Atomicity and Software Development This paper shows how the concept of atomicitycan ease the development of concurrent software. It illustrates by means of a case study howatomicity is used to reduce the complexity of concurrency by presenting simplified models orviews of the … CB Jones; D. Lomet; A. Romanovsky; G. Weikum: The Atomic Manifesto Thispaper is a manifesto for future research on "atomicity" in its many guises and is based on afive-day workshop on "Atomicity in System Design and Execution" that took place in SchlossDagstuhl in Germany in April 2004 … V. Podobnik; A. Petric; G. Jezic: An Agent-Based Solutionfor Dynamic Supply Chain Management Supply chain management (SCM) deals with planningand coordinating activities such as material procurement; product assembly; and the distributionof manufactured products. This paper offers an agent-based solution as a potentially …,*,*,*
Daniel A. Ford; IBM Almaden,Michael Franklin; Ophir Frieder; Norbert Fuhr; Dimitrios Georgakopoulos; Carol Goble; Theo Haerder; Willem Jonker; Yahiko Kambayashi; Jessie Kennedy; Martin Kersten; Hiroyuki Kitagawa; Masaru Kitsuregawa; Matthias Klusch; Mong Li Lee; Qing Li; Xiaoming Li; Ling Liu; Mengchi Liu; Frederick H Lochovsky; Hongjun Lu; Heiko Schuldt; Timos Sellis; Ming-Chien Shan; Timothy K Shih; Il-Yeol Song; Kian Lee Tan; Katsumi Tanaka; David Toman; Frank W Tompa; Shan Wang; Gerhard Weikum; Kam-Fai Wong; Masatoshi Yoshikawa; S Yu Philip; Osmar Zaiane; Carlo Zaniolo; Stan Zdonik; Aoying Zhou; Lizhu Zhou; Shuigeng Zhou,*,*,*,*
Mentor-lite Customizability: Tailoring a Light-Weight Workflow Management System to Workflow Application and Organizational Needs,Michael Gillmann; Jeanine Weissenfels; German Shegalov; Wolfgang Wonner; Gerhard Weikum,Abstract The Mentor-lite prototype has been developed within the research project"Architecture; Configuration; and Administration of Large Workflow Management Systems"funded by the German Science Foundation (DFG). A salient feature of Mentor-lite is its abilityto customize its workflow administration capabilities like worklist management and historymanagement to the specific needs of an application and the organization of the underlyingenterprise (s). The demo will show the feasibility of the presented approach bydemonstrating the tailoring of worklist management policies.,*,*,*
Das Web der Zukunft: Herausforderungen und Chancen für die Informatik,Gerhard Weikum,2. Die Antwortzeiten bei der Nutzung von Informationsdiensten müssen vorhersagbar undmit einer nahe bei eins liegenden Wahrscheinlichkeit garantierbar sein. Systeme müssen sokonfiguriert; optimiert und betrieben werden; dass zB 99 Prozent aller Benutzeraufträge inhöchstens zwei Sekunden bedient werden; und zwar auch während gelegentlicherLastspitzen zu den populärsten Tageszeiten. 3. Im Zeitalter der Globalisierung müssen E-Services eine extrem hohe Verfügbarkeit haben; eine Verfügbarkeit von 99; 99 Prozent etwaentspricht einer mittleren „Auszeit “von höchstens einer Stunde pro Jahr. Da transienteSoftwarefehler (sog.„Heisenbugs “[4]) in sehr großen Systemen auch mittelfristig nichtauszuschließen sind; liegt der Schlüssel zur Lösung bei der Replikation von Daten undServer-Komponenten zusammen mit sehr schnellen Recovery-Verfahren [5] …,*,*,*
Authors’ Addresses Klaus Berberich Max-Planck-Institut für Informatik Stuhlsatzenhausweg 85 66123 Saarbrücken,Srikanta Bedathur; Thomas Neumann; Gerhard Weikum,Abstract Text search over temporally versioned document collections such as web archiveshas received little attention as a research problem. As a consequence; there is no scalableand principled solution to search such a collection as of a specified time t. In this work; weaddress this shortcoming and propose an efficient solution for time-travel text search byextending the inverted file index to make it ready for temporal search. We introduceapproximate temporal coalescing as a tunable method to reduce the index size withoutsignificantly affecting the quality of results. In order to further improve the performance oftime-travel queries; we introduce two principled techniques to trade off index size for itsperformance. These techniques can be formulated as optimization problems that can besolved to near-optimality. Finally; our approach is evaluated in a comprehensive series of …,*,*,*
Data Engineering,Shady Elbassuoni; Maya Ramanath; Ralf Schenkel; Gerhard Weikum,Searching RDF Graphs with SPARQL and Keywords . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . ............................Shady Elbassuoni; Maya Ramanath; Ralf Schenkel and GerhardWeikum 16 Weighted Set-Based String Similarity . . . . . . . . . . . . . . . . . . . . . .Marios Hadjieleftheriouand Divesh Srivastava 25 Search-As-You-Type: Opportunities and Challenges . . . . . . . . . . . .. . . . . . . . . . . . . . . .Chen Li and Guoliang Li 37 Query Results Ready; NowWhat? . . . . . . . . . . . .… VLDB 2010 Conference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . 79 ICDE 2011 Conference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .back cover … Editor-in-Chief David B. Lomet Microsoft ResearchOne Microsoft Way Redmond; WA 98052; USA lomet@microsoft.com … Associate Editors SihemAmer-Yahia Yahoo! Research 111 40th St; 17th floor New York; NY 10018,*,*,*
Visualisierung einer verteilten Hashtabelle (CHORD),Alexander Prohaska; Gerhard Weikum,Kurzfassung Chord ist ein Protokoll für Peer-to-Peer Netzwerke. Es ist einfach; effizient undfrei skalierbar. Im Hintergrund laufen viele Vorgänge ab; die die Stabilität im Netzwerkaufrechterhalten; auch bei vielen ausfallenden und hinzukommenden Peers. In dervorliegenden Arbeit wurde eine grafische Oberfläche zu Chord geschrieben; um dieseVorgänge zu veranschaulichen. Dieses Programm soll die in einem Chordring ablaufendeKommunikation darstellen und es ermöglichen; ein Netzwerk; in dem Chord läuft; zubeobachten. Das Programm bietet eine Darstellung des Chordrings; die in vielen Punktenvom Benutzer angepasst werden kann. Der Benutzer entscheidet; welche Ereignisse oderVorgänge beobachtet und dokumentiert werden sollen; und welche uninteressant für ihnsind. Das Programm zeichnet die beobachteten Ereignisse in einer Liste auf. Außerdem …,*,*,*
Program Co-Chairs,Jeffrey Naughton; Gerhard Weikum; Henry Korth; Chungmin Chen; Paul Attie; Weiyi Meng; Cyril Orji; Honesty Young; Eugene Shekita,*,*,*,*
J. Burton; CB Jones,CB Jones; D Lomet; A Romanovsky; G Weikum; JW Coleman,J. Burton; CB Jones: Investigating Atomicity and Observability Using the fiction of atomicity asa design abstraction and then refining atomicity as we develop an implementation is widely usedin areas of concurrent computing such as database systems and transaction processing. In eachof these and similar … CB Jones; D. Lomet; A. Romanovsky; G. Weikum: The Atomic ManifestoThis paper is a manifesto for future research on "atomicity" in its many guises and is based ona five-day workshop on "Atomicity in System Design and Execution" that took place in SchlossDagstuhl in Germany in April 2004. ...,*,*,*
Expert Paper Data and Knowledge Discovery,Gerhard Weikum,Discovery of documents; data sources; facts; and opinions is at the very heart of digitalinformation and knowledge services. Being able to search; discover; compile; and analyserelevant information for a user's specific tasks is of utmost importance in science (eg;computational life sciences; digital humanities; etc.); business (eg; market and mediaanalytics; customer relationship management; etc.); and society at large (eg; consumerinformation; traffic logistics; health discussions; etc.). Traditionally; information discovery isbased on search engines; which in turn have mostly focused on finding documents ofvarious kinds: publications; Web pages; news articles; etc. Search engine technology hasbeen developed for both Internet/Web search and; with somewhat different requirements;enterprise search within companies and intranets of organisations. Digital library services …,*,*,*
Departement Informatik Institut für Informationssysteme,Peter Scheuermann; Gerhard Weikum; Peter Zabback,Abstract Parallel disk systems such as disk arrays provide opportunities for exploiting I/Oparallelism in two possible ways; namely via inter—request and intra—request parallelism.In this paper we argue for software—controlled disk arrays in which each disk can beaccessed individually and data partitioning as well as data allocation is completely underthe control of the file system. We discuss the main issues in performance tuning of suchsystems; namely striping and load balancing; and show their relationship to response timeand throughput. We outline the main components of an intelligent file system that performsstriping on a file-specific basis by taking into account the requirements of the applications;and performs load balancing by judicious file allocation and dynamic redistributions of thedata when access patterns change. Our system uses simple but effective heuristics that …,*,*,*
FC Filho; CMF Rubira,E Börger; ME Cambronero; V Valero; E Martínez; I Čavrak; A Stranjak; M Žagar; CB Jones; D Lomet; A Romanovsky; G Weikum,FC Filho; CMF Rubira: Implementing Coordinated Error Recovery for Distributed Object-OrientedSystems with AspectJ Exception handling is a very popular technique for incorporating fault toleranceinto software systems. However; its use for structuring concurrent; distributed systems is hinderedby the fact that the exception handling models of many mainstream … E. Börger: The Originsand the Development of the ASM Method for High Level System Design and Analysis The researchbelonging to the Abstract State Machines approach to system design and analysis is surveyedand documented in an annotated ASM bibliography. The survey covers the period from1984; when the idea for the concept of ASMs (under the name … ME Cambronero; V.Valero; E. Martínez: Design and Generation of Web Services Choreographies with Time ConstraintsIn this paper we show how UML 2.0 sequence diagrams can be used for the design of …,*,*,*
JEB Moss; R. Rajwar,CB Jones; D Lomet; A Romanovsky; G Weikum; C Fetzer; P Felber; C Herzeel; P Costanza; T D'Hondt; J Kienzle,JEB Moss; R. Rajwar: Atomicity as a First-Class System Provision We argue that atomicity;ie; atomic actions with most of the traditional "ACID" properties; namely atomicity;consistency; and isolation but perhaps not durability; should be provided as a fundamental firstclass resource in computer systems. This … CB Jones; D. Lomet; A. Romanovsky; G.Weikum: The Atomic Manifesto This paper is a manifesto for future research on "atomicity" inits many guises and is based on a five-day workshop on "Atomicity in System Design andExecution" that took place in Schloss Dagstuhl in Germany in April 2004 … C. Fetzer; P.Felber: Improving Program Correctness with Atomic Exception Handling Exception handlingis a powerful mechanisms for dealing with failures at runtime. It simplifies the development ofrobust programs by allowing the programmer to implement recovery actions and tolerate …,*,*,*
Datenbanksysteme für Business; Technologie und Web (BTW),Gerhard Weikum; Harald Schöning; Erhard Rahm,Die 10. BTW-Tagung der Gesellschaft für Informatik (GI) findet vom 26. bis zum 28. Februar2003 im traditionsreichen Leipzig statt; der Stadt von Johann Sebastian Bach undAuerbachs Keller; der Stadt der Völkerschlacht anno 1813 und der Stadt; in der diedeutsche Wiedervereinigung mit den Montagsdemonstrationen im Herbst 1989 ihrenAnfang nahm. Die BTW'2003 wird in den Räumen der Universität Leipzig am Augustusplatzausgerichtet; direkt im Zentrum der Stadt. Die Universität Leipzig wurde vor nahezu 600Jahren gegründet (1409) und verbindet Tradition und Innovation in besonderer Weise. DieInformatik wurde nach der Wende aufgebaut und hat sich mittlerweile an der Universität undin der deutschen Informatiklandschaft sehr gut etabliert.,*,*,*
Next-Generation Peer-to-Peer Search Engines (Position Paper),Gerhard Weikum,*,*,*,*
Programme Committee Members,Michele Adiba; Divyakant Agrawal; Peter Apers; Ricardo Baeza-Yates; Jose A Blakeley; Mokrane Bouzeghoub; Alex Buchmann; Peter Buneman; Michael Carey; Marco Casanova; Nick J Cercone; Sharma Chakravarthy; Jan Chomicki; Stavros Christodoulakis; Sophie Cluet; Armin B Cremers; Walter Cunto; Sergio Delgado; Klaus D&rich; Christos Faloutsos; Raymundo Forradellas; Antonio Furtado; Sumit Ganguly; Georges Gardarin; Narain Gehani; Goetz Graefe; Jim Gray; Ehud Gudes; Theo Haerder; Paula Hawthorn; Richard Hull; Sushi Jajodia; Christian Jensen; Manfred Jeusfeld; Yahiko Kambayayshi; Alfons Kemper; Ramamohanarao Kotagiri; Masaru Kitsuregawa; Michel Kuntz; Rosana Lanzelotte; Claudia Medeii; Alberto Mendelzon; Michele Missikoff; C Mohan; Ami Mono; Richard R Muntz; Erich Neuhold; Jack Orenstein; Maria Orlowska; Gultekin Ozsoyoglu; Alain Pirotte; Kenneth Ross; Ron Sacks-Davis; Felix Saltor; Betty Sal&erg; Joachim W Schmidt; Michael Schrefl; Amit P Sheth; Avi Silberschatz; Richard Snodgrass; Stefano Spaccapietra; VS Subrahmanian; TC Tay; Patrick Valduriez; Yannis Vassiliou; P Venkat Rangan; Victor Vianu; Gerhard Weikum; Jennifer Widom; Kam Fai Wong,Michele Adiba (France) Divyakant Agrawal (USA) Peter Apers (Netherlands) RicardoBaeza-Yates (Chile) David Bell (UK) Jose A. Blakeley (Mexico/USA) Mokrane Bouzeghoub(Prance) Alex Buchmann (Mexico/Germany) Peter Buneman (USA) Michael Carey (USA) MarcoCasanova (Brazil) Nick J. Cercone (Canada) Sharma Chakravarthy (USA) Jan Chomicki(USA) Stavros Christodoulakis (Greece) Sophie Cluet (France) Armin B. Cremers (Germany)Walter Cunto (Venezuela) S.Misbah Deen (UK) Sergio Delgado (Mexico) Klaus D&rich(Switzerland) Christos Faloutsos (USA) Raymundo Forradellas (Argentina) Antonio Furtado(Brazil) Sumit Ganguly (USA) Georges Gardarin (France) Narain Gehani (USA) Shahram Ghandeharizadeh(US A) Goetz Graefe (USA) Jim Gray (USA) Ehud Gudes (Israel) Theo Haerder (Germany) PaulaHawthorn (USA) Richard Hull (USA) Sushi1 Jajodia (USA) Christian Jensen (Denmark) …,*,*,*
Authors’ Addresses Gjergji Kasneci Max-Planck-Institut für Informatik Stuhlsatzenhausweg 85 66123 Saarbrücken,Fabian M Suchanek; Georgiana Ifrim; Maya Ramanath; Gerhard Weikum,Abstract The Web has the potential to become the world's largest knowledge base. In orderto unleash this potential; the wealth of information available on the web needs to beextracted and organized. There is a need for new querying techniques that are simple yetmore expressive than those provided by standard keyword-based search engines. Searchfor knowledge rather than Web pages needs to consider inherent semantic structures likeentities (person; organization; etc.) and relationships (isA; locatedIn; etc.). In this paper; wepropose NAGA; a new semantic search engine. NAGA's knowledge base; which isorganized as a graph with typed edges; consists of millions of entities and relationshipsautomatically extracted from Web-based corpora. A query language capable of expressingkeyword search for the casual user as well as graph queries with regular expressions for …,*,*,*
bhnlichkeitssuche auf XML Daten,Sergej Sizov; Anja Theobald; Gerhard Weikum,Zusammenfassung Anfragesprachen für XML; wie zB XPATH oder XML-QL; unterstützenBoolesches Retrieval; Anfrageergebnisse sind dabei ungeordnete Mengen von XML–Elementen; die die regulären Suchmuster einer Anfrage erfüllen. Dieses Suchparadigma istfür stark schematisierte;" geschlossene" XML-Dokumentkollektionen; zB elektronischeKataloge; geeignet. Für die Suche nach Informationen im World Wide Web oder in" offenen"Umgebungen; zB Intranets großer Unternehmen; ist jedoch Ranked Retrieval vorzuziehen;Anfrageergebnisse sind dabei Ranglisten von XML-Elementen; die nach absteigenderRelevanz sortiert sind. Web-Suchmaschinen; die auf Information-Retrieval-Konzeptenbasieren; sind andererseits nicht in der Lage; die zusätzlichen Informationen; die sich ausder Struktur von XML-Dokumenten und der semantischen Annotation durch …,*,*,*
M. Kirchberg,CB Jones; D Lomet; A Romanovsky; G Weikum; E Börger; K Gopinath; MK Narasinhan; JL Garcia Rosa; JM Adan-Coello,M. Kirchberg: Using Abstract State Machines to Model ARIES-based Transaction ProcessingTransaction management is an essential component of database management systems. It enablesmultiple users to access the database concurrently while preserving transactional propertiessuch as atomicity; consistency; isolation; and … CB Jones; D. Lomet; A. Romanovsky; G.Weikum: The Atomic Manifesto This paper is a manifesto for future research on "atomicity" inits many guises and is based on a five-day workshop on "Atomicity in System Design andExecution" that took place in Schloss Dagstuhl in Germany in April 2004 … E. Börger: TheOrigins and the Development of the ASM Method for High Level System Design and AnalysisThe research belonging to the Abstract State Machines approach to system design and analysisis surveyed and documented in an annotated ASM bibliography. The survey covers the …,*,*,*
S. Kell,S Hadjiefthymiades; I Varouxis; D Martakos; CB Jones; D Lomet; A Romanovsky; G Weikum; DK Tsolis; S Sioutas; L Drossos; TS Papatheodorou; E Soler; J Trujillo; C Blanco; E Fernández-Medina,S. Kell: A Survey of Practical Software Adaptation Techniques Software adaptation techniquesappear in many disparate areas of research literature; and under many guises. This paper enablesa clear and uniform understanding of the related research; in three ways. Firstly; it surveys abroad range of … S. Hadjiefthymiades; I. Varouxis; D. Martakos: Performance of RDBMS-WWWInterfaces under Heavy Workload The WWW is currently considered as the most promising andrapidly evolving software platform for the deployment of applications in wide area networks aswell as enterprise intranets. Interfacing legacy systems like RDBMS to the WWW has becomea … CB Jones; D. Lomet; A. Romanovsky; G. Weikum: The Atomic Manifesto This paper isa manifesto for future research on "atomicity" in its many guises and is based on a five-day workshopon "Atomicity in System Design and Execution" that took place in Schloss Dagstuhl in …,*,*,*
VLDB Endowment Board of Trustees,John Mylopoulos; Keith G Jeffery; Stanley Su; Rakesh Agrawal; Michael L Brodie; Michael J Carey; S tefano Ceri; Umeshwar Dayal; Klaus R Dittrich; Jim Gray; Yannis Ioannidis; Martin L Kersten; Masaru Kitsugerawa; Maria E Orlowska; Hans-J Schek; Timoleon K Sellis; Patrick Valduriez; Gerhard Weikum; Kyu-Young Whang; Jennifer Widom,Further information on the VLDB Endowment; its role and its activities is available on the WorldWide Web at: http://ww?.vldb.org … (This information is correct as of June; 1998),*,*,*
Self-tuning E-services: From Wishful Thinking to Viable Engineering-Position Paper,Gerhard Weikum,Automatic tuning of database and transaction processing systems has been an elusive goalfor three decades [1]. Despite many interesting approaches and partial solutions to specificissues (see; eg;[2]); we are still far from a breakthrough. There has been a lot of talk aboutzero-admin; self-tuning; care-free systems; but in practice good; predictable performance stilldepends on intensive and expensive feed and care by human system administrators andtuning experts. I claim that this" traditional" approach is viable only in conventional settingssuch as banking applications with a well-understood workload; fairly predictable; low-variance user behavior; and a limited number of simultaneously active clients. With Web-based e-services [3] such as auctions; brokerage; service outsourcing and e-businesssupply chains; the problem becomes much more difficult and also more pressing.,*,*,*
Authors’ Addresses Gjergji Kasneci Max-Planck Institute for Informatics Campus E1. 4 66123 Saarbrücken,Shady Elbassuoni; Gerhard Weikum,Many modern applications are faced with the task of knowledge discovery in largeentityrelationship (ER) graphs; such as domain-specific knowledge bases or socialnetworks. An important building block of many knowledge discovery tasks is that of findingthe “closest” relationships between k≥ 2 given entities. We have investigated this kind ofknowledge discovery task in our previous work [22]. A more general knowledge discoveryscenario on ER graphs is that of mining the most “informative” subgraph for k (≥ 2) givenentities of interest (ie query entities). Intuitively; this would be the subgraph that best explainsthe relations between the k given query entities. This knowledge discovery scenario is moregeneral than the one of [22] in that its focus is on whole subgraphs (and not on trees).Furthermore; in this scenario; the semantics plays a crucial role. Our notion of …,*,*,*
IO-Top-k at TREC 2006: Terabyte Track,Holger Bast Debapriyo Majumdar Ralf Schenkel; Martin Theobald; Gerhard Weikum,This paper describes the setup and results of our contribu- tion to the TREC 2006 TerabyteTrack. Our implemen- tation was based on the algorithms proposed in [1] “IO- Top-k: Index-AccessOptimized Top-K Query Processing; VLDB'06”; with a main focus on the efficiency track …1. INTRODUCTION IO-Top-k [1] extends the family of threshold algorithms (TA) [3; 4; 8] with asuite of new strategies. To retrieve the best-scoring (so-called top-k) answers to a multi-keywordquery under a monotonic aggregation of per-keyword scores; TA-style algorithms perform indexscans (so-called sorted accesses) over precomputed index lists; one for each keyword in thequery; which are sorted in descending order of per- keyword scores. The key point of TA is thatit aggregates scores on the fly; thus computes a lower bound for the total score of the currentrank-k result document and an upper bound for the total scores of all other candidate …,*,*,*
Every paper submitted was carefully reviewed by three referees. Much of the reviewing was done by members of the Program Committee; hower; invaluable help wa...,S Abiteboul; R Albrecht; MG Apers; P Atzeni; G Ausiello; P Bertolazzi; N Bidoit; J Biskup; P Bourret; U Bussolatti; AB Cremers; R Dahl; A Dearnley; L Dekeyzer; K Dittrich; H Eckert; JA Edgar; C Eick; K Elhardt; JL Encarnacao; HD Erich; T Fink; S Florek; J Foiseau; L Foterlunt; HP Godbersen; L Grunding; C Habel; C Hulten; G Jaeschke; M Jouve; P Kandzia; M Lacroix; E Lefons; K Lautenbach; H Lehmann; M Lenzerini; R Lesuisse; M Leszak; E Lindencrona; B Lundberg; V Lum; C Machgeels; D Maio; F Manola; A Martella; G Martella; HC Mayr; BE Meyer; S Miranda; M Moscarini; R Munz; JM Nicolas; P Ochsenschlager; A Olive; T Ottmann; P Paolini; MT Pazienza; G Pelagatti; B Pernici; D Plateau; R Prinoth; M Protasi; FA Rabitti; B Radig; A Reuter; P Richard; C Rolland; R Scha; G Schevernstuhl; A Schmitt; M Scholl; H Schweppe; A Solvberg; N Spyratos; B Steinholpz; F Steyer; W Stucky; R Studer; G Stuebel; M Talamo; M Terranova; P Tiberio; A Vashishta; A Verroust; F Visaggio; K Voss; H Weber; H Wedekind; G Weikum,Every paper submitted was carefully reviewed by three referees. Much of the reviewing was doneby members of the Program Committee; hower; invaluable help was provided by the refereeslisted below. Their assistance is gratefully acknowledged … Abiteboul S. Albrecht R. ApersMG Atzeni P. Ausiello G. Bertolazzi P. Bidoit N. Biskup J. Bourret P. Bussolatti U. Cremers ABDahl R. Dearnley A. Dekeyzer L . Dittrich K. Eckert H. Edgar JA Eick C. Elhardt K. EncarnacaoJL Erich HD Fink T. Florek S. Foiseau J. Foterlunt L. Gambosi Cl. Godbersen HP Grunding L.Habel C. Hulten C. Jaeschke G … Jouve M. Kandzia P. Lacroix M. Lefons E. Lautenbach K.Lehmann H. Lenzerini M. Lesuisse R. Leszak M. Lindencrona E. Lundberg B. Lum V. MachgeelsC . Maio D. Manola F. Martella A. Martella G. Mayr HC Meyer BE Miranda S. Moscarini M. MunzR. Nicolas JM Ochsenschlager P . Olive A. Ottmann T. Paolini P. Pazienza MT Pelagatti …,*,*,*
Data Engineering,JJ Sandvig; Bamshad Mobasher; Robin Burke; Juha Leino; Kari-Jouko Räihä; Tom Crecelius; Mouna Kacimi; Thomas Neumann; Josiane Xavier Parreira; Marc Spaniol; Gerhard Weikum,The Data Engineering Bulletin The Bulletin of the Technical Committee on Data Engineeringis published quarterly and is distributed to all TC members. Its scope includes the design;implementation; modelling; theory and application of database systems and theirtechnology. Letters; conference information; and news should be sent to the Editor-in-Chief.Papers for each issue are solicited by and should be sent to the Associate Editorresponsible for the issue. Opinions expressed in contributions are those of the authors anddo not necessarily reflect the positions of the TC on Data Engineering; the IEEE ComputerSociety; or the authors' organizations. The Data Engineering Bulletin web site is at http://tab.computer. org/tcde/bull_about. html.,*,*,*
NAGA: Searching and Ranking Knowledge,Gjergji Kasneci Fabian M Suchanek; Georgiana Ifrim; Maya Ramanath; Gerhard Weikum,Abstract—The Web has the potential to become the world's largest knowledge base. In orderto unleash this potential; the wealth of information available on the Web needs to beextracted and organized. There is a need for new querying techniques that are simple andyet more expressive than those provided by standard keyword-based search engines.Searching for knowledge rather than Web pages needs to consider inherent semanticstructures like entities (person; organization; etc.) and relationships (isA; locatedIn; etc.). Inthis paper; we propose NAGA; a new semantic search engine. NAGA builds on a knowledgebase; which is organized as a graph with typed edges; and consists of millions of entitiesand relationships extracted from Web-based corpora. A graph-based query languageenables the formulation of queries with additional semantic information. We introduce a …,*,*,*
Gustavo Alonso y Radek Vingralek z Divyakant Agrawal y Yuri Breitbart z,Amr El Abbadi; Hans-J Schek; Gerhard Weikum,*,*,*,*
The Good; the Bad; the Ugly; and the Beautiful,Mauro Sozio; Tom Crecelius; Josiane Xavier Parreira; Gerhard Weikum,ABSTRACT Computing authority; trust; and reputation scores in web graphs and socialnetworks; by means of eigenvector analysis; is computationally expensive and potentiallycritical regarding autonomy and privacy of data owners. Embedding the computation in apeer-to-peer network addresses these issues; using distributed algorithms. However; thenecessary data exchanges between peers give rise to the problem that malicious peers maycheat in order to manipulate the outcome of the global computation. We present a robust;distributed algorithm for computing authority scores by power-iteration for principaleigenvectors; with resilience to such misbehavior; based on general principles of replicationand randomization. Our solution is very general; we merely assume that the fraction ofdishonest peers is bounded and there is an unforgeable mechanism for checking peer …,*,*,*
Data Engineering,Surajit Chaudhuri Ailamaki; Sam Lightstone; Guy Lohman; Pat Martin; Ken Salem; Gerhard Weikum,Information management systems are growing rapidly in scale and complexity; while skilleddatabase administrators are becoming rarer and more expensive. Increasingly; the total costof ownership of information management systems is dominated by the cost of people; ratherthan hardware or software costs. This economic dynamic dictates that information systems ofthe future be more automated and simpler to use; with most administration tasks transparentto the user. Autonomic; or self-managing; systems provide a promising approach toachieving the goal of systems that are increasingly automated and easier to use. But howcan that be achieved? The aim of this workshop was to present and discuss ideas towardachieving self-managing information systems in an intimate; informal; and interactiveenvironment.,*,*,*
Authors’ Addresses Thomas Neumann Max-Planck-Institut für Informatik Campus E1 4 66123 Saarbrücken,Gerhard Weikum,Abstract RDF is a data model for schema-free structured information that is gainingmomentum in the context of Semantic-Web data; life sciences; and also Web 2.0 platforms.The “pay-as-you-go” nature of RDF and the flexible patternmatching capabilities of its querylanguage SPARQL entail efficiency and scalability challenges for complex queries includinglong join paths. This paper presents the RDF-3X engine; an implementation of SPARQL thatachieves excellent performance by pursuing a RISC-style architecture with streamlinedindexing and query processing. The physical design is identical for all RDF-3X databasesregardless of their workloads; and completely eliminates the need for index tuning byexhaustive indexes for all permutations of subject-property-object triples and their binaryand unary projections. These indexes are highly compressed; and the query processor …,*,*,*
