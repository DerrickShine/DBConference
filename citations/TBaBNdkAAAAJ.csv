IRON file systems,Vijayan Prabhakaran; Lakshmi N Bairavasundaram; Nitin Agrawal; Haryadi S Gunawi; Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau,Abstract Commodity file systems trust disks to either work or fail completely; yet moderndisks exhibit more complex failure modes. We suggest a new fail-partial failure model fordisks; which incorporates realistic localized faults such as latent sector errors and blockcorruption. We then develop and apply a novel failure-policy fingerprinting framework; toinvestigate how commodity file systems react to a range of more realistic disk failures. Weclassify their failure policies in a new taxonomy that measures their Internal RObustNess(IRON); which includes both failure detection and recovery techniques. We show thatcommodity file system failure policies are often inconsistent; sometimes buggy; andgenerally inadequate in their ability to recover from partial disk failures. Finally; we design;implement; and evaluate a prototype IRON file system; Linux ixt3; showing that …,ACM SIGOPS Operating Systems Review,2005,264
Transforming policies into mechanisms with infokernel,Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau; Nathan C Burnett; Timothy E Denehy; Thomas J Engle; Haryadi S Gunawi; James A Nugent; Florentina I Popovici,Abstract We describe an evolutionary path that allows operating systems to be used in amore flexible and appropriate manner by higher-level services. An infokernel exposes keypieces of information about its algorithms and internal state; thus; its default policies becomemechanisms; which can be controlled from user-level. We have implemented two prototypeinfokernels based on the linuxtwofour and netbsdver kernels; called infolinux and infobsd;respectively. The infokernels export key abstractions as well as basic information primitives.Using infolinux; we have implemented four case studies showing that policies within Linuxcan be manipulated outside of the kernel. Specifically; we show that the default file cachereplacement algorithm; file layout policy; disk scheduling algorithm; and TCP congestioncontrol algorithm can each be turned into base mechanisms. For each case study; we …,ACM SIGOPS Operating Systems Review,2003,82
What Bugs Live in the Cloud? A Study of 3000+ Issues in Cloud Systems,Haryadi S Gunawi; Mingzhe Hao; Tanakorn Leesatapornwongsa; Tiratat Patana-anake; Thanh Do; Jeffry Adityatama; Kurnia J Eliazar; Agung Laksono; Jeffrey F Lukman; Vincentius Martin; Anang D Satria,Abstract We conduct a comprehensive study of development and deployment issues of sixpopular and important cloud systems (Hadoop MapReduce; HDFS; HBase; Cassandra;ZooKeeper and Flume). From the bug repositories; we review in total 21;399 submittedissues within a three-year period (2011-2014). Among these issues; we perform a deepanalysis of 3655" vital" issues (ie; real issues affecting deployments) with a set of detailedclassifications. We name the product of our one-year study Cloud Bug Study database(CbsDB)[9]; with which we derive numerous interesting insights unique to cloud systems. Tothe best of our knowledge; our work is the largest bug study for cloud systems to date.,Proceedings of the ACM Symposium on Cloud Computing,2014,79
PREFAIL: A programmable tool for multiple-failure injection,Pallavi Joshi; Haryadi S Gunawi; Koushik Sen,Abstract As hardware failures are no longer rare in the era of cloud computing; cloudsoftware systems must" prevail" against multiple; diverse failures that are likely to occur.Testing software against multiple failures poses the problem of combinatorial explosion ofmultiple failures. To address this problem; we present PreFail; a programmable failure-injection tool that enables testers to write a wide range of policies to prune down the largespace of multiple failures. We integrate PreFail to three cloud software systems (HDFS;Cassandra; and ZooKeeper); show a wide variety of useful pruning policies that we canwrite for them; and evaluate the speed-ups in testing time that we obtain by using thepolicies. In our experiments; our testing approach with appropriate policies found all thebugs that one can find using exhaustive testing while spending 10X--200X less time than …,ACM SIGPLAN Notices,2011,77
EIO: Error Handling is Occasionally Correct.,Haryadi S Gunawi; Cindy Rubio-González; Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau; Ben Liblit,Abstract The reliability of file systems depends in part on how well they propagate errors. Wedevelop a static analysis technique; EDP; that analyzes how file systems and storage devicedrivers propagate error codes. Running our EDP analysis on all file systems and 3 majorstorage device drivers in Linux 2.6; we find that errors are often incorrectly propagated; 1153calls (13%) drop an error code without handling it.,FAST,2008,67
FATE and DESTINI: A framework for cloud recovery testing,Haryadi S Gunawi; Thanh Do; Pallavi Joshi; Peter Alvaro; Joseph M Hellerstein; Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau; Koushik Sen; Dhruba Borthakur,Abstract As the cloud era begins and failures become commonplace; failure recoverybecomes a critical factor in the availability; reliability and performance of cloud services.Unfortunately; recovery problems still take place; causing downtimes; data loss; and manyother problems. We propose a new testing framework for cloud recovery: FATE (FailureTesting Service) and DESTINI (Declarative Testing Specifications). With FATE; recovery issystematically tested in the face of multiple failures. With DESTINI; correct recovery isspecified clearly; concisely; and precisely. We have integrated our framework to severalcloud systems (eg; HDFS [33]); explored over 40;000 failure scenarios; wrote 74specifications; found 16 new bugs; and reproduced 51 old bugs.,Proceedings of NSDI’11: 8th USENIX Symposium on Networked Systems Design and Implementation,2011,65
Deconstructing commodity storage clusters,Haryadi S Gunawi; Nitin Agrawal; Andrea C Arpaci-Dusseau; J Schindler; RH Arpaci-Dusseau,The traditional approach for characterizing complex systems is to run standard workloadsand measure the resulting performance as seen by the end user. However; uniqueopportunities exist when characterizing a system that is itself constructed from standardizedcomponents: one can also look inside the system itself by instrumenting each of thecomponents. In this paper; we show how intra-box instrumentation can help one understandthe behavior of a large-scale storage cluster; the EMC Centera. In our analysis; we leveragestandard tools for tracing both the disk and network traffic emanating from each node of thecluster. By correlating this traffic with the running workload; we are able to infer the structureof the software system (eg; its write update protocol) as well as its policies (eg; how itperforms caching; replication; and load-balancing). Further; by imposing variable intra …,Computer Architecture; 2005. ISCA'05. Proceedings. 32nd International Symposium on,2005,64
SQCK: A Declarative File System Checker.,Haryadi S Gunawi; Abhishek Rajimwale; Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau,Abstract: The lowly state of the art for file system checking and repair does not match what isneeded to keep important data available for users. Current file system checkers; such ase2fsck; are complex pieces of imperfect code written in low-level languages. We introduceSQCK; a file system checker based on a declarative query language; declarative queries area natural match for the cross-checking that must be performed across the many structures ofa file system image. We show that SQCK is able to perform the same functionality as e2fsckwith surprisingly elegant and compact queries. We also show that SQCK can easily performmore useful repairs than e2fsck by combining information available across the file system.Finally; our prototype implementation of SQCK achieves this improved functionality withcomparable performance to e2fsck.,OSDI,2008,60
Improving file system reliability with I/O shepherding,Haryadi S Gunawi; Vijayan Prabhakaran; Swetha Krishnan; Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau,Abstract We introduce a new reliability infrastructure for file systems called I/O shepherding.I/O shepherding allows a file system developer to craft nuanced reliability policies to detectand recover from a wide range of storage system failures. We incorporate shepherding intothe Linux ext3 file system through a set of changes to the consistency managementsubsystem; layout engine; disk scheduler; and buffer cache. The resulting file system;CrookFS; enables a broad class of policies to be easily and correctly specified. Weimplement numerous policies; incorporating data protection techniques such as retry; parity;mirrors; checksums; sanity checks; and data structure repairs; even complex policies can beimplemented in less than 100 lines of code; confirming the power and simplicity of theshepherding framework. We also demonstrate that shepherding is properly integrated …,ACM SIGOPS Operating Systems Review,2007,60
Error propagation analysis for file systems,Cindy Rubio-González; Haryadi S Gunawi; Ben Liblit; Remzi H Arpaci-Dusseau; Andrea C Arpaci-Dusseau,Abstract Unchecked errors are especially pernicious in operating system file managementcode. Transient or permanent hardware failures are inevitable; and error-management bugsat the file system layer can cause silent; unrecoverable data corruption. We propose aninterprocedural static analysis that tracks errors as they propagate through file system code.Our implementation detects overwritten; out-of-scope; and unsaved unchecked errors.Analysis of four widely-used Linux file system implementations (CIFS; ext3; IBM JFS andReiserFS); a relatively new file system implementation (ext4); and shared virtual file system(VFS) code uncovers 312 error propagation bugs. Our flow-and context-sensitive approachproduces more precise results than related techniques while providing better diagnosticinformation; including possible execution paths that demonstrate each bug found.,ACM Sigplan Notices,2009,45
SAMC: Semantic-Aware Model Checking for Fast Discovery of Deep Bugs in Cloud Systems,Tanakorn Leesatapornwongsa; Mingzhe Hao; Pallavi Joshi; Jeffrey F Lukman; Haryadi S Gunawi,Abstract The last five years have seen a rise of implementationlevel distributed systemmodel checkers (dmck) for verifying the reliability of real distributed systems. Existing dmckshowever rarely exercise multiple failures due to the state-space explosion problem; and thusdo not address present reliability challenges of cloud systems in dealing with complexfailures. To scale dmck; we introduce semantic-aware model checking (SAMC); a white-boxprinciple that takes simple semantic information of the target system and incorporates thatknowledge into state-space reduction policies. We present four novel reduction policies:local-message independence (LMI); crash-message independence (CMI); crash recoverysymmetry (CRS); and reboot synchronization symmetry (RSS); which collectively alleviateredundant reorderings of messages; crashes; and reboots. SAMC is systematic; it does …,11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14),2014,44
Failure as a service (FaaS): A cloud service for large-scale; online failure drills,Haryadi S Gunawi; Thanh Do; Joseph M Hellerstein; Ion Stoica; Dhruba Borthakur; Jesse Robbins,Abstract Cloud computing is pervasive; but cloud service outages still take place. One mightsay that the computing forecast for tomorrow is “cloudy with a chance of failure.” One mainreason why major outages still occur is that there are many unknown large-scale failurescenarios in which recovery might fail. We propose a new type of cloud service; Failure as aService (FaaS); which allows cloud services to routinely perform large-scale failure drills inreal deployments.,University of California; Berkeley; Berkeley,2011,43
Limplock: understanding the impact of limpware on scale-out cloud systems,Thanh Do; Mingzhe Hao; Tanakorn Leesatapornwongsa; Tiratat Patana-anake; Haryadi S Gunawi,Abstract We highlight one often-overlooked cause of performance failure: limpware--"limping" hardware whose performance degrades significantly compared to its specification.We report anecdotes of degraded disks and network components seen in large-scaleproduction. To measure the system-level impact of limpware; we assembled limpbench; aset of benchmarks that combine data-intensive load and limpware injections. We benchmarkfive cloud systems (Hadoop; HDFS; ZooKeeper; Cassandra; and HBase) and find thatlimpware can severely impact distributed operations; nodes; and an entire cluster. From this;we introduce the concept of limplock; a situation where a system progresses slowly due tothe presence of limpware and is not capable of failing over to healthy components. We showhow each cloud system that we analyze can exhibit operation; node; and cluster limplock …,Proceedings of the 4th annual Symposium on Cloud Computing,2013,42
Why does the cloud stop computing?: Lessons from hundreds of service outages,Haryadi S Gunawi; Mingzhe Hao; Riza O Suminto; Agung Laksono; Anang D Satria; Jeffry Adityatama; Kurnia J Eliazar,Abstract We conducted a cloud outage study (COS) of 32 popular Internet services. Weanalyzed 1247 headline news and public post-mortem reports that detail 597 unplannedoutages that occurred within a 7-year span from 2009 to 2015. We analyzed outageduration; root causes; impacts; and fix procedures. This study reveals the broader availabilitylandscape of modern cloud services and provides answers to why outages still take placeeven with pervasive redundancies.,Proceedings of the Seventh ACM Symposium on Cloud Computing,2016,29
Deploying Safe User-Level Network Services with icTCP.,Haryadi S Gunawi; Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau,Abstract We present icTCP; an “information and control” TCP implementation that exposeskey pieces of internal TCP state and allows certain TCP variables to be set in a safe fashion.The primary benefit of icTCP is that it enables a variety of TCP extensions to be implementedat user-level while ensuring that extensions are TCP-friendly. We demonstrate the utility oficTCP through a collection of case studies. We show that by exposing information and safecontrol of the TCP congestion window; we can readily implement user-level versions of TCPVegas; TCP Nice; and the Congestion Manager; we show how user-level libraries can safelycontrol the duplicate acknowledgment threshold to make TCP more robust to packetreordering or more appropriate for wireless LANs; we also show how the retransmissiontimeout value can be adjusted dynamically. Finally; we find that converting a stock TCP …,OSDI,2004,25
Impact of disk corruption on open-source DBMS,Sriram Subramanian; Yupu Zhang; Rajiv Vaidyanathan; Haryadi S Gunawi; Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau; Jeffrey F Naughton,Despite the best intentions of disk and RAID manufacturers; on-disk data can still becomecorrupted. In this paper; we examine the effects of corruption on database managementsystems. Through injecting faults into the MySQL DBMS; we find that in certain cases;corruption can greatly harm the system; leading to untimely crashes; data loss; or evenincorrect results. Overall; of 145 injected faults; 110 lead to serious problems. More detailedobservations point us to three deficiencies: MySQL does not have the capability to detectsome corruptions due to lack of redundant information; does not isolate corrupted data fromvalid data; and has inconsistent reactions to similar corruption scenarios. To detect andrepair corruption; a DBMS is typically equipped with an offline checker. Unfortunately; theMySQL offline checker is not comprehensive in the checks it performs; misdiagnosing …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,23
TaxDC: A taxonomy of non-deterministic concurrency bugs in datacenter distributed systems,Tanakorn Leesatapornwongsa; Jeffrey F Lukman; Shan Lu; Haryadi S Gunawi,Abstract We present TaxDC; the largest and most comprehensive taxonomy of non-deterministic concurrency bugs in distributed systems. We study 104 distributed concurrency(DC) bugs from four widely-deployed cloud-scale datacenter distributed systems;Cassandra; Hadoop MapReduce; HBase and ZooKeeper. We study DC-bug characteristicsalong several axes of analysis such as the triggering timing condition and inputpreconditions; error and failure symptoms; and fix strategies; collectively stored as 2;083classification labels in TaxDC database. We discuss how our study can open up many newresearch directions in combating DC bugs.,ACM SIGPLAN Notices,2016,22
The Tail at Store: A Revelation from Millions of Hours of Disk and SSD Deployments.,Mingzhe Hao; Gokul Soundararajan; Deepak R Kenchammana-Hosekote; Andrew A Chien; Haryadi S Gunawi,Abstract We study storage performance in over 450;000 disks and 4;000 SSDs over 87 daysfor an overall total of 857 million (disk) and 7 million (SSD) drive hours. We find that storageperformance instability is not uncommon: 0.2% of the time; a disk is more than 2x slowerthan its peer drives in the same RAID group (and 0.6% for SSD). As a consequence; diskand SSD-based RAIDs experience at least one slow drive (ie; storage tail) 1.5% and 2.2% ofthe time. To understand the root causes; we correlate slowdowns with other metrics(workload I/O rate and size; drive event; age; and model). Overall; we find that the primarycause of slowdowns are the internal characteristics and idiosyncrasies of modern disk andSSD drives. We observe that storage tails can adversely impact RAID performance;motivating the design of tail-tolerant RAID. To the best of our knowledge; this work is the …,FAST,2016,22
Homogeneous Programming For Heterogeneous Multiprocessor Systems,*,Systems and methods establish communication and control between variousheterogeneous processors in a computing system so that an operating system can run anapplication across multiple heterogeneous processors. With a single set of developmenttools; software developers can create applications that will flexibly run on one CPU or oncombinations of central; auxiliary; and peripheral processors. In a computing system;application-only processors can be assigned a lean subordinate kernel to manage localresources. An application binary interface (ABI) shim is loaded with application binaryimages to direct kernel ABI calls to a local subordinate kernel or to the main OS kerneldepending on which kernel manifestation is controlling requested resources.,*,2008,18
Tiny-tail flash: Near-perfect elimination of garbage collection tail latencies in NAND SSDs,Shiqin Yan; Huaicheng Li; Mingzhe Hao; Michael Hao Tong; Swaminathan Sundararaman; Andrew A Chien; Haryadi S Gunawi,Abstract Flash storage has become the mainstream destination for storage users. However;SSDs do not always deliver the performance that users expect. The core culprit of flashperformance instability is the well-known garbage collection (GC) process; which causeslong delays as the SSD cannot serve (blocks) incoming I/Os; which then induces the long taillatency problem. We present tt F lash as a solution to this problem. tt F lash is a “tiny-tail”flash drive (SSD) that eliminates GC-induced tail latencies by circumventing GC-blockedI/Os with four novel strategies: plane-blocking GC; rotating GC; GC-tolerant read; and GC-tolerant flush. These four strategies leverage the timely combination of modern SSD internaltechnologies such as powerful controllers; parity-based redundancies; and capacitor-backed RAM. Our strategies are dependent on the use of intra-plane copyback …,ACM Transactions on Storage (TOS),2017,12
HARDFS: hardening HDFS with selective and lightweight versioning.,Thanh Do; Tyler Harter; Yingchao Liu; Haryadi S Gunawi; Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau,Abstract We harden the Hadoop Distributed File System (HDFS) against fail-silent (non fail-stop) behaviors that result from memory corruption and software bugs using a newapproach: selective and lightweight versioning (SLEEVE). With this approach; actionsperformed by important subsystems of HDFS (eg; namespace management) are checked bya second implementation of the subsystem that uses lightweight; approximate datastructures. We show that HARDFS detects and recovers from a wide range of fail-silentbehaviors caused by random bit flips; targeted corruptions; and real software bugs. Inparticular; HARDFS handles 90% of the fail-silent faults that result from random memorycorruption and correctly detects and recovers from 100% of 78 targeted corruptions and 5real-world bugs. Moreover; it recovers orders of magnitude faster than full reboot by using …,FAST,2013,10
The Case for Drill-Ready Cloud Computing,Tanakorn Leesatapornwongsa; Haryadi S Gunawi,Abstract As cloud computing has matured; more and more local applications are replaced byeasy-to-use on-demand services accessible via computer networks (aka cloud services).Running behind these services are massive hardware infrastructures and complexmanagement tasks (eg; recovery; software upgrades) that if not tested thoroughly can exhibitfailures that lead to major service disruptions. Some researchers estimate that 568 hours ofdowntime at 13 well-known cloud services since 2007 had an economic impact of more than$70 million [18]. Others predict worse: for every hour it is not up and running; a cloud servicecan take a hit between $1 to 5 million [32]. Moreover; an outage of a popular service canshutdown other dependent services [11; 37; 59]; leading to many more frustrated and furioususers.,Proceedings of the ACM Symposium on Cloud Computing,2014,9
The Case for Limping-Hardware Tolerant Clouds.,Thanh Do; Haryadi S Gunawi; T Do; T Harter; Y Liu; HS Gunawi; AC Arpaci-Dusseau; RH Arpaci-Dusseau,Abstract With the advent of cloud computing; thousands of machines are connected andmanaged collectively. This era is confronted with a new challenge: performance variability;primarily caused by large-scale management issues such as hardware failures; softwarebugs; and configuration mistakes. In this paper; we highlight one overlooked cause: limpinghardware–hardware whose performance degrades significantly compared to itsspecification. We present numerous cases of limping disks; network and processors seen inproduction; along with the negative impacts of such failures on existing large-scaledistributed systems. From these findings; we advocate the concept of limping-hardwaretolerant clouds.,HotCloud,2013,8
Towards Automatically Checking Thousands of Failures with Micro-specifications.,Haryadi S Gunawi; Thanh Do; Pallavi Joshi; Joseph M Hellerstein; Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau; Koushik Sen,Abstract Recent data-loss incidents have shown that existing large distributed systems arestill vulnerable to failures. To improve the situation; we propose two new testing approaches:failure testing service (FTS) and declarative testing specification (DTS). FTS enables us tosystematically push a system into thousands of failure scenarios; leading us to many criticalrecovery bugs. With DTS; we introduce “micro-specifications”; clear and concisespecifications written in Datalog style; which enables developers to easily write; refine; andmanage potentially hundreds of specifications.,HotDep,2010,6
Towards Pre-Deployment Detection of Performance Failures in Cloud Distributed Systems.,Riza O Suminto; Agung Laksono; Anang D Satria; Thanh Do; Haryadi S Gunawi,Modern distributed systems (“cloud systems”) have emerged as a dominant backbone formany today's applications. They come in different forms such as scale-out file systems; key-value stores; computing frameworks; synchronization and cluster management services. Asthese systems collectively become the “cloud operating system”; users expect highdependability including performance stability. Unfortunately; the complexity of the softwareand environment in which they must run has outpaced existing testing and debugging tools.Cloud systems must run at scale with different topologies; execute complex distributedprotocols; face load fluctuations and a wide range of hardware faults; and serve users withdiverse job characteristics. One type of important failures is performance failures; a situationwhere a system (eg; Hadoop) does not deliver the expected performance (eg; a job takes …,HotCloud,2015,5
Signature matching in network processing using simd/gpu architectures,R Smith; N Goyal; J Ormont; K Sankaralingam; C Estan,Abstract Deep packet inspection is becoming prevalent for modern network processingsystems. They inspect packet payloads for a variety of reasons; including intrusion detection;traffic policing; and load balancing. The focus of this paper is deep packet inspection inintrusion detection/prevention systems (IPSes). The performance critical operation in thesesystems is signature matching: matching payloads against signatures of vulnerabilities.Increasing network speeds of today's networks and the transition from simple string-basedsignatures to complex regular expressions has rapidly increased the performancerequirement of signature matching. To meet these requirements; solutions range fromhardwarecentric ASIC/FPGA implementations to software implementations using high-performance microprocessors. In this paper; we propose a programmable SIMD …,Int. Symp. on Performance Analysis of Systems and Software,2009,4
DCatch: Automatically Detecting Distributed Concurrency Bugs in Cloud Systems,Haopeng Liu; Guangpu Li; Jeffrey F Lukman; Jiaxin Li; Shan Lu; Haryadi S Gunawi; Chen Tian,Abstract In big data and cloud computing era; reliability of distributed systems is extremelyimportant. Unfortunately; distributed concurrency bugs; referred to as DCbugs; widely exist.They hide in the large state space of distributed cloud systems and manifest non-deterministically depending on the timing of distributed computation and communication.Effective techniques to detect DCbugs are desired. This paper presents a pilot solution;DCatch; in the world of DCbug detection. DCatch predicts DCbugs by analyzing correctexecution of distributed systems. To build DCatch; we design a set of happens-before rulesthat model a wide variety of communication and concurrency mechanisms in real-worlddistributed cloud systems. We then build runtime tracing and trace analysis tools toeffectively identify concurrent conflicting memory accesses in these systems. Finally; we …,Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems,2017,3
Towards reliable storage systems,Haryadi Sudirman Gunawi,Abstract Users are storing increasingly massive amounts of data. Storage softwarecomplexity is growing. The use of cheap and less reliable hardware is increasing. Thecombination of these trends presents us with a terrific challenge: How can we promise usersthat storage systems work robustly in spite of the complex failures that can arise? In the firstpart of this dissertation; we respond to this question with our analysis of three reliabilitycomponents present in many modern file systems: the file system checker (fsck); failuredetection and recovery policies (failure policy); and journaling. We find that thesesubsystems are deficient in handling partial disk failures: in the fsck analysis; we find thatsome repairs are buggy (making the repaired file system more corrupted) and some repairsare missing (leaving some corruptions unattended). In the failure policy analysis; we …,*,2009,3
Scalability Bugs: When 100-Node Testing is Not Enough,Tanakorn Leesatapornwongsa; Cesar A Stuardo; Riza O Suminto; Huan Ke; Jeffrey F Lukman; Haryadi S Gunawi,Abstract We highlight the problem of scalability bugs; a new class of bugs that appear in"cloud-scale" distributed systems. Scalability bugs are latent bugs that are cluster-scaledependent; whose symptoms typically surface in large-scale deployments; but not in smallor medium-scale deployments. The standard practice to test large distributed systems is todeploy them on a large number of machines (" real-scale testing"); which is difficult andexpensive. New methods are needed to reduce developers' burdens in finding; reproducing;and debugging scalability bugs. We propose" scale check;" an approach that helpsdevelopers find and replay scalability bugs at real scales; but do so only on one machineand still achieve a high accuracy (ie; similar observed behaviors as if the nodes aredeployed in real-scale testing).,Proceedings of the 16th Workshop on Hot Topics in Operating Systems,2017,2
Impact of Limpware on HDFS: A Probabilistic Estimation,Thanh Do; Haryadi S Gunawi,Abstract: With the advent of cloud computing; thousands of machines are connected andmanaged collectively. This era is confronted with a new challenge: performance variability;primarily caused by large-scale management issues such as hardware failures; softwarebugs; and configuration mistakes. In our previous work we highlighted one overlookedcause: limpware-hardware whose performance degrades significantly compared to itsspecification. We showed that limpware can cause severe impact in current scale-outsystems. In this report; we quantify how often these scenarios happen in Hadoop DistributedFile System.,arXiv preprint arXiv:1311.3322,2013,2
Finding Error-Handling Bugs in Systems Code Using Static Analysis,Cindy Rubio-González; Ben Liblit,ABSTRACT Run-time errors are unavoidable whenever software interacts with the physicalworld. Unchecked errors are especially pernicious in operating system file managementcode. Transient or permanent hardware failures are inevitable; and errormanagement bugsat the file system layer can cause silent; unrecoverable data corruption. Furthermore; evenwhen developers have the best of intentions; inaccurate documentation can misleadprogrammers and cause software to fail in unexpected ways. We use static program analysisto understand and make error handling in large systems more reliable. We apply ouranalyses to numerous Linux file systems and drivers; finding hundreds of confirmed error-handling bugs that could lead to serious problems such as system crashes; silent data lossand corruption.,PhD Forum of the Grace Hopper Celebration of Women in Computing; Portland; Oregon,2011,2
DryadLINQ: a system for general-purpose distributed data-parallel computing using a high-level language,Haryadi S Gunawi; Abhishek Rajimwale,main.,*,2008,2
MittOS: Supporting Millisecond Tail Tolerance with Fast Rejecting SLO-Aware OS Interface,Mingzhe Hao; Huaicheng Li; Michael Hao Tong; Chrisma Pakha; Riza O Suminto; Cesar A Stuardo; Andrew A Chien; Haryadi S Gunawi,Abstract MittOS provides operating system support to cut millisecond-level tail latencies fordata-parallel applications. In MittOS; we advocate a new principle that operating systemshould quickly reject IOs that cannot be promptly served. To achieve this; MittOS exposes afast rejecting SLO-aware interface wherein applications can provide their SLOs (eg; IOdeadlines). If MittOS predicts that the IO SLOs cannot be met; MittOS will promptly returnEBUSY signal; allowing the application to failover (retry) to another less-busy node withoutwaiting. We build MittOS within the storage stack (disk; SSD; and OS cache managements);but the principle is extensible to CPU and runtime memory managements as well. MittOS'no-wait approach helps reduce IO completion time up to 35% compared to wait-then-speculateapproaches.,Proceedings of the 26th Symposium on Operating Systems Principles,2017,1
Automatically detecting distributed concurrency errors in cloud systems,*,Abstract A method for detecting distributed concurrency errors in a distributed cloudcomputing system includes tracing operations that access objects in functions involving inter-process messaging; applying a set of happens-before rules to the traced operations.Analyzing the traced operations to identify concurrent operations that access a commonobject to generate a list of potential distributed concurrency errors (DCbugs). Pruning the listof DCbugs to remove DCbugs having only local effect and that do not generate run-timeerrors.,*,2018,*
The case of FEMU: cheap; accurate; scalable and extensible flash emulator,Huaicheng Li; Mingzhe Hao; Michael Hao Tong; Swaminatahan Sundararaman; Matias Bjørling; Haryadi S Gunawi; Matias Bj,ABSTRACT: FEMU is a software (QEMU-based) flash emulator for fostering future full-stacksoftware/hardware SSD research. FEMU is cheap (opensourced); relatively accurate (0.5-38% variance as a drop-in replacement of OpenChannel SSD); scalable (can support 32parallel channels/chips); and extensible (support internal-only and split-level SSD research).,Proceedings of the 16th USENIX Conference on File and Storage Technologies,2018,*
StrongBox: Confidentiality; Integrity; and Performance using Stream Ciphers for Full Drive Encryption,Bernard Dickens III; Haryadi S Gunawi; Ariel J Feldman; Henry Hoffmann,Abstract Full-drive encryption (FDE) is especially important for mobile devices because theycontain large quantities of sensitive data yet are easily lost or stolen. Unfortunately; thestandard approach to FDE—the AES block cipher in XTS mode—is 3–5x slower thanunencrypted storage. Authenticated encryption based on stream ciphers is already used asa faster alternative to AES in other contexts; such as HTTPS; but the conventional wisdom isthat stream ciphers are unsuitable for FDE. Used naively in drive encryption; stream ciphersare vulnerable to attacks; and mitigating these attacks with on-drive metadata is generallybelieved to ruin performance. In this paper; we argue that recent developments in mobilehardware invalidate this assumption; making it possible to use fast stream ciphers for FDE.Modern mobile devices employ solid-state storage with Flash Translation Layers (FTL) …,*,2018,*
Fail-Slow at Scale: Evidence of Hardware Performance Faults in Large Production Systems,Haryadi S Gunawi; Riza O Suminto; Russell Sears; Casey Golliher; Swaminathan Sundararaman; Xing Lin; Tim Emami; Weiguang Sheng; Nematollah Bidokhti; Caitie McCaffrey; Gary Grider; Parks M Fields; Kevin Harms; Robert B Ross; Andree Jacobson; Robert Ricci; Kirk Webb; Peter Alvaro; H Birali Runesha; Mingzhe Hao; Huaicheng Li,*,*,2018,*
Exploring the Challenges and Opportunities of Cloud Stacks in Dynamic Resource Environments,Fan Yang; Haryadi Gunawi; Andrew Chien,Traditional cloud stacks are designed to tolerate server or rack-level failures; that areunpredictable and uncorrelated. Such stacks successfully deliver highly-available cloudservices at global scale. The increasing criticality of cloud services to the overall worldeconomy is causing concern about the impact of power outages; cyber-attacks; configurationerrors; or other causes of datacenter or larger-scale failures on cloud availability. Recentexperience shows that these events can trigger cascading failures and global-scale serviceoutages. We study the impact of correlated; datacenter resource failures; exploringdistributed protocols (widely-used in Cassandra) across varied configurations and resourceavailability. Our study reveals that using such protocols to achieve high availability onresources with large-scale; correlated outages are costly in storage and update traffic …,Collaboration and Internet Computing (CIC); 2017 IEEE 3rd International Conference on,2017,*
Resilient cloud in dynamic resource environments,Fan Yang; Andrew A Chien; Haryadi S Gunawi,Abstract Traditional cloud stacks are designed to tolerate random; small-scale failures; andcan successfully deliver highly-available cloud services and interactive services to endusers. However; they fail to survive large-scale disruptions that are caused by major poweroutage; cyber-attack; or region/zone failures. Such changes trigger cascading failures andsignificant service outages. We propose to understand the reasons for these failures; andcreate reliable data services that can efficiently and robustly tolerate such large-scaleresource changes.,Proceedings of the 2017 Symposium on Cloud Computing,2017,*
PBSE: a robust path-based speculative execution for degraded-network tail tolerance in data-parallel frameworks,Riza O Suminto; Cesar A Stuardo; Alexandra Clark; Huan Ke; Tanakorn Leesatapornwongsa; Bo Fu; Daniar H Kurniawan; Vincentius Martin; Maheswara Rao G Uma; Haryadi S Gunawi,Abstract We reveal loopholes of Speculative Execution (SE) implementations under aunique fault model: node-level network throughput degradation. This problem appears inmany data-parallel frameworks such as Hadoop MapReduce and Spark. To address this; wepresent PBSE; a robust; path-based speculative execution that employs three keyingredients: path progress; path diversity; and path-straggler detection and speculation. Weshow how PBSE is superior to other approaches such as cloning and aggressivespeculation under the aforementioned fault model. PBSE is a general solution; applicable tomany data-parallel frameworks such as Hadoop/HDFS+ QFS; Spark and Flume.,Proceedings of the 2017 Symposium on Cloud Computing,2017,*
Manylogs: Improved CMR/SMR disk bandwidth and faster durability with scattered logs,Tiratat Patana-anake; Vincentius Martin; Nora Sandler; Cheng Wu; Haryadi S Gunawi,We introduce manylogs; a simple and novel concept of logging that deploys many scatteredlogs on disk such that small random writes can be appended into any log near the currentdisk head position (eg; the location of last large I/O). The benefit is two-fold: the small writesattain fast durability while the large I/Os still sustain large bandwidth. Manylogs also inspirea new principle: decoupling of durability and location constraints. For example; we can putjournal blocks in any scattered log as they only need durability but not location constraints(ie; eventually journal blocks will be freed). We can also allow applications to specify whichfiles that require only durability but not location constraints (eg; application commit-log files).We show the power of manylogs for Conventional-Magnetic Recording file system (MLFS)and block-level (MLB) layers and also Shingled-Magnetic Recording (MLSMR) layer …,Mass Storage Systems and Technologies (MSST); 2016 32nd Symposium on,2016,*
SAMC: a fast model checker for finding heisenbugs in distributed systems,Tanakorn Leesatapornwongsa; Haryadi S Gunawi,Abstract We present SAMC; an open-source model checker that can be integrated to manymodern distributed cloud systems. SAMC can find concurrency bugs caused by non-deterministic dis-tributed events. We have successfully integrated SAMC to Hadoop;ZooKeeper and Cassandra.,Proceedings of the 2015 International Symposium on Software Testing and Analysis,2015,*
What bugs live in the cloud?: a study of issues in scalable distributed systems,Haryadi Gunawi; Thanh Do; Agung Laksono; Mingzhe Hao; Tanakorn Leesatapornwongsa; Jeffrey Ferrari Lukman; Riza Suminto,Resumen We performed a detailed study of development and deployment issues of six open-source scalable distributed systems (scale-out systems) by analyzing 3655 vital issuesreported within a three-year span. The results of our study should be useful to systemdevelopers and operators; systems researchers; and tool builders in advancing the reliabilityof future scale-out systems. The database of our Cloud Bug Study (CbsDB) is publiclyavailable.,; login:: the magazine of USENIX & SAGE,2015,*
Smart Monitor System For Automatic Anomaly Detection@ Baidu,Xianping Qu,Abstract: Billions of requests are supported by hundreds of thousands of servers in Baidu.So many servers and modules bring a huge challenge to engineers for anomaly detection.When an anomaly occurs; various alarms and incidents are sent to engineers. It is verydifficult to find the root cause based on large non-organized monitoring data and alarms.Thus; we tried to build a smarter monitoring system named BIMS (Baidu IntelligentMonitoring System) to help engineers to analyze the problems and give the most possiblereasons for important anomaly such as revenue loss.,*,2015,*
Master and subordinate operating system kernels for heterogeneous multiprocessor systems,*,Systems and methods establish communication and control between variousheterogeneous processors in a computing system so that an operating system can run anapplication across multiple heterogeneous processors. With a single set of developmenttools; software developers can create applications that will flexibly run on one CPU or oncombinations of central; auxiliary; and peripheral processors. In a computing system;application-only processors can be assigned a lean subordinate kernel to manage localresources. An application binary interface (ABI) shim is loaded with application binaryimages to direct kernel ABI calls to a local subordinate kernel or to the main OS kerneldepending on which kernel manifestation is controlling requested resources.,*,2014,*
Improving File System Reliability and Availability with Continuous Checker and Repair,Haryadi S Gunawi,Despite the best efforts of the file and storage system community; file system images becomecorrupt and require repair. In particular; problems with many different parts of the file andstorage system stack can corrupt a file system image: disk media; mechanical components;drive firmware; the transport layer; bus controller; OS drivers; and the buggy file system codeitself [2; 9; 17; 23]. Traditionally; file systems rely on an offline checker utility; fsck [16]; torepair all inconsistencies caused by the corruption. Unfortunately; as the name suggests;offline fsck can only work when the file system is not running. Furthermore; it has a badreputation of being an extremely slow process [13]. Since file system downtime is usuallyavoided in reality; offline fsck is run very rarely (eg; every 30 mounts). As a result; theoccasionality of offline fsck is risky for reliability; corruptions are not detected early in time …,*,2011,*
Verifying File System Properties with Type Inference,Haryadi S Gunawi; Swetha Krishnan,Abstract The storage stack is not trustworthy due to errors that arise from a variety of sources:unreliable hardware; malicious errors and file system bugs. Today; software errors play adominant role due to their inherent complexity. In the first part of our project; we look towardsverifying a specific file system property: on-disk pointer manipulation. We utilize CQUAL; aframework for adding type qualifiers with type inference support; and apply our analysis tothe Linux ext2 file system. We find that adding qualifiers serves the valuable purpose ofensuring that on-disk pointers are accessed and manipulated correctly by the file system.Thus; we believe that the qualifiers we introduce would decrease the probability of bugsbeing introduced by file system programmers. We also describe our experience in usingCQUAL and discuss its limitations. Based on our experience with CQUAL; we come up …,*,2011,*
Measurement & Profiling Readings,Jennifer M Anderson; Lance M Berc; Jeffrey Dean; Sanjay Ghemawat; Monika R Henzinger; Shun-Tak A Leung; Richard L Sites; Mark T Vandevoorde; Carl A Waldspurger; Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau; Nathan C Burnett; Timothy E Denehy; Thomas J Engle; Haryadi S Gunawi; James A Nugent; Florentina I Popvici,Jennifer M. Anderson; Lance M. Berc; Jeffrey Dean; Sanjay Ghemawat; Monika R. Hen-zinger; Shun-Tak A. Leung; Richard L. Sites; Mark T. Vandevoorde; Carl A. Waldspurger; andWilliam E. Weihl. “Continuous Profiling: Where Have All the Cycles Gone?” In Proc. 16thSOSP; 1997; revised version in ACM Transactions on Computer Systems 15(4); November1997. http://portal.acm.org/citation.cfm?id=265925&dl=portal&dl=ACM – or http://pag.lcs.mit.edu/6.893/readings/anderson-tocs97.pdf … Andrea C. Arpaci-Dusseau; Remzi H.Arpaci-Dusseau; Nathan C. Burnett; Timothy E. Denehy; Thomas J. Engle; Haryadi S.Gunawi; James A. Nugent; and Florentina I. Popvici. “Transforming Policies into Mechanismswith Infokernel.” In Proc. 19th SOSP; 2003. http:// www.cs.rochester.edu/sosp2003/papers/p162-arpacidusseau.pdf … Marcos K. Aguilera; Jeffrey C. Mogul; Janet L. Wiener; Patrick …,*,*,*
Improving the Performance of Multi-Tenant Storage with I/O Sheltering,Tiratat Patana-anake; Haryadi S Gunawi,*,*,*,*
