MonetDB/XQuery: a fast XQuery processor powered by a relational engine,Peter Boncz; Torsten Grust; Maurice van Keulen; Stefan Manegold; Jan Rittinger; Jens Teubner,Abstract Relational XQuery systems try to re-use mature relational data managementinfrastructures to create fast and scalable XML database technology. This paper describesthe main features; key contributions; and lessons learned while implementing such asystem. Its architecture consists of (i) a range-based encoding of XML documents intorelational tables;(ii) a compilation technique that translates XQuery into a basic relationalalgebra;(iii) a restricted (order) property-aware peephole relational query optimizationstrategy; and (iv) a mapping from XML update statements into relational updates. Thus; thissystem implements all essential XML database functionalities (rather than a single feature)such that we can learn from the full consequences of our architectural decisions. Whileimplementing this system; we had to extend the state-of-the-art with a number of new …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,360
Staircase join: Teach a relational DBMS to watch its (axis) steps,Torsten Grust; Maurice van Keulen; Jens Teubner,Abstract Relational query processors derive much of their effectiveness from the awarenessof specific table properties like sort order; size; or absence of duplicate tuples. This textapplies (and adapts) this successful principle to database-supported XML and XPathprocessing: the relational system is made tree aware; ie; tree properties like subtree size;intersection of paths; inclusion or disjointness of subtrees are made explicit. We propose alocal change to the database kernel; the staircase join; which encapsulates the necessarytree knowledge needed to improve XPath performance. Staircase join operates on an XMLencoding which makes this knowledge available at the cost of simple integer operations(eg;+;≤). We finally report on quite promising experiments with a staircase join enhancedmain-memory database kernel.,Proceedings of the 29th international conference on Very large data bases-Volume 29,2003,224
Accelerating XPath evaluation in any RDBMS,Torsten Grust; Maurice van Keulen; Jens Teubner,Abstract This article is a proposal for a database index structure; the XPath accelerator; thathas been specifically designed to support the evaluation of XPath path expressions. Assuch; the index is capable to support all XPath axes (including ancestor; following;preceding-sibling; descendant-or-self; etc.). This feature lets the index stand out amongrelated work on XML indexing structures which had a focus on the child and descendantaxes only. The index has been designed with a close eye on the XPath semantics as well asthe desire to engineer its internals so that it can be supported well by existing relationaldatabase query processing technology: the index (a) permits set-oriented (or; rather;sequence-oriented) path evaluation; and (b) can be implemented and queried using well-established relational index structures; notably B-trees and R-trees. We discuss the …,ACM Transactions on Database Systems (TODS),2004,200
A probabilistic XML approach to data integration,Maurice Van Keulen; Ander De Keijzer; Wouter Alink,In mobile and ambient environments; devices need to become autonomous; managing andresolving problems without interference from a user. The database of a (mobile) device canbe seen as its knowledge about objects in the'real world'. Data exchange between smalland/or large computing devices can be used to supplement and update this knowledgewhenever a connection gets established. In many situations; however; data from differentdata sources referring to the same real world objects; may conflict. It is the task of the datamanagement system of the device to resolve such conflicts without interference from a user.In this paper; we take a first step in the development of a probabilistic XML DBMS. The mainidea is to drop the assumption that data in the database should be certain: subtrees in XMLdocuments may denote possible views on the real world. We formally define the notion of …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,156
Pathfinder: XQuery---the relational way,Peter Boncz; Torsten Grust; Maurice van Keulen; Stefan Manegold; Jan Rittinger; Jens Teubner,Abstract Relational query processors are probably the best understood (as well as the bestengineered) query engines available today. Although carefully tuned to process instances ofthe relational model (tables of tuples); these processors can also provide a foundation forthe evaluation of" alien"(non-relational) query languages: if a relational encoding of the aliendata model and its associated query language is given; the RDBMS may act like a special-purpose processor for the new language.,Proceedings of the 31st international conference on Very large data bases,2005,59
Qualitative effects of knowledge rules and user feedback in probabilistic data integration,Maurice van Keulen; Ander de Keijzer,Abstract In data integration efforts; portal development in particular; much development timeis devoted to entity resolution. Often advanced similarity measurement techniques are usedto remove semantic duplicates or solve other semantic conflicts. It proves impossible;however; to automatically get rid of all semantic problems. An often-used rule of thumbstates that about 90% of the development effort is devoted to semi-automatically resolvingthe remaining 10% hard cases. In an attempt to significantly decrease human effort at dataintegration time; we have proposed an approach that strives for agood enough'initialintegration which stores any remaining semantic uncertainty and conflicts in a probabilisticdatabase. The remaining cases are to be resolved with user feedback during query time.The main contribution of this paper is an experimental investigation of the effects and …,The VLDB Journal,2009,57
Tree awareness for relational DBMS kernels: Staircase join,Torsten Grust; Maurice Van Keulen,Abstract Relational database management systems (RDBMSs) derive much of theirefficiency from the versatility of their core data structure: tables of tuples. Such tables aresimple enough to allow for an efficient representation on all levels of the memory hierarchy;yet sufficiently generic to host a wide range of data types. If one can devise mappings from adata type τ to tables and from operations on τ to relational queries; an RDBMS may be apremier implementation alternative. Temporal intervals; complex nested objects; and spatialdata are sample instances for such types τ.,*,2003,44
Using element clustering to increase the efficiency of xml schema matching,Marko Smiljanic; Maurice van Keulen; Willem Jonker,Schema matching attempts to discover semantic mappings between elements of twoschemas. Elements are cross compared using various heuristics (eg; name; data-type; andstructure similarity). Seen from a broader perspective; the schema matching problem is acombinatorial problem with an exponential complexity. This makes the naive matchingalgorithms for large schemas prohibitively inefficient. In this paper we propose a clusteringbased technique for improving the efficiency of large scale schema matching. The techniqueinserts clustering as an intermediate step into existing schema matching algorithms.Clustering partitions schemas and reduces the overall matching load; and creates apossibility to trade between the efficiency and effectiveness. The technique can be used inaddition to other optimization techniques. In the paper we describe the technique …,Data Engineering Workshops; 2006. Proceedings. 22nd International Conference on,2006,36
ROX: run-time optimization of XQueries,Riham Abdel Kader; Peter Boncz; Stefan Manegold; Maurice van Keulen,Abstract Optimization of complex XQueries combining many XPath steps and joins iscurrently hindered by the absence of good cardinality estimation and cost models forXQuery. Additionally; the state-of-the-art of even relational query optimization still strugglesto cope with cost model estimation errors that increase with plan size; as well as with theeffect of correlated joins and selections. In this research; we propose to radically depart fromthe traditional path of separating the query compilation and query execution phases; byhaving the optimizer execute; materialize partial results; and use sampling based estimationtechniques to observe the characteristics of intermediates. The proposed technique takes asinput a Join Graph where the edges are either equi-joins or XPath steps; and the executionenvironment provides value-and structural-join algorithms; as well as structural and value …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,35
Formalizing the XML schema matching problem as a constraint optimization problem,Marko Smiljanić; Maurice Van Keulen; Willem Jonker,Abstract The first step in finding an efficient way to solve any difficult problem is making acomplete; possibly formal; problem specification. This paper introduces a formalspecification for the problem of semantic XML schema matching. Semantic schemamatching has been extensively researched; and many matching systems have beendeveloped. However; formal specifications of problems being solved by these systems donot exist; or are partial. In this paper; we analyze the problem of semantic schema matching;identify its main components and deliver a formal specification based on the constraintoptimization problem formalism. Throughout the paper; we consider the schema matchingproblem as encountered in the context of a large scale XML schema matching application.,International Conference on Database and Expert Systems Applications,2005,32
Duplicate detection in probabilistic data,Fabian Panse; Maurice Van Keulen; Ander De Keijzer; Norbert Ritter,Collected data often contains uncertainties. Probabilistic databases have been proposed tomanage uncertain data. To combine data from multiple autonomous probabilistic databases;an integration of probabilistic data has to be performed. Until now; however; data integrationapproaches have focused on the integration of certain source data (relational or XML).There is no work on the integration of uncertain source data so far. In this paper; we presenta first step towards a concise consolidation of probabilistic data. We focus on duplicatedetection as a representative and essential step in an integration process. We presenttechniques for identifying multiple probabilistic representations of the same real-worldentities.,Data Engineering Workshops (ICDEW); 2010 IEEE 26th International Conference on,2010,23
Process prediction in noisy data sets: a case study in a dutch hospital,Sjoerd Van Der Spoel; Maurice Van Keulen; Chintan Amrit,Abstract Predicting the amount of money that can be claimed is critical to the effectiverunning of an Hospital. In this paper we describe a case study of a Dutch Hospital where weuse process mining to predict the cash flow of the Hospital. In order to predict the cost of atreatment; we use different data mining techniques to predict the sequence of treatmentsadministered; the duration and the final” care product” or diagnosis of the patient. Whileperforming the data analysis we encountered three specific kinds of noise that we callsequence noise; human noise and duration noise. Studies in the past have discussed waysto reduce the noise in process data. However; it is not very clear what effect the noise has todifferent kinds of process analysis. In this paper we describe the combined effect ofsequence noise; human noise and duration noise on the analysis of process data; by …,International Symposium on Data-Driven Process Discovery and Analysis,2012,22
Storing and querying probabilistic XML using a probabilistic relational DBMS,Emiel S Hollander; M van Keulen,Abstract This work explores the feasibility of storing and querying probabilistic XML in aprobabilistic relational database. Our approach is to adapt known techniques for mappingXML to relational data such that the possible worlds are preserved. We show that thisapproach can work for any XML-to-relational technique by adapting a representativeschema-based (inlining) as well as a representative schemaless technique (XPathAccelerator). We investigate the maturity of probabilistic rela-tional databases for this taskwith experiments with one of the state-of-the-art systems; called Trio.,*,2010,21
Quality measures in uncertain data management,Ander de Keijzer; Maurice van Keulen,Abstract Many applications deal with data that is uncertain. Some examples are applicationsdealing with sensor information; data integration applications and healthcare applications.Instead of these applications having to deal with the uncertainty; it should be theresponsibility of the DBMS to manage all data including uncertain data. Several projects doresearch on this topic. In this paper; we introduce four measures to be used to assess andcompare important characteristics of data and systems: uncertainty density; answerdecisiveness and adapted precision and recall measures.,International Conference on Scalable Uncertainty Management,2007,18
An injection with tree awareness: adding staircase join to postgreSQL,Sabine Mayer; Torsten Grust; Maurice van Keulen; Jens Teubner,The syntactic wellformedness constraints of XML (opening and closing tags nest properly)imply that XML processors face the challenge to efficiently handle data that takes the shapeof ordered; unranked trees. Although RDBMSs have originally been designed to managetable-shaped data; we propose their use as XML and XPath processors. In our setup; thedatabase system employs a relational XML document encoding; the XPath accelerator [1];which maps information about the XML node hierarchy to a table; thus making it possible toevaluate XPath expressions on SQL hosts. Conventional RDBMSs; nevertheless; remainignorant of many interesting properties of the encoded tree data; and were thus found tomake no or poor use of these properties. This is why we devised a new join algorithm;staircase join [2]; which incorporates the tree-specific knowledge required for an efficient …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,18
Indeterministic Handling of Uncertain Decisions in Deduplication,Fabian Panse; Maurice van Keulen; Norbert Ritter,Abstract In current research and practice; deduplication is usually considered as adeterministic approach in which database tuples are either declared to be duplicates or not.In ambiguous situations; however; it is often not completely clear-cut; which tuples representthe same real-world entity. In deterministic approaches; many realistic possibilities may beignored; which in turn can lead to false decisions. In this article; we present anindeterministic approach for deduplication by using a probabilistic target model includingtechniques for proper probabilistic interpretation of similarity matching results. Thus; insteadof deciding for one of the most likely situations; all realistic situations are modeled in theresultant data. This approach minimizes the negative impact of false decisions. Moreover;the deduplication process becomes almost fully automatic and human effort can be …,Journal of Data and Information Quality (JDIQ),2013,17
A possible world approach to uncertain relational data,Ander De Keijzer; Maurice van Keulen,Data exchange between embedded systems and other small or large computing deviceincreases. Since data in different data sources may refer to the same real world objects; datacannot simply be merged. Furthermore; in many situations; conflicts in data about the samereal world objects need to be resolved without interference from a user. We report on anattempt to make a RDBMS probabilistic; ie; data in a relation represents all possible viewson the real world; in order to achieve unattended data integration. We define a probabilisticrelational data model and review standard SQL query primitives in the light of probabilisticdata. It appears that thinking in terms of'possible worlds' is powerful in determining theproper semantics of these query primitives.,Database and Expert Systems Applications; 2004. Proceedings. 15th International Workshop on,2004,16
Improving toponym disambiguation by iteratively enhancing certainty of extraction,Mena Badieh Habib; Maurice van Keulen,Abstract Named entity extraction (NEE) and disambiguation (NED) have received muchattention in recent years. Typical fields addressing these topics are information retrieval;natural language processing; and semantic web. This paper addresses two problems withtoponym extraction and disambiguation (as a representative example of named entities).First; almost no existing works examine the extraction and disambiguation interdependency.Second; existing disambiguation techniques mostly take as input extracted named entitieswithout considering the uncertainty and imperfection of the extraction process. It is the aim ofthis paper to investigate both avenues and to show that explicit handling of the uncertainty ofannotation has much potential for making both extraction and disambiguation more robust.We conducted experiments with a set of holiday home descriptions with the aim to extract …,*,2012,15
Unsupervised improvement of named entity extraction in short informal context using disambiguation clues,Mena B Habib; Maurice van Keulen,Abstract Short context messages (like tweets and SMS's) are a potentially rich source ofcontinuously and instantly updated information. Shortness and informality of such messagesare challenges for Natural Language Processing tasks. Most efforts done in this directionrely on machine learning techniques which are expensive in terms of data collection andtraining. In this paper we present an unsupervised Semantic Web-driven approach toimprove the extraction process by using clues from the disambiguation process. Forextraction we used a simple Knowledge-Base matching technique combined with aclustering-based approach for disambiguation. Experimental results on a self-collected setof tweets (as an example of short context messages) show improvement in extraction resultswhen using unsupervised feedback from the disambiguation process.,*,2012,15
Named Entity Extraction and Disambiguation: The Reinforcement Effect.,Mena Badieh Habib; Maurice van Keulen,Abstract Named entity extraction and disambiguation have received much attention in recentyears. Typical fields addressing these topics are information retrieval; natural languageprocessing; and semantic web. Although these topics are highly dependent; almost noexisting works examine this dependency. It is the aim of this paper to examine thedependency and show how one affects the other; and vice versa. We conductedexperiments with a set of descriptions of holiday homes with the aim to extract anddisambiguate toponyms as a representative example of named entities. We experimentedwith three approaches for disambiguation with the purpose to infer the country of the holidayhome. We examined how the effectiveness of extraction influences the effectiveness ofdisambiguation; and reciprocally; how filtering out ambiguous names (an activity that …,*,2011,15
Taming data explosion in probabilistic information integration,Ander De Keijzer; Maurice Van Keulen; Yiping Li,Abstract Data integration has been a challenging problem for decades. In an ambientenvironment; where many autonomous devices have their own information sources andnetwork connectivity is ad hoc and peer-to-peer; it even becomes a serious bottleneck. Toenable devices to exchange information without the need for interaction with a user at dataintegration time and without the need for extensive semantic annotations; a probabilisticapproach seems rather promising. It simply teaches the device how to cope with theuncertainty occurring during data integration. Unfortunately; without any kind of worldknowledge; almost everything becomes uncertain; hence maintaining all possibilitiesproduces huge integrated information sources. In this paper; we claim that only very simpleand generic rules are enough world knowledge to drastically reduce the amount of …,*,2006,15
Size estimation of non-cooperative data collections,Mohammadreza Khelghati; Djoerd Hiemstra; Maurice van Keulen,Abstract With the increasing amount of data in deep web sources (hidden from generalsearch engines behind web forms); accessing this data has gained more attention. In thealgorithms applied for this purpose; it is the knowledge of a data source size that enables thealgorithms to make accurate decisions in stopping the crawling or sampling processeswhich can be so costly in some cases [14]. This tendency to know the sizes of data sourcesis increased by the competition among businesses on the Web in which the data coverage iscritical. In the context of quality assessment of search engines [7]; search engine selection inthe federated search engines; and in the resource/collection selection in the distributedsearch field [19]; this information is also helpful. In addition; it can give an insight over someuseful statistics for public sectors like governments. In any of these mentioned scenarios …,Proceedings of the 14th International Conference on Information Integration and Web-based Applications & Services,2012,14
Managing uncertainty: The road towards better data interoperability,Maurice van Keulen,Abstract Data interoperability encompasses the many data management activities neededfor effective information management in anyones or any organizations everyday work suchas data cleaning; coupling; fusion; mapping; and information extraction. It is our convictionthat a significant amount of money and time in IT that is devoted to these activities; is aboutdealing with one problem:“semantic uncertainty”. Sometimes data is subjective; incomplete;not current; or incorrect; sometimes it can be interpreted in different ways; etc. In our opinion;clean correct data is only a special case; hence data management technology should treatdata quality problems as a fact of life; not as something to be repaired afterwards. Recentapproaches treat uncertainty as an additional source of information which should bepreserved to reduce its impact. We believe that the road towards better data …,it-Information Technology Methoden und innovative Anwendungen der Informatik und Informationstechnik,2012,14
A generic open world named entity disambiguation approach for tweets,Mena B Habib; M van Keulen,Abstract Social media is a rich source of information. To make use of this information; it issometimes required to extract and disambiguate named entities. In this paper we focus onnamed entity disambiguation (NED) in twitter messages. NED in tweets is challenging in twoways. First; the limited length of Tweet makes it hard to have enough context while manydisambiguation techniques depend on it. The second is that many named entities in tweetsdo not exist in a knowledge base (KB). In this paper we share ideas from informationretrieval (IR) and NED to propose solutions for both challenges. For the first problem wemake use of the gregarious nature of tweets to get enough context needed fordisambiguation. For the second problem we look for an alternative home page if there is noWikipedia page represents the entity. Given a mention; we obtain a list of Wikipedia …,*,2013,12
Concept extraction challenge: University of twente at# msm2013,Mena B Habib; Maurice van Keulen; Zhemin Zhu,Abstract Twitter messages are a potentially rich source of continuously and instantly updatedinformation. Shortness and informality of such messages are challenges for NaturalLanguage Processing tasks. In this paper we present a hybrid approach for Named EntityExtraction (NEE) and Classification (NEC) for tweets. The system uses the power of theConditional Random Fields (CRF) and the Support Vector Machines (SVM) in a hybrid wayto achieve better results. For named entity type classification we used AIDA disambiguationsystem to disambiguate the extracted named entities and hence find their type.,*,2013,12
IMPrECISE: Good-is-good-enough data integration,Ander de Keijzer; Maurice van Keulen,IMPrECISE is an XQuery module that adds probabilistic XML functionality to an existing XMLDBMS; in our case MonetDB/XQuery. We demonstrate probabilistic XML and dataintegration functionality of IMPrECISE. The prototype is configurable with domain knowledgesuch that the amount of uncertainty arising during data integration is reduced to anacceptable level; thus obtaining a" good is good enough" data integration with minimalhuman effort.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,12
Distributed XML Database Systems,Marko Smiljanic; Henk Blanken; Maurice van Keulen; Willem Jonker,Abstract Invention of XML as a universal standard for data representation triggeredenormously wide efforts for its adaptation into almost every IT activity. Databases took one ofthe focusing places of the XML research. This paper investigates the development path ofXML from its origins to its current place in distributed database systems. Throughenumeration of features and analysis of the problems related to XML and distributeddatabase systems it forms a platform for understanding the consequences of adopting XMLto this area of IT.,*,2002,12
The IMPRESS DDT: A database design toolbox based on a formal specification language,Jan Flokstra; Maurice van Keulen; Jacek Skowronek,The Database Design Tool prototype is being developed in the IMPRESS project (Espritproject 6355). The IMPRESS project started in May 1992 and aims at creating a lowlevelstorage manager tailored for multimedia applications; together with a library of efficientoperators; a programming environment; high-level design tools and methodology. The DDTis part of this last effort. The project focuses on the field of Technical Information Systems;where there is aneed for tools supporting modeling of complex objects. Designers in thisfield usually use incremental design or step by step prototyping; because this seems to bebest suited for users coping with complexity and uncertainty about their own needs orrequirements. The IMPRESS DDT aims at supporting the database design part of thisprocess.,ACM SIGMOD Record,1994,12
Analysis of the NIST database towards the composition of vulnerabilities in attack scenarios,Virginia Nunes Leal Franqueira; Maurice van Keulen,The composition of vulnerabilities in attack scenarios has been traditionally performedbased on detailed pre-and post-conditions. Although very precise; this approach isdependent on human analysis; is time consuming; and not at all scalable. We investigate theNIST National Vulnerability Database (NVD) with three goals:(i) understand the associationsamong vulnerability attributes related to impact; exploitability; privilege; type of vulnerabilityand clues derived from plaintext descriptions;(ii) validate our initial composition model whichis based on required access and resulting effect; and (iii) investigate the maturity of XMLdatabase technology for performing statistical analyses like this directly on the XML data. Inthis report; we analyse 27;273 vulnerability entries (CVE 1) from the NVD. Using onlynominal information; we are able to eg identify clusters in the class of vulnerabilities with …,*,2008,11
Deep web entity monitoring,Mohamamdreza Khelghati; Djoerd Hiemstra; Maurice Van Keulen,Accessing information is an essential factor in decision making processes occurring indifferent domains. Therefore; broadening the coverage of available information for thedecision makers is of a vital importance. In such a informationthirsty environment; accessingevery source of information is considered highly valuable. Nowadays; the main or the mostgeneral approach for finding and accessing information sources is searching queries overgeneral search engines such as Google; Yahoo; or Bing. However; these search engines donot cover all the data available on the Web. In addition to the fact that none of these searchengines cover all the webpages existing on the Web; they miss the data behind web searchforms. This data is defined as hidden web or deep web which is not accessible throughsearch engines. It is estimated that deep web contains data in a scale several times …,Proceedings of the 22nd International Conference on World Wide Web,2013,10
User feedback in probabilistic integration,Ander de Keijzer; Maurice van Keulen,Data integration approaches mostly attempt to resolve semantic uncertainty and conflictsbetween data sources during the data integration process. In some application areas; this isimpractical or even prohibitive. We propose a probabilistic XML approach that allowsstorage and querying of uncertain data. It requires only minimal user involvement duringdata integration; because most semantic uncertainty and conflicts can be resolved byexploiting user feedback on query results; thus effectively postponing user involvement toquery time when a user already interacts with the system. We show that repeated feedbackgradually improves the factual correctness of integrated data.,Database and Expert Systems Applications; 2007. DEXA'07. 18th International Workshop on,2007,10
MonetDB/XQuery—consistent and efficient updates on the pre/Post plane,Peter Boncz; Jan Flokstra; Torsten Grust; Maurice van Keulen; Stefan Manegold; Sjoerd Mullender; Jan Rittinger; Jens Teubner,Abstract Relational XQuery processors aim at leveraging mature relational DBMS queryprocessing technology to provide scalability and efficiency. To achieve this goal; variousstorage schemes have been proposed to encode the tree structure of XML documents in flatrelational tables. Basically; two classes can be identified:(1) encodings using fixed-lengthsurrogates; like the preorder ranks in the pre/post encoding [5] or the equivalentpre/size/level encoding [8]; and (2) encodings using variable-length surrogates; like; eg;ORDPATH [9] or P-PBiTree [12]. Recent research [1] showed a clear advantage of theformer for efficient evaluation of XPath location steps; exploiting techniques like cheap nodeorder tests; positional lookup; and node skipping in staircase join [7]. However; onceupdates are involved; variable-length surrogates are often considered the better choice …,International Conference on Extending Database Technology,2006,10
Moa: extensibility and efficiency in querying nested data,Maurice van Keulen; Jochem Vonk; Arjen P Vries; Jan Flokstra; Henk Ernst Blok,Abstract Advanced non-traditional application domains such as geographic informationsystems and digital library systems demand advanced data management support. In aneffort to cope with this demand; we present a novel multi-model DBMS architecture whichprovides efficient evaluation of queries on complexly structured data. A vital role in thisarchitecture is played by the Moa language featuring a nested relational data model basedon XNF2; in which we placed renewed interest. Furthermore; the architecture allowsextensibility on all of its levels providing the means to better integrate domain-specificalgorithms into the system. In addition to this; the extensibility of the Moa language isdesigned in a way that optimization obstacles due to blackbox treatment of ADTs is avoided.This combination of well-integrated domainspecific algorithms; extensibility open to …,*,2002,10
Towards geosocial recommender systems,Victor de Graaff; Maurice van Keulen; Rolf A de By,Abstract The usage of social networks sites (SNSs); such as Facebook; and geosocialnetworks (GSNs); such as Foursquare; has increased tremendously over the past years. Thewillingness of users to share their current locations and experiences facilitate the creation ofgeographical recommender systems based on user generated content (UGC). This idea hasbeen used to create a substantial amount of geosocial recommender systems (GRSs); suchas Gogobot; TripIt; and Trippy already; but can be applied to more complex scenarios; suchas the recommendation of products with a strong binding to their region; such as real estateor vacation destinations. This extended form of GRS development requires advancedfunctionality for information collection (from the web; other social media and allowing theusers to supply UGC inside the application); information enrichment (such as data quality …,Proceedings of the 4th International Workshop on Web Intelligence & Communities,2012,9
Handling uncertainty in information extraction,Maurice van Keulen; Mena B Habib,*,Proceedings of the 7th International Conference on Uncertainty Reasoning for the Semantic Web-Volume 778,2011,9
Overview of query optimization in XML database systems,R Abdel Kader; Maurice van Keulen,KNAW Narcis. Back to search results. Publication Overview of query optimizationin XML database systems (2007). . Pagina-navigatie: Main …,*,2007,9
Toponym Extraction and Disambiguation Enhancement Using Loops of Feedback,Mena B Habib; Maurice van Keulen,Abstract Toponym extraction and disambiguation have received much attention in recentyears. Typical fields addressing these topics are information retrieval; natural languageprocessing; and semantic web. This paper addresses two problems with toponym extractionand disambiguation. First; almost no existing works examine the extraction anddisambiguation interdependency. Second; existing disambiguation techniques mostly takeas input extracted named entities without considering the uncertainty and imperfection of theextraction process. In this paper we aim to investigate both avenues and to show that explicithandling of the uncertainty of annotation has much potential for making both extraction anddisambiguation more robust. We conducted experiments with a set of holiday homedescriptions with the aim to extract and disambiguate toponyms. We show that the …,*,2013,8
Information Extraction for Social Media,Mena B Habib; Maurice van Keulen,*,SWAIE 2014,2014,7
Neogeography: The challenge of channelling large and ill-behaved data streams,Mena B Habib; Peter Apers; Maurice van Keulen,Neogeography is the combination of user generated data and experiences with mappingtechnologies. In this paper we present a research project to extract valuable structuredinformation with a geographic component from unstructured user generated text in wikis;forums; or SMSes. The project intends to help workers communities in developing countriesto share their knowledge; providing a simple and cheap way to contribute and get benefitusing the available communication technology.,Data Engineering Workshops (ICDEW); 2011 IEEE 27th International Conference on,2011,7
Automated semantic trajectory annotation with indoor point-of-interest visits in urban areas,Victor de Graaff; Rolf A de By; Maurice van Keulen,Abstract User trajectories contain a wealth of implicit information. The places that peoplevisit; provide us with information about their preferences and needs. Furthermore; it providesus with information about the popularity of places; for example at which time of the year orday these places are frequently visited. The potential for behavioral analysis of trajectories iswidely discussed in literature; but all of these methods need a pre-processing step: thegeometric trajectory data needs to be transformed into a semantic collection or sequence ofvisited points-of-interest that is more suitable for data mining. Especially indoor activities inurban areas are challenging to detect from raw trajectory data. In this paper; we propose anew algorithm for the automated detection of visited points-of-interest. This algorithmextracts the actual visited points-of-interest well; both in terms of precision and recall …,Proceedings of the 31st Annual ACM Symposium on Applied Computing,2016,6
A hybrid approach for robust multilingual toponym extraction and disambiguation,Mena B Habib; Maurice van Keulen,Abstract Toponym extraction and disambiguation are key topics recently addressed by fieldsof Information Extraction and Geographical Information Retrieval. Toponym extraction anddisambiguation are highly dependent processes. Not only toponym extraction effectivenessaffects disambiguation; but also disambiguation results may help improving extractionaccuracy. In this paper we propose a hybrid toponym extraction approach based on HiddenMarkov Models (HMM) and Support Vector Machines (SVM). Hidden Markov Model is usedfor extraction with high recall and low precision. Then SVM is used to find false positivesbased on informativeness features and coherence features derived from the disambiguationresults. Experimental results conducted with a set of descriptions of holiday homes with theaim to extract and disambiguate toponyms showed that the proposed approach …,*,2013,6
Information extraction; data integration; and uncertain data management: The state of the art,Mena B Habib; Maurice van Keulen,Abstract Information Extraction; data Integration; and uncertain data management aredifferent areas of research that got vast focus in the last two decades. Many researchestackled those areas of research individually. However; information extraction systems shouldhave integrated with data integration methods to make use of the extracted information.Handling uncertainty in extraction and integration process is an important issue to enhancethe quality of the data in such integrated systems. This article presents the state of the art ofthe mentioned areas of research and shows the common grounds and how to integrateinformation extraction and data integration under uncertainty management cover.,CTIT Technical Report Series,2011,6
Rox: The robustness of a run-time xquery optimizer against correlated data,Riham Abdel Kader; Peter Boncz; Stefan Manegold; Maurice van Keulen,We demonstrate ROX; a run-time optimizer of XQueries; that focuses on finding the bestexecution order of XPath steps and relational joins in an XQuery. The problem of joinordering has been extensively researched; but the proposed techniques are stillunsatisfying. These either rely on a cost model which might result in inaccurate estimations;or explore only a restrictive number of plans from the search space. ROX is developed totackle these problems. ROX does not need any cost model; and defers query optimization torun-time intertwining optimization and execution steps. In every optimization step; samplingtechniques are used to estimate the cardinality of unexecuted steps and joins to make adecision which sequence of operators to process next. Consequently; each execution stepwill provide updated and accurate knowledge about intermediate results; which will be …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,6
Loop-lifted staircase join: from XPath to XQuery,PA Boncz; Torsten Grust; M van Keulen; Stefan Manegold; Jan Rittinger; Jens Teubner,textabstractVarious techniques have been proposed for efficient evaluation of XPathexpressions; where the XPath location steps are rooted in a single sequence of contextnodes. Among these techniques; the staircase join allows to evaluate XPath location stepsalong arbitrary axes in at most one scan over the XML document; exploiting the XPathaccelerator encoding (aka. pre/post encoding). In XQuery; however; embedded XPath sub-expressions occur in arbitrarily nested for-loops. Thus; they are rooted in multiple sequencesof context nodes (one per iteration). Consequently; the previously proposed algorithms needto be applied repeatedly; requiring multiple scans over the XML document encoding. In thiswork; we present loop-lifted staircase join; an extension of the staircase join that allows toefficiently evaluate XPath sub-expressions in arbitrarily nested XQuery iteration scopes …,Information Systems [INS],2005,6
Defining the xml schema matching problem for a personal schema based query answering system,Marko Smiljanic; Maurice van Keulen; Willem Jonker,Abstract In this report; we analyze the problem of personal schema matching. We define theingredients of the XML schema matching problem using constraint logic programming. Thisallows us to thourougly investigate specific matching problems. We do not have the ambitionto provide for a formalism that covers all kinds of schema matching problems. The target isspecifically personal schema matching using XML. The report is organized as follows.Chapter 2 provides a detailed description of our research domain-the Personal SchemaQuery Answering System. In chapter 3; we introduce a framework for defining the XMLschema matching problem. The XML schema matching problem is defined using thisframework in chapter 4. An important component of the XML schema matching problem isthe objective function; which is investigated in chapter 5. Chapter 6 presents the related …,*,2004,6
Moa and the multi-model architecture: a new perspective on NF2,Maurice van Keulen; Jochem Vonk; Arjen P de Vries; Jan Flokstra; Henk Ernst Blok,Abstract Advanced non-traditional application domains such as geographic informationsystems and digital library systems demand advanced data management support. In aneffort to cope with this demand; we present the concept of a novel multi-model DBMSarchitecture which provides evaluation of queries on complexly structured data withoutsacrificing efficiency. A vital role in this architecture is played by the Moa language featuringa nested relational data model based on XNF 2; in which we placed renewed interest.Furthermore; extensibility in Moa avoids optimization obstacles due to black-box treatment ofADTs. The combination of a mapping of queries on complexly structured data to an efficientphysical algebra expression via a nested relational algebra; extensibility open tooptimization; and the consequently better integration of domain-specific algorithms …,International Conference on Database and Expert Systems Applications,2003,6
Bridging the GAP between relational and native XML storage with staircase join,Jens Teubner; Maurice van Keulen; Torsten Grust,Several mapping schemes have recently been proposed to store XML data in relationaltables. Relational database systems are readily available and can handle vast amounts ofdata very efficiently; taking advantage of physical properties that are specific to the relationalmodel; like sortedness or uniqueness. Tables that originate from XML documents; however;carry some further properties that cannot be exploited by current relational query processors.We propose a new join algorithm that is specifically designed to operate on XML datamapped to relational tables. The staircase join is fully aware of the underlying tree propertiesand allows for I/O and cache optimal query execution. As a local change to the databasekernel; it can easily be plugged into any relational database and allows for variousoptimization strategies; eg selection pushdown. Experiments with our prototype; based on …,15. GI Workshop on Foundations of Database Systems,2003,6
The TM Manual; version 2.0; revision e,René Bal; Herman Balsters; RA de By; Alexander Bosschaart; Jan Flokstra; Maurice van Keulen; J Skowronek; B Termorshuizen,This is the TM language manual; describing version 2.0 of the database specificationlanguage TM. This document is a working document; which means that not all (althoughmany) of our ideas concerning the construction of the language have been thoroughlyevaluated. Most importantly; we lack a fine set of application case studies; which wouldundoubtedly result in further enhancements and tuning of the language. This is obviously acyclic problem; because how can we obtain good case studies if there is no manual? Thus;the main goal of this document is to get some people out in the field of databasespecification use TM; and report on their findings. We use TM as a high-level language forthe design and specification of object-oriented database schemas in an efficient andeffective manner. The TM language and its accompanying design tools enable users to …,*,1995,6
Generic knowledge-based analysis of social media for recommendations,Victor de Graaff; Anne van de Venis; Maurice van Keulen; RA de By,Abstract Recommender systems have been around for decades to help people find the bestmatching item in a pre-defined item set. Knowledge-based recommender systems are usedto match users based on information that links the two; but they often focus on a single;specific application; such as movies to watch or music to listen to. In this presentation; wepresent our Interest-Based Recommender System (IBRS). This knowledge-basedrecommender system provides recommendations that are generic in three dimensions: IBRSis (1) domain-independent;(2) language-independent; and (3) independent of the usedsocial medium. To match user interests with items; the first are derived from the user's socialmedia profile; enriched with a deeper semantic embedding obtained from the genericknowledge base DBpedia. These interests are used to extract personalized …,*,2015,5
Named Entity Extraction and Linking Challenge: University of Twente at# Microposts2014,Mena B Habib; Maurice van Keulen; Zhemin Zhu,Abstract Twitter is a potentially rich source of continuously and instantly updated information.Shortness and informality of tweets are challenges for Natural Language Processing (NLP)tasks. In this paper; we present a hybrid approach for Named Entity Extraction (NEE) andLinking (NEL) for tweets. Although NEE and NEL are two topics that are well studied inliterature; almost all approaches treated the two problems separately. We believe thatdisambiguation (linking) could help improving the extraction process. We call this potentialfor mutual improvement; the reinforcement effect. It mimics the way humans understandnatural language. Furthermore; our proposed approaches handles uncertainties involved inthe two processes by considering possible alternatives.,*,2014,5
Point of interest to region of interest conversion,Victor de Graaff; Rolf A de By; Maurice van Keulen; Jan Flokstra,Abstract Trajectories of people contain a vast amount of information on users' interests andpopularity of locations. To obtain this information; the places visited by the owner of thedevice on such a trajectory need to be recognized. However; the location information on apoint of interest (POI) in a database is normally limited to an address and a coordinate pair;rather than a polygon describing its boundaries. A region of interest can be used to intersecttrajectories to match trajectories with objects of interest. In the absence of expensive andoften not publicly available detailed spatial data like cadastral data; we need to approximatethis ROI. In this paper; we present several approaches to approximate the size and shape ofROIs; by integrating data from multiple public sources; a validation technique; and avalidation of these approaches against the cadastral data of the city of Enschede; The …,Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems,2013,5
Compression of probabilistic XML documents,Irma Veldman; Ander De Keijzer; Maurice Van Keulen,Abstract Database techniques to store; query and manipulate data that contains uncertaintyreceives increasing research interest. Such UDBMSs can be classified according to theirunderlying data model: relational; XML; or RDF. We focus on uncertain XML DBMS with asrepresentative example the Probabilistic XML model (PXML) of [10; 9]. The size of a PXMLdocument is obviously a factor in performance. There are PXML-specific techniques toreduce the size; such as a push down mechanism; that produces equivalent but morecompact PXML documents. It can only be applied; however; where possibilities aredependent. For normal XML documents there also exist several techniques for compressinga document. Since Probabilistic XML is (a special form of) normal XML; it might benefit fromthese methods even more. In this paper; we show that existing compression mechanisms …,International Conference on Scalable Uncertainty Management,2009,5
User feedback in probabilistic xml,Ander de Keijzer; Maurice van Keulen,Abstract Data integration is a challenging problem in many application areas. Approachesmostly attempt to resolve semantic uncertainty and conflicts between information sources aspart of the data integration process. In some application areas; this is impractical or evenprohibitive; for example; in an ambient environment where devices on an ad hoc basis haveto exchange information autonomously. We have proposed a probabilistic XML approachthat allows data integration without user involvement by storing semantic uncertainty andconflicts in the integrated XML data. As a consequence; the integrated information sourcerepresents all possible appearances of objects in the real world; the so-called possibleworlds. In this paper; we show how user feedback on query results can resolve semanticuncertainty and conflicts in the integrated data. Hence; user involvement is effectively …,*,2007,5
TwitterNEED: A hybrid approach for named entity extraction and disambiguation for tweet,Mena B Habib; Maurice van Keulen,Abstract Twitter is a rich source of continuously and instantly updated information. Shortnessand informality of tweets are challenges for Natural Language Processing tasks. In thispaper; we present TwitterNEED; a hybrid approach for Named Entity Extraction and NamedEntity Disambiguation for tweets. We believe that disambiguation can help to improve theextraction process. This mimics the way humans understand language and reduces errorpropagation in the whole system. Our extraction approach aims for high extraction recall first;after which a Support Vector Machine attempts to filter out false positives among theextracted candidates using features derived from the disambiguation phase in addition toother word shape and Knowledge Base features. For Named Entity Disambiguation; weobtain a list of entity candidates from the YAGO Knowledge Base in addition to top-ranked …,Natural Language Engineering,2015,4
Need4tweet: a twitterbot for tweets named entity extraction and disambiguation,Mena Habib; Maurice van Keulen,Abstract In this demo paper; we present NEED4Tweet; a Twitterbot for named entityextraction (NEE) and disambiguation (NED) for Tweets. The straightforward application ofstate-of-the-art extraction and disambiguation approaches on informal text widely used inTweets; typically results in significantly degraded performance due to the lack of formalstructure; the lack of sufficient context required; and the seldom entities involved. In thispaper; we introduce a novel framework that copes with the introduced challenges. We relyon contextual and semantic features more than syntactic features which are less informative.We believe that disambiguation can help to improve the extraction process. This mimics theway humans understand language.,Proceedings of ACL-IJCNLP 2015 System Demonstrations,2015,4
Increasing NER recall with minimal precision loss,Jasper Kuperus; Cor J Veenman; Maurice van Keulen,Named Entity Recognition (NER) is broadly used as a first step toward the interpretation oftext documents. However; for many applications; such as forensic investigation; recall iscurrently inadequate; leading to loss of potentially important information. Entity classambiguity cannot be resolved reliably due to the lack of context information or theexploitation thereof. Consequently; entity classification introduces too many errors; leadingto severe omissions in answers to forensic queries. We propose a technique based onmultiple candidate labels; effectively postponing decisions for entity classification to querytime. Entity resolution exploits user feedback: a user is only asked for feedback on entitiesrelevant to his/her query. Moreover; giving feedback can be stopped anytime when queryresults are considered good enough. We propose several interaction strategies that …,Intelligence and Security Informatics Conference (EISIC); 2013 European,2013,4
To use or not to use: guidelines for researchers using data from online social networking sites,Aimee van Wynsberghe; Henry Been; Maurice van Keulen,*,*,2013,4
Improving toponym extraction and disambiguation using feedback loop,Mena B Habib; Maurice van Keulen,Abstract This paper addresses two problems with toponym extraction and disambiguation.First; almost no existing works examine the extraction and disambiguation interdependency.Second; existing disambiguation techniques mostly take as input extracted toponymswithout considering the uncertainty and imperfection of the extraction process. It is the aim ofthis paper to investigate both avenues and to show that explicit handling of the uncertainty ofannotation has much potential for making both extraction and disambiguation more robust.,International Conference on Web Engineering,2012,4
Indeterministic handling of uncertain decisions in duplicate detection,Fabian Panse; Maurice van Keulen; Norbert Ritter,Abstract In current research; duplicate detection is usually considered as a deterministicapproach in which tuples are either declared as duplicates or not. However; most often it isnot completely clear whether two tuples represent the same real-world entity or not. Indeterministic approaches; however; this uncertainty is ignored; which in turn can lead tofalse decisions. In this paper; we present an indeterministic approach for handling uncertaindecisions in a duplicate detection process by using a probabilistic target schema. Thus;instead of deciding between multiple possible worlds; all these worlds can be modeled inthe resulting data. This approach minimizes the negative impacts of false decisions.Furthermore; the duplicate detection process becomes almost fully automatic and humaneffort can be reduced to a large extent. Unfortunately; a full-indeterministic approach is by …,*,2010,4
Supporting positional predicates in efficient XPath axis evaluation for DOM data structures,Torsten Grust; Jan Hidders; Philippe Michiels; Roel Vercammen; Maurice Van Keulen,Abstract In this technical report we propose algorithms for implementing the axes for elementnodes in XPath given a DOM-like representation of the document. First; we constructalgorithms for evaluating simple step expressions; withoout any (positional) predicates. Thetime complexity of these algorithms is at most O (l+ m) where l is the size of the input list andm the size of the output list. This improves upon results in [6] where also algorithms withlinear time complexity are presented; but these are linear in the size of the entire documentwhereas our algorithms are linear in the size of the intermediate results which are oftenmuch smaller. In a second phase we give a description of how the support for positionalpredicates can be added to the algorithms with a focus on maintaining the efficiency ofevaluation. Each algorithm assumes an input list that is sorted in document order and …,*,2004,4
Relational Approach to Logical Query Optimization of XPath.,Maurice van Keulen,ABSTRACT To be able to handle the ever growing volumes of XML documents; effectiveand efficient data management solutions are needed. Managing XML data in a relationalDBMS has great potential. Recently; effective relational storage schemes and indexstructures have been proposed as well as special-purpose join operators to speed upquerying of XML data using XPath/XQuery. In this paper; we address the topic of query planconstruction and logical query optimization. The claim of this paper is that standardrelational algebra extended with special-purpose join operators suffices for logical queryoptimization. We focus on the XPath accelerator storage scheme and associated staircasejoin operators; but the approach can be generalized easily.,TDM,2004,4
Two video analysis applications using foreground/background segmentation,Zoran Zivkovic; Milan Petkovic; R Van Mierlo; M van Keulen; F van der Heijden; W Jonker; E Rijnierse,Probably the most frequently solved problem when videos are analyzed is segmenting aforeground object from its background in an image. After some regions in an image aredetected as the foreground objects; some features are extracted that describe thesegmented regions. These features together with the domain knowledge are often enoughto extract the needed high-level semantics from the video material. In this paper we presenttwo automatic systems for video analysis and indexing. In both systems the segmentation ofthe foreground objects is the basic processing step. The extracted features are then used tosolve the problem. One system analyses traffic videos; the other tennis games.,Visual Information Engineering; 2003. VIE 2003. International Conference on,2003,4
Revisiting the formal foundation of probabilistic databases,Brend Wanders; Maurice van Keulen,Abstract One of the core problems in soft computing is dealing with uncertainty in data. Inthis paper; we revisit the formal foundation of a class of probabilistic databases with thepurpose to (1) obtain data model independence;(2) separate metadata on uncertainty andprobabilities from the raw data;(3) better understand aggregation; and (4) create moreopportunities for optimization. The paper presents the formal framework and validates datamodel independence by showing how to a obtain probabilistic Datalog as well as aprobabilistic relational algebra by applying the framework to their non-probabilisticcounterparts. We conclude with a discussion on the latter three goals.,*,2015,3
Uncertain Groupings: Probabilistic combination of grouping data,Brend Wanders; Maurice van Keulen; PE van der Vet,Abstract Probabilistic approaches for data integration have much potential [7]. We view dataintegration as an iterative process where data understanding gradually increases as thedata scientist continuously refines his view on how to deal with learned intricacies like dataconflicts. This paper presents a probabilistic approach for integrating data on groupings. Wefocus on a bio-informatics use case concerning homology. A bio-informatician has a largenumber of homology data sources to choose from. To enable querying combined knowledgecontained in these sources; they need to be integrated. We validate our approach byintegrating three real-world biological databases on homology in three iterations.,*,2014,3
Computer assisted extraction; merging and correlation of identities with tracks inspector,Jop Hofste; Hans Henseler; Maurice van Keulen,Abstract With the pervasiveness of computers and mobile devices; digital forensics becomesmore important in law enforcement. Detectives increasingly depend on the scarce support ofdigital specialists which impedes efficiency of criminal investigations. Tracks Inspector is acommercial solution that enables non-technical investigators to easily investigate digitalevidence using a web browser. We will demonstrate how Tracks Inspector can be used todiscover the most important persons and groups in case data by investigators withoutrequiring the help of digital forensics experts.,Proceedings of the Fourteenth International Conference on Artificial Intelligence and Law,2013,3
Named Entity Extraction and Disambiguation: The Missing Link,Mena B Habib; Maurice van Keulen,*,*,2013,3
Improving named entity disambiguation by iteratively enhancing certainty of extraction,Mena Badieh Habib; Maurice van Keulen,Abstract Named entity extraction and disambiguation have received much attention in recentyears. Typical fields addressing these topics are information retrieval; natural languageprocessing; and semantic web. This paper addresses two problems with named entityextraction and disambiguation. First; almost no existing works examine the extraction anddisambiguation interdependency. Second; existing disambiguation techniques mostly takeas input extracted named entities without considering the uncertainty and imperfection of theextraction process. It is the aim of this paper to investigate both avenues and to show thatexplicit handling of the uncertainty of annotation has much potential for making bothextraction and disambiguation more robust. We conducted experiments with a set of holidayhome descriptions with the aim to extract and disambiguate toponyms as a representative …,*,2011,3
Run-time Optimization for Pipelined Systems,R Abdel Kader; Maurice Van Keulen; Peter Boncz; Stefan Manegold,Abstract Traditional optimizers fail to pick good execution plans; when faced withincreasingly complex queries and large data sets. This failure is even more acute in thecontext of XQuery; due to the structured nature of the XML language. To overcome thevulnerabilities of traditional optimizers; we have previously proposed ROX; a Run-timeOptimizer for XQueries; which interleaves optimization and execution of full tables. ROX hasproved to be robust; even in the presence of strong correlations; but it has one limitation: ituses full materialization of intermediate results making it unsuitable for pipelined systems.Therefore; this paper proposes ROX-sampled; a variant of ROX; which executes small datasamples; thus generating smaller intermediates. We conduct extensive experiments whichproved that ROX-sampled is comparable to ROX in performance; and that it is still robust …,*,2010,3
Onzekere databases,Maurice van Keulen,Maurice van Keulen 'fact of life'. De uitdaging is om informatiesystemen soepeler metonvolkomenheden om te kunnen laten gaan. Dat is niet alleen een kwestie van controles endata cleaning; maar ook van robuustheid en flexibiliteit. Alle bovengenoemde soortenonvolkomenheden zijn te zien als onzekerheid: we weten simpelweg op een bepaaldmoment niet hoe het werkelijk zit. Door eerlijk en expliciet met die onzekerheid om te gaan;kan een informatiesysteem zich daaraan aanpassen en goed genoeg functioneren ondanksdat niet alle gegevens precies en correct beschikbaar zijn. Onzekere databases maken hetmogelijk om eerlijk en expliciet met onzekerheid om te gaan; zonder dat het de complexiteitvan de informatiesystemen significant verhoogt. Kort gezegd; een onzekere database maakthet mogelijk om niet alleen de data maar ook de onzekerheid over die data expliciet op te …,DB/M: database magazine,2010,3
Qualitative effects of knowledge rules in probabilistic data integration,Maurice van Keulen; Ander de Keijzer,Abstract One of the problems in data integration is data overlap: the fact that different datasources have data on the same real world entities. Much development time in dataintegration projects is devoted to entity resolution. Often advanced similarity measurementtechniques are used to remove semantic duplicates from the integration result or solve othersemantic conflicts; but it proofs impossible to get rid of all semantic problems in dataintegration. An often-used rule of thumb states that about 90% of the development effort isdevoted to solving the remaining 10% hard cases. In an attempt to significantly decreasehuman effort at data integration time; we have proposed an approach that stores anyremaining semantic uncertainty and conflicts in a probabilistic database enabling it toalready be meaningfully used. The main development effort in our approach is devoted to …,*,2008,3
Information Integration-the process of integration; evolution and versioning,Ander Keijzer; Maurice Keulen,Abstract At present; many information sources are available wherever you are. Most of thetime; the information needed is spread across several of those information sources.Gathering this information is a tedious and time consuming job. Automating this processwould assist the user in its task. Integration of the information sources provides a globalinformation source with all information needed present. All of these information sources alsochange over time. With each change of the information source; the schema of this sourcecan be changed as well. The data contained in the information source; however; cannot bechanged every time; due to the huge amount of data that would have to be converted inorder to conform to the most recent schema. In this report we describe the current methods toinformation integration; evolution and versioning. We distinguish between integration of …,*,2005,3
A probabilistic database extension,Ander Keijzer; Maurice Keulen,Abstract Data exchange between embedded systems and other small or large computingdevices increases. Since data in different data sources may refer to the same real worldobjects; data cannot simply be merged. Furthermore; in many situations; conflicts in dataabout the same real world objects need to be resolved without interference from a user. Inthis report; we report on an attempt to make a RDBMS probabilistic; ie; data in a relationrepresents all possible views on the real world; in order to achieve unattended dataintegration. We define a probabilistic relational data model and review standard SQL queryprimitives in the light of probabilistic data. It appears that thinking in terms of 'possible worlds'is powerful in determining the proper semantics of these query primitives. 1,*,2004,3
An architecture and methodology for the design and development of Technical Information Systems,R Capobianchi; M Mautref; Maurice van Keulen; Herman Balsters,Abstract In order to meet demands in the context of Technical Information Systems (TIS)pertaining to reliability; extensibility; maintainability; etc.; we have developed an architecturalframework with accompanying methodological guidelines for designing such systems. Withthe framework; we aim at complex multiapplication information systems using a repository toshare data among applications. The framework proposes to keep a strict separationbetween Man-Machine-Interface and Model data; and provides design and implementationsupport to do this effectively. The framework and methodological guidelines have beendeveloped in the context of the ESPRIT project IMPRESS. The project also provided for“testing grounds” in the form of a TIS for the Spanish Electricity company Iberdrola.,International Symposium on Methodologies for Intelligent Systems,1996,3
A framework for representation; validation and implementation of database application semantics,Maurice van Keulen; Jacek Skowronek; Peter MG Apers; Herman Balsters; Henk M Blanken; RA de By; Jan Flokstra,Abstract New application domains in data-processing environments pose new requirementson the methodologies; techniques and tools used to design them. The applications'semantics should be fully represented at an increasingly high level; and the representationshould be subject to rigorous validation and verification. We present a semanticrepresentation framework (including the language; methods and tools) for design of data-processing applications. The new features of the framework include a small number ofprecisely defined domain-independent concepts; high-level possibilities for describingbehavioural semantics (methods and constraints) and the validation and verification toolsincluded in the framework. We present examples of the use of the framework; including theuse of its tools.,*,1995,3
Towards complete coverage in focused web harvesting,Mohammadreza Khelghati; Djoerd Hiemstra; Maurice van Keulen,Abstract With the goal of harvesting all information about a given entity; in this paper; we tryto harvest all matching documents for a given query submitted on a search engine. Theobjective is to retrieve all information about for instance" Michael Jackson";" Islamic State";or" FC Barcelona" from indexed data in search engines; or hidden data behind web forms;using a minimum number of queries. Policies of web search engines usually do not allowaccessing all of the matching query search results for a given query. They limit the number ofreturned documents and the number of user requests. These limitations are also applied indeep web sources; for instance in social networks like Twitter. In this work; we propose anew approach which automatically collects information related to a given query from asearch engine; given the search engine's limitations. The approach minimizes the …,Proceedings of the 17th International Conference on Information Integration and Web-based Applications & Services,2015,2
Spatiotemporal Behavior Profiling: A Treasure Hunt Case Study,Victor de Graaff; Dieter Pfoser; Maurice van Keulen; Rolf A De By,Abstract Trajectories have been providing us with a wealth of derived information such astraffic conditions and road network updates. This work focuses on deriving user profilesthrough spatiotemporal analysis of trajectory data to provide insight into the quality ofinformation provided by users. The presented behavior profiling method assesses userparticipation characteristics in a treasure-hunt type event. Consisting of an analysis and aprofiling phase; analysis involves a timeline and a stay-point analysis; as well as a semantictrajectory inspection relating actual and expected paths. The analysis results are thengrouped around profiles that can be used to estimate the user performance in the activity.The proposed profiling method is evaluated by means of a student orientation treasure-huntactivity at the University of Twente; The Netherlands. The profiling method is used to …,*,2015,2
Hadoop for EEG Storage and Processing: A Feasibility Study,Ghita Berrada; Maurice van Keulen; Mena B Habib,Abstract Lots of heterogeneous complex data are collected for diagnosis purposes. Suchdata should be shared between all caregivers and; often at least partly automaticallyprocessed; due to its complexity; for its full potential to be harnessed. This paper is afeasibility study that assesses the potential of Hadoop as a medical data storage andprocessing platform using EEGs as example of medical data.,International Conference on Brain Informatics and Health,2014,2
Designing A General Deep Web Access Approach Based On A Newly Introduced Factor; Harvestability Factor (HF),Mohammadreza Khelghati; Maurice Keulen; Djoerd Hiemstra,Abstract The growing need of accessing more and more information draws attentions tohuge amount of data hidden behind web forms defined as deep web. To make this dataaccessible; harvesters have a crucial role. Targeting different domains and websitesenhances the need to have a general-purpose harvester which can be applied to differentsettings and situations. To develop such a harvester; a number of issues should beconsidered. Among these issues; business domain features; targeted websites' features;and the harvesting goals are the most influential ones. To consider all these elements in onebig picture; a new concept; called harvestability factor (HF); is introduced in this paper. TheHF is defined as an attribute of a website (HF_w) or a harvester (HF_h) representing theextent to which the website can be harvested or the harvester can harvest. The …,*,2014,2
Digital-forensics based pattern recognition for discovering identities in electronic evidence,Hans Henseler; Jop Hofsté; Maurice van Keulen,With the pervasiveness of computers and mobile devices; digital forensics becomes moreimportant in law enforcement. Detectives increasingly depend on the scarce support ofdigital specialists which impedes efficiency of criminal investigations. This paper proposesand algorithm to extract; merge and rank identities that are encountered in the electronicevidence during processing. Two experiments are described demonstrating that ourapproach can assist with the identification of frequently occurring identities so thatinvestigators can prioritize the investigation of evidence units accordingly.,*,2013,2
08421 Abstracts Collection--Uncertainty Management in Information Systems,Christoph Koch; Birgitta König-Ries; Volker Markl; Maurice van Keulen,Abstract From October 12 to 17; 2008 the Dagstuhl Seminar 08421'Uncertainty Managementin Information Systems''was held in Schloss Dagstuhl~--~ Leibniz Center for Informatics. Theabstracts of the plenary and session talks given during the seminar as well as those of theshown demos are put together in this paper.,Dagstuhl Seminar Proceedings,2009,2
Operating Systems,AL Schoute; M van Keulen,Schoute; AL (2007). Operating Systems. In TMA Bemelmans; M. van Keulen; RJ Kusters; &M. Looijen (Eds.); ICT-Zakboek (pp. 761-770). (PBNA Polyzakboekjes; No. Chapter VI.1.7). DenHaag: Reed Business Information … ICT-Zakboek. ed. / TMA Bemelmans; Maurice vanKeulen; RJ Kusters; M. Looijen. Den Haag : Reed Business Information; 2007. p. 761-770 (PBNAPolyzakboekjes; No. Chapter VI.1.7) … Schoute; AL 2007; Operating Systems. in TMABemelmans; M van Keulen; RJ Kusters & M Looijen (eds); ICT-Zakboek. PBNAPolyzakboekjes; no. Chapter VI.1.7; Reed Business Information; Den Haag; pp. 761-770 …ICT-Zakboek. ed. / TMA Bemelmans; Maurice van Keulen; RJ Kusters; M. Looijen. Den Haag: Reed Business Information; 2007. p. 761-770 (PBNA Polyzakboekjes; No. Chapter VI.1.7) …Schoute AL. Operating Systems. In Bemelmans TMA; van Keulen M; Kusters RJ; Looijen …,*,2007,2
Effectiveness Bounds for Non-Exhaustive Schema Matching Systems,Marko Smiljanic; Maurice van Keulen; Willem Jonker,Semantic validation of the effectiveness of a schema matching system is traditionallyperformed by comparing system-generated mappings with those of human evaluators. Thehuman effort required for validation quickly becomes huge in large scale environments. Theperformance of a matching system; however; is not solely determined by the quality of themappings; but also by the efficiency with which it can produce them. Improving efficiencyquickly leads to a trade-off between efficiency and effectiveness. Establishing or obtaining alarge test collection for measuring this trade-off is often a severe obstacle. In this paper; wepresent a technique for determining lower and upper bounds for effectiveness measures fora certain class of schema matching system improvements in order to lower the requiredvalidation effort. Effectiveness bounds for a matching system improvement are solely …,Data Engineering Workshops; 2006. Proceedings. 22nd International Conference on,2006,2
The Dodo query flattening system,Joeri Ruth; Maarten Fokkinga; Maurice Keulen,*,*,2004,2
CIRQUID: complex information retrieval queries in a database,Djoerd Hiemstra; Arjen P Vries; Henk Ernst Blok; Maurice Keulen; Willem Jonker; Martin L Kersten,Abstract The CIRQUID project plans to design and build a DBMS that seemlessly integratesrelevance-oriented querying of semi-structured data (XML) with traditional querying of thisdata. The project is funded by the Netherlands Organisation of Scientific Research (NWOproject number 612.061. 210).,*,2003,2
Deep Physiological Arousal Detection in a Driving Simulator using Wearable Sensors,Aaqib Saeed; Stojan Trajanovski; Maurice van Keulen; Jan van Erp,Abstract—Driving is an activity that requires considerable alertness. Insufficient attention;imperfect perception; inadequate information processing; and sub-optimal arousal arepossible causes of poor human performance. Understanding of these causes and theimplementation of effective remedies is of key importance to increase traffic safety andimprove driver's wellbeing. For this purpose; we used deep learning algorithms to detectarousal level; namely; under-aroused; normal and overaroused for professional truck driversin a simulated environment. The physiological signals are collected from 11 participants bywrist wearable devices. We presented a cost effective groundtruth generation scheme forarousal based on a subjective measure of sleepiness and score of stress stimuli. On thisdataset; we evaluated a range of deep neural network models for representation learning …,IEEE International Conference on Data Miningworkshop: Data Mining in Biomedical Informatics and Healthcare (DMBIH),2017,1
Efficient web harvesting strategies for monitoring deep web content,Mohammad Khelghati; Djoerd Hiemstra; Maurice van Keulen,Abstract Web content changes rapidly [18]. In Focused Web Harvesting [17] which aim it is toachieve a complete harvest for a given topic; this dynamic nature of the web createsproblems for users who need to access a set of all the relevant web data to their topics ofinterest. Whether you are a fan following your favorite idol or a journalist investigating atopic; you may need not only to access all the relevant information but also the recentchanges and updates. General search engines like Google apply several techniques toenhance the freshness of their crawled data. However; in focused web harvesting; we lackan efficient approach that detects changes for a given topic over time. In this paper; we focuson techniques that can keep the relevant content to a given query up-to-date. To do so; wetest four different approaches to efficiently harvest all the changed documents matching a …,Proceedings of the 18th International Conference on Information Integration and Web-based Applications and Services,2016,1
Lazy Evaluation for Concurrent OLTP and Bulk Transactions,Lesley Wevers; Marieke Huisman; Maurice van Keulen,Abstract Existing concurrency control systems cannot execute transactions with overlappingupdates concurrently. This is especially problematic for bulk updates; which usually overlapwith all concurrent transactions. To solve this; we have developed a concurrency controlmechanism based on lazy evaluation; which moves evaluation of operations from the writerto the reader. This allows readers to prioritize evaluation of those operations in which theyare interested; without loss of atomicity of transactions. To handle bulk operations; wedynamically split large transactions into transactions on smaller parts of the data. In thispaper we present an abstract lazy index structure for lazy transactions; and show howtransactions can be encoded to effectively use this data structure. Moreover; we discussevaluation strategies for lazy transactions; where trade-offs can be made between latency …,Proceedings of the 20th International Database Engineering & Applications Symposium,2016,1
JudgeD: a Probabilistic Datalog with Dependencies,B Wanders; M van Keulen; Jan Flokstra,Abstract We present JudgeD; a probabilistic datalog. A JudgeD program defines adistribution over a set of traditional datalog programs by attaching logical sentences toclauses to implicitly specify traditional data programs. Through the logical sentences;JudgeD provides a novel method for the expression of complex dependencies between bothrules and facts. JudgeD is implemented as a proof-of-concept in the language Python. Theimplementation allows connection to external data sources; and features both a Monte Carloprobability approximation as well as an exact solver supported by BDDs. Several directionsfor future work are discussed and the implementation is released under the MIT license.,*,2016,1
Analysis of the blocking behaviour of schema transformations in relational database systems,Lesley Wevers; Matthijs Hofstra; Menno Tammens; Marieke Huisman; Maurice van Keulen,Abstract In earlier work we have extended the TPC-C benchmark with basic and complexschema transformations. This paper uses this benchmark to investigate the blockingbehaviour of online schema transformations in PostgreSQL; MySQL and Oracle 11g. Firstwe discuss experiments using the data definition language of the DBMSs; which show thatall complex operations are blocking; while we have mixed results for basic transformations.Second; we look at a technique for online schema transformations by Ronström; based ontriggers. Our experiments show that pt-online-schema-change for MySQL andDBMS_REDEFINITION for Oracle can perform basic transformations without blocking;however; support for complex transformations is missing. To conclude; we provide a solutionoutline for complex non-blocking transformations.,East European Conference on Advances in Databases and Information Systems,2015,1
Harvesting all matching information to a given query from a deep website,Mohammadreza Khelghati; Djoerd Hiemstra; Maurice van Keulen,Abstract In this paper; the goal is harvesting all documents matching a given (entity) queryfrom a deep web source. The objective is to retrieve all information about for instance"Denzel Washington";" Iran Nuclear Deal"; or" FC Barcelona" from data hidden behind webforms. Policies of web search engines usually do not allow accessing all of the matchingquery search results for a given query. They limit the number of returned documents and thenumber of user requests. In this work; we propose a new approach which automaticallycollects information related to a given query from a search engine; given the search engine'slimitations. The approach minimizes the number of queries that need to be sent by applyinginformation from a large external corpus. The new approach outperforms existingapproaches when tested on Google; measuring the total number of unique documents …,*,2015,1
A benchmark for online non-blocking schema transformations,Lesley Wevers; Matthijs Hofstra; Menno Tammens; Marieke Huisman; Maurice van Keulen,Abstract This paper presents a benchmark for measuring the blocking behavior of schematransformations in relational database systems. As a basis for our benchmark; we havedeveloped criteria for the functionality and performance of schema transformationmechanisms based on the characteristics of state of the art approaches. To addresslimitations of existing approaches; we assert that schema transformations must becomposable while satisfying the ACID guarantees like regular database transactions.Additionally; we have identified important classes of basic and complex relational schematransformations that a schema transformation mechanism should be able to perform. Basedon these transformations and our criteria; we have developed a benchmark that extends thestandard TPC-C benchmark with schema transformations; which can be used to analyze …,*,2015,1
Incremental Data Uncertainty Handling Using Evidence Combination: A Case Study on Maritime Data Reasoning,Mena B Habib; Brend Wanders; Jan Flokstra; Maurice van Keulen,Semantic incompatibility is a conflict that occurs in the meanings of data. In this paper; wepropose an approach for data cleaning by resolving semantic incompatibility. Our approachapplies a dynamic and incremental enhancement of data quality. It checks thecoherency/conflict of the newly recorded facts/relations against the existing ones. It reasonsover the existing information and comes up with new discovered facts/relations. We choosemaritime data cleaning as a validation scenario.,*,2015,1
Towards Online and Transactional Relational Schema Transformations,Lesley Wevers; Matthijs Hofstra; Menno Tammens; Marieke Huisman; Maurice Keulen,Abstract In this paper; we want to draw the attention of the database community to theproblem of online schema changes: changing the schema of a database without blockingconcurrent transactions. We have identified important classes of relational schematransformations that we want to perform online; and we have identified general requirementsfor the mechanisms that execute these transformations. Using these requirements; we havedeveloped an experiment based on the standard TPC-C benchmark to assess the behaviourof existing systems. We look at PostgreSQL; which does not support online schemachanges; MySQL; which supports basic online schema changes; and pt-online-schema-change; which is a tool for MySQL that uses triggers to implement online schema changes.We found that none of the existing systems fulfill our requirements. In particular; existing …,*,2014,1
Uncertainty Handling in Named Entity Extraction and Disambiguation for Informal Text,Maurice van Keulen; Mena B Habib,Abstract Social media content represents a large portion of all textual content appearing onthe Internet. These streams of user generated content (UGC) provide an opportunity andchallenge for media analysts to analyze huge amount of new data and use them to infer andreason with new information. A main challenge of natural language is its ambiguity andvagueness. To automatically resolve ambiguity; the grammatical structure of sentences isused. However; when we move to informal language widely used in social media; thelanguage becomes more ambiguous and thus more challenging for automaticunderstanding. Information Extraction (IE) is the research field that enables the use ofunstructured text in a structured way. Named Entity Extraction (NEE) is a sub task of IE thataims to locate phrases (mentions) in the text that represent names of entities such as …,*,2014,1
Uncertainty Management in Information Systems–Executive Summary by the Organizers,Christoph Koch; Birgitta König-Ries; Volker Markl; Maurice van Keulen,Computer science has long pretended that information systems are perfect mirror images ofa perfect world. Database management systems; eg; work under the assumption that thedata stored represent a correct subset of the real world. Of course; this idealized assumptionis rarely true. Information systems contain–wrong information caused; eg; by data entryerrors: This is a common problem for instance in genomic databases–imprecise or falselyprecise information; eg; a measuring device will provide information with a certain precisiononly. Typically; information systems store the measured date; but do not store informationabout the conditions under which this data is true and the precision achieved.–incompleteinformation. A certain piece of information may not be available to the information system.–inconsistent information. Different information systems may contain contradictory …,Uncertainty Management in Information Systems: Dagstuhl Seminar 08421,2009,1
Report on the first VLDB workshop on Management of Uncertain Data (MUD),Ander de Keijzer; Maurice van Keulen; Alex Dekhtyar,Abstract On Monday September 24 th; we organized the first international VLDB workshopon Management of Uncertain Data [dKvKD07]. The idea of this workshop arose a yearearlier at the Twente Data Management Workshop on Uncertainty in Databases [dKvK06].The TDM is a bi-annual workshop organized by the Database group of the University ofTwente; for which each time a different topic is chosen. The participants of TDM 2006 wereenthusiastic about the topic" Uncertainty in Databases" and strongly expressed the wish fora follow-up co-located with an international conference. To fulfill this wish; we organized theMUD-workshop at VLDB.,ACM SIGMOD Record,2007,1
06472 Abstracts Collection--XQuery Implementation Paradigms,Peter A Boncz; Torsten Grust; Jérôme Siméon; Maurice van Keulen,Abstract From 19.11. 2006 to 22.11. 2006; the Dagstuhl Seminar 06472``XQueryImplementation Paradigms''was held in the International Conference and Research Center(IBFI); Schloss Dagstuhl. During the seminar; several participants presented their currentresearch; and ongoing work and open problems were discussed. Abstracts of thepresentations given during the seminar as well as abstracts of seminar results and ideas areput together in this paper. The first section describes the seminar topics and goals ingeneral. Links to extended abstracts or full papers are provided; if available.,Dagstuhl Seminar Proceedings,2007,1
ICT-zakboek,Theodorus Maria Aloysius Bemelmans; M van Keulen; RJ Kusters; M Looijen,Bemelmans; TMA; van Keulen; M.; Kusters; RJ; & Looijen; M. (Eds.) (2007). ICT-Zakboek. (3edruk ed.) (PBNA Polyzakboekjes; No. 2). Den Haag: Reed Business Information …Bemelmans; TMA (Editor); van Keulen; Maurice (Editor); Kusters; RJ (Editor); Looijen; M.(Editor) / ICT-Zakboek … 3e druk ed. Den Haag : Reed Business Information; 2007. 926 p.(PBNA Polyzakboekjes; No. 2) … Bemelmans; TMA; van Keulen; M; Kusters; RJ & Looijen;M (eds) 2007; ICT-Zakboek. PBNA Polyzakboekjes; no. 2; 3e druk edn; Reed BusinessInformation; Den Haag … ICT-Zakboek. / Bemelmans; TMA (Editor); van Keulen; Maurice(Editor); Kusters; RJ (Editor); Looijen; M. (Editor) … 3e druk ed. Den Haag : Reed BusinessInformation; 2007. 926 p. (PBNA Polyzakboekjes; No. 2) … Bemelmans TMA; (ed.); van KeulenM; (ed.); Kusters RJ; (ed.); Looijen M; (ed.). ICT-Zakboek. 3e druk ed. Den Haag: Reed …,*,2007,1
A Syntax-Directed Editor for TM,Jochem Vonk; M van Keulen; J Flokstra; H Balsters,Abstract This thesis describes the design and implementation of a support tool that will beintegrated into the Database Design Tool (DDT). This is an already existing toolset thatsupports the use of the object-oriented database speci cation language TM. The support tooldescribed in this thesis is a syntax-directed editor (SDE). The purpose of the SDE is tosupport a user; an unexperienced user in particular; in specifying expressions. The SDEpresents a list of syntax constructs that are allowed at speci c places; so the user does notneed to know the complete syntax of the language TM. This also prevents the creation ofsyntactically incorrect expressions. The choice to have a generic structure for theimplementation makes the tool language-independent. Although the SDE is implemented touse the language TM it can therefore be easily modi ed to be used with other languages. ii,Department of Computer Science,1995,1
Methodological guidelines for IMPRESS,F Ardorino; RA de By; R Capobianchi; Maurice van Keulen; M Mautref,KNAW Narcis. Back to search results. Publication MethodologicalGuidelines for IMPRESS (1995). Pagina-navigatie: Main …,*,1995,1
IMPRESS Database Design Tool-a high-level design toolset based on formal theory,Jan Flokstra; Maurice van Keulen; J Skowronek,Abstract This document presents the Database Design Tool prototype; developed at theUniversity of Twente. The Tool is used to specify databases in a graphical way; and is basedon a formal specification language TM (described in ECOOP'93 article [2]). TM and theDatabase Design Tool support object-oriented concepts such as classes; object; methodsand inheritance. The main point we want to state is that software engineering based on asound formal basis does not have to sacrifice ease-of-use and flexibility; we state that; on thecontrary; it is this formal basis which proves to be beneficial and profitable for the user-enabling faster and error-free software development.,European Conference on Object-Oriented Programming; ECOOP 1994,1994,1
Data-Driven Process Discovery and Analysis SIMPDA 2017,Paolo Ceravolo; Maurice Van Keulen; Kilian Stoffel,Page 1. 7thInternational Symposium on Data-Driven Process Discovery and Analysis SIMPDA2017 December 6-8; 2017 Neuchatel; Switzerland Editors: Paolo Ceravolo Maurice Van KeulenKilian Stoffel Page 2. II Foreword With the increasing automation of business processes; growingamounts of process data become available. This opens new research opportunities for businessprocess data analysis; mining; and modeling. The aim of the IFIP 2.6 - International Symposiumon Data- Driven Process Discovery and Analysis is to offer a forum where researchers from differentcommunities and the industry can share their insight in this hot new field. Submissions aim atcovering theoretical issues related to process representation; discovery and analysis; or providepractical and operational experiences in process discovery and analysis. Language for papersand presentations is English. In this sixth edition; 22 papers …,*,2017,*
Exploiting Natural Language Processing for Improving Health Processes,Maurice Van Keulen; Jeroen Geerdink; Gerard CM Linssen; Riemer HJA Slart; Onno Vijlbrief,*,7th International Symposium on Data-Driven Process Discovery and Analysis (SIMPDA 2017),2017,*
Truth assessment of objective facts extracted from tweets: A case study on world cup 2014 game facts,Bas Janssen; Mena Habib; Maurice Van Keulen,Abstract By understanding the tremendous opportunities to work with social media data andthe acknowledgment of the negative effects social media messages can have; a way ofassessing truth in claims on social media would not only be interesting but also veryvaluable. By making use of this ability; applications using social media data could besupported; or a selection tool in research regarding the spread of false rumors or'fake news'could be build. In this paper; we show that we can determine truth by using a statisticalclassifier supported by an architecture of three preprocessing phases. We base our researchon a dataset of Twitter messages about the FIFA World Cup 2014. We determine the truth ofa tweet by using 7 popular fact types (involving events in the matches in the tournamentsuch as scoring a goal) and we show that we can achieve an F1-score of 0.988 for the …,13th International Conference on Web Information Systems and Technologies; WEBIST 2017,2017,*
Detecting Hacked Twitter Accounts based on Behavioural Change,Meike Nauta; Mena Badieh Habib; Maurice van Keulen,Abstract Social media accounts are valuable for hackers for spreading phishing links;malware and spam. Furthermore; some people deliberately hack an acquaintance todamage his or her image. This paper describes a classification for detecting hacked Twitteraccounts. The model is mainly based on features associated with behavioural change suchas changes in language; source; URLs; retweets; frequency and time. We experiment with aTwitter data set containing tweets of more than 100 Dutch users including 37 who werehacked. The model detects 99% of the malicious tweets which proves that behaviouralchanges can reveal a hack and that anomaly-based features perform better than regularfeatures. Our approach can be used by social media systems such as Twitter toautomatically detect a hack of an account only a short time after the fact allowing the …,13th International Conference on Web Information Systems and Technologies; WEBIST 2017,2017,*
MTCB: A Multi-Tenant Customizable database Benchmark,Wim van der Zijden; Djoerd Hiemstra; Maurice van Keulen,ABSTRACT We argue that there is a need for Multi-Tenant Customizable OLTP systems.Such systems need a Multi-Tenant Customizable Database (MTC-DB) as a backing. Tostimulate the development of such databases; we propose the benchmark MTCB.Benchmarks for OLTP exist and multi-tenant benchmarks exist; but no MTC-DB benchmarkexists that accounts for customizability. We formulate seven requirements for the benchmark:realistic; unambiguous; comparable; correct; scalable; simple and independent. It focuseson performance aspects and produces nine metrics: Aulbach compliance; size on disk;tenants created; types created; attributes created; transaction data type instances createdper minute; transaction data type instances loaded by ID per minute; conjunctive searchesper minute and disjunctive searches per minute. We present a specification and an …,*,2017,*
Real-time measures of social interaction as predictors for team effectiveness,Stijn de Laat; Maaike Dorine Endedijk; Elze Gooitzen Ufkes; Maurice van Keulen; Reinout Everhard de Vries,2017. Paper presented at WAOP conference 2017; Nijmegen; Netherlands … Real-time measuresof social interaction as predictors for team effectiveness. / de Laat; Stijn ; Endedijk; MaaikeDorine; Ufkes; Elze Gooitzen; van Keulen; Maurice ; de Vries; Reinout Everhard … 2017. Paperpresented at WAOP conference 2017; Nijmegen; Netherlands … Powered by Pure; Scopus& Elsevier Fingerprint Engine™ © 2017 Elsevier BV.,WAOP conference 2017,2017,*
ISIS in the Eyes of the Dutch,Bas Hendrikse; Mena B Habib; Maurice van Keulen,Abstract The presence of militant group Islamic State of Iraq and Syria (ISIS) is growing.Terrorist attacks in Europe and an incoming stream of refugees in the south of the continentare some of the reasons Europe is getting socially involved in the Middle-Eastern war. Itmight seem that the Netherlands could become a target of the organization too. But areDutch citizens concerned about this? In this paper; we describe the reaction of the Dutch onISIS by analyzing what they say on Twitter about the organization. With the use of textclassification; topic modeling and visualization tools; we were able to retrieve Tweets aboutISIS and create a network graph displaying the ten main topics about ISIS which Dutchpeople tweeted about; in addition to a word cloud; displaying the words which were mostused in the Tweets. The visualizations are used to analyze what topics people discussed …,*,2017,*
Evidence combination for incremental decision-making processes,Ghita Berrada; Maurice van Keulen; Ander de Keijzer,Abstract. The establishment of a medical diagnosis is an incremental process highly fraughtwith uncertainty. At each step of this painstaking process; it may be beneficial to be able toquantify the uncertainty linked to the diagnosis and steadily update the uncertaintyestimation using available sources of information; for example user feedback; as theybecome available. Using the example of medical data in general and EEG data in particular;we show what types of evidence can affect discrete variables such as a medical diagnosisand build a simple and computationally efficient evidence combination model based on theDempster-Shafer theory.,CTIT technical report,2016,*
Handling Uncertainty in Relation Extraction: A Case Study on Tennis Tournament Results Extraction from Tweets,Jochem GJ Verburg; Mena B Habib; Maurice van Keulen,Abstract Relation extraction involves different types of uncertainty due to the imperfection ofthe extraction tools and the inherent ambiguity of unstructured text. In this paper; we discussseveral ways of handling uncertainties in relation extraction from social media. Our studycase is to extract tennis games' results for two Grand Slam tennis tournaments from tweets.Analysis has been done to find to what extent it is useful to use semantic web; domainknowledge; facts repetition; and authors' trustworthiness to improve the certainty of theextracted relations.,Proceedings of the 8th International Conference on Knowledge Capture,2015,*
Dealing with poor data quality of OSINT data in fraud risk analysis,Maurice van Keulen,Abstract Governmental organizations responsible for keeping certain types of fraud undercontrol; often use data-driven methods for both immediate detection of fraud; or for fraud riskanalysis aimed at more effectively targeting inspections. A blind spot in such methods; is thatthe source data often represents a'paper reality'. Fraudsters will attempt to disguisethemselves in the data they supply painting a world in which they do nothing wrong. Thisblind spot can be counteracted by enriching the data with traces and indicators frommore'real-world'sources such as social media and internet. One of the crucial datamanagement problems in accomplishing this enrichment is how to capture and handle dataquality problems. The presentation will start with a real-world example; which is also used asstarting point for a problem generalization in terms of information combination and …,*,2015,*
Stairwalker user manual,Dennis Muller; Jochem Elsinga; Maurice Keulen,Abstract Geographical data are typically visualized using various information layers that aredisplayed over a map. Interactive exploration by zooming and panning actions needs real-time re-calculation. A common operation in calculating with multidimensional data is thecomputation of aggregates. For layers containing aggregated information derived fromvoluminous data sets; such real-time exploration is impossible using standard databasetechnology. Calculations require too much time. The University of Twente has developed“Stairwalker‿: database technology that accurately aggregates data so that they cangeographically be explored in real-time. The technology is a plug-in to common open sourcetechnology. Its core is the pre-aggregate index: a database index that cleverly precalculatesaggregation values such that it can obtain exact aggregation results from voluminous …,*,2015,*
Designing A General Deep Web Harvester by Harvestability Factor,Mohammadreza Khelghati; Djoerd Hiemstra; Maurice van Keulen,Abstract. To make deep web data accessible; harvesters have a crucial role. Targetingdifferent domains and websites enhances the need of a general-purpose harvester whichcan be applied to different settings and situations. To develop such a harvester; a largenumber of issues should be addressed. To have all influential elements in one big picture; anew concept; called harvestability factor (HF); is introduced in this paper. The HF is definedas an attribute of a website (HFW) or a harvester (HFH) representing the extent to which thewebsite can be harvested or the harvester can harvest. The comprising elements of thesefactors are different websites' or harvesters' features. These elements are gathered fromliterature or introduced through the authors' experiments. In addition to enabling designersof evaluating where they products stand from the harvesting perspective; the HF can act …,*,2014,*
Towards Online Relational Schema Transformations,Lesley Wevers; M Hofstra; M Tammens; M Huisman; M van Keulen,Abstract Current relational database systems are ill-equipped for changing the structure ofdata while the database is in use. This is a real problem for systems for which we expect24/7 availability; such as telecommunication; payment; and control systems. As a result;developers tend to avoid making changes because of the downtime consequences. Theurgency to solve this problem is evident by a multitude of tools developed in industry; suchas pt-online-schema-change1 and oak-online-alter-table2. Also; MySQL recently addedlimited support for online schema changes3. Contributions: We want to draw the attention ofthe database community to the problem of online schema changes. We have definedrequirements for online schema change mechanisms; and we have experimentallyinvestigated existing solutions. Our results show that current solutions are unsatisfactory …,*,2014,*
Finding You on the Internet: an approach for finding on-line presences of people for fraud risk analysis,Henry Been; Maurice van Keulen,Abstract Fraud risk analysis on data from formal information sources; being a 'paper reality';suffers from blindness to false information. Moreover; the very act of providing falseinformation is a strong indicator for fraud. The technology presented in this paper providesone step towards the vision of harnessing real-world data from social media and internet forfraud risk analysis. We introduce a novel iterative search; monitor; and match approach forfinding on-line presences of people. A real-world experiment showed that Twitter accountscan be effectively found given only limited name and address data. We also present ananalysis of the ethical considerations surrounding the application of such technology forfraud risk analysis.,16th International Conference on Enterprise Information Systems (ICEIS 2014),2014,*
Sample-based XPath Ranking for Web Information Extraction,Oliver Jundt; Maurice Van Keulen,Abstract Web information extraction typically relies on a wrapper; ie; program code or aconfiguration that specifies how to extract some information from web pages at a specificwebsite. Manually creating and maintaining wrappers is a cumbersome and error-pronetask. It may even be prohibitive as some applications require information extraction frompreviously unseen websites. This paper approaches the problem of automatic on-the-flywrapper creation for websites that provide attribute data for objects in a 'search–searchresult page–detail page'setup. The approach is a wrapper induction approach which uses asmall and easily obtainable set of sample data for ranking XPaths on their suitability forextracting the wanted attribute data. Experiments show that the automatically generated top-ranked XPaths indeed extract the wanted data. Moreover; it appears that 20 to 25 input …,*,2013,*
How much data resides in a web collection: how to estimate size of a web collection,Mohammadreza Khelghati; Djoerd Hiemstra; Maurice Van Keulen,Abstract With increasing amount of data in deep web sources (hidden from general searchengines behind web forms); accessing this data has gained more attention. In the algorithmsapplied for this purpose; it is the knowledge of a data source size that enables the algorithmsto make accurate decisions in stopping crawling or sampling processes which can be socostly in some cases [4]. The tendency to know the sizes of data sources is increased by thecompetition among businesses on the Web in which the data coverage is critical. In thecontext of quality assessment of search engines [2]; search engine selection in the federatedsearch engines; and in the resource/collection selection in the distributed search field [6];this information is also helpful. In addition; it can give an insight over some useful statisticsfor public sectors like governments. In any of these mentioned scenarios; in case of facing …,*,2013,*
//Rondje Zilverling: COMMIT/TimeTrails,Maurice van Keulen; Victor Graaff; Zhemin Zhu; Andreas de By; Rolf; Wombacher; Jan Flokstra,Abstract Het TimeTrails-project3 gaat over data mining in grote hoeveelheden gegevensover gebeurtenissen in ruimte en tijd; dwz met coördinaten en time-stamps. Dergelijkegegevens worden doorgaans vergaard door mensen; sensoren en wetenschappelijkeobservaties. Gegevensanalyse richt zich vaak op de vier W's: Wie; Wat; Waar en Wanneer.Een belangrijke kwestie is het kunnen behappen van de grote hoeveelheden gegevens;dwz" big data". Vanuit de UT werken we; dwz de groepen EWI/DB en ITC/GIP; aan tweeapplicaties:* Het in kaart brengen van de mening van het publiek bij groteinfrastructuurproject zoals de aanleg van een nieuw stuk snelweg. Dit doen we met Twitter-analyse en data-visualisatie.• Het vinden van goede vakantiebestemmingen. Hierbij spelenSocial media; web harvesting en analyse van GPS-traces een rol.,I/O Vivat,2013,*
Semantic Enrichment of GPS Trajectories,Victor de Graaff; Maurice van Keulen; RA de By,Abstract Semantic annotation of GPS trajectories helps us to recognize the interests of thecreator of the GPS trajectories. Automating this trajectory annotation circumvents therequirement of additional user input. To annotate the GPS traces automatically; two types ofautomated input are required: 1) a collection of possible annotations; and 2) a collection ofGPS trajectories to annotate. The first type of input can be a set of points of interest (POIs);activities; weather types; etc. This collection is to be provided by an application developer;and can originate from the web; an external knowledge base; or an existing database; forexample. The type of annotation that we are interested in; is annotation with visitedlocations; in order to create a user profile at a later stage. We have collected POIs byscraping the web; using a self-configuring data harvester. This harvester is based on …,*,2012,*
Named Entity Extraction and Disambiguation from an Uncertainty Perspective,Mena Badieh Habib; Maurice van Keulen,Abstract Named entity extraction and disambiguation have received much attention in recentyears. Typical fields addressing these topics are information retrieval; natural languageprocessing; and semantic web. This work addresses two problems with named entityextraction and disambiguation. First; almost no existing works examine the extraction anddisambiguation interdependency. Second; existing disambiguation techniques mostly takeas input extracted named entities without considering the uncertainty and imperfection of theextraction process. It is the aim of this work to investigate both avenues and to show thatexplicit handling of the uncertainty of annotation has much potential for making bothextraction and disambiguation more robust. We conducted experiments with a set of holidayhome descriptions with the aim to extract and disambiguate toponyms as a representative …,*,2011,*
Integration of Biological Sources: Exploring the Case of Protein Homology,Tjeerd W Boerman; Maurice Van Keulen; PE van der Vet; Edouard I Severing,Abstract Data integration is a key issue in the domain of bioin-formatics; which deals withhuge amounts of heteroge-neous biological data that grows and changes rapidly. Thispaper serves as an introduction in the field of bioinformatics and the biological concepts itdeals with; and an exploration of the integration problems a bioinformatics scientist faces.We examine ProGMap; an integrated protein homology system used by bioin-formaticsscientists at Wageningen University; and several use cases related to protein homology. Akey issue we identify is the huge manual effort required to unify source databases into asingle resource. Un-certain databases are able to contain several possi-ble worlds; and ithas been proposed that they can be used to significantly reduce initial integration efforts. Wepropose several directions for future work where uncertain databases can be applied to …,*,2011,*
Proceedings of the Fifth International VLDB Workshop on Management of Uncertain Data (MUD),Ander Keijzer; Maurice van Keulen,*,*,2011,*
The Fourth International VLDB Workshop on Management of Uncertain Data,A de Keijzer; M van Keulen,*,*,2010,*
Working Group: Classification; Representation and Modeling.,S Das; C Koch; B König-Ries; Ander de Keijzer; V Markl; A Deshpande; M van Keulen; PJ Haas; IF Ilyas; T Neumann; D Olteanu; M Theobald; V Vassalos,KNAW Narcis. Back to search results. Publication Working Group: Classification;Representation and Modeling. (2009). Pagina-navigatie: Main …,*,2009,*
Proceedings of the Third International Workshop on Management of Uncertain Data (MUD2009),Ander de Keijzer; Maurice van Keulen,Enschede; The Netherlands : Centre for Telematics and Information Technology (CTIT);2009. 77 p. (CTIT Workshop Proceedings Series; No. WP-CTIT-09-14) … Proceedings of theThird International Workshop on Management of Uncertain Data (MUD2009). / de Keijzer; Ander(Editor); van Keulen; Maurice (Editor) … Enschede; The Netherlands : Centre for Telematicsand Information Technology (CTIT); 2009. 77 p. (CTIT Workshop Proceedings Series; No.WP-CTIT-09-14) … Powered by Pure; Scopus & Elsevier Fingerprint Engine™ © 2017 ElsevierBV.,*,2009,*
IMPrECISE: Good-is-good-enough Data Integration,C Koch; B König-Ries; V Markl; Maurice van Keulen,The IMPrECISE system is a probabilistic XML database system which supports near-automatic integration of XML documents. What is required of the user is to configure thesystem with a few simple knowledge rules allowing the system to sufficiently eliminatenonsense possibilities. We demonstrate the integration process under conditions withvarying degrees of confusion and different sets of rules. Even when an integrated documentstill contains much uncertainty; it can be queried effectively. The system produces asequence of possible result elements ranked by likelihood. User feedback on query resultsfurther reduces uncertainty which in a sense continues the semantic integration processincrementally. We demonstrate querying on integrated documents and measure answerquality with adapted precision and recall measures. The user feedback mechanism has …,*,2009,*
Probabilistic Data Integration,C Koch; B König-Ries; V Markl; M van Keulen,In data integration efforts such as in portal development; much development time is devotedto entity resolution. Often advanced similarity measurement techniques are used to removesemantic duplicates or solve other semantic conflicts. It proofs impossible; however; toautomatically get rid of all semantic problems. An often-used rule of thumb states that about90% of the development effort is devoted to semi-automatically resolving the remaining 10%hard cases. In an attempt to significantly decrease human effort at data integration time; wehave proposed an approach that strives for a'good enough'initial integration which storesany remaining semantic uncertainty and conflicts in a probabilistic XML database. Theremaining cases are to be resolved during use with user feedback. We conducted extensiveexperiments on the effects and sensitivity of rule denition; threshold tuning; and user …,*,2009,*
08421 Working Group: Imprecision; Diversity and Uncertainty: Disentangling Threads in Uncertainty Management,Myra Spiliopoulou; HJ Lenz; J Wijsen; M Renz; R Kruse; M Stern,Abstract We report on the results of Workgroup 1 on" Imprecision; Diversity and Uncertainty".We set the scene by elaborating on where uncertainty comes from and what the ground truthis. In real world applications; the data observed may not be as expected: they may violateconstraints; or; more generally; disagree with the anticipated model of the world. This leadsto two orthogonal cases: The data may be erroneous; ie they must be corrected. Or; themodel may outdated and must be adjusted to the data. After elaborating on this fundamentaldistinction; we address the issues of measuring uncertainty and exploiting uncertainty in realapplications. We conclude with a list of challenges that should be addressed when dealingwith uncertainty.,Uncertainty Management in Information Systems: Dagstuhl Seminar 08421,2009,*
Imprecision; Diversity and Uncertainty: Disentangling Threads in Uncertainty Management: 08421 Working Group,M Spiliopoulou; M Keulen; H-J Lenz; J Wijsen; M Renz; R Kruse; M Stern,*,*,2009,*
08421 Working Group: Report of the Probabilistic Databases Benchmarking,Christoph Koch; C Re; D Olteanu; H-J Lenz; M van Keulen; PJ Haas; Jeff Z Pan,Koch; C.; Re; C.; Olteanu; D.; Lenz; HJ; Haas; PJ; & Pan; JZ (2009). 08421 Working Group: Reportof the Probabilistic Databases Benchmarking. In C. Koch; B. König-Ries; V. Markl; & M. van Keulen(Eds.); Proceedings of Dagstuhl Seminar 08421 on Uncertainty Management in Information Systems(pp. -). (Dagstuhl Seminar Proceedings; No. 08421). Dagstuhl; Germany: Schloss Dagstuhl -Leibniz-Zentrum fuer Informatik … Koch; C.; Re; C.; Olteanu; D.; Lenz; HJ; Haas; PJ; Pan; JZ/ 08421 Working Group: Report of the Probabilistic Databases Benchmarking … Proceedingsof Dagstuhl Seminar 08421 on Uncertainty Management in Information Systems. ed. / C.Koch; B. König-Ries; V. Markl; Maurice van Keulen. Dagstuhl; Germany : Schloss Dagstuhl -Leibniz-Zentrum fuer Informatik; 2009. p. - (Dagstuhl Seminar Proceedings; No. 08421) …Koch; C; Re; C; Olteanu; D; Lenz; HJ; Haas; PJ & Pan; JZ 2009; 08421 Working Group …,*,2009,*
Probabilistic Data Integration,M van Keulen,The approach is based on the view that data quality problems (as they occur in anintegration process) can be modeled as uncertainty [1] and this uncertainty is considered animportant result of the integration process [2]. In a sense; data quality problems arisingduring the data integration process are not solved immediately; but explicitly represented inthe resulting integrated data. This data can be stored in a probabilistic database to bequeried directly resulting in possible or approximate answers [3]. A probabilistic database isa specific kind of DBMS that allows storage; querying and manipulation of uncertain data. Itkeeps track of alternatives and dependencies among them. While traditional data integrationmethods more or less explicitly consider uncertainty as a problem; as something to beavoided; probabilistic data integration treats uncertainty as an additional source of …,*,2009,*
Proceedings of the International Workshop on Quality in Databases and Management of Uncertain Data (QDBMUD2008),Ander de Keijzer; Maurice van Keulen; P Missier; X Lin,Abstract The ability to detect and correct errors in the data; and more broadly to developtechniques for data quality assessment; has long been recognized as critical to thefunctionality of a large number of applications; in areas ranging from business managementto data-intensive science. While many of the technical issues associated with data qualityhave been known for quite some time; novel applications still pose original challenges;while advances in data management technology offer ideas for novel approaches. The sixthin a workshop series dedicated specifically to problems of Quality in Databases; QDB'08 is aqualified forum for presenting and discussing novel ideas and solutions related to theproblems of assessing; monitoring; improving; and maintaining the quality of data. Previouseditions of the workshop were co-located with top-level data management conferences …,*,2008,*
Proceedings of the first international VLDB workshop on Management of Uncertain Data,Ander de Keijzer; Maurice van Keulen; Alex Dekhtyar,Enschede : Centre for Telematics and Information Technology (CTIT); 2007. 90 p. (ProceedingsWorkshop Series; No. WP-CTIT-07-08) … Proceedings of the first international VLDB workshopon Management of Uncertain Data. / de Keijzer; Ander (Editor); van Keulen; Maurice (Editor);Dekhtyar; A … Enschede : Centre for Telematics and Information Technology (CTIT); 2007.90 p. (Proceedings Workshop Series; No. WP-CTIT-07-08) … Powered by Pure; Scopus & ElsevierFingerprint Engine™ © 2018 Elsevier BV.,*,2007,*
Gegevensbanken,TMA Bemelmans; M van Keulen; RJ Kusters; M Looijen,*,ICT-Zakboek,2007,*
Computerondersteuning voor groepscommunicatie en samenwerken,Nicolaas Sikkel; TMA Bemelmans; Maurice van Keulen; RJ Kusters; M Looijen,Sikkel; N. (2007). Computerondersteuning voor groepscommunicatie en samenwerken. In TMABemelmans; M. van Keulen; RJ Kusters; & M. Looijen (Eds.); ICT-Zakboek (pp. 341-350). (PBNAPolyzakboekjes; No. Chapter III.12). Den Haag: Reed Business Information … Sikkel; Nicolaas/ Computerondersteuning voor groepscommunicatie en samenwerken … ICT-Zakboek. ed./ TMA Bemelmans; Maurice van Keulen; RJ Kusters; M. Looijen. Den Haag : Reed BusinessInformation; 2007. p. 341-350 (PBNA Polyzakboekjes; No. Chapter III.12) … Sikkel; N 2007;Computerondersteuning voor groepscommunicatie en samenwerken. in TMA Bemelmans; Mvan Keulen; RJ Kusters & M Looijen (eds); ICT-Zakboek. PBNA Polyzakboekjes; no. ChapterIII.12; Reed Business Information; Den Haag; pp. 341-350 … Computerondersteuning voorgroepscommunicatie en samenwerken. / Sikkel; Nicolaas … ICT-Zakboek. ed. / TMA …,PBNA Polyzakboekjes,2007,*
Computerondersteuning voor groepscommunicatie en samenwerken,Nicolaas Sikkel; TMA Bemelmans; Maurice van Keulen; RJ Kusters; M Looijen,Sikkel; N. (2007). Computerondersteuning voor groepscommunicatie en samenwerken. In TMABemelmans; M. van Keulen; RJ Kusters; & M. Looijen (Eds.); ICT-Zakboek (pp. 341-350). (PBNAPolyzakboekjes; No. Chapter III.12). Den Haag: Reed Business Information … Sikkel; Nicolaas/ Computerondersteuning voor groepscommunicatie en samenwerken … ICT-Zakboek. ed./ TMA Bemelmans; Maurice van Keulen; RJ Kusters; M. Looijen. Den Haag : Reed BusinessInformation; 2007. p. 341-350 (PBNA Polyzakboekjes; No. Chapter III.12) … Sikkel; N 2007;Computerondersteuning voor groepscommunicatie en samenwerken. in TMA Bemelmans; Mvan Keulen; RJ Kusters & M Looijen (eds); ICT-Zakboek. PBNA Polyzakboekjes; no. ChapterIII.12; Reed Business Information; Den Haag; pp. 341-350 … Computerondersteuning voorgroepscommunicatie en samenwerken. / Sikkel; Nicolaas … ICT-Zakboek. ed. / TMA …,PBNA Polyzakboekjes,2007,*
Computerondersteuning voor groepscommunicatie en samenwerken,Nicolaas Sikkel; TMA Bemelmans; Maurice van Keulen; RJ Kusters; M Looijen,Sikkel; N. (2007). Computerondersteuning voor groepscommunicatie en samenwerken. In TMABemelmans; M. van Keulen; RJ Kusters; & M. Looijen (Eds.); ICT-Zakboek (pp. 341-350). (PBNAPolyzakboekjes; No. Chapter III.12). Den Haag: Reed Business Information … Sikkel; Nicolaas/ Computerondersteuning voor groepscommunicatie en samenwerken … ICT-Zakboek. ed./ TMA Bemelmans; Maurice van Keulen; RJ Kusters; M. Looijen. Den Haag : Reed BusinessInformation; 2007. p. 341-350 (PBNA Polyzakboekjes; No. Chapter III.12) … Sikkel; N 2007;Computerondersteuning voor groepscommunicatie en samenwerken. in TMA Bemelmans; Mvan Keulen; RJ Kusters & M Looijen (eds); ICT-Zakboek. PBNA Polyzakboekjes; no. ChapterIII.12; Reed Business Information; Den Haag; pp. 341-350 … Computerondersteuning voorgroepscommunicatie en samenwerken. / Sikkel; Nicolaas … ICT-Zakboek. ed. / TMA …,PBNA Polyzakboekjes,2007,*
Ch. VI. 2 Programmatuur,A van Deursen; P Klint,*,*,2007,*
Computerondersteuning voor groepscommunicatie en samenwerken,Maurice Keulen; Klaas Sikkel,*,*,2007,*
Data warehousing en data mining,APJM Siebes; M van Keulen; TMA Bemelmans; RJ Kusters; M Looijen,KNAW Narcis. Back to search results. Publication Data warehousingen data mining (2007). Pagina-navigatie: Main …,ICT-Zakboek,2007,*
Procesautomatisering,M van Keulen; TMA Bemelmans; RJ Kusters; M Looijen,KNAW Narcis. Back to search results. PublicationProcesautomatisering (2007). Pagina-navigatie: Main …,ICT-Zakboek,2007,*
06472 Executive Summary--XQuery Implementation Paradigms,Peter A Boncz; Torsten Grust; Jérôme Siméon; Maurice van Keulen,Abstract Only a couple of weeks after the participants of seminar No. 06472 met in Dagstuhl;the W3C published the Final Recommendation documents that fix the XQuery 1.0 syntax;data model; formal semantics; built-in function library and the interaction with the XMLSchema Recommendations (see W3C's XQuery web site at http://www. w3. org/XML/Query/).With the language's standardization nearing its end and now finally in place; the manyefforts to construct correct; complete; and efficient implementations of XQuery finally got ridof the hindering" moving target''syndrome. This Dagstuhl seminar on the different XQueryimplementation paradigms that have emerged in the recent past; thus was as timely as itcould have possibly been.,Dagstuhl Seminar Proceedings,2007,*
Proceedings of the 2nd Twente Data Management Workshop (TDM'06) on Uncertainty in Databases,Ander de Keijzer; Maurice van Keulen,*,*,2006,*
Assignments for XML&DB 2 Theme “Probabilistic XML”,Maurice van Keulen; Ander de Keijzer; Djoerd Hiemstra,*,*,2006,*
Optimization of Pathfinder Queries (Opaque)[public version],Maurice van Keulen,Summary: The Opaque project focuses on query optimization for relational XQuery engines.Advances in techniques for storage and query processing for XML using existing relationalengines show that this approach has great potential for being able to manage the evergrowing volumes of XML data. Many storage models; index structures; and algorithms havebeen proposed to speed up queries. Currently; attention is shifting towards queryoptimization techniques to address the remaining performance problems; to which thisresearch hopes to make a contribution.,*,2006,*
Rule-based Information Integration,Ander de Keijzer; Maurice van Keulen,Abstract In this report; we show the process of information integration. We specificallydiscuss the language used for integration. We show that integration consists of two phases;the schema mapping phase and the data integration phase. We formally definetransformation rules; conversion; evolution and versioning. We further discuss theintegration process from a data point of view.,*,2005,*
Third Workshop on Ambient Databases,P Boncz; Maurice van Keulen; AH van Bunningen,Skip to main content University of Twente Research Information Logo …,*,2005,*
Organisatie Dutch-Belgian Database Day,P Boncz; Maurice van Keulen; AH van Bunningen,KNAW Narcis. Back to search results. Publication OrganisatieDutch-Belgian Database Day (2005). Pagina-navigatie: Main …,*,2005,*
MonetDB/XQuery; Technology preview open source release 1,P Boncz; S Manegold; Sjoerd Mullender; Maurice van Keulen; Jan Flokstra; T Grust; J Teubner; J Rittinger,Boncz; P; Manegold; S; Mullender; S; van Keulen; M; Flokstra; J; Grust; T; Teubner; J &Rittinger; J MonetDB/XQuery; Technology preview open source release 1 …MonetDB/XQuery; Technology preview open source release 1. / Boncz; P.; Manegold; S.;Mullender; Sjoerd; van Keulen; Maurice; Flokstra; Jan; Grust; T.; Teubner; J.; Rittinger; J … BonczP; Manegold S; Mullender S; van Keulen M; Flokstra J; Grust T et al. MonetDB/XQuery; Technologypreview open source release 1. 2005 … Powered by Pure; Scopus & Elsevier FingerprintEngine™ © 2017 Elsevier BV.,*,2005,*
Advanced SIKS-course XML: where databases and information retrieval meet,Djoerd Hiemstra; Maurice van Keulen,Skip to main content University of Twente Research Information Logo …,*,2005,*
Second Multimedian Workshop on Ambient Multimedia Databases,Maurice van Keulen,KNAW Narcis. Back to search results. Publication Second Multimedian Workshop onAmbient Multimedia Databases (2004). Pagina-navigatie: Main …,*,2004,*
Seventh EDBT Summper School on XML & Databases,Maurice van Keulen,Skip to main content University of Twente Research Information Logo …,*,2004,*
Zoeken in XML: van XPath naar SQL,H van Rein; Maurice van Keulen,KNAW Narcis. Back to search results. Publication Zoeken in XML:van XPath naar SQL (2004). Pagina-navigatie: Main …,DB/M: database magazine,2004,*
De hamer of de moker: welke databasesoftware te kiezen,Maurice van Keulen; A Offerman,Abstract Voor serversystemen is Linux als operating system zeer geschikt. Menig webserverwordt gehost op een Linux-gebaseerd systeem en steeds meer organisaties kiezen voorLinux voor hun back-office. Een essentiele component daarbij is het database managementsystem (DBMS). Het DBMS draagt zorg voor het beheer van uw kostbare gegevens.Decennialange ervaring met DBMS'en; die teruggaat tot de eerste mainframes; heeft totproducten geleid waarin die gegevens veilig en betrouwbaar opgeslagen liggen enwaarmee u er snelle toegang tot hebt. Voor het Linux-platform zijn zowel open-sourceDBMS'en beschikbaar; zoals MySQL en PostgreSQL; als Linux-versies van bekendecommerciele producten; zoals Oracle en IBM DB2. Er bestaat geen eenduidige winnaar vande prijs voor het beste DBMS voor Linux. Er is altijd een trade-off die u zult moeten …,Linux news,2002,*
Report on the development issues: Techniques,Maurice van Keulen; J Vonk,Skip to main content University of Twente Research Information Logo …,Deliverable D6 of the SUMMER project,2002,*
An integrated DMMDBMS prototype supporting video retrieval and security,Maurice van Keulen; J Vonk,Skip to main content University of Twente Research Information Logo …,Deliverable D5 of the SUMMER project,2002,*
SUMMER demo,Maurice van Keulen,Skip to main content University of Twente Research Information Logo …,*,2001,*
DMMDBMS components and architectures,Maurice van Keulen,KNAW Narcis. Back to search results. Publication DMMDBMS componentsand architectures (2001). Pagina-navigatie: Main …,*,2001,*
Requirements analysis based on application domains,Maurice van Keulen,KNAW Narcis. Back to search results. Publication Requirements analysis basedon application domains (2001). Pagina-navigatie: Main …,*,2001,*
XML: De eend met de gouden eieren?,Maurice Keulen,*,I/O Vivat,2001,*
View integration,Herman Balsters; Maarten Fokkinga; Maurice Van Keulen,Problem statement Suppose that we are to build one information system (database; say) forseveral users that each have their own view of the world. The idea is to integrate the views ofall individual users into one view (which is satisfactory for all users); and build theinformation system for that view. The problem that we are faced with is this:,*,1999,*
Object-Oriented Programming: A Unified Foundation-Guiseppe Castagna,Maurice van Keulen,KNAW Narcis. Back to search results. Publication Object-Oriented Programming: A UnifiedFoundation - Guiseppe Castagna (1998). Pagina-navigatie: Main …,Objet,1998,*
Trends in tools voor gegevensmodellering,Maurice van Keulen; HW Brand,KNAW Narcis. Back to search results. Publication Trends in tools voorgegevensmodellering (1998). Pagina-navigatie: Main …,Informatie,1998,*
Formal operation definition in object-oriented databases,Maurice van Keulen,Abstract Despite the rapid progress of the previous decades; information systemengineering is still not as mature as many other engineering disciplines like electronics. Aprominent aspect in which this can be observed is the toolset an engineer has to hisdisposal for design and development. For example; an electronics engineer can performadvanced analyses on a schematic circuit design to predict the behaviour of the circuit intime. Tools developed for information system engineering have not yet arrived at acomparable level of maturity. One of the causes of the inability to develop such tools is thatthe specification languages used to decribe a design fall short on several aspects comparedto the circuit diagrams:(1) the specifications are in some circumstances susceptible tomultiple interpretations; ie; their meaning is not clear with the mathematical precision …,*,1997,*
Formal operation definition in object-oriented databases,Maurice van Keulen,Abstract Despite the rapid progress of the previous decades; information systemengineering is still not as mature as many other engineering disciplines like electronics. Aprominent aspect in which this can be observed is the toolset an engineer has to hisdisposal for design and development. For example; an electronics engineer can performadvanced analyses on a schematic circuit design to predict the behaviour of the circuit intime. Tools developed for information system engineering have not yet arrived at acomparable level of maturity. One of the causes of the inability to develop such tools is thatthe specification languages used to decribe a design fall short on several aspects comparedto the circuit diagrams:(1) the specifications are in some circumstances susceptible tomultiple interpretations; ie; their meaning is not clear with the mathematical precision …,*,1997,*
How do we type an object-oriented query result,H Balsters; Maurice van Keulen,KNAW Narcis. Back to search results. Publication How do we type anobject-oriented query result (abstract) (1996). Pagina-navigatie: Main …,Dagstuhl Seminar Reports,1996,*
An architecture and methodology for the design and development of Technical Information Systems,H Balsters; R Capobianchi; M Mautref; Maurice van Keulen,KNAW Narcis. Back to search results. Publication An architecture and methodology forthe design and development of... (1995). Pagina-navigatie: Main …,CTIT technical reports series,1995,*
TM Manual: version 2.0 revision e,H Balsters; RA de By; Maurice van Keulen; J Skowronek,Balsters; H.; de By; RA; van Keulen; M.; & Skowronek; J. (1995). TM Manual: version 2.0 revisione. (IMPRESS deliverable; No. UT-TECH-T79-001-R4). Enschede … Balsters; H.; de By; RA;van Keulen; Maurice; Skowronek; J. / TM Manual: version 2.0 revision e … Enschede;1995. (IMPRESS deliverable; No. UT-TECH-T79-001-R4) … Balsters; H; de By; RA; vanKeulen; M & Skowronek; J 1995; TM Manual: version 2.0 revision e. IMPRESS deliverable;no. UT-TECH-T79-001-R4; Enschede … TM Manual: version 2.0 revision e. / Balsters; H.; deBy; RA; van Keulen; Maurice; Skowronek; J … Enschede; 1995. (IMPRESS deliverable; No.UT-TECH-T79-001-R4) … Balsters H; de By RA; van Keulen M; Skowronek J. TM Manual: version2.0 revision e. Enschede; 1995. (IMPRESS deliverable; UT-TECH-T79-001-R4) … Poweredby Pure; Scopus & Elsevier Fingerprint Engine™ © 2017 Elsevier BV.,IMPRESS deliverable,1995,*
Design and Validation of Reliable Complex Computer Systems,Maurice van Keulen; J Skowronek; Peter MG Apers; H Balsters; Henk Blanken; RA de By; Jan Flokstra,KNAW Narcis. Back to search results. Publication Design and Validation of ReliableComplex Computer Systems (1995). Pagina-navigatie: Main …,Memoranda informatica,1995,*
Impress/UT-soft-D41-001-R1; Database Design Tool; deliverable S41,Peter MG Apers; Henk Blanken; H Balsters; Jan Flokstra; RT Boon; J Skowronek; Maurice van Keulen,Apers; PMG; Blanken; H; Balsters; H; Flokstra; J; Boon; RT; Skowronek; J & van Keulen; MImpress/UT-soft-D41-001-R1; Database Design Tool; deliverable S41 … Impress/UT-soft-D41-001-R1; Database Design Tool; deliverable S41. / Apers; Peter MG; Blanken; Henk;Balsters; H.; Flokstra; Jan; Boon; RT; Skowronek; J.; van Keulen; Maurice … Apers PMG; BlankenH; Balsters H; Flokstra J; Boon RT; Skowronek J et al. Impress/UT-soft-D41-001-R1; DatabaseDesign Tool; deliverable S41. 1994 … Powered by Pure; Scopus & Elsevier FingerprintEngine™ © 2017 Elsevier BV.,*,1994,*
Impress/UT-Soft-D34-001-R1; TMQL Implementation; deliverable S34,Peter MG Apers; Henk Blanken; H Balsters; Jan Flokstra; RT Boon; J Skowronek; Maurice van Keulen,Apers; PMG; Blanken; H; Balsters; H; Flokstra; J; Boon; RT; Skowronek; J & van Keulen; MImpress/UT-Soft-D34-001-R1; TMQL Implementation; deliverable S34 … Impress/UT-Soft-D34-001-R1; TMQL Implementation; deliverable S34. / Apers; Peter MG; Blanken; Henk;Balsters; H.; Flokstra; Jan; Boon; RT; Skowronek; J.; van Keulen; Maurice … Apers PMG; BlankenH; Balsters H; Flokstra J; Boon RT; Skowronek J et al. Impress/UT-Soft-D34-001-R1; TMQLImplementation; deliverable S34. 1994 … Powered by Pure; Scopus & Elsevier FingerprintEngine™ © 2017 Elsevier BV.,*,1994,*
Results of an examination of the GTI,Maurice van Keulen,KNAW Narcis. Back to search results. Publication Results of anexamination of the GTI (1993). Pagina-navigatie: Main …,*,1993,*
Some methodological guidelines for IMPRESS,F Ardorino; M Mautref; RA de By; Maurice van Keulen,Skip to main content University of Twente Research Information Logo …,*,1993,*
Global design of the prototyping environment,Maurice van Keulen,Skip to main content University of Twente Research Information Logo …,IMPRESS/UT-MEMO,1993,*
The TM manual; version 2.0,Peter MG Apers; RA de By; Henk Blanken; H Balsters; J Skowronek; HJ Steenhagen; Maurice van Keulen; AN Wilschut; Jan Flokstra; RJ Blok,KNAW Narcis. Back to search results. Publication The TM manual;version 2.0 (1993). Pagina-navigatie: Main …,*,1993,*
Struggle for LIFE: A Rapid prototype for TM in LIFE,M van Keulen,*,*,1992,*
Deep Physiological Arousal Detection in a Driving Simulator using Wearable Devices,Aaqib Saeed; Stojan Trajanovski; Maurice van Keulen; Jan van Erp,Abstract—Driving is an activity that requires considerable alertness. Insufficient attention;imperfect perception; inadequate information processing; and sub-optimal arousal arepossible causes of poor human performance. Understanding of these causes and theimplementation of effective remedies is of key importance to increase traffic safety andimprove driver's wellbeing. For this purpose; we used deep learning algorithms to detectarousal level; namely; under-aroused; normal and overaroused for professional truck driversin a simulated environment. The physiological signals are collected from 11 participants bywrist wearable devices. We presented a cost effective groundtruth generation scheme forarousal based on a subjective measure of sleepiness and score of stress stimuli. On thisdataset; we evaluated a range of deep neural network models for representation learning …,*,*,*
Pay-as-you-go data integration for bioinformatics,Brend Wanders Paul van der Vet; Maurice van Keulen,Page 1. Brend Wanders Paul van der Vet Maurice van Keulen <b.wanders@utwente.nl>Pay-as-you-go data integration for bioinformatics ACO1 ACO2 IDH3A IDH3B IDH3G IDH1 IDH2SUCLA2 SUCLG1 SUCLG2 SUCLA2P1 LOC283398 SDHA SDHB SDHD SDHC FH MDH1 MDH2MDH1B CS SLC35G3 citrate cis-aconitate D-threo-isocitrate 3-carboxy-1-hydroxypropyl-ThPPsuccinyl-CoA 2-oxosuccinate succinate fumarate (S)-malate oxaloacetate ACO1 ACO2 IREB2ACO1 ACO2 OGDH OGDHL DLST SUCLA2 SUCLG1 SUCLG2 SUCLA2 SUCLG1 SUCLG2 IDH1IDH2 IDH2 IDH1 IDH2 IDH1 IDH2 OGDH OGDHL SDHA SDHB SDHD SDHC SDHA SDHB SDHDSDHC H2O H2O NAD H+ NADH NAD NADP CO2 NADPH H+ NADH CO2 H+ ubiquinol ubiquinoneFADH2 FAD+ CoA ATP ADP GDP IDP ITP acetyl-CoA H2O H+ CoA NAD NADH H+ H2O Pi CoAPi GTP CoA NADP H+ NADPH NAD CoA NADH CO2 H+ …,*,*,*
Neogeography: The challenge of channelling large and ill-behaved data streams,Mena Badieh Habib Morgan; Peter MG Apers; Maurice Van Keulen,Abstract Neogeography is the combination of user generated data and experiences withmapping technologies. In this article we present a research project to extract valuablestructured information with a geographic component from unstructured user generated textin wikis; forums; or SMSes. The extracted information should be integrated together to form acollective knowledge about certain domain. This structured information can be used furtherto help users from the same domain who want to get information using simple questionanswering system. The project intends to help workers communities in developing countriesto share their knowledge; providing a simple and cheap way to contribute and get benefitusing the available communication technology.,*,*,*
Imprecision; Diversity and Uncertainty: Disentangling Threads in Uncertainty Management,Myra Spiliopoulou; Maurice Van Keulen; Hans-Joachim Lenz; Jef Wijsen; Matthias Renz; Rudolf Kruse; Mirco Stern,The motivation of the workgroup was to couple uncertainty management with the notion ofchange. This encompassed integration; aggregation and mining upon uncertain data. Targetquestions were measuring uncertainty and defining and measuring the quality of uncertaindata. We have left out ambiguousness (in the sense of unclear terms and vagueness) as anorthogonal issue. Uncertainty management in the context of reasoning (cf. among else[GS98]) has been a source of inspiration; since that area encompasses appropriatetheoretical underpinnings for addressing some of the questions studied here.,*,*,*
Item Removed,Marko Smiljanic; Henk Blanken; Maurice van Keulen; Willem Jonker,*,*,*,*
Workshop FlexDBIST 2011,Gloria Bordogna; Giuseppe Psaila; Elena Baralis; Patrick Bosc; Barbara Catania; Richard Chbeir; Eliseo Clementini; Karin Coninx; Alfredo Cuzzocrea; Ander de Keijzer; Guy de Tré; Fernando Ferri; Sara Foresti; Pablo Garcia Bringas; Paolo Garza; Patrizia Grifoni; Dion Hoe-Lian Goh; Enrique Herrera Viedma; Maria J Martin-Bautista; Rosa Meo; Barbara Oliboni; Javier Parapar; Gerardo Pelosi; Ilia Petrov; Olivier Pivert; Chiara Renso; Crawford Revie; Maurice van Keulen; Slawomir Zadrozny,Program Committee Chairs Gloria Bordogna; CNR Consiglio Nazionale delle Ricerche; ItalyGiuseppe Psaila; University of Bergamo; Italy … Program Committee Elena Baralis; Politecnicodi Torino; Italy Patrick Bosc; IRISA/ENSSAT; France Mohand Boughanem; IRIT; France BarbaraCatania; University of Genova; Italy Richard Chbeir; Bourgogne University; (France EliseoClementini; University of L'Aquila; Italy Karin Coninx; Hasselt University; Belgium AlfredoCuzzocrea; CNR-ICAR and University of Calabria; Italy Ander de Keijzer; University ofTwente; The Netherlands Guy de Tré; Ghent University; Belgium Fernando Ferri;IRPPS-CNR; Italy Sara Foresti; University of Milan; Italy Pablo Garcia Bringas; University ofDeusto; Spain Paolo Garza; Politecnico di Torino; Italy Patrizia Grifoni; IRPPS-CNR; Italy DionHoe-Lian Goh; Nanyang Technological University; Singapore Enrique Herrera Viedma …,*,*,*
Pay-‐as-‐you-‐go data integration for bio-‐informatics (PAYDIBI),M van Keulen; P van der Vet,Scientific research in bio-‐informatics is often data-‐driven and supported by biologicaldatabases. A biological database contains factual information collected from scientificexperiments and computational analyses about areas including genomics; proteomics;metabolomics; microarray gene expression; and phylogenetics. Information contained inbiological databases includes gene function; structure; localization (both cellular andchromosomal); clinical effects of mutations as well as similarities of biological sequencesand structures. 2 Much effort is involved in keeping them up-‐to-‐date; extending them andin creating new ones (see for example[1];[2]). In a growing number of research projects;researchers like to ask combined questions; ie; questions that require the combination ofinformation from more than one database.[3] estimates that in scientific workflows“in total …,*,*,*
