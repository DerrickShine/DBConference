SAP HANA database: data management for modern business applications,Franz Färber; Sang Kyun Cha; Jürgen Primsch; Christof Bornhövd; Stefan Sigg; Wolfgang Lehner,Abstract The SAP HANA database is positioned as the core of the SAP HANA Appliance tosupport complex business analytical processes in combination with transactionallyconsistent operational workloads. Within this paper; we outline the basic characteristics ofthe SAP HANA database; emphasizing the distinctive features that differentiate the SAPHANA database from other classical relational database management systems. On thetechnical side; the SAP HANA database consists of multiple data processing engines with adistributed query processing environment to provide the full spectrum of data processing--from classical relational data supporting both row-and column-oriented physicalrepresentations in a hybrid engine; to graph and text processing for semi-and unstructureddata management within the same system. From a more application-oriented perspective …,ACM Sigmod Record,2012,313
Modeling large scale OLAP scenarios,Wolfgang Lehner,Abstract In the recent past; different multidimensional data models were introduced to modelOLAP ('Online Analytical Processing') scenarios. Design problems arise; when the modeledOLAP scenarios become very large and the dimensionality increases; which greatlydecreases the support for an efficient ad-hoc data analysis process. Therefore; we extendthe classical multidimensional model by grouping functionally dependent attributes withinsingle dimensions; yielding in real orthogonal dimensions; which are easy to create and tomaintain on schema design level. During the multidimensional data analysis phase; thistechnique yields in nested data cubes reflecting an intuitive two-step navigation process:classification-oriented 'drill-down'/'roll-up'and description-oriented 'split'/'merge'operators ondata cubes. Thus; the proposed Nested Multidimensional Data Model provides great …,International Conference on Extending Database Technology,1998,246
Normal forms for multidimensional databases,Wolfgang Lehner; Jens Albrecht; Hartmut Wedekind,In the area of online analytical processing (OLAP); the concept of multidimensionaldatabases is receiving much popularity. Thus; a couple of different multidimensional datamodels were proposed from the research as well as from the commercial product side; eachemphasizing different perspectives. However; very little work has been done investigatingguidelines for good schema design within such a multidimensional data model. Based on alogical reconstruction of multidimensional schema design; this paper proposes twomultidimensional normal forms. These normal forms define modeling constraints forsummary attributes describing the cells within a multidimensional data cube and constraintsto model complex dimensional structures appropriately. Multidimensional schemascompliant to these normal forms do not only ensure the validity of analytical computations …,Scientific and Statistical Database Management; 1998. Proceedings. Tenth International Conference on,1998,194
The SAP HANA Database--An Architecture Overview.,Franz Färber; Norman May; Wolfgang Lehner; Philipp Große; Ingo Müller; Hannes Rauhe; Jonathan Dees,Abstract Requirements of enterprise applications have become much more demanding.They require the computation of complex reports on transactional data while thousands ofusers may read or update records of the same data. The goal of the SAP HANA database isthe integration of transactional and analytical workload within the same databasemanagement system. To achieve this; a columnar engine exploits modern hardware(multiple CPU cores; large main memory; and caches); compression of database content;maximum parallelization in the database kernel; and database extensions required byenterprise applications; eg; specialized data structures for hierarchies or support for domainspecific languages. In this paper we highlight the architectural concepts employed in theSAP HANA database. We also report on insights gathered with the SAP HANA database …,IEEE Data Eng. Bull.,2012,191
Efficient transaction processing in SAP HANA database: the end of a column store myth,Vishal Sikka; Franz Färber; Wolfgang Lehner; Sang Kyun Cha; Thomas Peh; Christof Bornhövd,Abstract The SAP HANA database is the core of SAP's new data management platform. Theoverall goal of the SAP HANA database is to provide a generic but powerful system fordifferent query scenarios; both transactional and analytical; on the same data representationwithin a highly scalable execution environment. Within this paper; we highlight the mainfeatures that differentiate the SAP HANA database from classical relational databaseengines. Therefore; we outline the general architecture and design criteria of the SAP HANAin a first step. In a second step; we challenge the common belief that column store datastructures are only superior in analytical workloads and not well suited for transactionalworkloads. We outline the concept of record life cycle management to use different storageformats for the different stages of a record. We not only discuss the general concept but …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,145
Efficient exploitation of similar subexpressions for query processing,Jingren Zhou; Per-Ake Larson; Johann-Christoph Freytag; Wolfgang Lehner,Abstract Complex queries often contain common or similar subexpressions; either within asingle query or among multiple queries submitted as a batch. If so; query execution time canbe improved by evaluating a common subexpression once and reusing the result in multipleplaces. However; current query optimizers do not recognize and exploit similarsubexpressions; even within the same query. We present an efficient; scalable; andprincipled solution to this long-standing optimization problem. We introduce a light-weightand effective mechanism to detect potential sharing opportunities among expressions.Candidate covering subexpressions are constructed and optimization is resumed todetermine which; if any; such subexpressions to include in the final query plan. The chosensubexpression (s) are computed only once and the results are reused to answer other …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,94
Representing data quality in sensor data streaming environments,Anja Klein; Wolfgang Lehner,Abstract Sensors in smart-item environments capture data about product conditions andusage to support business decisions as well as production automation processes. Achallenging issue in this application area is the restricted quality of sensor data due tolimited sensor precision and sensor failures. Moreover; data stream processing to meetresource constraints in streaming environments introduces additional noise and decreasesthe data quality. In order to avoid wrong business decisions due to dirty data; qualitycharacteristics have to be captured; processed; and provided to the respective businesstask. However; the issue of how to efficiently provide applications with information aboutdata quality is still an open research problem. In this article; we address this problem bypresenting a flexible model for the propagation and processing of data quality. The …,Journal of Data and Information Quality (JDIQ),2009,72
RiTE: Providing on-demand data for right-time data warehousing,Christian Thomsen; Torben Bach Pedersen; Wolfgang Lehner,Data warehouses (DWs) have traditionally been loaded with data at regular time intervals;eg; monthly; weekly; or daily; using fast bulk loading techniques. Recently; the trend is toinsert all (or only some) new source data very quickly into DWs; called near-realtime DWs(right-time DWs). This is done using regular INSERT statements; resulting in too low insertspeeds. There is thus a great need for a solution that makes inserted data available quickly;while still providing bulk-load insert speeds. This paper presents RiTE (" Right-Time ETL"); amiddleware system that provides exactly that. A data producer (ETL) can insert data thatbecomes available to data consumers on demand. RiTE includes an innovative main-memory based catalyst that provides fast storage and offers concurrency control. A numberof policies controlling the bulk movement of data based on user requirements for …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,69
The state of open data,Katrin Braunschweig; Julian Eberius; Maik Thiele; Wolfgang Lehner,ABSTRACT Following the Open Data trend; governments and public agencies have startedmaking their data available to the public using web portals; web services or REST interfaces.Ideally; making this data available on the web would lead to more transparency;participation and innovation throughout society. However; just publishing the data on theweb is not enough. To truly advance the open society; the publication platforms need to fulfillcertain legal; administrative as well as technical requirements. In this paper we present asurvey of existing Open Government Data platforms; focusing on the technical aspects. Westudied over fifty Open Data repositories operated by national; regional and communalgovernments; as well as international organizations. Features such as standardization;discoverability and machinereadability of data were taken into account. Furthermore; a …,Limits of current open data platforms,2012,57
On solving the view selection problem in distributed data warehouse architectures,Andreas Bauer; Wolfgang Lehner,The use of materialized views in a data warehouse installation is a common tool to speed upmostly aggregation queries. The problems coming along with materialized aggregate viewshave triggered a huge variety of proposals; such as picking the optimal set of aggregationcombinations; transparently rewriting user queries to take advantage of the summary data;or synchronizing pre-computed summary data as soon as the base data changes. The paperfocuses on the problem of view selection in the context of distributed data warehousearchitectures. While much research was done with regard to the view selection problem inthe central case; we are not aware to any other work discussing the problem of viewselection in distributed data warehouse systems. The paper proposes an extension of theconcept of an aggregation lattice to capture the distributed semantics. Moreover; we …,Scientific and Statistical Database Management; 2003. 15th International Conference on,2003,55
Data mining with the SAP NetWeaver BI accelerator,Thomas Legler; Wolfgang Lehner; Andrew Ross,Abstract The new SAP NetWeaver Business Intelligence accelerator is an engine thatsupports online analytical processing. It performs aggregation in memory and in queryruntime over large volumes of structured data. This paper first briefly describes theaccelerator and its main architectural features; and cites test results that indicate its power.Then it describes in detail how the accelerator may be used for data mining. The acceleratorcan perform data mining in the same large repositories of data and using the same compactindex structures that it uses for analytical processing. A first such implementation of datamining is described and the results of a performance evaluation are presented. Associationrule mining in a distributed architecture was implemented with a variant of the BUC icebergcubing algorithm. Test results suggest that useful online mining should be possible with …,Proceedings of the 32nd international conference on Very large data bases,2006,54
The Graph Story of the SAP HANA Database.,Michael Rudolf; Marcus Paradies; Christof Bornhövd; Wolfgang Lehner,*,BTW,2013,50
Partition-based workload scheduling in living data warehouse environments,Maik Thiele; Ulrike Fischer; Wolfgang Lehner,Abstract The demand for so-called living or real-time data warehouses is increasing in manyapplication areas such as manufacturing; event monitoring and telecommunications. Inthese fields; users normally expect short response times for their queries and high freshnessfor the requested data. However; meeting these fundamental requirements is challengingdue to the high loads and the continuous flow of write-only updates and read-only queriesthat might be in conflict with each other. Therefore; we present the concept of workloadbalancing by election (WINE); which allows users to express their individual demands on thequality of service and the quality of data; respectively. WINE exploits these information tobalance and prioritize both types of transactions—queries and updates—according to thevarying user needs. A simulation study shows that our proposed algorithm outperforms …,Information Systems,2009,49
Incremental maintenance of summary tables with complex grouping expressions,*,A method; apparatus; and article of manufacture for the incremental maintenance ofsummary tables with complex grouping expressions where the change(insert/delete/update) of a single row of the base data can affect multiple rows in thesummary table. The invention applies the complex grouping expression to the raw delta;yielding a delta stream consisting of multiple (but distinct) grouping combinations. Theinvention then inserts/deletes/updates delta values into/from the existing summary table sothat each grouping combination of the delta stream modifies its corresponding groupingcombination in the summary table.,*,2004,48
SAP HANA distributed in-memory database system: Transaction; session; and metadata management,Juchang Lee; Yong Sik Kwon; Franz Färber; Michael Muehle; Chulwon Lee; Christian Bensberg; Joo Yeon Lee; Arthur H Lee; Wolfgang Lehner,One of the core principles of the SAP HANA database system is the comprehensive supportof distributed query facility. Supporting scale-out scenarios was one of the major designprinciples of the system from the very beginning. Within this paper; we first give an overviewof the overall functionality with respect to data allocation; metadata caching and queryrouting. We then dive into some level of detail for specific topics and explain features andmethods not common in traditional disk-based database systems. In summary; the paperprovides a comprehensive overview of distributed query processing in SAP HANA databaseto achieve scalability to handle large databases and heterogeneous types of workloads.,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,45
Improving in-memory database index performance with Intel® Transactional Synchronization Extensions,Tomas Karnagel; Roman Dementiev; Ravi Rajwar; Konrad Lai; Thomas Legler; Benjamin Schlegel; Wolfgang Lehner,The increasing number of cores every generation poses challenges for high-performance in-memory database systems. While these systems use sophisticated high-level algorithms topartition a query or run multiple queries in parallel; they also utilize low-levelsynchronization mechanisms to synchronize access to internal database data structures.Developers often spend significant development and verification effort to improveconcurrency in the presence of such synchronization. The Intel® TransactionalSynchronization Extensions (Intel® TSX) in the 4th Generation Core™ Processors enablehardware to dynamically determine whether threads actually need to synchronize even inthe presence of conservatively used synchronization. This paper evaluates the effectivenessof such hardware support in a commercial database. We focus on two index …,High Performance Computer Architecture (HPCA); 2014 IEEE 20th International Symposium on,2014,43
Management of multidimensional aggregates for efficient online analytical processing,Jens Albrecht; Andreas Bauer; Oliver Deyerling; H Gunzel; W Lehner; L Schlesinger,Proper management of multidimensional aggregates is a fundamental prerequisite forefficient OLAP. The experimental OLAP server CUBESTAR whose concepts are described;was designed exactly for that purpose. All logical query processing is based solely on aspecific algebra for multidimensional data. However; a relational database system is usedfor the physical storage of the data. Therefore; in popular terms; CUBESTAR can beclassified as a ROLAP system. In comparison to commercially available systems;CUBESTAR is superior in two aspects. First; the implemented multidimensional data modelallows more adequate modeling of hierarchical dimensions; because properties which applyonly to certain dimensional elements can be modeled context-sensitively. This fact isreflected by an extended star schema on the relational side. Second; CUBESTAR …,Database Engineering and Applications; 1999. IDEAS'99. International Symposium Proceedings,1999,43
Multi-objective scheduling for real-time data warehouses,Maik Thiele; Andreas Bader; Wolfgang Lehner,Abstract The issue of write-read contention is one of the most prevalent problems whendeploying real-time data warehouses. With increasing load; updates are increasinglydelayed and previously fast queries tend to be slowed down considerably. However;depending on the user requirements; we can improve the response time or the data qualityby scheduling the queries and updates appropriately. If both criteria are to be consideredsimultaneously; we are faced with a so-called multi-objective optimization problem. Wetransformed this problem into a knapsack problem with additional inequalities and solved itefficiently. Based on our solution; we developed a scheduling approach that provides theoptimal schedule with regard to the user requirements at any given point in time. Weevaluated our scheduling in an extensive experimental study; where we compared our …,Computer Science-Research and Development,2009,42
Data modeling for Precision Dairy Farming within the competitive field of operational and analytical tasks,Christian Schulze; Joachim Spilke; Wolfgang Lehner,Abstract The interdisciplinary concept of Precision Dairy Farming sets very high standardsfor data management. Special consideration during its implementation must therefore begiven to support both operational and analytical data uses (eg; OLAP). The inclusion of bothdata views results in the data modeling being a hybrid of two conceptual design models. Incontrast to previous design concepts; we will assume a parallel modeling process for bothviews; which results in a shared logical data schema. This is the only way to effectively avoidredundancies and inconsistencies on both the schema and data levels. Using an ongoingapplication as an example; we will explain both methods and results. In doing so; we willmake use of the Entity-Relationship Model (E/RM) for modeling operational data. We willalso make use of E/RM's multi-dimensional extension; the multi-dimensional Entity …,Computers and electronics in agriculture,2007,42
A dip in the reservoir: Maintaining sample synopses of evolving datasets,Rainer Gemulla; Wolfgang Lehner; Peter J Haas,Abstract Perhaps the most flexible synopsis of a database is a random sample of the data;such samples are widely used to speed up processing of analytic queries and data-miningtasks; enhance query optimization; and facilitate information integration. In this paper; westudy methods for incrementally maintaining a uniform random sample of the items in adataset in the presence of an arbitrary sequence of insertions and deletions. For" stable"datasets whose size remains roughly constant over time; we provide a novel samplingscheme; called" random pairing"(RP) which maintains a bounded-size uniform sample byusing newly inserted data items to compensate for previous deletions. The RP algorithm isthe first extension of the almost 40-year-old reservoir sampling algorithm to handledeletions. Experiments show that; when dataset-size fluctuations over time are not too …,Proceedings of the 32nd international conference on Very large data bases,2006,42
Real-time scheduling for data stream management systems,Sven Schmidt; Thomas Legler; Daniel Schaller; Wolfgang Lehner,Quality-aware management of data streams is gaining more and more importance with theamount of data produced by streams growing continuously. The resources required for datastream processing depend on different factors and are limited by the environment of the datastream management system (DSMS). Thus; with a potentially unbounded amount of streamdata and limited processing resources; some of the data stream processing tasks(originating from different users) may not be satisfyingly answered; and therefore; usersshould be enabled to negotiate a certain quality for the execution of their stream processingtasks. After the negotiation process; it is the responsibility of the Data Stream ManagementSystem to meet the quality constraints by using adequate resource reservation andscheduling techniques. Within this paper; we consider different aspects of real-time …,Real-Time Systems; 2005.(ECRTS 2005). Proceedings. 17th Euromicro Conference on,2005,42
fAST refresh using mass query optimization,Wolfgang Lehner; Bobbie Cochrane; Hamid Pirahesh; Markos Zaharioudakis,Automatic summary tables (ASTs); more commonly known as materialized views; are widelyused to enhance query performance; particularly for aggregate queries. Such queriesaccess a huge number of rows to retrieve aggregated summary data while performingmultiple joins in the context of a typical data warehouse star schema. To keep ASTsconsistent with their underlying base data; the ASTs are either immediately synchronized orfully recomputed. This paper proposes an optimization strategy for simultaneously refreshingmultiple ASTs; thus avoiding multiple scans of a large fact table (one pass for ASTcomputation). A query stacking strategy detects common sub-expressions using theavailable query matching technology of DB2. Since exact common sub-expressions arerare; the novel query sharing approach systematically generates common …,Data Engineering; 2001. Proceedings. 17th International Conference on,2001,41
An alternative relational OLAP modeling approach,Andreas Bauer; Wolfgang Hümmer; Wolfgang Lehner,Abstract Schema design is one of the fundamentals in database theory and practice as well.In this paper; we discuss the problem of locally valid dimensional attributes in a classificationhierarchy of a typical OLAP scenario. In a first step; we show that the traditional star andsnowflake schema approach is not feasible in this very natural case of a hierarchy.Therefore; we sketch two alternative modeling approaches resulting in practical solutionsand a seamless extension of the traditional star and snowflake schema approach: In a purerelational approach; we replace each dimension table of a star/snowflake schema by a set ofviews directly reflecting the classification hierarchy. The second approach takes advantageof the object-relational extensions. Using object-relational techniques in the context for therelational representation of a multidimensional OLAP scenario is a novel approach and …,International Conference on Data Warehousing and Knowledge Discovery,2000,41
Data management in the mirabel smart grid system,Matthias Boehm; Lars Dannecker; Andreas Doms; Erik Dovgan; Bogdan Filipič; Ulrike Fischer; Wolfgang Lehner; Torben Bach Pedersen; Yoann Pitarch; Laurynas Šikšnys; Tea Tušar,Abstract Nowadays; Renewable Energy Sources (RES) are attracting more and moreinterest. Thus; many countries aim to increase the share of green energy and have to facewith several challenges (eg; balancing; storage; pricing). In this paper; we address thebalancing challenge and present the MIRABEL project which aims to prototype an EnergyData Management System (EDMS) which takes benefit of flexibilities to efficiently balanceenergy demand and supply. The EDMS consists of millions of heterogeneous nodes thateach incorporates advanced components (eg; aggregation; forecasting; scheduling;negotiation). We describe each of these components and their interaction. Preliminaryexperimental results confirm the feasibility of our EDMS.,Proceedings of the 2012 Joint EDBT/ICDT Workshops,2012,39
Cardinality estimation using sample views with quality assurance,Per-Ake Larson; Wolfgang Lehner; Jingren Zhou; Peter Zabback,Abstract Accurate cardinality estimation is critically important to high-quality queryoptimization. It is well known that conventional cardinality estimation based on histograms orsimilar statistics may produce extremely poor estimates in a variety of situations; forexample; queries with complex predicates; correlation among columns; or predicatescontaining user-defined functions. In this paper; we propose a new; general cardinalityestimation technique that combines random sampling and materialized view technology toproduce accurate estimates even in these situations. As a major innovation; we exploitfeedback information from query execution and process control techniques to assure thatestimates remain statistically valid when the underlying data changes. Experimental resultsbased on a prototype implementation in Microsoft SQL Server demonstrate the practicality …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,39
Qstream: Deterministic querying of data streams,Sven Schmidt; Henrike Berthold; Wolfgang Lehner,Abstract Current developments in processing data streams are based on the best-effortprinciple and therefore not adequate for many application areas. When sensor data isgathered by interface hardware and is used for triggering data-dependent actions; the datahas to be queried and processed not only in an efficient but also in a deterministic way. Ourstreaming system prototype embodies novel data processing techniques. It is based on anoperator component model and runs on top of a real-time capable environment. Thisenables us to provide real Quality-of-Service for data stream queries.,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,39
In-memory databases in business information systems,Peter Loos; Jens Lechtenbörger; Gottfried Vossen; Alexander Zeier; Jens Krüger; Jürgen Müller; Wolfgang Lehner; Donald Kossmann; Benjamin Fabian; Oliver Günther; Robert Winter,In-memory databases are developed to keep the entire data in main memory. Compared totraditional database systems; read access is now much faster since no I/O access to a harddrive is required. In terms of write access; mechanisms are available which provide datapersistence and thus secure transactions. In-memory databases have been available for awhile and have proven to be suitable for particular use cases. With increasing storagedensity of DRAM modules; hardware systems capable of storing very large amounts of datahave become affordable. In this context the question arises whether in-memory databasesare suitable for business information system applications. Hasso Plattner; who developedthe HANA in-memory database; is a trailblazer for this approach. He sees a lot of potentialfor novel concepts concerning the development of business information systems. One …,Business & Information Systems Engineering,2011,38
Fast integer compression using SIMD instructions,Benjamin Schlegel; Rainer Gemulla; Wolfgang Lehner,Abstract We study algorithms for efficient compression and decompression of a sequence ofintegers on modern hardware. Our focus is on universal codes in which the codeword lengthis a monotonically non-decreasing function of the uncompressed integer value; such codesare widely used for compressing" small integers". In contrast to traditional integercompression; our algorithms make use of the SIMD capabilities of modern processors byencoding multiple integer values at once. More specifically; we provide SIMD versions ofboth null suppression and Elias gamma encoding. Our experiments show that theseversions provide a speedup from 1.5 x up to 6.7 x for decompression; while maintaining asimilar compression performance.,Proceedings of the Sixth International Workshop on Data Management on New Hardware,2010,38
Setting goals and choosing metrics for recommender system evaluations,Gunnar Schröder; Maik Thiele; Wolfgang Lehner,ABSTRACT Recommender systems have become an important personalization techniqueon the web and are widely used especially in e-commerce applications. However; operatorsof web shops and other platforms are challenged by the large variety of available algorithmsand the multitude of their possible parameterizations. Since the quality of therecommendations that are given can have a significant business impact; the selection of arecommender system should be made based on well-founded evaluation data. Theliterature on recommender system evaluation offers a large variety of evaluation metrics butprovides little guidance on how to choose among them. This paper focuses on the oftenneglected aspect of clearly defining the goal of an evaluation and how this goal relates tothe selection of an appropriate metric. We discuss several well-known accuracy metrics …,UCERSTI2 Workshop at the 5th ACM Conference on Recommender Systems; Chicago; USA,2011,37
Fast Sorted-Set Intersection using SIMD Instructions.,Benjamin Schlegel; Thomas Willhalm; Wolfgang Lehner,ABSTRACT In this paper; we focus on sorted-set intersection which is an important part inmany algorithms; eg; RID-list intersection; inverted indexes; and others. In contrast totraditional scalar sorted-set intersection algorithms that try to reduce the number ofcomparisons; we propose a parallel algorithm that relies on speculative execution ofcomparisons. In general; our algorithm requires more comparisons but less instructions thanscalar algorithms that translates into a better overall speed. We achieve this by utilizingefficient single-instruction-multiple-data (SIMD) instructions that are available in manyprocessors. We provide different sorted-set intersection algorithms for different integer datatypes. We propose versions that use uncompressed integer values as input and output aswell as a version that uses a tailor-made data layout for even faster intersections. In our …,ADMS@ VLDB,2011,37
Sampling time-based sliding windows in bounded space,Rainer Gemulla; Wolfgang Lehner,Abstract Random sampling is an appealing approach to build synopses of large datastreams because random samples can be used for a broad spectrum of analytical tasks.Users are often interested in analyzing only the most recent fraction of the data stream inorder to avoid outdated results. In this paper; we focus on sampling schemes that samplefrom a sliding window over a recent time interval; such windows are a popular and highlycomprehensible method to model recency. In this setting; the main challenge is to guaranteean upper bound on the space consumption of the sample while using the allotted spaceefficiently at the same time. The difficulty arises from the fact that the number of items in thewindow is unknown in advance and may vary significantly over time; so that the samplingfraction has to be adjusted dynamically. We consider uniform sampling schemes; which …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,37
Efficient In-Memory Indexing with Generalized Prefix Trees.,Matthias Boehm; Benjamin Schlegel; Peter Benjamin Volk; Ulrike Fischer; Dirk Habich; Wolfgang Lehner,Abstract: Efficient data structures for in-memory indexing gain in importance due to (1) theexponentially increasing amount of data;(2) the growing main-memory capacity; and (3) thegap between main-memory and CPU speed. In consequence; there are high performancedemands for in-memory data structures. Such index structures are used—with minorchanges—as primary or secondary indices in almost every DBMS. Typically; tree-based orhash-based structures are used; while structures based on prefix-trees (tries) are neglectedin this context. For tree-based and hash-based structures; the major disadvantages areinherently caused by the need for reorganization and key comparisons. In contrast; themajor disadvantage of trie-based structures in terms of high memory consumption (createdand accessed nodes) could be improved. In this paper; we argue for reconsidering prefix …,BTW,2011,36
The Cube-query-language (CQL) for multidimensional statistical and scientific database systems,Andreas Bauer; Wolfgang Lehner,Abstract The" Cube-Query-Language"(CQL) is a new query language for flexible access toMultidimensional Database Systems. CQL provides a high level user interface for specifyingqueries in the context of multidimensional data analysis. The multidimensional view iswidely accepted for typical decision support requirements like" Online AnalyticalProcessing". CQL directly supports this view and circumvents the problems of formulatingmultiple groupby's and equijoins in a typical SQL-notation. Furthermore; in comparison tocurrently available OLAP-solutions; CQL provides a SQL like query interface; making it easyfor an experienced SQL-programmer; to formulate decision support queries efficiently. Thepaper details the two-step query processing found in the CQL approach:" data querying"and" data presentation". This description of the query language design is accompanied …,*,1997,36
Representing data quality for streaming and static data,Anja Klein; Hong-Hai Do; Gregor Hackenbroich; Marcel Karnstedt; Wolfgang Lehner,In smart item environments; multitude of sensors are applied to capture data about productconditions and usage to guide business decisions as well as production automationprocesses. A big issue in this application area is posed by the restricted quality of sensordata due to limited sensor precision as well as sensor failures and malfunctions. Decisionsderived on incorrect or misleading sensor data are likely to be faulty. The issue of how toefficiently provide applications with information about data quality (DQ) is still an openresearch problem. In this paper; we present a flexible model for the efficient transfer andmanagement of data quality for streaming as well as static data. We propose a data streammetamodel to allow for the propagation of data quality from the sensors up to the respectivebusiness application without a significant overhead of data. Furthermore; we present the …,Data Engineering Workshop; 2007 IEEE 23rd International Conference on,2007,35
BPEL DT—Data-aware extension for data-intensive service applications,Dirk Habich; Sebastian Richly; Steffen Preissler; Mike Grasselt; Wolfgang Lehner; Albert Maier,Abstract Aside from business processes; the service-oriented approach—currently realizedwith Web services and BPEL—should be utilizable for data-intensive applications as well.Fundamentally; data-intensive applications are characterized by (i) a sequence of functionaloperations processing large amounts of data and (ii) the delivery and transformation of hugedata sets between those functional activities. However; for the efficient handling of massivedata sets; a significant amount of data infrastructure is required and the predefined 'byvalue'data semantic within the invocation of Web services and BPEL is not well suited forthis context. To tackle this problem on the BPEL level; we developed a seamless extensionto BPEL—the 'BPEL data transitions'.,*,2008,34
Maintenance of cube automatic summary tables,Wolfgang Lehner; Richard Sidle; Hamid Pirahesh; Roberta Wolfgang Cochrane,Abstract Materialized views (or Automatic Summary Tables—ASTs) are commonly used toimprove the performance of aggregation queries by orders of magnitude. In contrast toregular tables; ASTs are synchronized by the database system. In this paper; we presenttechniques for maintaining cube ASTs. Our implementation is based on IBM DB2 UDB.,ACM SIGMOD Record,2000,34
SAP HANA: The evolution from a modern main-memory data platform to an enterprise application platform,Vishal Sikka; Franz Färber; Anil Goel; Wolfgang Lehner,Abstract SAP HANA is a pioneering; and one of the best performing; data platform designedfrom the grounds up to heavily exploit modern hardware capabilities; including SIMD; andlarge memory and CPU footprints. As a comprehensive data management solution; SAPHANA supports the complete data life cycle encompassing modeling; provisioning; andconsumption. This extended abstract outlines the vision and planned next step of the SAPHANA evolution growing from a core data platform into an innovative enterprise applicationplatform as the foundation for current as well as novel business applications in both on-premise and on-demand scenarios. We argue that only a holistic system design rigorouslyapplying co-design at different levels may yield a highly optimized and sustainable platformfor modern enterprise applications.,Proceedings of the VLDB Endowment,2013,33
Robust real-time query processing with QStream,Sven Schmidt; Thomas Legler; Sebastian Schär; Wolfgang Lehner,Abstract Processing data streams with Quality-of-Service (QoS) guarantees is an emergingarea in existing streaming applications. Although it is possible to negotiate the result qualityand to reserve the required processing resources in advance; it remains a challenge toadapt the DSMS to data stream characteristics which are not known in advance or aredifficult to obtain. Within this paper we present the second generation of our QStream DSMSwhich addresses the above challenge by using a real-time capable operating systemenvironment for resource reservation and by applying an adaptation mechanism if the datastream characteristics change spontaneously.,Proceedings of the 31st international conference on Very large data bases,2005,33
Improving query response time in scientific databases using data aggregation-a case study,Wolfgang Lehner; Thomas Ruf; Michael Teschke,Although most state-of-the-art database systems have no inherent limitations wrt the amountof data they can handle; the huge data quantities typically found in scientific databaseapplications often exceed the feasibility level from a practical point of view when queryperformance is the issue. One theoretically well-known concept of improving query responsetime in scientific database applications is using the categorization and classification facilitiesoften found in scientific computing domains for storing data aggregations that allow tosubstitute expensive access to raw data by the use of stored aggregated values. The resultsof an empirical performance study carried out in the application domain of market researchare presented which substantiate the practical importance of such work. Using real marketresearch data; it is shown that query response time can be shortened in an order of …,Database and Expert Systems Applications; 1996. Proceedings.; Seventh International Workshop on,1996,33
Memory-efficient frequent-itemset mining,Benjamin Schlegel; Rainer Gemulla; Wolfgang Lehner,Abstract Efficient discovery of frequent itemsets in large datasets is a key component ofmany data mining tasks. In-core algorithms---which operate entirely in main memory andavoid expensive disk accesses---and in particular the prefix tree-based algorithm FP-growthare generally among the most efficient of the available algorithms. Unfortunately; theirexcessive memory requirements render them inapplicable for large datasets with manydistinct items and/or itemsets of high cardinality. To overcome this limitation; we propose twonovel data structures---the CFP-tree and the CFP-array---; which reduce memoryconsumption by about an order of magnitude. This allows us to process significantly largerdatasets in main memory than previously possible. Our data structures are based onstructural modifications of the prefix tree that increase compressability; an optimized …,Proceedings of the 14th International Conference on Extending Database Technology,2011,32
On-line analytical processing in distributed data warehouses,Jens Albrecht; Wolfgang Lehner,The concepts of'data warehousing'and'on-line analytical processing'have seen a growinginterest in the research and commercial product community. Today; the trend moves awayfrom complex centralized data warehouses to distributed data marts integrated in a commonconceptual schema. However; as the first part of this paper demonstrates; there are manyproblems and little solutions for large distributed decision support systems in worldwideoperating corporations. After showing the benefits and problems of the distributed approach;this paper outlines possibilities for achieving performance in distributed online analyticalprocessing. Finally; the architectural framework of the prototypical distributed OLAP systemCUBESTAR is outlined.,Database Engineering and Applications Symposium; 1998. Proceedings. IDEAS'98. International,1998,32
Maintaining bernoulli samples over evolving multisets,Rainer Gemulla; Wolfgang Lehner; Peter J Haas,Abstract Random sampling has become a crucial component of modern data managementsystems. Although the literature on database sampling is large; there has been relativelylittle work on the problem of maintaining a sample in the presence of arbitrary insertions anddeletions to the underlying dataset. Most existing maintenance techniques apply either tothe insert-only case or to datasets that do not contain duplicates. In this paper; we provide ascheme that maintains a Bernoulli sample of an underlying multiset in the presence of anarbitrary stream of updates; deletions; and insertions. Importantly; the scheme never needsto access the underlying multiset. Such Bernoulli samples are easy to manipulate; and arewell suited to parallel processing environments. Our method can be viewed as anenhancement of the" counting sample" scheme developed by Gibbons and Matias for …,Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2007,31
SOFORT: A hybrid SCM-DRAM storage engine for fast data recovery,Ismail Oukid; Daniel Booss; Wolfgang Lehner; Peter Bumbulis; Thomas Willhalm,Abstract Storage Class Memory (SCM) has the potential to significantly improve databaseperformance. This potential has been well documented for throughput [4] and response time[25; 22]. In this paper we show that SCM has also the potential to significantly improve restartperformance; a shortcoming of traditional main memory database systems. We presentSOFORT; a hybrid SCM-DRAM storage engine that leverages full capabilities of SCM bydoing away with a traditional log and updating the persisted data in place in smallincrements. We show that we can achieve restart times of a few seconds independent ofinstance size and transaction volume without significantly impacting transaction throughput.,Proceedings of the Tenth International Workshop on Data Management on New Hardware,2014,30
Data-grey-boxweb services in data-centric environments,Dirk Habich; S Richly; M Grasselt,In data-centric environments; for example; in the field of scientific computing; thetransmission of large amount of structured data to Web services is required. In service-oriented environments (SOA); the Simple Object Access Protocol (SOAP) is commonly usedas the main transport protocol. However; the resulting'by value'data transmission approachis not efficiently applicable in data-centric environments. One challenging bottleneck ofSOAP arises from the XML serialization and deserialization when processing large SOAPmessages. In this paper; we present an extended Web service framework which explicitlyconsiders the data aspects of functional Web services. Aside from the possibility to integratespecialized data transfer methods in SOA; this framework allows the efficient and scalabledata handling and processing within Web services. In this case; we combine the …,Web Services; 2007. ICWS 2007. IEEE International Conference on,2007,30
Fptree: A hybrid SCM-DRAM persistent and concurrent b-tree for storage class memory,Ismail Oukid; Johan Lasperas; Anisoara Nica; Thomas Willhalm; Wolfgang Lehner,Abstract The advent of Storage Class Memory (SCM) is driving a rethink of storage systemstowards a single-level architecture where memory and storage are merged. In this context;several works have investigated how to design persistent trees in SCM as a fundamentalbuilding block for these novel systems. However; these trees are significantly slower thanDRAM-based counterparts since trees are latency-sensitive and SCM exhibits higherlatencies than DRAM. In this paper we propose a novel hybrid SCM-DRAM persistent andconcurrent B-Tree; named Fingerprinting Persistent Tree (FPTree) that achieves similarperformance to DRAM-based counterparts. In this novel design; leaf nodes are persisted inSCM while inner nodes are placed in DRAM and rebuilt upon recovery. The FPTree usesFingerprinting; a technique that limits the expected number of in-leaf probed keys to one …,Proceedings of the 2016 International Conference on Management of Data,2016,29
Eris: A NUMA-aware in-memory storage engine for analytical workloads,Thomas Kissinger; Tim Kiefer; Benjamin Schlegel; Dirk Habich; Daniel Molka; Wolfgang Lehner,*,Proceedings of the VLDB Endowment,2014,29
KISS-Tree: smart latch-free in-memory indexing on modern architectures,Thomas Kissinger; Benjamin Schlegel; Dirk Habich; Wolfgang Lehner,Abstract Growing main memory capacities and an increasing number of hardware threads inmodern server systems led to fundamental changes in database architectures. Mostimportantly; query processing is nowadays performed on data that is often completely storedin main memory. Despite of a high main memory scan performance; index structures are stillimportant components; but they have to be designed from scratch to cope with the specificcharacteristics of main memory and to exploit the high degree of parallelism. Currentresearch mainly focused on adapting block-optimized B+-Trees; but these data structureswere designed for secondary memory and involve comprehensive structural maintenancefor updates. In this paper; we present the KISS-Tree; a latch-free in-memory index that isoptimized for a minimum number of memory accesses and a high number of concurrent …,Proceedings of the Eighth International Workshop on Data Management on New Hardware,2012,29
Instant Recovery for Main Memory Databases.,Ismail Oukid; Wolfgang Lehner; Thomas Kissinger; Thomas Willhalm; Peter Bumbulis,ABSTRACT With the emergence of new hardware technologies; new opportunities arise andexisting database architectures have to be rethought to fully exploit them. In particular;recovery mechanisms of current main-memory database systems are tuned to efficientlywork on block-oriented; high-latency storage devices. These devices create a bottleneckduring transaction processing. In this paper; we investigate the opportunities given by theupcoming Storage Class Memory (SCM) technology for database system recoverymechanisms. In contrast to traditional block-oriented devices; SCM is byte-addressable andoffers a latency close to that of DRAM. We propose a novel main-memory databasearchitecture that directly operates in SCM; eliminates the need for logging mechanisms; andprovides a way to trade recovery time with the overall query performance. We …,CIDR,2015,28
Clustering uncertain data with possible worlds,Peter Benjamin Volk; Frank Rosenthal; Martin Hahmann; Dirk Habich; Wolfgang Lehner,The topic of managing uncertain data has been explored in many ways. Differentmethodologies for data storage and query processing have been proposed. As theavailability of management systems grows; the research on analytics of uncertain data isgaining in importance. Similar to the challenges faced in the field of data management;algorithms for uncertain data mining also have a high performance degradation compared totheir certain algorithms. To overcome the problem of performance degradation; the MCDBapproach was developed for uncertain data management based on the possible worldscenario. As this methodology shows significant performance and scalability enhancement;we adopt this method for the field of mining on uncertain data. In this paper; we introduce aclustering methodology for uncertain data and illustrate current issues with this approach …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,28
-Combi-Operator—Database Support for Data Mining Applications,Alexander Hinneburg; Dirk Habich; Wolfgang Lehner,This chapter identifies the data intensive subproblem of aggregating high-dimensional datain all possible low-dimensional projections; which occurs in several established data miningtechniques. It explores that existing OLAP SQL-extensions are insufficient for high-dimensional data and proposes a new SQL-operator; which seamlessly fits into the set ofexisting OLAP group by operators. The main drawbacks of the existing operators are (1) verylarge query size and (2) suboptimal performance. This chapter proposes efficientimplementations for the operator; which take the limited resources of main memory intoaccount. It demonstrates on a number of real and synthetic data sets that for the identifiedsubproblem; the new implementations yield a large speedup over existing methods built incommercially available database systems.COMBI-Operator-Database Support for Data …,*,2003,28
DrillBeyond: enabling business analysts to explore the web of open data,Julian Eberius; Maik Thiele; Katrin Braunschweig; Wolfgang Lehner,Abstract Following the Open Data trend; governments and public agencies have startedmaking their data available on the Web and established platforms such as data. gov or data.un. org. These Open Data platforms provide a huge amount of data for various topics suchas demographics; transport; finance or health in various data formats. One typical usagescenario for this kind of data is their integration into a database or data warehouse in orderto apply data analytics. However; in today's business intelligence tools there is an evidentlack of support for so-called situational or ad-hoc data integration. In this demonstration wewill therefore present DrillBeyond; a novel database and information retrieval engine whichallows users to query a local database as well as the Web of Open Data in a seamless andintegrated way with standard SQL. The audience will be able to pose queries to our …,Proceedings of the VLDB Endowment,2012,25
CROSS-DB: a feature-extended multidimensional data model for statistical and scientific databases,Wolfgang Lehner; Thomas Ruf; Michael Teschke,Abstract Statistical and scientifi computing applications exhibit characteristics that arefinahmentally different fmm classical database system application domains. The CROSS-DBdata model presented in this paper is optimized for use in such applications by providingadvanced data mo&lling methods and application-oriented query fmilities; thus providingafiamework for optimized alzta nuuuzgement procedures. CROSS-DB (which stands forClassifiationoriented Redundancy-based Optimization of Statistical and ScientificDataBases) is based on a multidimenswnal data view. The model differs j? om otherapproaches by o~ ering two complementary rnechanisrnsfor structuring qualifyinginformation; classification and feature description. Using these ntechanisms results in anormalized; low-dimensional database schema which ensures both modelling …,Proceedings of the fifth international conference on Information and knowledge management,1996,25
Integrated resource management for data stream systems,Henrike Berthold; Sven Schmidt; Wolfgang Lehner; Claude-Joachim Hamann,Abstract Data stream systems have to deal with massive data volumes. To perform severalqueries in parallel or to perform even a single query; resources must be planned carefullyand the resulting quality-of-service (QoS) is lower than the best one. Typical QoS measuresare the output delay and the amount of data in the stream used for the processing. In thispaper; we introduce a model which allows to describe stream operators and the streamsbetween the operators of an operator graph belonging to a stream query. The model allowsus to calculate the resources consumed by a query graph given a certain result quality.Furthermore; it can be used to determine in advance if the quality-of-service requirement of agiven query can be met with the actual available system resources. This model is the basisfor building QoS-guaranteeing systems.,Proceedings of the 2005 ACM symposium on Applied computing,2005,24
Cache-efficient aggregation: Hashing is sorting,Ingo Müller; Peter Sanders; Arnaud Lacurie; Wolfgang Lehner; Franz Färber,Abstract For decades researchers have studied the duality of hashing and sorting for theimplementation of the relational operators; especially for efficient aggregation. Dependingon the underlying hardware and software architecture; the specifically implementedalgorithms; and the data sets used in the experiments; different authors came to differentconclusions about which is the better approach. In this paper we argue that in terms of cacheefficiency; the two paradigms are actually the same. We support our claim by showing thatthe complexity of hashing is the same as the complexity of sorting in the external memorymodel. Furthermore we make the similarity of the two approaches obvious by designing analgorithmic framework that allows to switch seamlessly between hashing and sorting duringexecution. The fact that we mix hashing and sorting routines in the same algorithmic …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,23
Scalable frequent itemset mining on many-core processors,Benjamin Schlegel; Tomas Karnagel; Tim Kiefer; Wolfgang Lehner,Abstract Frequent-itemset mining is an essential part of the association rule mining process;which has many application areas. It is a computation and memory intensive task with manyopportunities for optimization. Many efficient sequential and parallel algorithms wereproposed in the recent years. Most of the parallel algorithms; however; cannot cope with thehuge number of threads that are provided by large multiprocessor or many-core systems. Inthis paper; we provide a highly parallel version of the well-known Eclat algorithm. It runs onboth; multiprocessor systems and many-core coprocessors; and scales well up to a verylarge number of threads---244 in our experiments. To evaluate mcEclat's performance; weconducted many experiments on realistic datasets. mcEclat achieves high speedups of up to11.5 x and 100x on a 12-core multiprocessor system and a 61-core Xeon Phi many-core …,Proceedings of the Ninth International Workshop on Data Management on New Hardware,2013,22
Pathways to servers of the future: highly adaptive energy efficient computing (HAEC),Gerhard Fettweis; Wolfgang Nagel; Wolfgang Lehner,Abstract The Special Session on" Pathways to Servers of the Future" outlines a newresearch program set up at Technische Universität Dresden addressing the increasingenergy demand of global internet usage and the resulting ecological impact of it. Theprogram pursues a novel holistic approach that considers hardware as well as softwareadaptivity to significantly increase energy efficiency; while suitably addressing applicationdemands. The session presents the research challenges and industry perspective.,Proceedings of the conference on design; automation and test in Europe,2012,22
Maintaining bounded-size sample synopses of evolving datasets,Rainer Gemulla; Wolfgang Lehner; Peter J Haas,Abstract Perhaps the most flexible synopsis of a database is a uniform random sample of thedata; such samples are widely used to speed up processing of analytic queries and data-mining tasks; enhance query optimization; and facilitate information integration. The ability tobound the maximum size of a sample can be very convenient from a system-design point ofview; because the task of memory management is simplified; especially when manysamples are maintained simultaneously. In this paper; we study methods for incrementallymaintaining a bounded-size uniform random sample of the items in a dataset in thepresence of an arbitrary sequence of insertions and deletions. For" stable" datasets whosesize remains roughly constant over time; we provide a novel sampling scheme; called"random pairing"(RP); that maintains a bounded-size uniform sample by using newly …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,22
A decathlon in multidimensional modeling: Open issues and some solutions,Wolfgang Hümmer; Wolfgang Lehner; Andreas Bauer; Lutz Schlesinger,Abstract The concept of multidimensional modeling has proven extremely successful in thearea of Online Analytical Processing (OLAP) as one of many applications running on top of adata warehouse installation. Although many different modeling techniques expressed inextended multidimensional data models were proposed in the recent past; we feel that manyhot issues are not properly reflected. In this paper we address ten common problemsreaching from defects within dimensional structures over multidimensional structures to newanalytical requirements and more.,International Conference on Data Warehousing and Knowledge Discovery,2002,22
Experimental Evaluation of NUMA Effects on Database Management Systems.,Tim Kiefer; Benjamin Schlegel; Wolfgang Lehner,Abstract: NUMA systems with multiple CPUs and large main memories are common today.Consequently; database management systems (DBMSs) in data centers are deployed onNUMA systems. They serve a wide range of database use-cases; single large applicationshaving high performance needs as well as many small applications that are consolidated onone machine to save resources and increase utilization. Database servers often show anatural partitioning in the data that is accessed; eg; caused by multiple applicationsaccessing only their data. Knowledge about these partitions can be used to allocate adatabase's memory on the different nodes accordingly: a strategy that increases memorylocality and reduces expensive communication between CPUs. In this work; we show thatpartitioning a database's memory with respect to the data's access patterns can improve …,BTW,2013,21
Towards agile BI: applying in-memory technology to data warehouse architectures.,Tobias Knabke; Sebastian Olbrich; W Lehner; G Piller,Abstract: Confronted with increased market dynamics and hence frequently changingsystem environments; today's decision support systems face the demand to respect suchdevelopments. Developing and maintaining so called agile business intelligence (BI)systems is a major challenge for information technology and organizations; since theunderlying assumption of BI is to support mostly long term decisions in a non-volatile andintegrated way. Hence; current approaches towards agility often focus on shortenedimplementation times using agile methods like Extreme Programming (XP) or Scrum. Giventhe existing BI architectures and environments; these methods are not fully applicable. Thus;this paper focuses on the architecture of agile BI. To achieve this goal; we derive criteria foragile BI. Then; a semi-virtual data warehouse architecture with an in-memory database …,IMDM,2011,21
Database as a service (DBaaS),Wolfgang Lehner; Kai-Uwe Sattler,Modern Web or “Eternal-Beta” applications necessitate a flexible and easy-to-use datamanagement platform that allows the evolutionary development of databases andapplications. The classical approach of relational database systems following strictly theACID properties has to be extended by an extensible and easy-to-use persistency layer withspecialized DB features. Using the underlying concept of Software as a Service (SaaS) alsoenables an economic advantage based on the “economy of the scale “; where applicationand system environments only need to be provided once but can be used by thousands ofusers. Within this tutorial; we are looking at the current state-of-the-art from differentperspectives. We outline foundations and techniques to build database services based onthe SaaS-paradigm. We discuss requirements from a programming perspective; show …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,21
Cardinality estimation in database systems using sample views,*,A system and method that facilitates and effectuates estimating the result of performing adata analysis operation on a set of data. Employing an approximation of the data analysisoperation on a statistically valid random sample view of the data allows for a statisticallyaccurate estimate of the result to be obtained. Sequential sampling in the view enables theapproximated operation to evaluate accuracy conditions at intervals during the scan of thesample view and obtain the estimated result without having to scan the entire sample view.Feedback regarding the accuracy of the estimated result can be captured when the dataanalysis operation is performed against the set of data. Process control techniques can beemployed with the feedback to maintain the statistical validity of the sample view.,*,2008,21
Web-scale data management for the cloud,Wolfgang Lehner; Kai-Uwe Sattler; Kai-Uwe Sattler,The efficient management of data is a core asset of almost every organization. Data iscollected; stored; manipulated; and analysed to drive the business and derive support for thedecision making process. Establishing and running the data management platform within alarger organization is a complex; time-and budget-intensive tasks. Not only do datamanagement systems have to be installed; deployed; and populated with different sets ofdata. The data management platform of an organization has to constantly maintained on atechnical as well as on a content level. Changing applications and business requirementshave to be reflected within the technical setup; new software versions have to be installed;hardware has to be exchanged etc. Although the benefit of an efficient data managementplatform can be enormous not only in terms of a direct controlling of the business or a …,*,2013,20
K-ary search on modern processors,Benjamin Schlegel; Rainer Gemulla; Wolfgang Lehner,Abstract This paper presents novel tree-based search algorithms that exploit the SIMDinstructions found in virtually all modern processors. The algorithms are a natural extensionof binary search: While binary search performs one comparison at each iteration; therebycutting the search space in two halves; our algorithms perform k comparisons at a time andthus cut the search space into k pieces. On traditional processors; this so-called k-ary searchprocedure is not beneficial because the cost increase per iteration offsets the cost reductiondue to the reduced number of iterations. On modern processors; however; multiple scalaroperations can be executed simultaneously; which makes k-ary search attractive. In thispaper; we provide two different search algorithms that differ in terms of efficiency andmemory access patterns. Both algorithms are first described in a platform independent …,Proceedings of the Fifth International Workshop on Data Management on New Hardware,2009,20
Dipbench toolsuite: A framework for benchmarking integration systems,Matthias Bohm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,So far the optimization of integration processes between heterogeneous data sources is stillan open challenge. A first step towards sufficient techniques was the specification of auniversal benchmark for integration systems. This DIPBench allows to compare solutionsunder controlled conditions and would help generate interest in this research area.However; we see the requirement for providing a sophisticated toolsuite in order to minimizethe effort for benchmark execution. This demo illustrates the use of the DIPBench toolsuite.We show the macro-architecture as well as the micro-architecture of each tool. Furthermore;we also present the first reference benchmark implementation using a federated DBMS.Thereby; we discuss the impact of the defined benchmark scale factors. Finally; we want togive guidance on how to benchmark other integration systems and how to extend the …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,19
Set-derivability of multidimensional aggregates,Jens Albrecht; Holger Günzel; Wolfgang Lehner,Abstract A common optimization technique in data warehouse environments is the use ofmaterialized aggregates. Aggregate processing becomes complex; if partitions ofaggregates or queries are materialized and reused later. Most problematic are theimplication problems regarding the restriction predicates. We show that in the presence ofhierarchies in a multidimensional environment an efficient algorithm can be given toconstruct-or to derive-an aggregate from one or more overlapping materialized aggregatepartitions (set-derivability).,International Conference on Data Warehousing and Knowledge Discovery,1999,19
Towards scalable real-time analytics: an architecture for scale-out of OLxP workloads,Anil K Goel; Jeffrey Pound; Nathan Auch; Peter Bumbulis; Scott MacLean; Franz Färber; Francis Gropengiesser; Christian Mathis; Thomas Bodner; Wolfgang Lehner,Abstract We present an overview of our work on the SAP HANA Scale-out Extension; a noveldistributed database architecture designed to support large scale analytics over real-timedata. This platform permits high performance OLAP with massive scale-out capabilities;while concurrently allowing OLTP workloads. This dual capability enables analytics overreal-time changing data and allows fine grained user-specified service level agreements(SLAs) on data freshness. We advocate the decoupling of core database components suchas query processing; concurrency control; and persistence; a design choice made possibleby advances in high-throughput low-latency networks and storage devices. We provide fullACID guarantees and build on a logical timestamp mechanism to provide MVCC-basedsnapshot isolation; while not requiring synchronous updates of replicas. Instead; we use …,Proceedings of the VLDB Endowment,2015,18
GRAPHITE: an extensible graph traversal framework for relational database management systems,Marcus Paradies; Wolfgang Lehner; Christof Bornhövd,Abstract Graph traversals are a basic but fundamental ingredient for a variety of graphalgorithms and graph-oriented queries. To achieve the best possible query performance;they need to be implemented at the core of a database management system that aims atstoring; manipulating; and querying graph data. Increasingly; modern business applicationsdemand native graph query and processing capabilities for enterprise-critical operations ondata stored in relational database management systems. In this paper we propose anextensible graph traversal framework (GRAPHITE) as a central graph processingcomponent on a common storage engine inside a relational database management system.We study the influence of the graph topology on the execution time of graph traversals andderive two traversal algorithm implementations specialized for different graph topologies …,Proceedings of the 27th International Conference on Scientific and Statistical Database Management,2015,18
In-Memory-Datenmanagement in betrieblichen Anwendungssystemen,Peter Loos; Jens Lechtenbörger; Gottfried Vossen; Alexander Zeier; Jens Krüger; Jürgen Müller; Wolfgang Lehner; Donald Kossmann; Benjamin Fabian; Oliver Günther; Robert Winter,In-Memory-Datenbanken halten den gesamten Datenbestand permanent im Hauptspeichervor. Somit können lesende Zugriffe weitaus schneller erfolgen als bei traditionellenDatenbanksystemen; da keine I/O-Zugriffe auf die Festplatte erfolgen müssen. Fürschreibende Zugriffe wurden Mechanismen entwickelt; die Persistenz und somitTransaktionssicherheit gewährleisten. In-Memory-Datenbanken werden seit geraumer Zeitentwickelt und haben sich in speziellen Anwendungen bewährt. Mit zunehmenderSpeicherdichte von DRAM-Bausteinen sind Hardwaresysteme wirtschaftlich erschwinglich;deren Hauptspeicher einen kompletten betrieblichen Datenbestand aufnehmen können.Somit stellt sich die Frage; ob In-Memory-Datenbanken auch in betrieblichenAnwendungssystemen eingesetzt werden können. Hasso Plattner; der mit HANA eine In …,Wirtschaftsinformatik,2011,18
Context-aware parameter estimation for forecast models in the energy domain,Lars Dannecker; Robert Schulze; Matthias Böhm; Wolfgang Lehner; Gregor Hackenbroich,Abstract Continuous balancing of energy demand and supply is a fundamental prerequisitefor the stability and efficiency of energy grids. This balancing task requires accurateforecasts of future electricity consumption and production at any point in time. For thispurpose; database systems need to be able to rapidly process forecasting queries and toprovide accurate results in short time frames. However; time series from the electricitydomain pose the challenge that measurements are constantly appended to the time series.Using a naive maintenance approach for such evolving time series would mean a re-estimation of the employed mathematical forecast model from scratch for each newmeasurement; which is very time consuming. We speed-up the forecast model maintenanceby exploiting the particularities of electricity time series to reuse previously employed …,International Conference on Scientific and Statistical Database Management,2011,18
Linked bernoulli synopses: Sampling along foreign keys,Rainer Gemulla; Philipp Rösch; Wolfgang Lehner,Abstract Random sampling is a popular technique for providing fast approximate queryanswers; especially in data warehouse environments. Compared to other types of synopses;random sampling bears the advantage of retaining the dataset's dimensionality; it alsoassociates probabilistic error bounds with the query results. Most of the available samplingtechniques focus on table-level sampling; that is; they produce a sample of only a singledatabase table. Queries that contain joins over multiple tables cannot be answered withsuch samples because join results on random samples are often small and skewed. On thecontrary; schema-level sampling techniques by design support queries containing joins. Inthis paper; we introduce Linked Bernoulli Synopses; a schema-level sampling schemebased upon the well-known Join Synopses. Both schemes rely on the idea of maintaining …,International Conference on Scientific and Statistical Database Management,2008,18
Error-aware density-based clustering of imprecise measurement values,Dirk Habich; Peter B Volk; Wolfgang Lehner; Ralf Dittmann; Clemens Utzny,Manufacturing process development is under constant pressure to achieve a good yield forstable processes. The development of new technologies; especially in the field ofphotomask and semiconductor development; is at its phys-ical limits. In this area; data; egsensor data; has to be collected and analyzed for each process in order to ensure processquality. With increasing complexity of manufactur-ing processes; the volume of data that hasto be evaluated rises accordingly. The complexity and data volume exceeds the possibility ofa manual data analysis. At this point; data mining techniques become interesting. Theapplication of current techniques is complex because most of the data is captured withsensor measurement tools. Therefore; every measured value contains a specific error. In thispaper we propose an error-aware extension of the density-based al-gorithm DBSCAN …,Data Mining Workshops; 2007. ICDM Workshops 2007. Seventh IEEE International Conference on,2007,18
Precision Dairy Farming–integrativer Ansatz für eine nachhaltige Milcherzeugung,Joachim Spilke; Wolfgang Büscher; Reiner Doluschitz; Rolf-Dieter Fahr; Wolfgang Lehner,Der Agrar-und Ernährungssektor steht heute mehr denn je vor der Herausforderung einerdauerhaften Verbindung von Verbraucher-und Tierschutz; Qualitätssicherung sowieNachhaltigkeit. Die gesellschaftliche Stellung und Akzeptanz der Landwirtschaft hängenwesentlich davon ab; wie diese Herausforderungen gemeistert werden. Für dieMilcherzeugung gilt das wegen ihrer dominanten betrieblichen Wettbewerbskraft und dergroßen Bedeutung für den Agrarsektor in besonderem Maße. Der aus Sicht desTierschutzes ebenso wie aus betriebs-und arbeitswirtschaftlichen Gründen zwingendeÜbergang zur Laufstallhaltung; aber auch tendenziell steigende Herdengrößen; dürfen nichtzu einer Abnahme der tierindividuellen Betreuungsintensität führen. Vielmehr zwingen vorallem die derzeit hohe Frequenz von Mastitis und hiermit verbundene tierschutzrelevante …,Zeitschrift für Agrarinformatik,2003,18
A plan for OLAP,Bernhard Jaecksch; Wolfgang Lehner; Franz Faerber,Abstract So far; data warehousing has often been discussed in the light of complex OLAPqueries and as reporting facility for operative data. We argue that business planning as ameans to generate plan data is an equally important cornerstone of a data warehousesystem; and we propose it to be a first-class citizen within an OLAP engine. We introduce anabstract model describing relevant aspects of the planning process in general and therequirements it poses to a planning engine. Furthermore; we show that business planninglends itself well to parallelization and benefits from a column-store much like traditionalOLAP does. We then develop a physical model specifically targeted at a highly parallelcolumn-store; and with our implementation; we show nearly linear scaling behavior.,Proceedings of the 13th International Conference on Extending Database Technology,2010,17
Supporting the ETL-process by Web Service technologies,Lutz Schlesinger; Florian Irmert; Wolfgang Lehner,Extracting data from heterogeneous data sources and transferring data into the datawarehouse system is one of the most cost intensive tasks in setting up and operating a datawarehouse. Special tools may be used to connect different sources and target systems. Inthis paper; we propose an architecture which enables the flexible integration of data sourcesinto any target database system. The approach is based on the idea of splitting the classicalwrapping module into a source specific and a target specific part and establishing thecommunication between these components based on Web Service technology. We describethe general architecture; the use of Web Service technology to describe and dynamicallyintegrate participating data sources and the deployment within a specific database system.,International Journal of Web and Grid Services,2005,17
Efficiently synchronizing multidimensional schema data,Lutz Schlesinger; Andreas Bauer; Wolfgang Lehner; G Ediberidze; M Gutzmann,Abstract Most existing concepts in data warehousing provide a central data¿ base systemstoring gathered raw data and redundantly computed materialized views. While in currentsystem architectures client tools are sending queries to a central data warehouse systemand are only used to graphically present the result; the steady rise in power of personalcomputers and the expansion of network bandwidth makes it possible to store replicatedparts of the data warehouse at the client thus saving network bandwidth and utilizing localcom¿ puting power. Within such a scenario a-potentially mobile-client does not need to beconnected to a central server while performing local analyses. Although this scenario seemsattractive; several pro¿ blems arise by introducing such an architecture: For exampleschema data could be changed or new fact data could be available. This paper is …,Proceedings of the 4th ACM international workshop on Data warehousing and OLAP,2001,17
SLACID-sparse linear algebra in a column-oriented in-memory database system,David Kernert; Frank Köhler; Wolfgang Lehner,Abstract Scientific computations and analytical business applications are often based onlinear algebra operations on large; sparse matrices. With the hardware shift of the primarystorage from disc into memory it is now feasible to execute linear algebra queries directly inthe database engine. This paper presents and compares different approaches of storingsparse matrices in an in-memory column-oriented database system. We show that a systemlayout derived from the compressed sparse row representation integrates well with acolumnar database design and that the resulting architecture is moreover amenable to awide range of non-numerical use cases when dictionary encoding is used. Dynamic matrixmanipulation operations; like online insertion or deletion of elements; are not covered bymost linear algebra frameworks. Therefore; we present a hybrid architecture that consists …,Proceedings of the 26th International Conference on Scientific and Statistical Database Management,2014,16
An application-specific instruction set for accelerating set-oriented database primitives,Oliver Arnold; Sebastian Haas; Gerhard Fettweis; Benjamin Schlegel; Thomas Kissinger; Wolfgang Lehner,Abstract The key task of database systems is to efficiently manage large amounts of data. Ahigh query throughput and a low query latency are essential for the success of a databasesystem. Lately; research focused on exploiting hardware features like superscalar executionunits; SIMD; or multiple cores to speed up processing. Apart from these softwareoptimizations for given hardware; even tailor-made processing circuits running on FPGAsare built to run mostly stateless query plans with incredibly high throughput. A similar idea;which was already considered three decades ago; is to build tailor-made hardware like adatabase processor. Despite their superior performance; such application-specificprocessors were not considered to be beneficial because general-purpose processorseventually always caught up so that the high development costs did not pay off. In this …,Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data,2014,16
First steps towards a systematical optimized strategy for solar energy supply forecasting,Robert Ulbricht; Ulrike Fischer; Wolfgang Lehner; Hilko Donker,Abstract. The capacity of renewable energy sources constantly increases world-wide andchallenges the maintenance of the electric balance between power demand and supply. Toallow for a better integration of solar energy supply into the power grids; a lot of researchwas dedicated to the development of precise forecasting approaches. However; there is stillno straightforward and easy-to-use recommendation for a standardized forecasting strategy.In this paper; a classification of solar forecasting solutions proposed in the literature isprovided for both weather-and energy forecast models. Subsequently; we describe our ideaof a standardized forecasting process and the typical parameters possibly influencing theselection of a specific model. We discuss model combination as an optimization option andevaluate this approach comparing two statistical algorithms in a case study. Finally; we …,Proceedings of the 2013 ECML/PKDD International Workshop on Data Analytics for Renewable Energy Integration (DARE),2013,16
Data mining in a multidimensional environment,Holger Günzel; Jens Albrecht; Wolfgang Lehner,Abstract Data Mining and Data Warehousing are two hot topics in the database researcharea. Until recently; conventional data mining algorithms were primarily developed for arelational environment. But a data warehouse database is based on a multidimensionalmodel. In our paper we apply this basis for a seamless integration of data mining in themultidimensional model for the example of discovering association rules. Furthermore; wepropose this method as a userguided technique because of the clear structure both of modeland data. We present both the theoretical basis and efficient algorithms for data mining inthe multidimensional data model. Our approach uses directly the requirements ofdimensions; classifications and sparsity of the cube. Additionally we give heuristics foroptimizing the search for rules.,East European Conference on Advances in Databases and Information Systems,1999,16
An Architecture for Distributed OLAP,Jens Albrecht; W Lehner; H Gunzel,CERN Accelerating science. Sign in; Directory. CERN Document Server. Access articles; reportsand multimedia content in HEP. Main menu. Search; Submit; Help; Personalize: Your alerts; Yourbaskets; Your comments; Your searches. Home > An Architecture for Distributed OLAP. Information;Discussion (0); Files. Article. Title; An Architecture for Distributed OLAP. Author(s); Albrecht; J ;Gunzel; H ; Lehner; W. In: International Conference on Parallel and Distributed ProcessingTechniques and Applications; Las Vegas; NV; USA; 13 - 16 Jul 1998; pp.1482-1488. Back tosearch. Record created 2000-09-14; last modified 2016-06-30. Similar records. Add to personalbasket; Export as BibTeX; MARC; MARCXML; DC; EndNote; NLM; RefWorks. Share on social …,*,1998,16
Forecasting the data cube: A model configuration advisor for multi-dimensional data sets,Ulrike Fischer; Christopher Schildt; Claudio Hartmann; Wolfgang Lehner,Forecasting time series data is crucial in a number of domains such as supply chainmanagement and display advertisement. In these areas; the time series data to forecast istypically organized along multiple dimensions leading to a high number of time series thatneed to be forecasted. Most current approaches focus only on selection and optimizing aforecast model for a single time series. In this paper; we explore how we can utilize timeseries at different dimensions to increase forecast accuracy and; optionally; reduce modelmaintenance overhead. Solving this problem is challenging due to the large space ofpossibilities and possible high model creation costs. We propose a model configurationadvisor that automatically determines the best set of models; a model configuration; for agiven multi-dimensional data set. Our approach is based on a general process that …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,15
F2DB: the flash-forward database system,Ulrike Fischer; Frank Rosenthal; Wolfgang Lehner,Forecasts are important to decision-making and risk assessment in many domains. Sincecurrent database systems do not provide integrated support for forecasting; it is usually doneoutside the database system by specially trained experts using forecast models. However;integrating model-based forecasting as a first-class citizen inside a DBMS speeds up theforecasting process by avoiding exporting the data and by applying database-relatedoptimizations like reusing created forecast models. It especially allows subsequentprocessing of forecast results inside the database. In this demo; we present our prototypeF2DB based on PostgreSQL; which allows for transparent processing of forecast queries.Our system automatically takes care of model maintenance when the underlying datasetchanges. In addition; we offer optimizations to save maintenance costs and increase …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,15
Method for maintaining a sample synopsis under arbitrary insertions and deletions,*,A method of incrementally maintaining a stable; bounded; uniform random sample S from adataset R; in the presence of arbitrary insertions and deletions to the dataset R; and withoutaccesses to the dataset R; comprises a random pairing method in which deletions areuncompensated until compensated by a subsequent insertion (randomly paired to thedeletion) by including the insertion's item into S if and only if the uncompensated deletion'sitem was removed from S (ie; was in S so that it could be removed). A method for resizing asample to a new uniform sample of increased size while maintaining a bound on the samplesize and balancing cost between dataset accesses and transactions to the dataset is alsodisclosed. A method for maintaining uniform; bounded samples for a dataset in the presenceof growth in size of the dataset is additionally disclosed.,*,2010,15
Using cloud technologies to optimize data-intensive service applications,Dirk Habich; Wolfgang Lehner; Sebastian Richly; Uwe Assmann,The role of data analytics increases in several application domains to cope with the largeamount of captured data. Generally; data analytics are data-intensive processes; whoseefficient execution is a challenging task. Each process consists of a collection of relatedstructured activities; where huge data sets have to be exchanged between several looselycoupled services. The implementation of such processes in a service-oriented environmentoffers some advantages; but the efficient realization of data flows is difficult. Therefore; weuse this paper to propose a novel SOA-aware approach with a special focus on the dataflow. The tight interaction of new cloud technologies with SOA technologies enables us tooptimize the execution of data-intensive service applications by reducing the data exchangetasks to a minimum. Fundamentally; our core concept to optimize the data flows is found …,Cloud Computing (CLOUD); 2010 IEEE 3rd International Conference on,2010,15
Evaluation of load scheduling strategies for real-time data warehouse environments,Maik Thiele; Wolfgang Lehner,Abstract The demand for so-called living or real-time data warehouses is increasing in manyapplication areas; including manufacturing; event monitoring and telecommunications. Infields like these; users normally expect short response times for their queries and highfreshness for the requested data. However; it is truly challenging to meet both requirementsat the same time because of the continuous flow of write-only updates and read-only queriesas well as the latency caused by arbitrarily complex ETL processes. To optimize the updateflow in terms of data freshness maximization and load minimization; we propose twoalgorithms—local and global scheduling—that operate on the basis of different systeminformation. We want to discuss the benefits and drawbacks of both approaches in detailand derive recommendations regarding the optimal scheduling strategy for any given …,International Workshop on Business Intelligence for the Real-Time Enterprise,2009,15
Towards Self-Optimization of Message Transformation Processes.,Matthias Böhm; Dirk Habich; Uwe Wloka; Jürgen Bittner; Wolfgang Lehner,Abstract. The Message Transformation Model (MTM); for modeling complex messagetransformation processes in data centric application scenarios; provides strong capabilitiesfor describing the data and control flow; transactional behavior and even the interaction withexternal systems. Thus; this general model could be used for different integration platformslike EAI-Servers; Message-Brokers and Subscription-Systems; as well as for ETL-Tools. Inthis paper; we describe self-optimization strategies for MTM processes to determine anoptimal executable process. Our proposed strategies can be distinguished into rule-basedand workload-based techniques. Aside from theoretical consideration; we describeimplementation aspects within the integration platform Trans-ConnectR©. Furthermore; wepresent some first evaluation results.,ADBIS Research Communications,2007,15
Source-aware Join Strategies of Sensor Data Streams.,Sven Schmidt; Marc Fiedler; Wolfgang Lehner,Abstract The growing number of data produced as streams requires sophisticated datastream processing. One challenge comprises join operations on data streams. Basicconcepts are derived from data base management systems and are extended in terms ofwindow techniques to handle infinite data streams. Within our paper; we show that existingwindow-based join algorithms are not sufficient for processing data streams of varioussensor data sources. As a solution; we propose novel join strategies; which are orientedtowards three basic data stream classes. Thereby; we focus on the temporal relationbetween the stream tuples and introduce the bandwidth-based stream resampling. A real-world example for data streams originating from a casting process accompanies our paper.,SSDBM,2005,15
Top-k entity augmentation using consistent set covering,Julian Eberius; Maik Thiele; Katrin Braunschweig; Wolfgang Lehner,Abstract Entity augmentation is a query type in which; given a set of entities and a largecorpus of possible data sources; the values of a missing attribute are to be retrieved. State ofthe art methods return a single result that; to cover all queried entities; is fused from apotentially large set of data sources. We argue that queries on large corpora ofheterogeneous sources using information retrieval and automatic schema matchingmethods can not easily return a single result that the user can trust; especially if the result iscomposed from a large number of sources that user has to verify manually. We thereforepropose to process these queries in a Top-k fashion; in which the system produces multipleminimal consistent solutions from which the user can choose to resolve the uncertainty of thedata sources and methods used. In this paper; we introduce and formalize the problem of …,Proceedings of the 27th International Conference on Scientific and Statistical Database Management,2015,14
The HELLS-join: a heterogeneous stream join for extremely large windows,Tomas Karnagel; Dirk Habich; Benjamin Schlegel; Wolfgang Lehner,Abstract Upcoming processors are combining different computing units in a tightly-coupledapproach using a unified shared memory hierarchy. This tightly-coupled combination leadsto novel properties with regard to cooperation and interaction. This paper demonstrates theadvantages of those processors for a stream-join operator as an important data-intensiveexample. In detail; we propose our HELLS-Join approach employing all heterogeneousdevices by outsourcing parts of the algorithm on the appropriate device. Our HELLS-Joinperforms better than CPU stream joins; allowing wider time windows; higher streamfrequencies; and more streams to be joined as before.,Proceedings of the Ninth International Workshop on Data Management on New Hardware,2013,14
Enhancing Named Entity Extraction by Effectively Incorporating the Crowd.,Katrin Braunschweig; Maik Thiele; Julian Eberius; Wolfgang Lehner,Abstract: Named entity extraction is an established research area in the field of informationextraction. When tailored to a specific domain and with sufficient pre-labeled training data;state-of-the-art extraction algorithms have achieved near human performance. However;when presented with semi-structured data; informal text or unknown domains where trainingdata is not available; extraction results can deteriorate significantly. Recent research hasfocused on crowdsourcing as an alternative to automatic named entity extraction or as a toolto generate the required training data. While humans easily adapt to semi-structured dataand informal style; a crowd-based approach also introduces new issues due to monetarycosts or spamming. We address these issues by combining automatic named entityextraction algorithms with crowdsourcing into a hybrid approach. We have conducted a …,BTW Workshops,2013,14
Real-time business intelligence in the MIRABEL smart grid system,Ulrike Fischer; Dalia Kaulakienė; Mohamed E Khalefa; Wolfgang Lehner; Torben Bach Pedersen; Laurynas Šikšnys; Christian Thomsen,Abstract The so-called smart grid is emerging in the energy domain as a solution to providea stable; efficient and sustainable energy supply accommodating ever growing amounts ofrenewable energy like wind and solar in the energy production. Smart grid systems arehighly distributed; manage large amounts of energy related data; and must be able to reactrapidly (but intelligently) when conditions change; leading to substantial real-time businessintelligence challenges. This paper discusses these challenges and presents datamanagement solutions in the European smart grid project MIRABEL. These solutionsinclude real-time time series forecasting; real-time aggregation of the flexibilities in energysupply and demand; managing subscriptions for forecasted and flexibility data; efficientstorage of time series and flexibilities; and real-time analytical query processing spanning …,International Workshop on Business Intelligence for the Real-Time Enterprise,2012,14
Flexible Information Management; Exploration and Analysis in SAP HANA.,Christof Bornhövd; Robert Kubis; Wolfgang Lehner; Hannes Voigt; Horst Werner,Abstract: Data management is not limited anymore to towering data silos full of perfectlystructured; well integrated data. Today; we need to process and make sense of data fromdiverse sources (public and on-premise); in different application contexts; with differentschemas; and with varying degrees of structure and quality. Because of the necessity todefine a rigid data schema upfront; fixed-schema database systems are not a good fit forthese new scenarios. However; schema is still essential to give data meaning and toprocess data purposefully. In this paper; we describe a schema-flexible database systemthat combines a flexible data model with a powerful data query; analysis; and manipulationlanguage that provides both required schema information and the flexibility required formodern information processing and decision support.,DATA,2012,14
GPU-Based Speculative Query Processing for Database Operations.,Peter Benjamin Volk; Dirk Habich; Wolfgang Lehner,ABSTRACT With an increasing amount of data and user demands for fast query processing;the optimization of database operations continues to be a challenging task. A commonoptimization method is to leverage parallel hardware architectures. With the introduction ofgeneral-purpose GPU computing; massively parallel hardware has become available withincommodity hardware. To efficiently exploit this technology; we introduce the method ofspeculative query processing. This speculative query processing works on; but is not limitedto; a prefix tree structure to efficiently support heavily used database index operations.Fundamentally; our developed approach traverse a prefix tree structure in a speculative;parallel way instead of a step-by-step traversing. To show the benefits and opportunities ofour novel approach; we present an exhaustive evaluation on a graphical processing unit.,ADMS@ VLDB,2010,14
Building the dresden web table corpus: A classification approach,Julian Eberius; Katrin Braunschweig; Markus Hentsch; Maik Thiele; Ahmad Ahmadov; Wolfgang Lehner,In recent years; researchers have recognized relational tables on the Web as an importantsource of information. To assist this research we developed the Dresden Web TablesCorpus (DWTC); a collection of about 125 million data tables extracted from the CommonCrawl (CC) which contains 3.6 billion web pages and is 266TB in size. As the vast majorityof HTML tables are used for layout purposes and only a small share contains genuine tableswith different surface forms; accurate table detection is essential for building a large-scaleWeb table corpus. Furthermore; correctly recognizing the table structure (eg horizontallistings; matrices) is important in order to understand the role of each table cell;distinguishing between label and data cells. In this paper; we present an extensive tablelayout classification that enables us to identify the main layout categories of Web tables …,Big Data Computing (BDC); 2015 IEEE/ACM 2nd International Symposium on,2015,13
SynopSys: large graph analytics in the SAP HANA database through summarization,Michael Rudolf; Marcus Paradies; Christof Bornhövd; Wolfgang Lehner,Abstract Graph-structured data is ubiquitous and with the advent of social networkingplatforms has recently seen a significant increase in popularity amongst researchers.However; also many business applications deal with this kind of data and can thereforebenefit greatly from graph processing functionality offered directly by the underlyingdatabase. This paper summarizes the current state of graph data processing capabilities inthe SAP HANA database and describes our efforts to enable large graph analytics in thecontext of our research project SynopSys. With powerful graph pattern matching support atthe core; we envision OLAP-like evaluation functionality exposed to the user in the form ofeasy-to-apply graph summarization templates. By combining them; the user is able toproduce concise summaries of large graph-structured datasets. We also point out open …,First International Workshop on Graph Data Management Experiences and Systems,2013,13
Buzzard: A numa-aware in-memory indexing system,Lukas M Maas; Thomas Kissinger; Dirk Habich; Wolfgang Lehner,Abstract With the availability of large main memory capacities; in-memory index structureshave become an important component of modern data management platforms. Currentresearch even suggests index-based query processing as an alternative or supplement fortraditional tuple-at-a-time processing models. However; while simple sequential scanoperations can fully exploit the high bandwidth provided by main memory; indexes aremainly latency bound and spend most of their time waiting for memory accesses.,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,13
How to optimize the quality of sensor data streams,Anja Klein; Wolfgang Lehner,Present data stream management systems allow the automatic recording and processing ofhuge data volumes to guide any kind of process control or business decision. However; acrucial problem is posed by data quality deficiencies due to imprecise sensors;environmental influences; transfer failures; etc. If not handled carefully; they lead tomisguided decisions and inappropriate actions. In this paper; we present the quality-drivenoptimization of stream processing to improve the resulting quality of data and service. First;we present the optimization objectives and discuss the parameterization of streamprocessing operators to define the underlying optimization problem. We develop the genericoptimization framework and present the quality-driven evolution strategy (QES). Finally; weshow that the designed optimization scales very well with regard to processing complexity …,Computing in the Global Information Technology; 2009. ICCGI'09. Fourth International Multi-Conference on,2009,13
Sample synopses for approximate answering of group-by queries,Philipp Rösch; Wolfgang Lehner,Abstract With the amount of data in current data warehouse databases growing steadily;random sampling is continuously gaining in importance. In particular; interactive analyses oflarge datasets can greatly benefit from the significantly shorter response times ofapproximate query processing. Typically; those analytical queries partition the data intogroups and aggregate the values within the groups. Further; with the commonly used roll-upand drill-down operations a broad range of group-by queries is posed to the system; whichmakes the construction of highly-specialized synopses difficult. In this paper; we propose ageneral-purpose sampling scheme that is biased in order to answer group-by queries withhigh accuracy. While existing techniques focus on the size of the group when computing itssample size; our technique is based on its standard deviation. The basic idea is that the …,Proceedings of the 12th international conference on extending database technology: advances in database technology,2009,13
Derby/s: a DBMS for sample-based query answering,Anja Klein; Rainer Gemulla; Philipp Rösch; Wolfgang Lehner,Abstract Although approximate query processing is a prominent way to cope with therequirements of data analysis applications; current database systems do not provideintegrated and comprehensive support for these techniques. To improve this situation; wepropose an SQL extension---called SQL/S---for approximate query answering using randomsamples; and present a prototypical implementation within the engine of the open-sourcedatabase system Derby---called Derby/S. Our approach significantly reduces the requiredexpert knowledge by enabling the definition of samples in a declarative way; the choice ofthe specific sampling scheme and its parametrization is left to the system. SQL/S introducesnew DDL commands to easily define and administrate random samples subject to a givenset of optimization criteria. Derby/S automatically takes care of sample maintenance if the …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,13
Deferred maintenance of disk-based random samples,Rainer Gemulla; Wolfgang Lehner,Abstract Random sampling is a well-known technique for approximate processing of largedatasets. We introduce a set of algorithms for incremental maintenance of large randomsamples on secondary storage. We show that the sample maintenance cost can be reducedby refreshing the sample in a deferred manner. We introduce a novel type of log file whichfollows the intuition that only a “sample” of the operations on the base data has to beconsidered to maintain a random sample in a statistically correct way. Additionally; wedevelop a deferred refresh algorithm which updates the sample by using fast sequential diskaccess only; and which does not require any main memory. We conducted an extensive setof experiments and found; that our algorithms reduce maintenance cost by several orders ofmagnitude.,International Conference on Extending Database Technology,2006,13
Query optimization by using derivability in a data warehouse environment,Jens Albrecht; Wolfgang Hümmer; Wolfgang Lehner; Lutz Schlesinger,ABSTRACT Materialized summary tables and cached query results are frequently used forthe optimization of aggregate queries in a data warehouse. Query rewriting techniques areincorporated into database systems to use those materialized views and thus avoid theaccess of the possibly huge raw data. A rewriting is only possible if the query is derivablefrom these views. Several approaches can be found in the literature to check the derivabilityand find query rewritings. The specific application scenario of a data warehouse with itsmultidimensional perspective allows the consideration of much more semantic information;eg structural dependencies within the dimension hierarchies and different characteristics ofmeasures. The motivation of this article is to use this information to present conditions forderivability in a large number of relevant cases which go beyond previous approaches.,Proceedings of the 3rd ACM international workshop on Data warehousing and OLAP,2000,13
Towards integrated data analytics: Time series forecasting in DBMS,Ulrike Fischer; Lars Dannecker; Laurynas Siksnys; Frank Rosenthal; Matthias Boehm; Wolfgang Lehner,Abstract Integrating sophisticated statistical methods into database management systems isgaining more and more attention in research and industry in order to be able to cope withincreasing data volume and increasing complexity of the analytical algorithms. Oneimportant statistical method is time series forecasting; which is crucial for decision makingprocesses in many domains. The deep integration of time series forecasting offers additionaladvanced functionalities within a DBMS. More importantly; however; it allows foroptimizations that improve the efficiency; consistency; and transparency of the overallforecasting process. To enable efficient integrated forecasting; we propose to enhance thetraditional 3-layer ANSI/SPARC architecture of a DBMS with forecasting functionalities. Thisarticle gives a general overview of our proposed enhancements and presents how …,Datenbank-Spektrum,2013,12
Two-phase clustering strategy for gene expression data sets,Dirk Habich; Thomas Wächter; Wolfgang Lehner; Christian Pilarsky,Abstract In the context of genome research; the method of gene expression analysis hasbeen used for several years. Related microarray experiments are conducted all over theworld; and consequently; a vast amount of microarray data sets are produced. Havingaccess to this variety of repositories; researchers would like to incorporate this data in theiranalyses to increase the statistical significance of their results. In this paper; we present anew two-phase clustering strategy which is based on the combination of local clusteringresults to obtain a global clustering. The advantage of such a technique is that eachmicroarray data set can be normalized and clustered separately. The set of different relevantlocal clustering results is then used to calculate the global clustering result. Furthermore; wepresent an approach based on technical as well as biological quality measures to …,Proceedings of the 2006 ACM symposium on Applied computing,2006,12
Extending Data Warehouses by Semi-Consistent Database Views,Lutz Schlesinger; Wolfgang Lehner,Abstract The extremely expensive process of integrating data to achieve a consolidateddatabase is one of the main problems in building a data warehouse. Since operational datasources may exhibit a high update rate; the data warehouse information is out of date by theorder of magnitude. To avoid out-dated information the alternative is to query the datasources directly; which results in higher query runtimes or in the complete failure producinga result if data sources are currently not available. This paper discusses an approach toclose this gap: multiple snapshots of participating data sources are cached in a middlewarelayer and user queries are routed to this set of snapshots which are providing an almostconsistent global database view. We describe the architecture of our information middlewareapproach; develop different join semantics to combine different data sources; and …,Proceedings of the 4th International Workshop on Design and Management of Data Warehouses (DMDW'02; Toronto; Canada,2002,12
CoDEL–a relationally complete language for database evolution,Kai Herrmann; Hannes Voigt; Andreas Behrend; Wolfgang Lehner,Abstract Software developers adapt to the fast-moving nature of software systems with agiledevelopment techniques. However; database developers lack the tools and concepts tokeep pace. Data; already existing in a running product; needs to be evolved accordingly;usually by manually written SQL scripts. A promising approach in database research is touse a declarative database evolution language; which couples both schema and dataevolution into intuitive operations. Existing database evolution languages focus on usabilitybut did not aim for completeness. However; this is an inevitable prerequisite for reasonabledatabase evolution to avoid complex and error-prone workarounds. We argue that relationalcompleteness is the feasible expressiveness for a database evolution language. Buildingupon an existing language; we introduce CoDEL. We define its semantic using relational …,East European Conference on Advances in Databases and Information Systems,2015,11
Energy-efficient databases using sweet spot frequencies,Sebastian Götz; Thomas Ilsche; Jorge Cardoso; Josef Spillner; Thomas Kissinger; Uwe Aßmann; Wolfgang Lehner; Wolfgang E Nagel; Alexander Schill,Database management systems (DBMS) are typically tuned for high performance andscalability. Nevertheless; carbon footprint and energy efficiency are also becomingincreasing concerns. Unfortunately; existing studies mainly present theoretical contributionsbut fall short on proposing practical techniques. These could be used by administrators orquery optimizers to increase the energy efficiency of the DBMS. Thus; this paper exploresthe effect of so-called sweet spots; which are energy-efficient CPU frequencies; on theenergy required to execute queries. From our findings; we derive the Sweet Spot Technique;which relies on identifying energy-efficient sweet spots and the optimal number of threadsthat minimizes energy consumption for a query or an entire database workload. Thetechnique is simple and has a practical implementation leading to energy savings of up to …,Utility and Cloud Computing (UCC); 2014 IEEE/ACM 7th International Conference on,2014,11
Dynamic fine-grained scheduling for energy-efficient main-memory queries,Iraklis Psaroudakis; Thomas Kissinger; Danica Porobic; Thomas Ilsche; Erietta Liarou; Pınar Tözün; Anastasia Ailamaki; Wolfgang Lehner,Abstract Power and cooling costs are some of the highest costs in data centers today; whichmake improvement in energy efficiency crucial. Energy efficiency is also a major designpoint for chips that power whole ranges of computing devices. One important goal in thisarea is energy proportionality; arguing that the system's power consumption should beproportional to its performance. Currently; a major trend among server processors; whichstems from the design of chips for mobile devices; is the inclusion of advanced powermanagement techniques; such as dynamic voltage-frequency scaling; clock gating; andturbo modes. A lot of recent work on energy efficiency of database management systems isfocused on coarse-grained power management at the granularity of multiple machines andwhole queries. These techniques; however; cannot efficiently adapt to the frequently …,Proceedings of the Tenth International Workshop on Data Management on New Hardware,2014,11
Qppt: Query processing on prefix trees,Thomas Kissinger; Benjamin Schlegel; Dirk Habich; Wolfgang Lehner,ABSTRACT Modern database systems have to process huge amounts of data and shouldprovide results with low latency at the same time. To achieve this; data is nowadays typicallyhold completely in main memory; to benefit of its high bandwidth and low access latency thatcould never be reached with disks. Current in-memory databases are usually columnstoresthat exchange columns or vectors between operators and suffer from a high tuplereconstruction overhead. In this paper; we present the indexed table-at-a-time processingmodel that makes indexes the first-class citizen of the database system. The processingmodel comprises the concepts of intermediate indexed tables and cooperative operators;which make indexes the common data exchange format between plan operators. To keepthe intermediate index materialization costs low; we employ optimized prefix trees that …,*,2013,11
Model-based integration of past & future in timetravel,Mohamed E Khalefa; Ulrike Fischer; Torben Bach Pedersen; Wolfgang Lehner,Abstract We demonstrate TimeTravel; an efficient DBMS system for seamless integratedquerying of past and (forecasted) future values of time series; allowing the user to view pastand future values as one joint time series. This functionality is important for advancedapplication domain like energy. The main idea is to compactly represent time series asmodels. By using models; the TimeTravel system answers queries approximately on pastand future data with error guarantees (absolute error and confidence) one order ofmagnitude faster than when accessing the time series directly. In addition; it efficientlysupports exact historical queries by only accessing relevant portions of the time series. Thisis unlike existing approaches; which access the entire time series to exactly answer thequery. To realize this system; we propose a novel hierarchical model index structure. As …,Proceedings of the VLDB Endowment,2012,11
OPEN—Enabling non-expert users to extract; integrate; and analyze open data,Katrin Braunschweig; Julian Eberius; Maik Thiele; Wolfgang Lehner,Abstract Government initiatives for more transparency and participation have lead to anincreasing amount of structured data on the web in recent years. Many of these datasetshave great potential. For example; a situational analysis and meaningful visualization of thedata can assist in pointing out social or economic issues and raising people's awareness.Unfortunately; the ad-hoc analysis of this so-called Open Data can prove very complex andtime-consuming; partly due to a lack of efficient system support. On the one hand; searchfunctionality is required to identify relevant datasets. Common document retrieval techniquesused in web search; however; are not optimized for Open Data and do not address thesemantic ambiguity inherent in it. On the other hand; semantic integration is necessary toperform analysis tasks across multiple datasets. To do so in an ad-hoc fashion; however …,Datenbank-Spektrum,2012,11
Private table database virtualization for dbaas,Tim Kiefer; Wolfgang Lehner,Growing number of applications store data in relational databases. Moving databaseapplications to the cloud faces challenges related to flexible and scalable management ofdata. The obvious strategy of hosting legacy database management systems (DMBSs) onvirtualized cloud resources leads to sub optimal utilization and performance. However; thelayered architecture inside the DBMS allows for virtualization and consolidation above theOS level which can lead to significantly better system utilization and applicationperformance. Finding an optimal database cloud solution requires finding an assignmentfrom virtual to physical resources as well as configurations for all components. Our goal is toprovide a virtualization advisor that aids in setting up and operating a database cloud. Byformulating analytic cost; workload; and resource models performance of cloud-hosted …,Utility and Cloud Computing (UCC); 2011 Fourth IEEE International Conference on,2011,11
How to juggle columns: an entropy-based approach for table compression,Marcus Paradies; Christian Lemke; Hasso Plattner; Wolfgang Lehner; Kai-Uwe Sattler; Alexander Zeier; Jens Krueger,Abstract Many relational databases exhibit complex dependencies between data attributes;caused either by the nature of the underlying data or by explicitly denormalized schemas. Indata warehouse scenarios; calculated key figures may be materialized or hierarchy levelsmay be held within a single dimension table. Such column correlations and the resultingdata redundancy may result in additional storage requirements. They may also result in badquery performance if inappropriate independence assumptions are made during querycompilation. In this paper; we tackle the specific problem of detecting functionaldependencies between columns to improve the compression rate for column-baseddatabase systems; which both reduces main memory consumption and improves queryperformance. Although a huge variety of algorithms have been proposed for detecting …,Proceedings of the Fourteenth International Database Engineering & Applications Symposium,2010,11
Indexing forecast models for matching and maintenance,Ulrike Fischer; Frank Rosenthal; Matthias Boehm; Wolfgang Lehner,Abstract Forecasts are important to decision-making and risk assessment in many domains.There has been recent interest in integrating forecast queries inside a DBMS. Answering aforecast query requires the creation of forecast models. Creating a forecast model is anexpensive process and may require several scans over the base data as well as expensiveoperations to estimate model parameters. However; if forecast queries are issuedrepeatedly; answer times can be reduced significantly if forecast models are reused. Due tothe possibly high number of forecast queries; existing models need to be found quickly.Therefore; we propose a model index that efficiently stores forecast models and allows forthe efficient reuse of existing ones. Our experiments illustrate that the model index shows anegligible overhead for update transactions; but it yields significant improvements during …,Proceedings of the Fourteenth International Database Engineering & Applications Symposium,2010,11
Workload-based optimization of integration processes,Matthias Boehm; Uwe Wloka; Dirk Habich; Wolfgang Lehner,Abstract The efficient execution of integration processes between distributed;heterogeneous data sources and applications is a challenging research area of datamanagement. These integration processes are an abstraction for workflow-based integrationtasks; used in EAI servers and WfMS. The major problem are significant workload changesduring runtime. The performance of integration processes strongly depends on thosedynamic workload characteristics; and hence workload-based optimization is important.However; existing approaches of workflow optimization only address the rule-basedoptimization and disregard changing workload characteristics. To overcome the problem ofinefficient process execution in the presence of workload shifts; here; we present anapproach for the workload-based optimization of instance-based integration processes …,Proceedings of the 17th ACM conference on Information and knowledge management,2008,11
Model-Driven Generation and Optimization of Complex Integration Processes.,Matthias Böhm; Uwe Wloka; Dirk Habich; Wolfgang Lehner,Abstract: The integration of heterogeneous systems is still one of the main challenges in thearea of data management. Its importance is based on the trend towards heterogeneoussystem environments; where the different levels of integration approaches result in a largenumber of different integration systems. Due to these proprietary solutions and the lack of astandard for data-intensive integration processes; the model-driven development—followingthe paradigm of the Model-Driven Architecture (MDA)—is advantageous. This papercontributes to the model-driven development of complex and data-intensive integrationprocesses. In addition; we illustrate optimization possibilities offered by this model-drivenapproach and discuss first evaluation results.,ICEIS (1),2008,11
Lightweight data compression algorithms: An experimental survey,Patrick Damme; Dirk Habich; J Hildebrand; Wolfgang Lehner,ABSTRACT Lightweight data compression algorithms are frequently applied in in-memorydatabase systems to tackle the growing gap between processor speed and main memorybandwidth. In recent years; the vectorization of basic techniques such as delta coding andnull suppression has considerably enlarged the corpus of available algorithms. As a result;today there is a large number of algorithms to choose from; while different algorithms aretailored to different data characteristics. However; a comparative evaluation of thesealgorithms under different data characteristics has never been sufficiently conducted in theliterature. To close this gap; we conducted an exhaustive experimental survey by evaluatingseveral state-of-the-art compression algorithms as well as cascades of basic techniques. Wesystematically investigated the influence of the data properties on the performance and …,Proc. EDBT,2017,10
DrillBeyond: processing multi-result open world SQL queries,Julian Eberius; Maik Thiele; Katrin Braunschweig; Wolfgang Lehner,Abstract In a traditional relational database management system; queries can only bedefined over attributes defined in the schema; but are guaranteed to give single; definitiveanswer structured exactly as specified in the query. In contrast; an information retrievalsystem allows the user to pose queries without knowledge of a schema; but the result will bea top-k list of possible answers; with no guarantees about the structure or content of theretrieved documents. In this paper; we present DrillBeyond; a novel IR/RDBMS hybridsystem; in which the user seamlessly queries a relational database together with a largecorpus of tables extracted from a web crawl. The system allows full SQL queries over therelational database; but additionally allows the user to use arbitrary additional attributes inthe query that need not to be defined in the schema. The system then processes this semi …,Proceedings of the 27th International Conference on Scientific and Statistical Database Management,2015,10
A framework for user-centered declarative etl,Vasileios Theodorou; Alberto Abelló; Maik Thiele; Wolfgang Lehner,Abstract As business requirements evolve with increasing information density and velocity;there is a growing need for efficiency and automation of Extract-Transform-Load (ETL)processes. Current approaches for the modeling and optimization of ETL processes provideplatform-independent optimization solutions for the (semi-) automated transition amongdifferent abstraction levels; focusing on cost and performance. However; the suggestedrepresentations are not abstract enough to communicate business requirements and therole of the process quality in a user-centered perspective has not yet been adequatelyexamined. In this paper; we introduce a novel methodology for the end-to-end design of ETLprocesses that takes under consideration both functional and non-functional requirements.Based on existing work; we raise the level of abstraction for the conceptual representation …,Proceedings of the 17th international workshop on data warehousing and OLAP,2014,10
Heterogeneity-aware operator placement in column-store DBMS,Tomas Karnagel; Dirk Habich; Benjamin Schlegel; Wolfgang Lehner,Abstract Due to the tremendous increase in the amount of data efficiently managed bycurrent database systems; optimization is still one of the most challenging issues indatabase research. Today's query optimizer determine the most efficient composition ofphysical operators to execute a given SQL query; whereas the underlying hardware consistsof a multi-core CPU. However; hardware systems are more and more shifting towardsheterogeneity; combining a multi-core CPU with various computing units; eg; GPU or FPGAcores. In order to efficiently utilize the provided performance capability of suchheterogeneous hardware; the assignment of physical operators to computing units gainsimportance. In this paper; we propose a heterogeneity-aware physical operator placementstrategy (HOP) for in-memory columnar database systems in a heterogeneous …,Datenbank-Spektrum,2014,10
Quality measures for ETL processes,Vasileios Theodorou; Alberto Abelló; Wolfgang Lehner,Abstract ETL processes play an increasingly important role for the support of modernbusiness operations. These business processes are centred around artifacts with highvariability and diverse lifecycles; which correspond to key business entities. The apparentcomplexity of these activities has been examined through the prism of Business ProcessManagement; mainly focusing on functional requirements and performance optimization.However; the quality dimension has not yet been thoroughly investigated and there is aneed for a more human-centric approach to bring them closer to business-usersrequirements. In this paper we take a first step towards this direction by defining a soundmodel for ETL process quality characteristics and quantitative measures for eachcharacteristic; based on existing literature. Our model shows dependencies among …,International Conference on Data Warehousing and Knowledge Discovery,2014,10
Rsql-a query language for dynamic data types,Tobias Jäkel; Thomas Kühn; Hannes Voigt; Wolfgang Lehner,Abstract Database Management Systems (DBMS) are used by software applications; tostore; manipulate; and retrieve large sets of data. However; the requirements of currentsoftware systems pose various challenges to established DBMS. First; most softwaresystems organize their data by means of objects rather than relations leading to increasedmaintenance; redundancy; and transformation overhead when persisting objects torelational databases. Second; complex objects are separated into several objects resultingin Object Schizophrenia and hard to persist Distributed State. Last but not least; currentsoftware systems have to cope with increased complexity and changes. These challengeshave lead to a general paradigm shift in the development of software systems. Unfortunately;classical DBMS will become intractable; if they are not adapted to the new requirements …,Proceedings of the 18th International Database Engineering & Applications Symposium,2014,10
Demonstrating efficient query processing in heterogeneous environments,Tomas Karnagel; Matthias Hille; Mario Ludwig; Dirk Habich; Wolfgang Lehner; Max Heimel; Volker Markl,Abstract The increasing heterogeneity in hardware systems gives developers manyopportunities to add more functionality and computational power to the system. As aconsequence; modern database systems will need to be able to adapt to a wide variety ofheterogeneous architectures. While porting single operators to accelerator architectures iswell-understood; a more generic approach is needed for the whole database system. In priorwork; we presented a generic hardware-oblivious database system; where the operatorscan be executed on the main processor as well as on a large number of acceleratorarchitectures. However; to achieve fully heterogeneous query processing; placementdecisions are needed for the database operators. We enhance the presented system withheterogeneity-aware operator placement (HOP) to take a major step towards designing a …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,10
DeExcelerator: a framework for extracting relational data from partially structured documents,Julian Eberius; Christoper Werner; Maik Thiele; Katrin Braunschweig; Lars Dannecker; Wolfgang Lehner,Abstract Of the structured data published on the web; for instance as datasets on Open DataPlatforms such as data. gov; but also in the form of HTML tables on the general web; only asmall part is in a relational form. Instead the data is intermingled with formatting; layout andtextual metadata; ie; it is contained in partially structured documents. This makestransformation into a true relational form necessary; which is a precondition for most forms ofdata analysis and data integration. Studying data. gov as an example source for partiallystructured documents; we present a classification of typical normalization problems. We thenpresent the DeExcelerator; which is a framework for extracting relations from partiallystructured documents such as spreadsheets and HTML tables.,Proceedings of the 22nd ACM international conference on Information & Knowledge Management,2013,10
MulTe: a multi-tenancy database benchmark framework,Tim Kiefer; Benjamin Schlegel; Wolfgang Lehner,Abstract Multi-tenancy in relational databases has been a topic of interest for a couple ofyears. On the one hand; ever increasing capabilities and capacities of modern hardwareeasily allow for multiple database applications to share one system. On the other hand;cloud computing leads to outsourcing of many applications to service architectures; which inturn leads to offerings for relational databases in the cloud; as well. The ability to benchmarkmulti-tenancy database systems (MT-DBMSs) is imperative to evaluate and comparesystems and helps to reveal otherwise unnoticed shortcomings. With several tenants sharinga MT-DBMS; a benchmark is considerably different compared to classic databasebenchmarks and calls for new benchmarking methods and performance metrics.Unfortunately; there is no single; well-accepted multi-tenancy benchmark for MT-DBMSs …,Technology Conference on Performance Evaluation and Benchmarking,2012,10
Forcasting evolving time series of energy demand and supply,Lars Dannecker; Matthias Böhm; Wolfgang Lehner; Gregor Hackenbroich,Abstract Real-time balancing of energy demand and supply requires accurate and efficientforecasting in order to take future consumption and production into account. Thesebalancing capabilities are reasoned by emerging energy market developments; which alsopose new challenges to forecasting in the energy domain not addressed so far: First; real-time balancing requires accurate forecasts at any point in time. Second; the hierarchicalmarket organization motivates forecasting in a distributed system environment. In this paper;we present an approach that adapts forecasting to the hierarchical organization of today'senergy markets. Furthermore; we introduce a forecasting framework; which allows efficientforecasting and forecast model maintenance of time series that evolve due to continuousstreams of measurements. This framework includes model evaluation and adaptation …,East European Conference on Advances in Databases and Information Systems,2011,10
Offline Design Tuning for Hierarchies of Forecast Models.,Ulrike Fischer; Matthias Boehm; Wolfgang Lehner,Abstract: Forecasting of time series data is crucial for decision-making processes in manydomains as it allows the prediction of future behavior. In this context; a model is fit to theobserved data points of the time series by estimating the model parameters. The computedparameters are then utilized to forecast future points in time. Existing approaches integrateforecasting into traditional relational query processing; where a forecast query requests thecreation of a forecast model. Models of continued interest should be deployed only once andused many times afterwards. This however leads to additional maintenance costs as modelsneed to be kept up-to-date. Costs can be reduced by choosing a well-defined subset ofmodels and answering queries using derivation schemes. In contrast to materialized viewselection; model selection opens a whole new problem area as results are approximate …,BTW,2011,10
Pairwise element computation with MapReduce,Tim Kiefer; Peter Benjamin Volk; Wolfgang Lehner,Abstract In this paper; we present a parallel method to evaluate functions on pairs ofelements. It is a challenge to partition the Cartesian product of a set with itself in order toparallelize the function evaluation on all pairs. Our solution uses (a) replication of setelements to allow for partitioning and (b) aggregation of the results gathered for differentcopies of an element. Based on an execution model with nodes that execute tasks on localdata without online communication; we present a generic algorithm and show how it can beimplemented with MapReduce. Three different distribution schemes that define thepartitioning of the Cartesian product are introduced; compared; and evaluated. Any one ofthe distribution schemes can be used to derive and implement a specific algorithm forparallel pairwise element computation.,Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing,2010,10
GignoMDA: exploiting cross-layer optimization for complex database applications,Dirk Habich; Sebastian Richly; Wolfgang Lehner,Abstract Database Systems are often used as persistent layer for applications. This impliesthat database schemas are generated out of transient programming class descriptions. Thebasic idea of the MDA approach generalizes this principle by providing a framework togenerate applications (and database schemas) for different programming platforms. Withinour GignoMDA project [3]--which is subject of this demo proposal--we have extended classicconcepts for code generation. That means; our approach provides a single point of truthdescribing all aspects of database applications (eg database schema; projectdocumentation;...) with great potential for cross-layer optimization. These new cross-layeroptimization hints are a novel way for the challenging global optimization issue of multi-tierdatabase applications. The demo at VLDB comprises an in-depth explanation of our …,Proceedings of the 32nd international conference on Very large data bases,2006,10
Adaptive work placement for query processing on heterogeneous computing resources,Tomas Karnagel; Dirk Habich; Wolfgang Lehner,Abstract The hardware landscape is currently changing from homogeneous multi-coresystems towards heterogeneous systems with many different computing units; each withtheir own characteristics. This trend is a great opportunity for data-base systems to increasethe overall performance if the heterogeneous resources can be utilized efficiently. Toachieve this; the main challenge is to place the right work on the right computing unit.Current approaches tackling this placement for query processing assume that datacardinalities of intermediate results can be correctly estimated. However; this assumptiondoes not hold for complex queries. To overcome this problem; we propose an adaptiveplacement approach being independent of cardinality estimation of intermediate results. Ourapproach is incorporated in a novel adaptive placement sequence. Additionally; we …,Proceedings of the VLDB Endowment,2017,9
Context-aware parameter estimation for forecast models,*,Methods; systems; and computer-readable storage media for providing at least oneparameter for use with a forecast model. Implementations include actions of receiving a firstcontext vector; the first context vector including a plurality of context attributes that describe afirst context; retrieving a first parameter vector from a repository based on the first contextvector; the repository electronically storing a plurality of parameter vector; each parametervector being associated with a respective context and including one or more parameters;parameterizing the forecast model based on parameters provided in the first parametervector to provide a parameterized forecast model; optimizing the parameterized forecastmodel to provide an optimized forecast model; and forecasting one or more values using theoptimized forecast model.,*,2016,9
Answering “why empty?” and “why so many?” queries in graph databases,Elena Vasilyeva; Maik Thiele; Christof Bornhövd; Wolfgang Lehner,Abstract Graph databases provide schema-flexible storage and support complex; expressivequeries. However; the flexibility and expressiveness in these queries come at additionalcosts: queries can result in unexpected empty answers or too many answers; which aredifficult to resolve manually. To address this; we introduce subgraph-based solutions forgraph queries “Why Empty?” and “Why So Many?” that give an answer about which part of agraph query is responsible for an unexpected result. We also extend our solutions toconsider the specifics of the used graph model and to increase efficiency and experimentallyevaluate them in an in-memory column database.,Journal of Computer and System Sciences,2016,9
Column-specific context extraction for web tables,Katrin Braunschweig; Maik Thiele; Julian Eberius; Wolfgang Lehner,Abstract Relational Web tables have become an important resource for applications such asfactual search and entity augmentation. A major challenge for an automatic identification ofrelevant tables on the Web is the fact that many of these tables have missing or non-informative column labels. Research has focused largely on recovering the meaning ofcolumns by inferring class labels from the instances using external knowledge bases. Thetable context; which often contains additional information on the table's content; is frequentlyconsidered as an indicator for the general content of a table; but not as a source for column-specific details.,Proceedings of the 30th Annual ACM Symposium on Applied Computing,2015,9
Local vs. Global Optimization: Operator Placement Strategies in Heterogeneous Environments.,Tomas Karnagel; Dirk Habich; Wolfgang Lehner,ABSTRACT In several parts of query optimization; like join enumeration or physical operatorselection; there is always the question of how much optimization is needed and how largethe performance benefits are. In particular; a decision for either global optimization (eg;during query optimization) or local optimization (during query execution) has to be taken. Inthis way; heterogeneity in the hardware environment is adding a further optimization aspectwhile it is yet unknown; how much optimization is actually required for that aspect. Generally;several papers have shown that heterogeneous hardware environments can be usedefficiently by applying operator placement for OLAP queries. However; whether it is better toapply this placement in a local or global optimization strategy is still an open question. Totackle this challenge; we examine both strategies for a column-store database system in …,EDBT/ICDT Workshops,2015,9
Joining business rules and business processes,Dirk Habich; Sebastian Richly; Birgit Demuth; Franziska Gietl; Joachim Spilke; Wolfgang Lehner; Uwe Assmann,Abstract. In today's world; the spheres of business rules and business processes are tooloosely coupled. From our point of view; an appropriate joining of both spheres would provebeneficial for various application domains. Therefore; we propose an integrated approach tojoin rules and processes in a business-aware manner from a modeling and executionperspective. Our developed solution consists of four steps:(1) enhancement of classicalbusiness processes with SBVR annotations;(2) automatic integration of SBVR vocabularyinto the business process world using domain-specific language transformations;(3)transformation of business rules to OCL constraints; and (4) manual mapping of domain-specific concepts to Web-service data structures. Based on our developed approach;several advantages arise; eg; the independent modification of business rules without …,Proceedings of IT,2010,9
How to control clustering results? flexible clustering aggregation,Martin Hahmann; Peter B Volk; Frank Rosenthal; Dirk Habich; Wolfgang Lehner,Abstract One of the most important and challenging questions in the area of clustering ishow to choose the best-fitting algorithm and parameterization to obtain an optimal clusteringfor the considered data. The clustering aggregation concept tries to bypass this problem bygenerating a set of separate; heterogeneous partitionings of the same data set; from whichan aggregate clustering is derived. As of now; almost every existing aggregation approachcombines given crisp clusterings on the basis of pair-wise similarities. In this paper; weregard an input set of soft clusterings and show that it contains additional information that isefficiently useable for the aggregation. Our approach introduces an expansion of mentionedpair-wise similarities; allowing control and adjustment of the aggregation process and itsresult. Our experiments show that our flexible approach offers adaptive results; improved …,International Symposium on Intelligent Data Analysis,2009,9
DIPBench: An independent benchmark for data-intensive integration processes,Matthias Bohm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,The integration of heterogeneous data sources is one of the main challenges within the areaof data engineering. Due to the absence of an independent and universal benchmark fordata-intensive integration processes; we propose a scalable benchmark; called DIPBench(Data Intensive Integration Process Benchmark); for evaluating the performance ofintegration systems. This benchmark could be used for subscription systems; like replicationservers; distributed and federated DBMS or message-oriented middleware platforms likeEnterprise Application Integration (EAI) servers and Extraction Transformation Loading(ETL) tools. In order to reach the mentioned universal view for integration processes; thebenchmark is designed in a conceptual; process-driven way. The benchmark comprises 15integration process types. We specify the source and target data schemas and provide a …,Data Engineering Workshop; 2008. ICDEW 2008. IEEE 24th International Conference on,2008,9
Designing random sample synopses with outliers,Philipp Rosch; Rainer Gemulla; Wolfgang Lehner,Random sampling is one of the most widely used means to build synopses of large datasetsbecause random samples can be used for a wide range of analytical tasks. Unfortunately;the quality of the estimates derived from a sample is negatively affected by the presence of"outliers" in the data. In this paper; we show how to circumvent this shortcoming byconstructing outlier-aware sample synopses. Our approach extends the well-known outlierindexing scheme to multiple aggregation columns.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,9
Contracting in the Days of eBusiness,Wolfgang Hümmer; Wolfgang Lehner; Hartmut Wedekind,Abstract Putting electronic business on a sound foundation---model theoretically as well astechnologically---has to be seen as a central challenge for research as well as forcommercial development. This paper concentrates on the discovery and the negotiationphase of concluding an agreement based on a contract. We present a methodology how tocome seamlessly from a many-to-many relationship in the discovery phase to a one-to-onerelationship in the contract negotiation phase. Making the content of the contracts persistentis achieved by reconstructing contract templates by means of mereologic (logic of the whole-part relation). Possibly nested sub-structures of the contract template are taken as a basis fornegotiation in a dialogical way. For the negotiation itself the contract templates are extendedby implications (logical) and sequences (topical).,ACM Sigmod Record,2002,9
Template-based Time Series Generation with Loom.,Lars Kegel; Martin Hahmann; Wolfgang Lehner,ABSTRACT Time series analysis and forecasting are important techniques for decision-making in many domains. They are typically evaluated on given sets of time series that havea constant size and specified characteristics. Synthetic datasets are relevant because theyare flexible in both size and characteristics. In this demo; we present our prototype Loom;that generates datasets with respect to the user's configuration of categorical informationand time series characteristics. The prototype allows for comparison of different analysistechniques.,EDBT/ICDT Workshops,2016,8
Relationships for dynamic data types in RSQL,Tobias Jäkel; Thomas Kühn; Stefan Hinkel; Hannes Voigt; Wolfgang Lehner,Currently; there is a mismatch between the conceptual model of an information system andits implementation in a database management system (DBMS). Most of the conceptualmodeling languages relate their conceptual entities with relationships; but relationaldatabase management systems solely rely on the notion of relations to model both; entitiesand relationships. To make things worse; real world objects are not static as assumed insuch modeling languages; but change over time. Thus; modeling languages were enrichedto model those scenarios; as well. However; mapping these models onto relationaldatabases requires the use of object-relational mapping engines; which in turn hide thesemantics of the conceptual model from the DBMS. Consequently; traditional relationaldatabase systems cannot directly ensure specific consistency constraints and thus lose …,Datenbanksysteme für Business; Technologie und Web (BTW 2015),2015,8
Plan operator specialization using reflective compiler techniques,Carl-Philip Haensch; Thomas Kissinger; Dirk Habich; Wolfgang Lehner,Query-specific code generation has become a well-established approach to speed up queryexecution. However; this approach has two major drawbacks:(1) code generators are ingeneral hard to write and maintain;(2) code generators lack the ability to deal with customoperators. To overcome these limitations; we suggest to return to the traditional executionapproach with precompiled generic operators which are parametrized and composed toquery plans at query compile time. Nevertheless; to optimize such plan operators and speedup their execution; we introduce a novel specialization approach using reflective compilertechniques. Employing code annotations and an additional compiler pass; we are able totrack and replace low-level load instructions that refer to operator parameters which remainconstant during execution time. By dissolving such up-to-now unknown constant …,Datenbanksysteme für Business; Technologie und Web (BTW 2015),2015,8
HASHI: An Application Specific Instruction Set Extension for Hashing.,Oliver Arnold; Sebastian Haas; Gerhard Fettweis; Benjamin Schlegel; Thomas Kissinger; Tomas Karnagel; Wolfgang Lehner,ABSTRACT Hashing is one of the most relevant operations within query processing. Almostall core database operators like groupby; selections; or different join implementations rely onhighly efficient hash implementations. In this paper; we present a way to significantlyimprove performance and energy efficiency of hash operations using specialized instructionset extensions for the Tensilica Xtensa LX5 core. To show the applicability of instruction setextensions; we implemented a bit extraction hashing scheme for 32-bit integer keys as wellas the CityHash function for string values. We identify the individual parts of the algorithmsrequired to be optimized; we describe our hashing-specific instruction set; and finally give acomprehensive experimental evaluation. We observed that the hash implementation usingthe hashing-specific instruction set (1) is up to two orders of magnitudes faster than the …,ADMS@ VLDB,2014,8
Leveraging flexible data management with graph databases,Elena Vasilyeva; Maik Thiele; Christof Bornhövd; Wolfgang Lehner,Abstract Integrating up-to-date information into databases from different heterogeneous datasources is still a time-consuming and mostly manual job that can only be accomplished byskilled experts. For this reason; enterprises often lack information regarding the currentmarket situation; preventing a holistic view that is needed to conduct sound data analysisand market predictions. Ironically; the Web consists of a huge and growing number ofvaluable information from diverse organizations and data providers; such as the LinkedOpen Data cloud; common knowledge sources like Freebase; and social networks. Onedesirable usage scenario for this kind of data is its integration into a single database in orderto apply data analytics. However; in today's business intelligence tools there is an evidentlack of support for so-called situational or ad-hoc data integration. What we need is a …,First International Workshop on Graph Data Management Experiences and Systems,2013,8
A high-throughput in-memory index; durable on flash-based SSD: insights into the winning solution of the SIGMOD programming contest 2011,Thomas Kissinger; Benjamin Schlegel; Matthias Boehm; Dirk Habich; Wolfgang Lehner,Abstract Growing memory capacities and the increasing number of cores on modernhardware enforces the design of new in-memory indexing structures that reduce the numberof memory transfers and minimizes the need for locking to allow massive parallel access.However; most applications depend on hard durability constraints requiring a persistentmedium like SSDs; which shorten the latency and throughput gap between main memoryand hard disks. In this paper; we present our winning solution of the SIGMOD ProgrammingContest 2011. It consists of an in-memory indexing structure that provides a balancedread/write performance as well as non-blocking reads and single-lock writes.Complementary to this index; we describe an SSD-optimized logging approach to fit harddurability requirements at a high throughput rate.,ACM SIGMOD Record,2012,8
Efficient in-database maintenance of ARIMA models,Frank Rosenthal; Wolfgang Lehner,Abstract Forecasting is an important analysis task and there is a need of integrating timeseries models and estimation methods in database systems. The main issue is thecomputationally expensive maintenance of model parameters when new data is inserted. Inthis paper; we examine how an important class of time series models; the AutoRegressiveIntegrated Moving Average (ARIMA) models; can be maintained with respect to inserts.Therefore; we propose a novel approach; on-demand estimation; for the efficientmaintenance of maximum likelihood estimates from numerically implemented estimators.We present an extensive experimental evaluation on both real and synthetic data; whichshows that our approach yields a substantial speedup while sacrificing only a limitedamount of predictive accuracy.,International Conference on Scientific and Statistical Database Management,2011,8
Cost-based vectorization of instance-based integration processes,Matthias Boehm; Dirk Habich; Steffen Preissler; Wolfgang Lehner; Uwe Wloka,Abstract Integration processes are workflow-based integration tasks. The inefficiency ofthese processes is often caused by low resource utilization and significant waiting times forexternal systems. With the aim to overcome these problems; we proposed the concept ofprocess vectorization. There; instance-based integration processes are transparentlyexecuted with the pipes-and-filters execution model. The term vectorization is used in thesense of processing a sequence (vector) of messages by one standing process. Although ithas been shown that process vectorization achieves a significant throughput improvement;this concept has two major drawbacks. First; the theoretical performance of a vectorizedintegration process mainly depends on the performance of the most cost-intensive operator.Second; the practical performance strongly depends on the number of used threads and …,Information Systems,2011,8
Cost-based vectorization of instance-based integration processes,Matthias Boehm; Dirk Habich; Steffen Preissler; Wolfgang Lehner; Uwe Wloka,Abstract The inefficiency of integration processes—as an abstraction of workflow-basedintegration tasks—is often reasoned by low resource utilization and significant waiting timesfor external systems. With the aim to overcome these problems; we proposed the concept ofprocess vectorization. There; instance-based integration processes are transparentlyexecuted with the pipes-and-filters execution model. Here; the term vectorization is used inthe sense of processing a sequence (vector) of messages by one standing process.Although it has been shown that process vectorization achieves a significant throughputimprovement; this concept has two major drawbacks. First; the theoretical performance of avectorized integration process mainly depends on the performance of the most cost-intensive operator. Second; the practical performance strongly depends on the number of …,East European Conference on Advances in Databases and Information Systems,2009,8
GCIP: Exploiting the generation and optimization of integration processes,Matthias Boehm; Uwe Wloka; Dirk Habich; Wolfgang Lehner,Abstract As a result of the changing scope of data management towards the management ofhighly distributed systems and applications; integration processes have gained inimportance. Such integration processes represent an abstraction of workflow-basedintegration tasks. In practice; integration processes are pervasive and the performance ofcomplete IT infrastructures strongly depends on the performance of the central integrationplatform that executes the specified integration processes. In this area; the three majorproblems are:(1) significant development efforts;(2) low portability; and (3) inefficientexecution. To overcome those problems; we follow a model-driven generation approach forintegration processes. In this demo proposal; we want to introduce the so-called GCIPFramework (Generation of Complex Integration Processes) which allows the modeling of …,Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology,2009,8
Building a real Data warehouse for Market Research,Jens Albrecht; Wolfgang Lehner; Michael Teschke; Thomas Kirsche,This paper reflects the results of the evaluation phase of building a data production systemfor the retail research division of the GfK; Europe's largest market research company. Theapplication specific requirements like end-user needs or data volume are very different fromdata warehouses discussed in the literature; making it a real data warehouse. In a casestudy; these requirements are compared with state-of-the-art solutions offered by leadingsoftware vendors. Each of the common architectures (MOLAP; ROLAP; HOLAP) wasrepresented by a product. The result of this comparison is that all systems have to bemassively tailored to GfK's needs; especially to cope with meta data management or themaintenance of aggregations.,Database and Expert Systems Applications; 1997. Proceedings.; Eighth International Workshop on,1997,8
SAP HANA-From Relational OLAP Database to Big Data Infrastructure.,Norman May; Wolfgang Lehner; Shahul Hameed; Nitesh Maheshwari; Carsten Müller; Sudipto Chowdhuri; Anil K Goel,ABSTRACT SAP HANA started as one of the best-performing database engines for OLAPworkloads strictly pursuing a main-memory centric architecture and exploiting hardwaredevelopments like large number of cores and main memories in the TByte range. Within thispaper; we outline the steps from a traditional relational database engine to a Big Datainfrastructure comprising different methods to handle data of different volume; coming in withdifferent velocity; and showing a fairly large degree of variety. In order to make thepresentation of this transformation process more tangible; we discuss two major technicaltopics–HANA native integration points as well as extension points for collaboration withHadoop-based data management infrastructures. The overall of goal of this paper is to (a)review current application patterns and resulting technical challenges as well as to (b) …,EDBT,2015,7
Bringing linear algebra objects to life in a column-oriented in-memory database,David Kernert; Frank Köhler; Wolfgang Lehner,Abstract Large numeric matrices and multidimensional data arrays appear in many sciencedomains; as well as in applications of financial and business warehousing. Commonapplications include eigenvalue determination of large matrices; which decompose into aset of linear algebra operations. With the rise of in-memory databases it is now feasible toexecute these complex analytical queries directly in a relational database system without theneed of transfering data out of the system and being restricted by hard disc latencies forrandom accesses. In this paper; we present a way to integrate linear algebra operations andlarge matrices as first class citizens into an in-memory database following a two-layeredarchitectural model. The architecture consists of a logical component receiving manipulationstatements and linear algebra expressions; and of a physical layer; which autonomously …,*,2015,7
GraphMCS: Discover the Unknown in Large Data Graphs.,Elena Vasilyeva; Maik Thiele; Christof Bornhövd; Wolfgang Lehner,ABSTRACT Graph databases implementing the property graph model provide schema-flexible storage and support complex; expressive queries like shortest path; reachability; andgraph isomorphism queries. However; both the flexibility and expressiveness in thesequeries come with additional costs: queries can result in an unexpected; empty answer. Tounderstand the reason of an empty answer; a user normally has to create alternativequeries; which is a cumbersome and time-consuming task. To address this; we introduce diff-queries; a new kind of graph queries; that give an answer about which part of a query graphis represented in a data graph and which part is missing. We propose a new algorithm forprocessing diff-queries; which detects maximum common subgraphs between a query graphand a data graph and computes the difference between them. In addition; we present …,EDBT/ICDT Workshops,2014,7
Stream Join Processing on Heterogeneous Processors.,Tomas Karnagel; Benjamin Schlegel; Dirk Habich; Wolfgang Lehner,Abstract: The window-based stream join is an important operator in all data streamingsystems. It has often high resource requirements so that many efficient sequential as well asparallel versions of it were proposed in the literature. The parallel stream join operatorsrecently gain increasing interest because hardware is getting more and more parallel. Mostof these operators; however; are only optimized for processors with homogeneous executionunits (eg; multi-core processors). Newly available processors with heterogeneous executionunits cannot be exploited whereas such processors provide typically a very high peakperformance. In this paper; we propose an initial variant of a window-based stream joinoperator that is optimized for processors with heterogeneous execution units. We provide anefficient load balancing approach to utilize all available execution units of a processor …,BTW Workshops,2013,7
The planning OLAP model–A multidimensional model with planning support,Bernhard Jaecksch; Wolfgang Lehner,Abstract A wealth of multidimensional OLAP models has been suggested in the past;tackling various problems of modeling multidimensional data. However; all of these modelsfocus on navigational and query operators for grouping; selection and aggregation. Weargue that planning functionality is; next to reporting and analysis; an important part of OLAPin many businesses and as such should be represented as part of a multidimensionalmodel. Navigational operators are not enough for planning; instead new factual data iscreated or existing data is changed. To our knowledge we are the first to suggest amultidimensional model with support for planning. Because the main data entities of atypical multidimensional model are used both by planning and reporting; we concentrate onthe extension of an existing model; where we add a set of novel operators that support an …,*,2013,7
Real-time BI and situational analysis,Maik Thiele; Wolfgang Lehner,Abstract In the past; data-warehouse systems served as information providers for keymanagement members and knowledge workers; today; they are the central platform for theenterprise-wide integrated information provision. Aside from strategic analyses on historicaldata; this encompasses; above all; the submission of real-time data to operationalprocesses. The clear separation of the operational and the analytical world; as it has beenpromoted until now; will thus become obsolete in the future. This development (ie; themerging of the operational and the analytical world in context of Web technologies) is topicof this chapter. For this purpose; we take a closer look at current business intelligence (BI)and data warehouse trends from the perspective of both the applications and the databasesystems. We discuss scenarios which show the emerging trend to real-time business …,*,2012,7
Optimizing Multiple Top-K Queries over Joins.,Dirk Habich; Wolfgang Lehner; Alexander Hinneburg,Abstract Advanced Data Mining applications require more and more support from relationaldatabase engines. Especially clustering applications in high dimensional features spacedemand a proper support of multiple Top-k queries in order to perform projected clustering.Although some research tackles to problem of optimizing restricted ranking (top-k) queries;there is no solution considering more than one single ranking criterion. This deficit-optimizing multiple Top-k queries over joins-is targeted by this paper from two perspectives.On the one hand; we propose a minimal but quite handy extension of SQL to expressmultiple top-k queries. On the other hand; we propose an optimized hash join strategy toefficiently execute this type of queries. Extensive experiments conducted in this context showthe feasibility of our proposal.,SSDBM,2005,7
Debeaq-debugging empty-answer queries on large data graphs,Elena Vasilyeva; Thomas Heinze; Maik Thiele; Wolfgang Lehner,The large volume of freely available graph data sets impedes the users in analyzing them.For this purpose; they usually pose plenty of pattern matching queries and study theiranswers. Without deep knowledge about the data graph; users can createfailing'queries;which deliver empty answers. Analyzing the causes of these empty answers is a time-consuming and complicated task especially for graph queries. To help users in debuggingthesefailing'queries; there are two common approaches: one is focusing on discoveringmissing subgraphs of a data graph; the other one tries to rewrite the queries such that theydeliver some results. In this demonstration; we will combine both approaches and give theusers an opportunity to discover why empty results were delivered by the requested queries.Therefore; we propose DebEAQ; a debugging tool for pattern matching queries; which …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,6
Hybrid SCM-DRAM Transactional Storage Engine For Fast Data Recovery,*,A data recovery system and method are disclosed. Primary data is stored a database in byte-addressable NVRAM; where the database includes one or more persistent tables of data ina byte-addressable; RAM format; and a persistent memory allocator that maps persistentmemory pointers of the persistent memory to virtual memory pointers of a virtual memoryassociated with the database. Secondary data is stored in volatile DRAM. A failure recoveryincludes recovering the persistent memory allocator; mapping the persistent memory to thevirtual memory to recover primary data using their persistent memory pointers; translatingthe persistent memory pointers to virtual memory pointers; undoing changes to the primarydata made by unfinished transactions of the query execution at the time of failure of one ofthe one or more queries; and reconstructing the secondary data from the primary data.,*,2015,6
Towards a hybrid imputation approach using web tables,Ahmad Ahmadov; Maik Thiele; Julian Eberius; Wolfgang Lehner; Robert Wrembel,Data completeness is one of the most important data quality dimensions and an essentialpremise in data analytics. With new emerging Big Data trends such as the data lake concept;which provides a low cost data preparation repository instead of moving curated data into adata warehouse; the problem of data completeness is additionally reinforced. Whiletraditionally the process of filling in missing values is addressed by the data imputationcommunity using statistical techniques; we complement these approaches by using externaldata sources from the data lake or even the Web to lookup missing values. In this paper wepropose a novel hybrid data imputation strategy that; takes into account the characteristics ofan incomplete dataset and based on that chooses the best imputation approach; ie either astatistical approach such as regression analysis or a Web-based lookup or a combination …,Big Data Computing (BDC); 2015 IEEE/ACM 2nd International Symposium on,2015,6
Query processing on low-energy many-core processors,Annett Ungethüm; Dirk Habich; Tomas Karnagel; Wolfgang Lehner; Nils Asmussen; Marcus Völp; Benedikt Nöthen; Gerhard Fettweis,Aside from performance; energy efficiency is an increasing challenge in database systems.To tackle both aspects in an integrated fashion; we pursue a hardware/software co-designapproach. To fulfill the energy requirement from the hardware perspective; we utilize a low-energy processor design offering the possibility to us to place hundreds to millions of chipson a single board without any thermal restrictions. Furthermore; we address the performancerequirement by the development of several database-specific instruction set extensions tocustomize each core; whereas each core does not have all extensions. Therefore; ourhardware foundation is a low-energy processor consisting of a high number ofheterogeneous cores. In this paper; we introduce our hardware setup on a system level andpresent several challenges for query processing. Based on these challenges; we …,Data Engineering Workshops (ICDEW); 2015 31st IEEE International Conference on,2015,6
Top-k differential queries in graph databases,Elena Vasilyeva; Maik Thiele; Christof Bornhövd; Wolfgang Lehner,Abstract The sheer volume as well as the schema complexity of today's graph databasesimpede the users in formulating queries against these databases and often cause queries to“fail” by delivering empty answers. To support users in such situations; the concept ofdifferential queries can be used to bridge the gap between an unexpected result (eg anempty result set) and the query intention of users. These queries deliver missing parts of aquery graph and; therefore; work with such scenarios that require users to specify a querygraph. Based on the discovered information about a missing query subgraph; users mayunderstand which vertices and edges are the reasons for queries that unexpectedly returnempty answers; and thus can reformulate the queries if needed. A study showed that theresult sets of differential queries are often too large to be manually introspected by users …,East European Conference on Advances in Databases and Information Systems,2014,6
Smix: Self-managing indexes for dynamic workloads,Hannes Voigt; Thomas Kissinger; Wolfgang Lehner,Abstract As databases accumulate growing amounts of data at an increasing rate; adaptiveindexing becomes more and more important. At the same time; applications and their useget more agile and flexible; resulting in less steady and less predictable workloadcharacteristics. Being inert and coarse-grained; state-of-the-art index tuning techniquesbecome less useful in such environments. Especially the full-column indexing paradigmresults in many indexed but never queried records and prohibitively high storage andmaintenance costs. In this paper; we present Self-Managing Indexes; a novel; adaptive; fine-grained; autonomous indexing infrastructure. In its core; our approach builds on a novelaccess path that automatically collects useful index information; discards useless indexinformation; and competes with its kind for resources to host its index information …,Proceedings of the 25th International Conference on Scientific and Statistical Database Management,2013,6
Energy-efficient in-memory database computing,Wolfgang Lehner,Abstract The efficient and flexible management of large datasets is one of the corerequirements of modern business applications. Having access to consistent and up-to-dateinformation is the foundation for operational; tactical; and strategic decision making. Withinthe last few years; the database community sparked a large number of extremely innovativeresearch projects to push the envelope in the context of modern database systemarchitectures. In this paper; we outline requirements and influencing factors to identify someof the hot research topics in database management systems. We argue that---even after 30years of active database research---the time is right to rethink some of the core architecturalprinciples and come up with novel approaches to meet the requirements of the next decadesin data management. The sheer number of diverse and novel (eg; scientific) application …,Proceedings of the Conference on Design; Automation and Test in Europe,2013,6
Sample-based forecasting exploiting hierarchical time series,Ulrike Fischer; Frank Rosenthal; Wolfgang Lehner,Abstract Time series forecasting is challenging as sophisticated forecast models arecomputationally expensive to build. Recent research has addressed the integration offorecasting inside a DBMS. One main benefit is that models can be created once and thenrepeatedly used to answer forecast queries. Often forecast queries are submitted on higheraggregation levels; eg; forecasts of sales over all locations. To answer such a forecastquery; we have two possibilities. First; we can aggregate all base time series (sales inAustria; sales in Belgium...) and create only one model for the aggregate time series.Second; we can create models for all base time series and aggregate the base forecastvalues. The second possibility might lead to a higher accuracy but it is usually too expensivedue to a high number of base time series. However; we actually do not need all base …,Proceedings of the 16th International Database Engineering & Applications Sysmposium,2012,6
Evolving ensemble-clustering to a feedback-driven process,Martin Hahmann; Dirk Habich; Wolfgang Lehner,Data clustering is a highly used knowledge extraction technique and is applied in more andmore application domains. Over the last years; a lot of algorithms have been proposed thatare often complicated and/or tailored to specific scenarios. As a result; clustering hasbecome a hardly accessible domain for non-expert users; who face major difficulties likealgorithm selection and parameterization. To overcome this issue; we develop a novelfeedback-driven clustering process using a new perspective of clustering. By substitutingparameterization with user-friendly feedback and providing support for result interpretation;clustering becomes accessible and allows the step-by-step construction of a satisfying resultthrough iterative refinement.,Data Mining Workshops (ICDMW); 2010 IEEE International Conference on,2010,6
Enabling Real-Time Business Intelligence,Malu Castellanos; Dayal Umeshwar; Renee Miller,In today's competitive and highly dynamic environment; analyzing data to understand howthe business is performing; and to predict outcomes and trends have become critical. Thetraditional approach to reporting is no longer adequate. Instead users now demand easy-to-use intelligent platforms and applications capable of analyzing realtime data to provideinsight and actionable information at the right time. The end goal is to support better andtimelier decision making; enabled by the availability of up-todate; high-quality information.Although there has been progress in this direction and many companies are introducingproducts toward meeting this goal; there is still a long way to go. In particular; the wholelifecycle of business intelligence requires innovative techniques and methodologies capableof dealing with the requirements imposed by these new generation BI applications. From …,*,2010,6
Drift-aware ensemble regression,Frank Rosenthal; Peter Benjamin Volk; Martin Hahmann; Dirk Habich; Wolfgang Lehner,Abstract Regression models are often required for controlling production processes bypredicting parameter values. However; the implicit assumption of standard regressiontechniques that the data set used for parameter estimation comes from a stationary jointdistribution may not hold in this context because manufacturing processes are subject tophysical changes like wear and aging; denoted as process drift. This can cause theestimated model to deviate significantly from the current state of the modeled system. In thispaper; we discuss the problem of estimating regression models from drifting processes andwe present ensemble regression; an approach that maintains a set of regression models—estimated from different ranges of the data set—according to their predictive performance.We extensively evaluate our approach on synthetic and real-world data.,International Workshop on Machine Learning and Data Mining in Pattern Recognition,2009,6
Query processing in data warehouses,Wolfgang Lehner,In general; the term Quadtree refers to a class of representations of geometric entities (suchas points; line segments; polygons; regions) in a space of two (or more) dimensions; thatrecursively decompose the space containing these entities into blocks until the data in eachblock satisfy some condition (with respect; for example; to the block size; the number of blockentities; the characteristics of the block entities; etc.). In a more restricted sense; the termQuadtree (Octree) refers to a tree data-structure in which each internal node has four (eight)children and is used for the representation of geometric entities in a two (three) dimensionalspace. The root of the tree represents the whole space/region. Each child of a noderepresents a subregion of the subregion of its parent. The subregions of the siblingsconstitute a partition of the parent's regions. Several variations of quadtrees are possible …,*,2009,6
Model-driven development of complex and data-intensive integration processes,Matthias Böhm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Abstract Due to the changing scope of data management from centrally stored data towardsthe management of distributed and heterogeneous systems; the integration takes place ondifferent levels. The lack of standards for information integration as well as applicationintegration resulted in a large number of different integration models and proprietarysolutions. With the aim of a high degree of portability and the reduction of developmentefforts; the model-driven development—following the Model-Driven Architecture (MDA)—isadvantageous in this context as well. Hence; in the GCIP project (Generation of ComplexIntegration Processes); we focus on the model-driven generation and optimization ofintegration tasks using a process-based approach. In this paper; we contribute detailedgeneration aspects and finally discuss open issues and further challenges.,*,2008,6
An Advanced Transaction Model for Recovery Processing of Integration Processes.,Matthias Böhm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Abstract. Integration processes are increasingly used in order to integrate distributed andheterogeneous systems. Although transactional behavior of workflows has been discussedextensively; recovery processing has been disregarded so far. Due to the huge number ofdifferent integration systems and models; there are also different transaction concepts withoverlapping functionalities available. However; there is the need for a discussion of problemcategories and guarantees in order to consolidate the existing transaction concepts. In thispaper; we survey possible anomalies of recovery processing in message-orientedmiddleware and—in conclusion—we define the comprehensive transaction model SIR fordata-intensive—but instance-based—integration processes. This model includes thedefinition of specific transaction levels; which are the precondition for the integration …,ADBIS (local proceedings),2008,6
Querying asynchronously updated sensor data sets under quantified constraints,Lutz Schlesinger; Wolfgang Lehner,ABSTRACT Sensor databases are being widely deployed for measurement; detection andsurveillance applications. The resulting data streams are usually collected in a centralizedsensor database system holding the data belonging to the same subject or the samelocation. It is common practice to evaluate the data only locally. To discover new results;modern requirements imply to combine data at different sensors focussing on differentsubjects. The natural way of realizing a homogenous view and a centralized access point toa distributed set of database systems is the concept of a federated or a data warehousesystem. However; the tremendously high data refresh rate disallows the usage of thesetraditional replicated approaches with its strong consistency property. Instead we propose asystem that handles packaged updates to reduce the replication frequency and deals with …,GeoSensor Networks,2004,6
Towards a role-based contextual database,Tobias Jäkel; Thomas Kühn; Hannes Voigt; Wolfgang Lehner,Abstract Traditional modeling approaches and information systems assume static entitiesthat represent all information and attributes at once. However; due to the evolution ofinformation systems to increasingly context-aware and self-adaptive systems; thisassumption no longer holds. To cope with the required flexibility; the role concept wasintroduced. Although researchers have proposed several role modeling approaches; theyusually neglect the contextual characteristics of roles and their representation in databasemanagement systems. Unfortunately; these systems do not rely on a conceptual model of aninformation system; rather they model this information by their own means leading totransformation and maintenance overhead. So far; the challenges posed by dynamiccomplex entities; their first class implementation; and their contextual characteristics lack …,East European Conference on Advances in Databases and Information Systems,2016,5
Index structure to accelerate graph traversal,*,A system; computer-implemented method; and computer-readable storage medium forgenerating a block-based index; are provided. A block index is generated where the blockindex comprises a plurality of blocks and a block corresponds to a section of a graph columnthat stores a value. A block range vector is also generated for the index where the blockrange vector includes range information for the block that corresponds to the section of thegraph and where the block-based index facilitates traversal of the graph column thatsearches for the value by constraining the traversal to the section of the graph.,*,2015,5
From web tables to concepts: A semantic normalization approach,Katrin Braunschweig; Maik Thiele; Wolfgang Lehner,Abstract Relational Web tables; embedded in HTML or published on data platforms; havebecome an important resource for many applications; including question answering or entityaugmentation. To utilize the data; we require some understanding of what the tables areabout. Previous research on recovering Web table semantics has largely focused on simpletables; which only describe a single semantic concept. However; there is also a significantnumber of de-normalized multi-concept tables on the Web. Treating these as single-concepttables results in many incorrect relations being extracted. In this paper; we propose anormalization approach to decompose multi-concept tables into smaller single-concepttables. First; we identify columns that represent keys or identifiers of entities. Then; we utilizethe table schema as well as intrinsic data correlations to identify concept boundaries and …,International Conference on Conceptual Modeling,2015,5
A benchmark framework for data compression techniques,Patrick Damme; Dirk Habich; Wolfgang Lehner,Abstract Lightweight data compression is frequently applied in main memory databasesystems to improve query performance. The data processed by such systems is highlydiverse. Moreover; there is a high number of existing lightweight compression techniques.Therefore; choosing the optimal technique for a given dataset is non-trivial. Existingapproaches are based on simple rules; which do not suffice for such a complex decision. Incontrast; our vision is a cost-based approach. However; this requires a detailed cost model;which can only be obtained from a systematic benchmarking of many compressionalgorithms on many different datasets. A naïve benchmark evaluates every algorithm underconsideration separately. This yields many redundant steps and is thus inefficient. Wepropose an efficient and extensible benchmark framework for compression techniques …,Technology Conference on Performance Evaluation and Benchmarking,2015,5
Highspeed graph processing exploiting main-memory column stores,Matthias Hauck; Marcus Paradies; Holger Fröning; Wolfgang Lehner; Hannes Rauhe,Abstract A popular belief in the graph database community is that relational databasemanagement systems are generally ill-suited for efficient graph processing. This might applyfor analytic graph queries performing iterative computations on the graph; but does notnecessarily hold true for short-running; OLTP-style graph queries. In this paper we arguethat; instead of extending a graph database management system with traditional relationaloperators—predicate evaluation; sorting; grouping; and aggregations among others—oneshould consider adding a graph abstraction and graph-specific operations; such as graphtraversals and pattern matching; to relational database management systems. We use anexemplary query from the interactive query workload of the ldbc social network benchmarkand run it against our enhanced in-memory; columnar relational database system to …,European Conference on Parallel Processing,2015,5
SynopSys: foundations for multidimensional graph analytics,Michael Rudolf; Hannes Voigt; Christof Bornhövd; Wolfgang Lehner,Abstract The past few years have seen a tremendous increase in often irregularly structureddata that can be represented most naturally and efficiently in the form of graphs. Makingsense of incessantly growing graphs is not only a key requirement in applications like socialmedia analysis or fraud detection but also a necessity in many traditional enterprisescenarios. Thus; a flexible approach for multidimensional analysis of graph data is needed.Whereas many existing technologies require up-front modelling of analytical scenarios andare difficult to adapt to changes; our approach allows for ad-hoc analytical queries of graphdata. Extending our previous work on graph summarization; in this position paper we lay thefoundation for large graph analytics to enable business intelligence on graph-structureddata.,*,2015,5
Flexs–a logical model for physical data layout,Hannes Voigt; Alfred Hanisch; Wolfgang Lehner,Abstract Driven by novel application domains and hardware trends database research anddevelopment set off to many novel and specialized architectures. Particularly in the area ofphysical data layout; specialized solutions have shown exceptional performance for specificapplications. This trend is great for research and development and for those in need of top-level performance first and foremost. For those with moderate performance needs; however;a universal but flexible database system has the benefit of lower TCO. Regarding physicaldata layout; the more general systems are fairly inflexible compared to the variety of physicaldata layouts available in specialized systems. Particularly; the macroscopic characteristics;ie; how the data is grouped and clustered; are generally hard-coded and cannot be changedby configuration. We present Flexs; a declarative storage description language for the …,*,2015,5
SpMacho-Optimizing Sparse Linear Algebra Expressions with Probabilistic Density Estimation.,David Kernert; Frank Köhler; Wolfgang Lehner,ABSTRACT In the age of statistical and scientific databases; there is an emerging trend ofintegrating analytical algorithms into database systems. Many of these algorithms are basedon linear algebra with large; sparse matrices. However; linear algebra expressions oftencontain multiplications of more then two matrices. The execution of sparse matrix chains isnontrivial; since the runtime depends on the parenthesization and on physical properties ofintermediate results. Our approach targets to overcome the burden for data scientists ofselecting appropriate algorithms; matrix storage representations; and execution paths. In thispaper; we present a sparse matrix chain optimizer (SpMachO) that creates an executionplan; which is composed of multiplication operators and transformations between sparseand dense matrix storage representations. We introduce a comprehensive cost model for …,EDBT,2015,5
A query; a minute: Evaluating performance isolation in cloud databases,Tim Kiefer; Hendrik Schön; Dirk Habich; Wolfgang Lehner,Abstract Several cloud providers offer reltional databases as part of their portfolio. It ishowever not obvious how resource virtualization and sharing; which is inherent to cloudcomputing; influence performance and predictability of these cloud databases. Cloudproviders give little to no guarantees for consistent execution or isolation from other users.To evaluate the performance isolation capabilities of two commercial cloud databases; weran a series of experiments over the course of a week (a query; a minute) and reportvariations in query response times. As a baseline; we ran the same experiments on adedicated server in our data center. The results show that in the cloud single outliers are upto 31 times slower than the average. Additionally; one can see a point in time after which theaverage performance of all executed queries improves by 38%.,Technology Conference on Performance Evaluation and Benchmarking,2014,5
Modular Data Clustering-Algorithm Design beyond MapReduce.,Martin Hahmann; Dirk Habich; Wolfgang Lehner,ABSTRACT In the context of Big Data; flexible and adjustable data analytics become moreand more important; whereas an efficient; scalable and fault-tolerant execution is requiredas well. To fulfill the flexibility as well as the execution requirements; the specification of theanalysis methods have to be in an appropriate and easy adjustable manner. TheMapReduce approach has demonstrated that such flexible specification as well as scalableexecution is possible and applicable. However; the MapReduce programming model is toogeneric and complicates the specification from a data analysis point of view. Therefore; wepropose a novel programming approach using well-defined modular building blocks for aspecific and highly utilized data analysis domain named data clustering in this paper. Ourapproach offers many advantages:(i) a unified and specific instruction set for data …,EDBT/ICDT Workshops,2014,5
Optimized renewable energy forecasting in local distribution networks,Robert Ulbricht; Ulrike Fischer; Wolfgang Lehner; Hilko Donker,Abstract The integration of renewable energy sources (RES) into local energy distributionnetworks becomes increasingly important. Renewable energy highly depends on weatherconditions; making it difficult to maintain stability in such networks. To still enable efficientplanning and balancing; forecasts of energy supply are essential. However; typicaldistribution networks contain a variety of heterogeneous RES installations (eg wind; solar;water); each providing different characteristics and weather dependencies. Additionally;advanced meters; which allow the communication of final-granular production curves to thenetwork operator; are not available at all RES sites. Despite these heterogeneities andmissing measurements; reliable forecasts over the whole local distribution network have tobe provided. This poses high challenges on choosing the right input parameters …,Proceedings of the Joint EDBT/ICDT 2013 Workshops,2013,5
Multidimensionale Datenbanksysteme: Modellierung und Verarbeitung,Wolfgang Lehner,Ziel des Buches ist es; ein Framework für eine modellseitig mächtige und gleichzeitigeffiziente multidimensionale Datenanalyse; basierend auf einem relationalenDatenbanksystem; zu beschreiben. Die multidimensionale Datenanalyse ist im Kontext des"Decision Support" für eine flexible und umfassende betriebs-und volkswirtschaftlicheInformationsversorgung einer Organisation verantwortlich und reflektiert somit auftechnischer Ebene die zentrale Anwendung basierend auf einer" Data Warehouse"-Datenbasis.,*,2013,5
Advanced Analytics with the SAP HANA Database.,Philipp Große; Wolfgang Lehner; Norman May,Abstract: MapReduce as a programming paradigm provides a simple-to-use yet verypowerful abstraction encapsulated in two second-order functions: Map and Reduce. Assuch; they allow defining single sequentially processed tasks while at the same time hidingmany of the framework details about how those tasks are parallelized and scaled out. In thispaper we discuss four processing patterns in the context of the distributed SAP HANAdatabase that go beyond the classic MapReduce paradigm. We illustrate them using sometypical Machine Learning algorithms and present experimental results that demonstrate howthe data flows scale out with the number of parallel tasks.,DATA,2013,5
Optimizing notifications of subscription-based forecast queries,Ulrike Fischer; Matthias Böhm; Wolfgang Lehner; Torben Bach Pedersen,Abstract Integrating sophisticated statistical methods into database management systems isgaining more and more attention in research and industry. One important statistical methodis time series forecasting; which is crucial for decision management in many domains. In thiscontext; previous work addressed the processing of ad-hoc and recurring forecast queries.In contrast; we focus on subscription-based forecast queries that arise when an application(subscriber) continuously requires forecast values for further processing. Forecast queriesexhibit the unique characteristic that the underlying forecast model is updated with each newactual value and better forecast values might be available. However;(re-) sending newforecast values to the subscriber for every new value is infeasible because this can causesignificant overhead at the subscriber side. The subscriber therefore wishes to be notified …,International Conference on Scientific and Statistical Database Management,2012,5
Data-Warehousing 3.0-Die Rolle von Data-Warehouse-Systemen auf Basis von In-Memory Technologie.,Maik Thiele; Wolfgang Lehner; Dirk Habich,Abstract: In diesem Beitrag widmen wir uns der Frage; welche Rolle aktuelle Trends derHard-und Software für Datenbanksysteme spielen; um als Enabler für neuartige Konzepteim Umfeld des Data-Warehousing zu dienen. Als zentraler Schritt der Evolution im Kontextdes Data-Warehousing wird dabei die enge Kopplung zu operativen Systemen gesehen;um eine direkte Rückkopplung bzw. Einbettung in operationale Geschäftsprozesse zurealisieren. In diesem Papier diskutieren wir die Fragen; wie In-Memory-Technologie dasKonzept von Echtzeit-DWH-Systemen unterstützt bzw. ermöglicht. Dazu stellen wir zumeinen eine Referenzarchitektur für DWH-Systeme vor; die insbesondere push-undpullbasierte Datenversorgung berücksichtigt. Zum anderen diskutieren wir die konkreteRolle von In-Memory-Systemen mit Blick auf konkrete Aspekte wie der Frage optionaler …,IMDM,2011,5
Hybrid data-flow graphs for procedural domain-specific query languages,Bernhard Jaecksch; Franz Faerber; Frank Rosenthal; Wolfgang Lehner,Abstract Domain-specific query languages (DSQL) let users express custom business logic.Relational databases provide a limited set of options to execute business logic. Usually;stored procedures or a series of queries with some glue code. Both methods havedrawbacks and often business logic is still executed on application side transferring largeamounts of data between application and database; which is expensive. We translate aDSQL into a hybrid data-flow execution plan; containing relational operators mixed withprocedural ones. A cost model is used to drive the translation towards an optimal mixture ofrelational and procedural plan operators.,Proceedings of the 23rd international conference on Scientific and statistical database management,2011,5
Visual decision support for ensemble clustering,Martin Hahmann; Dirk Habich; Wolfgang Lehner,Abstract The continuing growth of data leads to major challenges for data clustering inscientific data management. Clustering algorithms must handle high data volumes/dimensionality; while users need assistance during their analyses. Ensemble clusteringprovides robust; high-quality results and eases the algorithm selection andparameterization. Drawbacks of available concepts are the lack of facilities for resultadjustment and the missing support for result interpretation. To tackle these issues; we havealready published an extended algorithm for ensemble clustering that uses soft clusterings.In this paper; we propose a novel visualization; tightly coupled to this algorithm; thatprovides assistance for result adjustments and allows the interpretation of clusterings fordata sets of arbitrary size.,International Conference on Scientific and Statistical Database Management,2010,5
Vectorizing instance-based integration processes,Matthias Boehm; Dirk Habich; Steffen Preissler; Wolfgang Lehner; Uwe Wloka,Abstract The inefficiency of integration processes—as an abstraction of workflow-basedintegration tasks—is often reasoned by low resource utilization and significant waiting timesfor external systems. Due to the increasing use of integration processes within ITinfrastructures; the throughput optimization has high influence on the overall performance ofsuch an infrastructure. In the area of computational engineering; low resource utilization isaddressed with vectorization techniques. In this paper; we introduce the concept ofvectorization in the context of integration processes in order to achieve a higher degree ofparallelism. Here; transactional behavior and serialized execution must be ensured. Inconclusion of our evaluation; the message throughput can be significantly increased.,International Conference on Enterprise Information Systems,2009,5
Exploiting graphic card processor technology to accelerate data mining queries in sap netweaver bia,Christoph Weyerhaeuser; Tobias Mindnich; Franz Faerber; Wolfgang Lehner,Within Business Intelligence contexts; the importance of data mining algorithms iscontinuously increasing; particularly from the perspective of applications and users thatdemand novel algorithms on the one hand and an efficient implementation exploiting novelsystem architectures on the other hand. Within this paper; we focus on the latter issue andreport our experience with the exploitation of graphic card processor technology within theSAP NetWeaver Business Intelligence Accelerator (BIA). The BIA represents a highlydistributed analytical engine that supports OLAP and data mining processing primitives. Thesystem organizes data entities in column-wise fashion and its operation is completely main-memory-based. Since case studies have shown that classic data mining queries spend alarge portion of their runtime on scanning and filtering the data as a necessary …,Data Mining Workshops; 2008. ICDMW'08. IEEE International Conference on,2008,5
Data-aware soa for gene expression analysis processes,Dirk Habich; Sebastian Richly; Wolfgang Lehner; Uwe Aßmann; Mike Grasselt; Albert Maier; Christian Pilarsky,In the context of genome research; the method of gene expression analysis has been usedfor several years. Related microarray experiments are conducted all over the world; andconsequently; a vast amount of microarray data sets are produced. Having access to thisvariety of repositories; researchers would like to incorporate this data in their analysesprocesses to increase the statistical significance of their results. Such analyses processesare typical examples of data-intensive processes. In general; data-intensive processes arecharacterized by (i) a sequence of functional operations processing large amount of dataand (ii) the transportation and transformation of huge data sets between the functionaloperations. To support data-intensive processes; an efficient and scalable environment isrequired; since the performance is a key factor today. The service-oriented architecture …,Services; 2007 IEEE Congress on,2007,5
Processing reporting function views in a data warehouse environment,Wolfgang Lehner; Wolfgang Hummer; Lutz Schlesinger,Reporting functions reflect a novel technique to formulate sequence-oriented queries inSQL. They extend the classical way of grouping and applying aggregation functions byadditionally providing a column-based ordering; partitioning; and windowing mechanism.The application area of reporting functions ranges from simple ranking queries (TOP (n)-analyses) over cumulative (Year-To-Date-analyses) to sliding window queries. We discussthe problem of deriving reporting function queries from materialized reporting function views;which is one of the most important issues in efficiently processing queries in a datawarehouse environment. Two different derivation algorithms; including their relationalmappings are introduced and compared in a test scenario.,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,5
Divide and aggregate: caching multidimensional objects.,Wolfgang Lehner; Jens Albrecht; Wolfgang Hümmer,Abstract A common optimization technique in the OLAP application domain is the use ofsummary aggregate data. For an appropriate support of analysis hot spots; we propose aquery based aggregate cache of 'multidimensional objects'. The major drawback of previousquery caching methods is that new queries must be completely subsumed by a singlecached object in order to utilize it. Our solution is the introduction of 'setderivability'whichallows the combination of several aggregates to derive a single query. The set of cachedmultidimensional objects is dynamically determined using both users access patterns andsemantical knowledge of dimensional structures. Experimental results show that averagecost reductions of over 50% are easily reached while spending only 10% of additionalstorage for summary data.,DMDW,2000,5
On the design and implementation of the multidimensional cubestore storage manager,Wolfgang Lehner; Wolfgang Sporer,Abstract: CUBEStore is a storage manager designed within the CUBESTAR project to fit thespecial needs of multidimensional applications as found in the area of Statistical andScientific DataBases (SSDB). Some of the goals being achieved within this project were built-in support for multiple dimensions and good performance for range queries regardless of thedimensionality of the data. The general characteristics of SSDB-applications; the derivedrequirements with regard to a storage management system and the specific openarchitecture as well as the reference implementation of CUBEStore are described in thispaper.,15th IEEE Symposium on Mass Storage Systems (MSS'98),1998,5
Living in parallel realities: Co-existing schema versions with a bidirectional database evolution language,Kai Herrmann; Hannes Voigt; Andreas Behrend; Jonas Rausch; Wolfgang Lehner,Abstract We introduce end-to-end support of co-existing schema versions within onedatabase. While it is state of the art to run multiple versions of a continuously developedapplication concurrently; it is hard to do the same for databases. In order to keep multiple co-existing schema versions alive--which are all accessing the same data set--developersusually employ handwritten delta code (eg views and triggers in SQL). This delta code ishard to write and hard to maintain: if a database administrator decides to adapt the physicaltable schema; all handwritten delta code needs to be adapted as well; which is expensiveand error-prone in practice. In this paper; we present InVerDa: developers use the simplebidirectional database evolution language BiDEL; which carries enough information togenerate all delta code automatically. Without additional effort; new schema versions …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,4
Quality measures for ETL processes: from goals to implementation,Vasileios Theodorou; Alberto Abelló; Wolfgang Lehner; Maik Thiele,Summary Extraction transformation loading (ETL) processes play an increasingly importantrole for the support of modern business operations. These business processes are centredaround artifacts with high variability and diverse lifecycles; which correspond to keybusiness entities. The apparent complexity of these activities has been examined throughthe prism of business process management; mainly focusing on functional requirements andperformance optimization. However; the quality dimension has not yet been thoroughlyinvestigated; and there is a need for a more human-centric approach to bring them closer tobusiness-users requirements. In this paper; we take a first step towards this direction bydefining a sound model for ETL process quality characteristics and quantitative measures foreach characteristic; based on existing literature. Our model shows dependencies among …,Concurrency and computation: practice and experience,2016,4
Work-energy profiles: General approach and in-memory database application,Annett Ungethüm; Thomas Kissinger; Dirk Habich; Wolfgang Lehner,Abstract Recent energy-related hardware developments trend towards offering more andmore configuration opportunities for the software to control its own energy consumption.Existing research so far mainly focused on finding the most energy-efficient hardwareconfiguration for specific operators or entire queries in the database domain. However; theconfiguration opportunities influence the energy consumption as well as the processingperformance. Thus; treating energy efficiency and performance as independent optimizationgoals offers a lot of drawbacks. To overcome these drawbacks; we introduce a model basedapproach in this paper which enables us to select a hardware configuration offering the bestenergy efficiency for a requested performance. Our model is a work-energy-profile being aset of useful work done during a fixed time span and the required energy for this work for …,Technology Conference on Performance Evaluation and Benchmarking,2016,4
Limitations of intra-operator parallelism using heterogeneous computing resources,Tomas Karnagel; Dirk Habich; Wolfgang Lehner,Abstract The hardware landscape is changing from homogeneous multi-core systemstowards wildly heterogeneous systems combining different computing units; like CPUs andGPUs. To utilize these heterogeneous environments; database query execution has to adaptto cope with different architectures and computing behaviors. In this paper; we investigatethe simple idea of partitioning an operator's input data and processing all data partitions inparallel; one partition per computing unit. For heterogeneous systems; data has to bepartitioned according to the performance of the computing units. We define a way tocalculate the partition sizes; analyze the parallel execution exemplarily for two databaseoperators; and present limitations that could hinder significant performance improvements.The findings in this paper can help system developers to assess the possibilities and …,East European Conference on Advances in Databases and Information Systems,2016,4
HW/SW-database-codesign for compressed bitmap index processing,Sebastian Haas; Tomas Karnagel; Oliver Arnold; Erik Laux; Benjamin Schlegel; Gerhard Fettweis; Wolfgang Lehner,Compressed bitmap indices are heavily used in scientific and commercial database systemsbecause they largely improve query performance for various workloads. Early researchfocused on finding tailor-made index compression schemes that are amenable for modernprocessors. Improving performance further typically comes at the expense of a lowercompression rate; which is in many applications not acceptable because of memorylimitations. Alternatively; tailor-made hardware allows to achieve a performance that canonly hardly be reached with software running on general-purpose CPUs. In this paper; wewill show how to create a custom instruction set framework for compressed bitmapprocessing that is generic enough to implement most of the major compressed bitmapindices. For evaluation; we implemented WAH; PLWAH; and COMPAX operations using …,Application-specific Systems; Architectures and Processors (ASAP); 2016 IEEE 27th International Conference on,2016,4
Energy elasticity on heterogeneous hardware using adaptive resource reconfiguration LIVE,Annett Ungethüm; Thomas Kissinger; Willi-Wolfram Mentzel; Dirk Habich; Wolfgang Lehner,Abstract Energy awareness of database systems has emerged as a critical research topic;since energy consumption is becoming a major limiter for their scalability. Recent energy-related hardware developments trend towards offering more and more configurationopportunities for the software to control its own energy consumption. Existing research so farmainly focused on leveraging this configuration spectrum to find the most energy-efficientconfiguration for specific operators or entire queries. In this demo; we introduce the conceptof energy elasticity and propose the energy-control loop as an implementation of thisconcept. Energy elasticity refers to the ability of software to behave energy-proportional andenergy-efficient at the same time while maintaining a certain quality of service. Thus; oursystem does not draw the least energy possible but the least energy necessary to still …,Proceedings of the 2016 International Conference on Management of Data,2016,4
On testing persistent-memory-based software,Ismail Oukid; Daniel Booss; Adrien Lespinasse; Wolfgang Lehner,Abstract Leveraging Storage Class Memory (SCM) as a universal memory--ie as memoryand storage at the same time--has deep implications on database architectures. It becomespossible to store a single copy of the data in SCM and directly operate on it at a finegranularity. However; exposing the whole database with direct access to the applicationdramatically increases the risk of data corruption. In this paper we propose a lightweight on-line testing framework that helps find and debug SCM-related errors that can occur uponsoftware or power failures. Our testing framework simulates failures in critical code pathsand achieves fast code coverage by leveraging call stack information to limit duplicatetesting. It also partially covers the errors that might arise as a result of reordered memoryoperations. We show through an experimental evaluation that our testing framework is …,Proceedings of the 12th International Workshop on Data Management on New Hardware,2016,4
A machine learning approach for layout inference in spreadsheets,Elvis Koci; Maik Thiele; Óscar Romero Moral; Wolfgang Lehner,Abstract Spreadsheet applications are one of the most used tools for content generation andpresentation in industry and the Web. In spite of this success; there does not exist acomprehensive approach to automatically extract and reuse the richness of data maintainedin this format. The biggest obstacle is the lack of awareness about the structure of the data inspreadsheets; which otherwise could provide the means to automatically understand andextract knowledge from these files. In this paper; we propose a classification approach todiscover the layout of tables in spreadsheets. Therefore; we focus on the cell level;considering a wide range of features not covered before by related work. We evaluated theperformance of our classifiers on a large dataset covering three different corpora fromvarious domains. Finally; our work includes a novel technique for detecting and repairing …,IC3K 2016: Proceedings of the 8th International Joint Conference on Knowledge Discovery; Knowledge Engineering and Knowledge Management: volume 1: KDIR,2016,4
The orchestration stack: the impossible task of designing software for unknown future post-CMOS hardware,Marcus Völp; Sascha Klüppelholz; Jeronimo Castrillon; Hermann Härtig; Nils Asmussen; Uwe Aßmann; Franz Baader; Christel Baier; Gerhard Fettweis; Jochen Fröhlich; Andrés Goens; Sebastian Haas; Dirk Habich; Mattis Hasler; Immo Huismann; Tomas Karnagel; Sven Karol; Wolfgang Lehner; Linda Leuschner; Matthias Lieber; Siqi Ling; Steffen Märcker; Johannes Mey; Wolfgang Nagel; Benedikt Nöthen; Rafael Penaloza; Michael Raitza; Jörg Stiller; Annett Ungethüm; Axel Voigt,Future systems based on post-CMOS technologies will be wildly heterogeneous; withproperties largely unknown today. This paper presents our design of a newhardware/software stack to address the challenge of preparing software development forsuch systems. It combines well-understood technologies from different areas; eg; network-on-chips; capability operating systems; flexible programming models and model checking. Wedescribe our approach and provide details on key technologies.,*,2016,4
Sparse Linear Algebra in Column-Oriented In-Memory Database,*,Embodiments relate to storing sparse matrices in an in-memory column-oriented databasesystem. Specifically; recent hardware shifts of primary storage from disc into memory; allowexecution of linear algebra queries directly in the database engine. Dynamic matrixmanipulation operations (like online insertion or deletion of elements) are not covered bymost linear algebra frameworks. Therefore a hybrid architecture comprises a read-optimizedmain structure; and a write-optimized delta structure. The resulting system layout derivedfrom the Compressed Sparse Row (CSR) representation; integrates well with a columnardatabase design. Moreover; the resulting architecture is amenable to a wide range of non-numerical use cases when dictionary encoding is used. Performance in specific examples isevaluated for dynamic sparse matrix workloads; by applying work flows of nuclear …,*,2015,4
Graph traversal operator inside a column store,*,A system; computer-implemented method; and a computer-readable storage medium for adata graph traversal are provided. The input parameters for traversing the data graph arereceived. The data graph having a set of vertices and a set of edges are stored in a columnbased format in a memory cache of a computer device based on the input parameters istraversed. The traversal generates a set of traversed vertices that are the result of the graphtraversal.,*,2015,4
POIESIS: a tool for quality-aware ETL process redesign,Vasileios Theodorou; Alberto Abelló Gamazo; Maik Thiele; Wolfgang Lehner,Abstract We present a tool; called POIESIS; for automatic ETL process enhancement. ETLprocesses are essential data-centric activities in modern business intelligence environmentsand they need to be examined through a viewpoint that concerns their quality characteristics(eg; data quality; performance; manageability) in the era of Big Data. POIESIS responds tothis need by providing a user-centered environment for quality-aware analysis and redesignof ETL flows. It generates thousands of alternative flows by adding flow patterns to the initialflow; in varying positions and combinations; thus creating alternative design options in amultidimensional space of different quality attributes. Through the demonstration of POIESISwe introduce the tool's capabilities and highlight its efficiency; usability and modifiability;thanks to its polymorphic design.© 2015; Copyright is with the authors.,Proceedings of the 18th International Conference on Extending Database Technology,2015,4
On-demand re-optimization of integration flows,Matthias Boehm; Dirk Habich; Wolfgang Lehner,Abstract Integration flows are used to propagate data between heterogeneous operationalsystems or to consolidate data into data warehouse infrastructures. In order to meet theincreasing need of up-to-date information; many messages are exchanged over time. Theefficiency of those integration flows is therefore crucial to handle the high load of messagesand to reduce message latency. State-of-the-art strategies to address this performancebottleneck are based on incremental statistic maintenance and periodic cost-based re-optimization. This also achieves adaptation to unknown statistics and changing workloadcharacteristics; which is important since integration flows are deployed for long timehorizons. However; the major drawbacks of periodic re-optimization are many unnecessaryre-optimization steps and missed optimization opportunities due to adaptation delays. In …,Information Systems,2014,4
Online bit flip detection for in-memory b-trees on unreliable hardware,Till Kolditz; Thomas Kissinger; Benjamin Schlegel; Dirk Habich; Wolfgang Lehner,Abstract Hardware vendors constantly decrease the feature sizes of integrated circuits toobtain better performance and energy efficiency. Due to cosmic rays; low voltage or heatdissipation; hardware--both processors and memory--becomes more and more unreliableas the error rate increases. From a database perspective bit flip errors in main memory willbecome a major challenge for modern in-memory database systems; which keep all theirenterprise data in volatile; unreliable main memory. Although existing hardware error controltechniques like ECC-DRAM are able to detect and correct memory errors; their detectionand correction capabilities are limited. Moreover; hardware error correction faces majordrawbacks in terms of acquisition costs; additional memory utilization; and latency. In thispaper; we argue that slightly increasing data redundancy at the right places by …,Proceedings of the Tenth International Workshop on Data Management on New Hardware,2014,4
GRATIN: accelerating graph traversals in main-memory column stores,Marcus Paradies; Michael Rudolf; Christof Bornhövd; Wolfgang Lehner,Abstract Native graph query and processing capabilities have become indispensable formodern business applications in enterprise-critical operations on data that is stored inrelational database management systems. Traversal operations are a basic ingredient ofgraph algorithms and graph queries. As a consequence; they are fundamental for queryinggraph data in a relational database management system. In this paper we present gratin; aconcise secondary index structure to speedup graph traversals in main-memory columnstores. Conventional approaches for graph traversals rely on repeated full column scans;making it an inefficient approach for deep traversals on very large graphs. To tackle thischallenge; we devise a novel and adaptive block-based index to handle graphs efficiently.Most importantly; gratin is updateable in constant time and allows supporting evolving …,Proceedings of Workshop on GRAph Data management Experiences and Systems,2014,4
ERIS live: a NUMA-aware in-memory storage engine for tera-scale multiprocessor systems,Tim Kiefer; Thomas Kissinger; Benjamin Schlegel; Dirk Habich; Daniel Molka; Wolfgang Lehner,Abstract The ever-growing demand for more computing power forces hardware vendors toput an increasing number of multiprocessors into a single server system; which usuallyexhibits a non-uniform memory access (NUMA). In-memory database systems running onNUMA platforms face several issues such as the increased latency and the decreasedbandwidth when accessing remote main memory. To cope with these NUMA-related issues;a DBMS has to allow flexible data partitioning and data placement at runtime. In thisdemonstration; we present ERIS; our NUMA-aware in-memory storage engine. ERIS usesan adaptive partitioning approach that exploits the topology of the underlying NUMAplatform and significantly reduces NUMA-related issues. We demonstrate throughputnumbers and hardware performance counter evaluations of ERIS and a NUMA-unaware …,Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data,2014,4
Non-uniformity issues and workarounds in bounded-size sampling,Rainer Gemulla; Peter J Haas; Wolfgang Lehner,Abstract A variety of schemes have been proposed in the literature to speed up queryprocessing and analytics by incrementally maintaining a bounded-size uniform sample froma dataset in the presence of a sequence of insertion; deletion; and update transactions.These algorithms vary according to whether the dataset is an ordinary set or a multiset andwhether the transaction sequence consists only of insertions or can include deletions andupdates. We report on subtle non-uniformity issues that we found in a number of these priorbounded-size sampling schemes; including some of our own. We provide workarounds thatcan avoid the non-uniformity problem; these workarounds are easy to implement and incurnegligible additional cost. We also consider the impact of non-uniformity in practice anddescribe simple statistical tests that can help detect non-uniformity in new algorithms.,The VLDB Journal,2013,4
PcApriori: scalable Apriori for multiprocessor systems,Benjamin Schlegel; Tim Kiefer; Thomas Kissinger; Wolfgang Lehner,Abstract Frequent-itemset mining is an important part of data mining. It is a computationaland memory intensive task and has a large number of scientific and statistical applicationareas. In many of them; the datasets can easily grow up to tens or even several hundredgigabytes of data. Hence; efficient algorithms are required to process such amounts of data.In the recent years; there have been proposed many efficient sequential mining algorithms;which however cannot exploit current and future systems providing large degrees ofparallelism. Contrary; the number of parallel frequent-itemset mining algorithms is rathersmall and most of them do not scale well as the number of threads is largely increased. Inthis paper; we present a highly-scalable mining algorithm that is based on the well-knownApriori algorithm; it is optimized for processing very large datasets on multiprocessor …,Proceedings of the 25th International Conference on Scientific and Statistical Database Management,2013,4
Forecasting in hierarchical environments,Robert Lorenz; Lars Dannecker; Philipp Rösch; Wolfgang Lehner; Gregor Hackenbroich; Benjamin Schlegel,Abstract Forecasting is an important data analysis technique and serves as the basis forbusiness planning in many application areas such as energy; sales and traffic management.The currently employed statistical models already provide very accurate predictions; but theforecasting calculation process is very time consuming. This is especially true since manyapplication domains deal with hierarchically organized data. Forecasting in theseenvironments is especially challenging due to ensuring forecasting consistency betweenhierarchy levels; which leads to an increased data processing and communication effort. Forthis purpose; we introduce our novel hierarchical forecasting approach; where we proposeto push forecast models to the entities on the lowest hierarch level and reuse these modelsto efficiently create forecast models on higher hierarchical levels. With that we avoid the …,Proceedings of the 25th International Conference on Scientific and Statistical Database Management,2013,4
Publish-time data integration for open data platforms,Julian Eberius; Patrick Damme; Katrin Braunschweig; Maik Thiele; Wolfgang Lehner,Abstract Platforms for publication and collaborative management of data; such as Data. govor Google Fusion Tables; are a new trend on the web. They manage very large corpora ofdatasets; but often lack an integrated schema; ontology; or even just common publicationstandards. This results in inconsistent names for attributes of the same meaning; whichconstrains the discovery of relationships between datasets as well as their reusability.Existing data integration techniques focus on reuse-time; ie; they are applied when a userwants to combine a specific set of datasets or integrate them with an existing database. Incontrast; this paper investigates a novel method of data integration at publish-time; wherethe publisher is provided with suggestions on how to integrate the new dataset with thecorpus as a whole; without resorting to a manually created mediated schema or ontology …,Proceedings of the 2nd International Workshop on Open Data,2013,4
Partitioning and multi-core parallelization of multi-equation forecast models,Lars Dannecker; Matthias Böehm; Wolfgang Lehner; Gregor Hackenbroich,Abstract Forecasting is an important analysis technique used in many application domainssuch as electricity management; sales and retail and; traffic predictions. The employedstatistical models already provide very accurate predictions; but recent developments inthese domains pose new requirements on the calculation speed of the forecast models.Especially; the often used multi-equation models tend to be very complex and theirestimation is very time consuming. To still allow the use of these highly accurate forecastmodels; it is necessary to improve the data processing capabilities of the involved datamanagement systems. For this purpose; we introduce a partitioning approach for multi-equation forecast models that considers the specific data access pattern of these models tooptimize the data storage and memory access. With the help of our approach we avoid …,International Conference on Scientific and Statistical Database Management,2012,4
Hybride Datenbankarchitekturen am Beispiel der neuen SAP In-Memory-Technologie,Franz Färber; Bernhard Jäcksch; Christian Lemke; Philipp Große; Wolfgang Lehner,Zusammenfassung Die Verfügbarkeit neuer Technologien wie Multi-Core; SSD oder großeHauptspeicherkapazitäten bieten eine Gelegenheit; die klassischen Architekturansätze vonDatenbanksystemen zu überdenken und an bestimmten Stellen zu korrigieren. In diesemBeitrag stellen wir die Grobstruktur der neuen hauptspeicherzentrierten SAP Technologieals einen Ansatz einer kommerziellen Umsetzung moderner Architekturkonzepte vor.Zentrales Design-Kriterium ist dabei ein hybrider Ansatz; um eine möglichst hohe Anzahlvon Anforderungsvarianten optimal zu unterstützen. Nach einer Einleitung führt der Artikeldurch die wichtigsten Architekturkomponenten und illustriert den grundsätzlichen Aufbaudes Systems. Für einen „deep dive “werden zwei Bereiche in Teil 3 und 4 des Artikels imDetail diskutiert. Dabei greift der Artikel zum einen den Aspekt der physischen …,Datenbank-Spektrum,2010,4
Cherry picking in database languages,Bernhard Jaecksch; Franz Faerber; Wolfgang Lehner,Abstract To avoid expensive round-trips between the application layer and the databaselayer it is crucial that data-intensive processing and calculations happen close to where thedata resides--ideally within the database engine. However; each application has its owndomain and provides domain-specific languages (DSL) as a user interface to keepinteractions confined within the well-known metaphors of the respective domain. Revealingthe innards of the underlying data layer by forcing users to formulate problems in terms of ageneral database language is often not an option. To bridge that gap; we propose anapproach to transform and directly compile a DSL into a general database execution planusing graph transformations. We identify the commonalities and mismatches betweendifferent models and show which parts can be cherry-picked for direct translation. Finally …,Proceedings of the Fourteenth International Database Engineering & Applications Symposium,2010,4
Multi-flow optimization via horizontal message queue partitioning,Matthias Boehm; Dirk Habich; Wolfgang Lehner,Abstract Integration flows are increasingly used to specify and execute data-intensiveintegration tasks between heterogeneous systems and applications. There are manydifferent application areas such as near real-time ETL and data synchronization betweenoperational systems. For the reasons of an increasing amount of data; highly distributed ITinfrastructures; as well as high requirements for up-to-dateness of analytical query resultsand data consistency; many instances of integration flows are executed over time. Due tothis high load; the performance of the central integration platform is crucial for an ITinfrastructure. With the aim of throughput maximization; we propose the concept of multi-flowoptimization (MFO). In this approach; messages are collected during a waiting time andexecuted in batches to optimize sequences of plan instances of a single integration flow …,International Conference on Enterprise Information Systems,2010,4
Annotationsbasierte Prozessmodellierung in SOA-dargestellt an einem Beispiel aus dem Precision Dairy Farming.,Franziska Gietl; Joachim Spilke; Dirk Habich; Wolfgang Lehner,Abstract: Bei der Entwicklung einer serviceorientierten Architektur im Bereich des PrecisionDairy Farmings haben wir uns mit der Modellierung unternehmensübergreifender Prozessemit Hilfe der Business Process Modeling Notation (BPMN) beschäftigt. Da dieseModellierung stellenweise sehr abstrakt ist; schlagen wir einen angepasstenModellierungsansatz unter der Verwendung von Annotationen vor. Damit könnennotwendige Bedingungen direkt dem betreffenden Objekt zugeordnet werden; wodurch dieModellierung fachbezogener und damit für den Nutzer transparenter wird.,GIL Jahrestagung,2010,4
Stream-Based Web Service Invocation.,Steffen Preißler; Hannes Voigt; Dirk Habich; Wolfgang Lehner,Abstract: Service-oriented architectures (SOA) based on Web service technology play anincreasingly important role in many different application areas. The current serviceinvocation methodology suffers from performance problems and heavy resourceconsumption when services are used to process large amounts of data. A number ofsolutions to this problem have been proposed. Unfortunately; these modified invocationmethodologies are applicable only to a limited number of business scenarios; because theydo not provide all features of the traditional methodology. In this paper; we introduce a newservice invocation methodology that allows large volume data processing and does not limitapplicability. Our approach macerates the existing request–response paradigm andincorporates stream semantics into the Web service invocation methodology without …,BTW,2009,4
Constrained dynamic physical database design,Hannes Voigt; Wolfgang Lehner; Kenneth Salem,Abstract Physical design has always been an important part of database administration.Today's commercial database management systems offer physical design tools; whichrecommend a physical design for a given workload. However; these tools work only withstatic workloads and ignore the fact that workloads; and physical designs; may change overtime. Research has now begun to focus on dynamic physical design; which can account fortime-varying workloads. In this paper; we consider a dynamic but constrained approach tophysical design. The goal is to recommend dynamic physical designs that reflect majorworkload trends but that are not tailored too closely to the details of the input workloads. Toachieve this; we constrain the number of changes that are permitted in the recommendeddesign. In this paper we present our definition of the constrained dynamic physical design …,SMDB’08,2008,4
An approach for incremental semi-supervised svm,Wael Emara; Mehmed Kantardzic Marcel Karnstedt; Kai-Uwe Sattler; Dirk Habich; Wolfgang Lehner,In this paper we propose an approach for incremental learning of semi-supervised SVM. Theproposed approach makes use of the locality of radial basis function kernels to do local andincremental training of semi-supervised support vector machines. The algorithm introducesa se-quential minimal optimization based implementation of the branch and boundtechnique for training semi-supervised SVM problems. The novelty of our approach lies inthe,Data Mining Workshops; 2007. ICDM Workshops 2007. Seventh IEEE International Conference on,2007,4
QDM: A Generic QoS-Aware Data Model for Real-Time Data Stream Processing,Sven Schmidt; Benjamin Schlegel; Wolfgang Lehner,Data stream processing addresses a huge variety of application; ranging from processingcomplex-structured business events to high-volume RFID tag info streams. Some applicationareas; especially in telecommunication or manufacturing; require guarantees with regard topre-defined service constraints. In this paper; we outline the cornerstones of a QoS-awaredata model comprising functional as well as non-functional properties to describe qualityconstraints. We will look at the model from a structural point of view as well as from theoperational perspective. Since the proposed model is specific with regard to QoS constraintand generic with regard to specific operators; it may serve as a blue-print for a broadspectrum of data stream systems.,Digital Telecommunications; 2007. ICDT'07. Second International Conference on,2007,4
Query optimization for data warehouse system with different data distribution strategies,Thomas Legler; W Lehner; A Ross,*,*,2007,4
GignoMDA-Generation of Complex Database Applications.,Sebastian Richly; Dirk Habich; Wolfgang Lehner,Abstract Complex database applications feature a large amount of structural and content-related aspects; and with each project; these aspects either have to be implementedcompletely again or must be realized by adopting and adjusting available program code.Based on the MDA concept (Model-Driven Architecture); the GignoMDA Project aims at theenrichment of the automatic generation of complex 3-layer applications through theconsideration of nonfunctional properties. Aside from the automation aspect; the optimalmapping of annotated UML models to multi-layer architectures plays a central role here.That means; our approach provides a single point of truth describing all aspects of databaseapplications (eg database schema; project documentation; etc.) with a great potential ofcross-layer optimization. These new cross-layer optimization hints as non-functional …,Grundlagen von Datenbanken,2006,4
Eyes4Ears-More than a Classical Music Retrieval System,Dirk Habich; Wolfgang Lehner; Alexander Hinneburg; Philip Kitzmantel; Matthias Kimpl,Abstract Content-based similarity search for music retrieval attracted a lot attention in recentinformation retrieval research. Most music applications (eg several commercial web portals)offer to search music files; which however is limited to key-word-based search on subjectslike genre or artist. Other similarity search approaches base on abstract metrics; which aredefined on feature vectors representing psycho-acoustic or physical properties. However; itis still an open problem to adapt the search towards the user own music similarity measure.This paper presents a new music retrieval system which is more than a classical musicmanagement system. The system architecture is based on a client/server architecture andoffers methods to manage a large music database efficiently. In addition to key-word-basedsearch methods the Eyes4Ears system provides also methods for content-based similarity …,Proceedings of 5th Open Workshop of Musicnetwork-Integration of Music in Multimedia Applications,2005,4
Data Management in a Connected World,W Lehner,Data management systems play the most crucial role in building large application systems.Since modern applications are no longer single monolithic software blocks but highlyflexible and configurable collections of cooperative services; the data management layeralso has to adapt to these new requirements. Therefore; within recent years; datamanagement systems have faced a tremendous shift from the central management ofindividual records in a transactional way to a platform for data integration; federation; searchservices; and data analysis. This book addresses these new issues in the area of datamanagement from multiple perspectives; in the form of individual contributions; and itoutlines future challenges in the context of data management. These contributions arededicated to Prof. em. Dr. Dr.-Ing. E. h. Hartmut Wedekind on the occasion of his 70th …,*,2005,4
Xml stream processing quality,Sven Schmidt; Rainer Gemulla; Wolfgang Lehner,Abstract Systems for selective dissemination of information (SDI) are used to efficiently filter;transform; and route incoming XML documents according to pre-registered XPath profiles tosubscribers. Recent work focuses on the efficient implementation of the SDI core/filteringengine. Surprisingly; all systems are based on the best effort principle: The resulting XMLdocument is delivered to the consumer as soon as the filtering engine has successfullyfinished. In this paper; we argue that a more specific Quality-of-Service consideration has tobe applied to this scenario. We give a comprehensive motivation of quality of service in SDI-systems; discuss the two most critical factors of XML document size and shape and XPathstructure and length; and finally outline our current prototype of a Quality-of-Service-basedSDI-system implementation based on a real-time operating system and an extention of …,International XML Database Symposium,2003,4
Database support for 3D-protein data set analysis,Alexander Hinneburg; Wolfgang Lehner,The progress in genome research demands for an adequate infrastructure to analyze thedata sets. Database systems reflect a key technology to organize data and speed up theanalysis process. This paper discusses the role of a relational database system based onthe problem of finding frequent substructures in multi-dimensional protein databases. Thespecific problem consists of producing a set of association rules regarding frequentsubstructures with different lengths and gaps between the amino acid residues of a protein.From a database point of view; the process of finding association rules building the base fora more in-depth analysis of the data material is split into two parts. The first part performs adiscretization of the conformational angle space of a single amino acid residue bycomputing the nearest neighbor of a given set of representatives. The second part …,Scientific and Statistical Database Management; 2003. 15th International Conference on,2003,4
Maintenance of automatic summary tables in ibm db2/udb,Wolfgang Lehner; Richard Sidle; Hamid Pirahesh; Roberta Cochrane,Abstract Materialized views are commonly used to improve the performance of aggregationqueries by orders of magnitude. In contrast to regular tables; materialized views are notdirectly updateable by the user; but are indirectly synchronized by the database systemitself. In this paper we present an overview of the maintenance strategies for 'AutomaticSummary Tables'; the materialized view implementation in IBM's DB2/UDB databasesystem. In the first part; we focus on the incremental maintenance method; providing a wayto synchronize materialized views based on the joins of only the changes of the base tables(deltas) with all other tables of the view definition. The second part of the paper outlinesoptimization techniques to improve the full recomputation of a set of materialized views.*current address is: University of Erlangen-Nuremberg; Martensstr. 3; D-91058 Erlangen …,ACM SIGMOD Intl. Conf. on Management of Data,2000,4
Über Aufbau und Auswertung multidimensionaler Daten (Kurzbeitrag),Wolfgang Lehner; Michael Teschke; Hartmut Wedekind,Kurzfassung Der Aufbau multidimensionaler Daten in extensionalen und intensionalenZusammenhängen wird mit Hilfe der Generalisierungshierarchie von Smith/Smithbeschrieben. Es zeigt sich; daß Auswertungen multidimensionaler Daten sowohlklassifikationsbezogen; dh extensional; als auch eigenschaftsbezogen; dh intensional;orientiert sind. Ein adäquater Auswertungsprozeß wird auf relationaler Basis beschriebenund ein dazu notwendiger Substitutionsoperator definiert. Insbesondere durch dieintensionale Analyse werden Datenwürfel sehr groß; was eine volle Auswertung sehr zeit-und speicheraufwendig macht und deshalb gegenüber 'Ad-hoc'-Auswertungenzurückzustellen ist.,*,1997,4
Data structure engineering for byte-addressable non-volatile memory,Ismail Oukid; Wolfgang Lehner,Abstract Storage Class Memory (SCM) is emerging as a viable alternative to traditionalDRAM; alleviating its scalability limits; both in terms of capacity and energy consumption;while being non-volatile. Hence; SCM has the potential to become a universal memory;blurring well-known storage hierarchies. However; along with opportunities; SCM bringsmany challenges. In this tutorial we will dissect SCM challenges and provide an in-depthview of existing programming models that circumvent them; as well as novel data structuresthat stem from these models. We will also elaborate on fail-safety testing challenges--anoften overlooked; yet important topic. Finally; we will discuss SCM emulation techniques forend-to-end testing of SCM-based software components. In contrast to surveys investigatingthe use of SCM in database systems; this tutorial is designed as a programming guide for …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,3
A database accelerator for energy-efficient query processing and optimization,Sebastian Haas; Oliver Arnold; Stefan Scholze; Sebastian Höppner; Georg Ellguth; Andreas Dixius; Annett Ungethüm; Eric Mier; Benedikt Nöthen; Emil Matúš; Stefan Schiefer; Love Cederstroem; Fabian Pilz; Christian Mayr; René Schüffny; Wolfgang Lehner; Gerhard P Fettweis,Data processing on a continuously growing amount of information and the increasing powerrestrictions have become an ubiquitous challenge in our world today. Besides parallelcomputing; a promising approach to improve the energy efficiency of current systems is tointegrate specialized hardware. This paper presents a Tensilica RISC processor extendedwith an instruction set to accelerate basic database operators frequently used in moderndatabase systems. The core was taped out in a 28 nm SLP CMOS technology and allowsenergy-efficient query processing as well as query optimization by applying selectivityestimation techniques. Our chip measurements show an 1000x energy improvement onselected database operators compared to state-of-the-art systems.,Nordic Circuits and Systems Conference (NORCAS); 2016 IEEE,2016,3
Compression-aware in-memory query processing: vision; system design and beyond,Juliana Hildebrandt; Dirk Habich; Patrick Damme; Wolfgang Lehner,Abstract In-memory database systems have to keep base data as well as intermediateresults generated during query processing in main memory. In addition; the effort to accessintermediate results is equivalent to the effort to access the base data. Therefore; theoptimization of intermediate results is interesting and has a high impact on the performanceof the query execution. For this domain; we propose the continuous use of lightweightcompression methods for intermediate results and have the aim of developing a balancedquery processing approach based on compressed intermediate results. To minimize theoverall query execution time; it is important to find a balance between the reduced transfertimes and the increased computational effort. This paper provides an overview and presentsa system design for our vision. Our system design addresses the challenge of integrating …,*,2016,3
Efficient approximate olap querying over time series,Kasun S Perera; Martin Hahmann; Wolfgang Lehner; Torben Bach Pedersen; Christian Thomsen,Abstract The ongoing trend for data gathering not only produces larger volumes of data; butalso increases the variety of recorded data types. Out of these; especially time series; egvarious sensor readings; have attracted attention in the domains of business intelligenceand decision making. As OLAP queries play a major role in these domains; it is desirable toalso execute them on time series data. While this is not a problem on the conceptual level; itcan become a bottleneck with regards to query run-time. In general; processing OLAPqueries gets more computationally intensive as the volume of data grows. This is a particularproblem when querying time series data; which generally contains multiple measuresrecorded at fine time granularities. Usually; this issue is addressed either by scaling uphardware or by employing workload based query optimization techniques. However …,Proceedings of the 20th International Database Engineering & Applications Symposium,2016,3
Modeling and querying spatial data warehouses on the semantic web,Nurefşan Gür; Katja Hose; Torben Bach Pedersen; Esteban Zimányi,Abstract The Semantic Web (SW) has drawn the attention of data enthusiasts; and alsoinspired the exploitation and design of multidimensional data warehouses; in anunconventional way. Traditional data warehouses (DW) operate over static data. Howevermultidimensional (MD) data modeling approach can be dynamically extended by definingboth the schema and instances of MD data as RDF graphs. The importance and applicabilityof MD data warehouses over RDF is widely studied yet none of the works support a spatiallyenhanced MD model on the SW. Spatial support in DWs is a desirable feature for enhancedanalysis; since adding encoded spatial information of the data allows to query with spatialfunctions. In this paper we propose to empower the spatial dimension of data warehousesby adding spatial data types and topological relationships to the existing QB4OLAP …,Joint International Semantic Technology Conference,2015,3
Direct transformation techniques for compressed data: general approach and application scenarios,Patrick Damme; Dirk Habich; Wolfgang Lehner,Abstract Lightweight data compression techniques like dictionary or run-length compressionplay an important role in main memory database systems. Having decided for acompression scheme for a dataset; the transformation to another scheme is very inefficienttoday. The common approach works as follows: First; the compressed data is decompressedusing the source decompression algorithm resulting in the materialization of the raw data inmain memory. Second; the compression algorithm of the destination scheme is applied. Thisindirect way relies on existing algorithms; but is very inefficient; since the wholeuncompressed data has to be materialized as an intermediate step. To overcome thesedrawbacks; we propose a novel approach called direct transformation; which avoids thematerialization of the whole uncompressed data. Our techniques are cache optimized to …,East European Conference on Advances in Databases and Information Systems,2015,3
Managed Query Processing within the SAP HANA Database Platform,Norman May; Alexander Böhm; Meinolf Block; Wolfgang Lehner,Abstract The SAP HANA database extends the scope of traditional database engines as itsupports data models beyond regular tables; eg text; graphs or hierarchies. Moreover; SAPHANA also provides developers with a more fine-grained control to define their databaseapplication logic; eg exposing specific operators which are difficult to express in SQL.Finally; the SAP HANA database implements efficient communication to dedicated clientapplications using more effective communication mechanisms than available with standardinterfaces like JDBC or ODBC. These features of the HANA database are complemented bythe extended scripting engine–an application server for server-side JavaScript applications–that is tightly integrated into the query processing and application lifecycle management. Asa result; the HANA platform offers more concise models and code for working with the …,Datenbank-Spektrum,2015,3
Relaxation of subgraph queries delivering empty results,Elena Vasilyeva; Maik Thiele; Adrian Mocan; Wolfgang Lehner,Abstract Graph databases with the property graph model are used in multiple domainsincluding social networks; biology; and data integration. They provide schema-flexiblestorage for data of a different degree of a structure and support complex; expressive queriessuch as subgraph isomorphism queries. The exibility and expressiveness of graphdatabases make it difficult for the users to express queries correctly and can lead tounexpected query results; eg empty results. Therefore; we propose a relaxation approach forsubgraph isomorphism queries that is able to automatically rewrite a graph query; such thatthe rewritten query is similar to the original query and returns a non-empty result set. Indetail; we present relaxation operations applicable to a query; cardinality estimationheuristics; and strategies for prioritizing graph query elements to be relaxed. To …,Proceedings of the 27th International Conference on Scientific and Statistical Database Management,2015,3
Modeling large time series for efficient approximate query processing,Kasun S Perera; Martin Hahmann; Wolfgang Lehner; Torben Bach Pedersen; Christian Thomsen,Abstract Evolving customer requirements and increasing competition force businessorganizations to store increasing amounts of data and query them for information at anygiven time. Due to the current growth of data volumes; timely extraction of relevantinformation becomes more and more difficult with traditional methods. In addition;contemporary Decision Support Systems (DSS) favor faster approximations over slowerexact results. Generally speaking; processes that require exchange of data becomeinefficient when connection bandwidth does not increase as fast as the volume of data. Inorder to tackle these issues; compression techniques have been introduced in many areasof data processing. In this paper; we outline a new system that does not query completedatasets but instead utilizes models to extract the requested information. For time series …,International Conference on Database Systems for Advanced Applications,2015,3
Towards a web-scale data management ecosystem demonstrated by SAP HANA,Franz Faerber; Jonathan Dees; Martin Weidner; Stefan Baeuerle; Wolfgang Lehner,Over the years; data management has diversified and moved into multiple directions; mainlycaused by a significant growth in the application space with different usage patterns; amassive change in the underlying hardware characteristics; and-last but not least-growingdata volumes to be processed. A solution matching these constraints has to cope with amultidimensional problem space including techniques dealing with a large number ofdomain-specific data types; data and consistency models; deployment scenarios; andprocessing; storage; and communication infrastructures on a hardware level. Specializeddatabase engines are available and are positioned in the market optimizing a particulardimension on the one hand while relaxing other aspects (eg web-scale deployment withrelaxed consistency). Today it is common sense; that there is no single engine which can …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,3
A study of partitioning and parallel UDF execution with the SAP HANA database,Philipp Große; Norman May; Wolfgang Lehner,Abstract Large-scale data analysis relies on custom code both for preparing the data foranalysis as well as for the core analysis algorithms. The map-reduce framework offers asimple model to parallelize custom code; but it does not integrate well with relationaldatabases. Likewise; the literature on optimizing queries in relational databases has largelyignored user-defined functions (UDFs). In this paper; we discuss annotations for user-defined functions that facilitate optimizations that both consider relational operators andUDFs. In this paper we focus on optimizations that enable the parallel execution of relationaloperators and UDFs for a number of typical patterns. A study on real-world data investigatesthe opportunities for parallelization of complex data flows containing both relationaloperators and UDFs.,Proceedings of the 26th International Conference on Scientific and Statistical Database Management,2014,3
Online horizontal partitioning of heterogeneous data,Kai Herrmann; Hannes Voigt; Wolfgang Lehner,Abstract In an increasing number of use cases; databases face the challenge of managingheterogeneous data. Heterogeneous data is characterized by a quickly evolving variety ofentities without a common set of attributes. These entities do not show enough regularity tobe captured in a traditional database schema. A common solution is to centralize the diverseentities in a universal table. Usually; this leads to a very sparse table. Although today'stechniques allow efficient storage of sparse universal tables; query efficiency is still aproblem. Queries that address only a subset of attributes have to read the whole universaltable including many irrelevant entities. A solution is to use a partitioning of the table; whichallows pruning partitions of irrelevant entities before they are touched. Creating andmaintaining such a partitioning manually is very laborious or even infeasible; due to the …,it–Information Technology,2014,3
ECAST: A Benchmark Framework for Renewable Energy Forecasting Systems.,Robert Ulbricht; Ulrike Fischer; Lars Kegel; Dirk Habich; Hilko Donker; Wolfgang Lehner,ABSTRACT The increasing capacities of renewable energy sources and the opportunitiesemerging from the smart grid technology lead to new challenges for energy forecasters.Energy output fluctuates stronger compared to conventional power production. More timeseries data is available through the usage of sensor technology. New supply forecastingapproaches are developed to better address those characteristics; but meaningfulbenchmarks of such solutions are rare. Conducting detailed evaluations is time-intensiveand unattractive to customers as this is mostly handwork. We define and discussrequirements for efficient and reliable benchmarks of renewable energy supply forecastingtools. To cope with those requirements; we introduce the automated benchmark frameworkECAST as our proposed solution. The system's capability is demonstrated on a real-world …,EDBT/ICDT Workshops,2014,3
Transactional data management services for the cloud,Wolfgang Lehner; Kai-Uwe Sattler,Abstract Since the early years of database applications; transactional processing has beenone of the main use cases of database systems. Applications like booking; billing; fundtransfer in banking; and order processing–usually known as Online TransactionalProcessing (OLTP) applications–require not only to manipulate data but also to ensureatomicity and consistency even in case of concurrent updates.,*,2013,3
Identifying and weighting integration hypotheses on open data platforms,Julian Eberius; Katrin Braunschweig; Maik Thiele; Wolfgang Lehner,Abstract Open data platforms such as data. gov or opendata. socrata. com provide a hugeamount of valuable information; publicly available to anyone. This data has the potential todrive innovation and lead to a more democratic and transparent society. Still; the platforms itis offered on have some unique problems: Their free-for-all nature; the lack of publishingstandards and the multitude of domains and authors represented on these platforms lead tonew integration and standardization problems; such as duplicated or partitioned datasets. Atthe same time; crowd-based data integration techniques are emerging as new way ofdealing with data integration problems. However; these methods still require input in form ofspecific questions or tasks that can be passed to the crowd. This paper identifies severalclasses of integration problems on Open Data Platforms; and proposes a method for …,Proceedings of the First International Workshop on Open Data,2012,3
Adaptive index buffer,Hannes Voigt; Tobias Jaekel; Thomas Kissinger; Wolfgang Lehner,With rapidly increasing datasets and more dynamic workloads; adaptive partial indexingbecomes an important way to keep indexing efficiently. During times of changing workloads;the query performance suffers from inefficient tables scans while the index tuningmechanism adapts the partial index. In this paper we present the Adaptive Index Buffer. TheAdaptive Index Buffer reduces the cost of table scans by quickly indexing tuples in memoryuntil the partial index has adapted to the workload again. We explain the basic operatingmode of an Index Buffer and discuss how it adapts to changing workload situations. Further;we present three experiments that show the Index Buffer at work.,Data Engineering Workshops (ICDEW); 2012 IEEE 28th International Conference on,2012,3
System and method for maintaining and utilizing Bernoulli samples over evolving multisets,*,One embodiment of the present invention provides a method for incrementally maintaining aBernoulli sample S with sampling rate q over a multiset R in the presence of update; delete;and insert transactions. The method includes processing items inserted into R usingBernoulli sampling and augmenting S with tracking counters during this processing. Itemsdeleted from R are processed by using the tracking counters and by removing newly deleteditems from S using a calculated probability while maintaining a degree of uniformity in S.,*,2012,3
Browsing Robust Clustering-Alternatives.,Martin Hahmann; Dirk Habich; Wolfgang Lehner,Abstract. In the last years; new clustering approaches utilizing the notion of multipleclusterings have gained attention. Two general directions—each with its individual benefits—are identifiable:(i) extraction of multiple alternative clustering solutions from one dataset and(ii) combination of multiple clusterings of a dataset into one robust consensussolution. In thispaper; we propose a novel hybrid approach to generate and browse robust; alternativeclustering results. Our hybrid approach is based on frequent-groupings as specialization offrequent-itemset mining. In this way; the different benefits of the existing directions arecombined; offering new opportunities for knowledge extraction.,MultiClust@ ECML/PKDD,2011,3
Next Generation Database Programming and Execution Environment.,Dirk Habich; Matthias Boehm; Maik Thiele; Benjamin Schlegel; Ulrike Fischer; Hannes Voigt; Wolfgang Lehner,ABSTRACT The database research is always on the move. In order to integrate novelconcepts; the significance of the database programmability aspect more and moreincreases. The programmability aspect focuses on internal components as well as onprinciple to push-down application logic to the database system. In this paper; we propose anovel database programming model and a corresponding database architecture frameworkenabling extensibility and a better integration of application code into DBMS. In detail; wepresent a scripting language pyDBL which is unified utilizable to implement physicaldatabase operators; query plans and even complete applications. We demonstrate theapplicability of our approach in terms of a moderate performance overhead.,DBPL,2011,3
Cardinality estimation in ETL processes,Maik Thiele; Tim Kiefer; Wolfgang Lehner,Abstract The cardinality estimation in ETL processes is particularly difficult. Aside from thewell-known SQL operators; which are also used in ETL processes; there are a variety ofoperators without exact counterparts in the relational world. In addition to those; we findoperators that support very specific data integration aspects. For such operators; there areno well-examined statistic approaches for cardinality estimations. Therefore; we propose ablack-box approach and estimate the cardinality using a set of statistic models for eachoperator. We discuss different model granularities and develop an adaptive cardinalityestimation framework for ETL processes. We map the abstract model operators to specificstatistic learning approaches (regression; decision trees; support vector machines; etc.) andevaluate our cardinality estimations in an extensive experimental study.,Proceedings of the ACM twelfth international workshop on Data warehousing and OLAP,2009,3
Robust and distributed top-n frequent-pattern mining with SAP BW accelerator,Thomas Legler; Wolfgang Lehner; Jan Schaffner; Jens Krüger,Abstract Mining for association rules and frequent patterns is a central activity in data mining.However; most existing algorithms are only moderately suitable for real-world scenarios.Most strategies use parameters like minimum support; for which it can be very difficult todefine a suitable value for unknown datasets. Since most untrained users are unable orunwilling to set such technical parameters; we address the problem of replacing theminimum-support parameter with top-n strategies. In our paper; we start by extending a top-nimplementation of the ECLAT algorithm to improve its performance by using heuristic searchstrategy optimizations. Also; real-world datasets are often distributed and modern databasearchitectures are switching from expensive SMPs to cheaper shared-nothing blade servers.Thus; most mining queries require distribution handling. Since partitioning can be forced …,Proceedings of the VLDB Endowment,2009,3
Innovative Process Execution in Service-oriented Environments.,Dirk Habich; Steffen Preissler; Hannes Voigt; Wolfgang Lehner,Abstract: Today's information systems are often built on the foundation of service-orientedenvironments. Although the fundamental purpose of an information system is the processingof data and information; the service-oriented architecture (SOA) does not treat data as a corefirst class citizen. Current SOA technologies support neither the explicit modeling of dataflows in common business process modeling languages (such as BPMN) nor the usage ofspecialized data transformation and propagation technologies (for instance ETL-tools) onthe process execution layer (BPEL). In this paper; we introduce our data-aware approach onthe execution perspective as well as on the modeling perspective of business processes.,ICEIS (1),2009,3
Model-driven generation of dynamic adapters for integration platforms,Matthias Böhm; Jürgen Bittner; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Abstract. The concept of Enterprise Application Integration (EAI) is widely used forintegrating heterogeneous applications and systems via message-based communication.Typically; EAI servers provide a huge set of specific inbound and outbound adapters usedfor interacting with the external systems and for converting proprietary message formats.However; one major shortcoming in currently available products is the monolithic design ofthose adapters; resulting in performance deficits caused by the need for data independence.Further; also functional restrictions must be noticed here. In this paper; we give (1) a detailedproblem characterization; followed by (2) a discussion of alternative data representationsand adapter architectures; and (3) we introduce our model-driven DIEFOS (dataindependence; efficiency and functional flexibility using feature-oriented software …,1st International Workshop on Model Driven Interoperability for Sustainable Information Systems,2008,3
Exploiting self-monitoring sample views for cardinality estimation,Per-Ake Larson; Wolfgang Lehner; Jingren Zhou; Peter Zabback,Abstract Good cardinality estimates are critical for generating good execution plans duringquery optimization. Complex predicates; correlations between columns; and user-definedfunctions are extremely hard to handle when using the traditional histogram approach. Thisdemo illustrates the use of sample views for cardinality estimations as prototyped inMicrosoft SQL Server. We show the creation of sample views; discuss how they areexploited during query optimization; and explain their potential effect on query plans. Inaddition; we also show our implementation of maintenance policies using statistical qualitycontrol techniques based on query feedback.,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,3
Ein Nachrichtentransformationsmodell für komplexe Transformationsprozesse in datenzentrischen Anwendungsszenarien.,Matthias Böhm; Jürgen Bittner; Uwe Wloka; Dirk Habich; Wolfgang Lehner,Abstract: Die horizontale Integration von Systemen durch eine nachrichtenbasierteKommunikation über Middleware-Produkte stellt eine; sich immer weiter verbreitende; Artder Anwendungsintegration dar; um eine hinreichend lose Kopplung der partizipierendenSysteme und Anwendungen zu gewährleisten. Für die Beschreibung derartigerIntegrationsprozesse kommen zunehmend funktional-orientierteProzessbeschreibungssprachen wie beispielsweise WSBPEL zum Einsatz; welcheallerdings Defizite bei der Beschreibung von datenzentrischen Anwendungsszenarienoffenbaren. Dieses Papier leistet einen Beitrag zur systematischen Modellbildung fürkomplexe Nachrichtentransformationen in datenzentrischen Prozessen. Die Realisierbarkeitder Ergebnisse wird dabei an der Integrationsplattform TransConnect R,BTW,2007,3
Euro-Par 2006 Parallel Processing: 12th International Euro-Par Conference; Dresden; Germany; August 28-September 1; 2006; Proceedings,Wolfgang E Nagel; Wolfgang V Walter; Wolfgang Lehner,This book constitutes the refereed proceedings of the 12th International Conference onParallel Computing; Euro-Par 2006. The book presents 110 carefully reviewed; revisedpapers. Topics include support tools and environments; performance prediction andevaluation; scheduling and load balancing; compilers for high performance; parallel anddistributed databases; data mining and knowledge discovery; grid and cluster computing:models; middleware and architectures; parallel computer architecure and instruction-levelparallelism; distributed systems and algorithms; and more.,*,2006,3
Materialized views in the presence of reporting functions,Dirk Habich; Wolfgang Lehner; Michael Just,Materialized views are a well-known optimization strategy with the potential for massiveimprovements in query processing time; especially for aggregation queries over largetables. To realize this potential; the query optimizer has to know how and when to exploitmaterialized views. Reporting functions represent a novel technique to formulate sequence-oriented queries in SQL. They provide a column-wise ordering; partitioning; and windowingmechanism for aggregation functions and therefore extend the well-known way of groupingand applying simple aggregation functions. Up to now; current work has not considered thefrequently used reporting functions in data warehouse environments. In this paper; weintroduce materialized reporting function views and show how to rewrite queries withreporting functions as well as aggregation queries to this new kind of materialized view …,Scientific and Statistical Database Management; 2006. 18th International Conference on,2006,3
Optimistic coarse-grained cache semantics for data marts,Maik Thiele; Jens Albrecht; Wolfgang Lehner,Data marts and caching are two closely related concepts in the domain of multi-dimensionaldata. Both store pre-computed data to provide fast response times for complex OLAPqueries; and for both it must be guaranteed that every query can be completely processed.However; they differ extremely in their update behaviour which we utilise to build a specificdata mart extended by cache semantics. In this paper; we introduce a novel cacheexploitation concept for data marts-coarse-grained caching-in which the containednesscheck for a multi-dimensional query is done through the comparison of the expected and theactual cardinalities. Therefore; we subdivide the multi-dimensional data into coarsepartitions; the so called cubletets; which allow to specify the completeness criteria forincoming queries. We show that during query processing; the completeness check is …,Scientific and Statistical Database Management; 2006. 18th International Conference on,2006,3
Hierarchical group-based sampling,Rainer Gemulla; Henrike Berthold; Wolfgang Lehner,Abstract Approximate query processing is an adequate technique to reduce response timesand system load in cases where approximate results suffice. In database literature; samplinghas been proposed to evaluate queries approximately by using only a subset of the originaldata. Unfortunately; most of these methods consider either only certain problems arising dueto the use of samples in databases (eg data skew) or only join operations involving multiplerelations. We describe how well-known sampling techniques dealing with group-byoperations can be combined with foreign-key joins such that the join is computed after thegeneration of the sample. In detail; we show how senate sampling and small groupsampling can be combined efficiently with the idea of join synopses. Additionally; weintroduce different algorithms which maintain the sample if the underlying data changes …,British National Conference on Databases,2005,3
Hybrid modeling of operational and analytical data at precision dairy farming,C Schulze; J Spilke; W Lehner,Abstract The concept of Precision Dairy Farming requires a suitable type of datamanagement for its practical implementation. During the modeling process; both operativeand analytical use of data have to be considered. By using the entity-relationship model(E/RM); the analytical view cannot be modeled adequately. Analysis support (eg OLAP)requires a multi-dimensional view; which has been modeled by us with the help of the multi-dimensional extension of the E/RM (mE/RM). In this paper; we present the results of themodeling process with a concrete example. While doing so; we will introduce requirements;deficits and necessary extensions. The method shown here is important for a variety of tasksin the field of precision farming beyond the scenario we use.,Tagungsband EFITA/WCCA2005,2005,3
Informationsbedarfsanalyse als Grundlage der Datenmodellierung in Rahmen des Precision Dairy Farming.,Christian Schulze; Joachim Spilke; Wolfgang Lehner,Abstract: Aus dem Konzept des Precision Dairy Farming leiten sich hohe Anforderungen andie Gestaltung der Informationsverarbeitung ab. Grundlage einer Umsetzung kann nur eineintegrierende und konsistente Datenbasis sein. Neben prozessorientierten; betriebsinternenAufgaben müssen ebenso erweiterte; überbetriebliche Belange Berücksichtigung finden.Erster Schritt zur Entwicklung eines angepassten Datenmodells ist die Analyse desInformationsbedarfes. Die dargestellten Ergebnisse der Informationsbedarfsanalyseverdeutlichen die Komplexität der Thematik.,GIL Jahrestagung,2004,3
Scintra: A model for quantifying inconsistencies in grid-organized sensor database systems,Lutz Schlesinger; Wolfgang Lehner,Abstract Sensor data sets are usually collected in a centralized sensor database system orreplicated cached in a distributed system to speed up query evaluation. However; a highdata refresh rate disallows the usage of traditional replicated approaches with its strongconsistency property. Instead we propose a combination of grid computing technology withsensor database systems. Each node holds cached data of other grid members. Sincecached information may become stale fast; the access to outdated data may sometimes beacceptable if the user has knowledge about the degree of inconsistency if unsynchronizeddata are combined. The contribution of this paper is the presentation and discussion of amodel for describing inconsistencies in grid organized sensor database systems.,European Conference on Parallel Processing,2003,3
WebService-basierte Integration externer Datenquellen in relationale Datenbanksysteme.,Lutz Schlesinger; Wolfgang Lehner,Kurzfassung: Traditionell werden Datenbanksysteme als reine Datenverwaltungssysteme inunterschiedlichsten Anwendungsgebieten verwendet. Die Nutzung verteilt gehaltenerInformationen und eine Verknüpfung mit lokalen Daten wird insbesondere bei der Rolle derDatenbanksysteme als zentrale Integrationsplattform immer wichtiger. Währendkommerzielle Systeme proprietäre Schnittstellen als Erweiterungsmechanismus anbieten;wird in diesem Beitrag ein Rahmenwerk und eine Implementierung vorgestellt; welchedurch Nutzung einer satzorientierten Schnittstelle nach dem ONC-Protokoll sowohl eineenge Kopplung auf Basis von Java-Objekten als auch eine lose Kopplung durch Nutzungvon WebService-Technologien ermöglicht. Die Leistung des Systems besteht somit darin;dass beliebige über WebService bereitgestellte Dienste; die eine minimale vorgegebene …,XMIDX,2003,3
Building An Information Marketplace using a Content and Memory based Publish/Subscribe System,Wolfgang Lehner; Wolfgang Hümmer; Michael Redert,Abstract The well-known publish/subscribe paradigm offers a reliable and scalable groupcommunication mechanism. This paper proposes the PubScribe framework using a contentand memory based publish/subscribe system to provide an XML-based access open foreverybody to participate as a publisher as well as a subscriber of new information. Inaddition to pure event notification or message brokering systems; the PubScribe frameworktargets at processing published messages and deriving result sets for registeredsubscriptions. The processing strategy aims to support'Business Intelligence'applications byproviding appropriate operators as well as local memory inside the brokerage component.The paper discusses different subscription semantics; the communication model; theprocessing model; and finally gives an overview of the prototypical implementation.,*,2001,3
Use and reuse of association rules in an OLAP environment,Holger Günzel; Jens Albrecht; Wolfgang Lehner,*,Proceedings of the 2000 information resources management association international conference on Challenges of information technology management in the 21st century,2000,3
A redundancy-based optimization approach for aggregation in multidimensional scientific and statistical databases,Wolfgang Lehner; Thomas Ruf,Abstract Large data volumes; flexible drill-down analysis and short query response times;which are predominant characteristics of Scientific and Statistical Data Base (SSDB)applications; require new optimization techniques as compared to traditional DBMS. Thispaper describes formally an optimization approach in the SSDB domain which is based onthe re-use of materialized results of former queries to process aggregate queries along aclassification hierarchy. The description of the approach is embedded in the CROSS-DBmodel (which stands for Classification-oriented; Redundancy-based Optimization ofScientific and Statistical Data-Bases). It will be shown that the approach taken can improvequery response time by orders of magnitude while adding tolerable storage andmaintenance overhead to the database.,*,1997,3
Robust and simple database evolution,Kai Herrmann; Hannes Voigt; Jonas Rausch; Andreas Behrend; Wolfgang Lehner,Abstract Software developers adapt to the fast-moving nature of software systems with agiledevelopment techniques. However; database developers lack the tools and concepts tokeep the pace. Whenever the current database schema is evolved; the already existing dataneeds to be evolved as well. This is usually realized with manually written SQL scripts;which is error-prone and explains significant costs in software projects. A promising solutionare declarative database evolution languages; which couple both schema and dataevolution into intuitive operations. Existing database evolution languages focus on usabilitybut do not strive for completeness. However; this is an inevitable prerequisite to avoidcomplex and error-prone workarounds. We present CoDEL which is based on an existinglanguage but is relationally complete. We precisely define its semantic using relational …,Information Systems Frontiers,2018,2
Memory management techniques for large-scale persistent-main-memory systems,Ismail Oukid; Daniel Booss; Adrien Lespinasse; Wolfgang Lehner; Thomas Willhalm; Grégoire Gomes,Abstract Storage Class Memory (SCM) is a novel class of memory technologies that promiseto revolutionize database architectures. SCM is byte-addressable and exhibits latenciessimilar to those of DRAM; while being non-volatile. Hence; SCM could replace both mainmemory and storage; enabling a novel single-level database architecture without thetraditional I/O bottleneck. Fail-safe persistent SCM allocation can be considered conditiosine qua non for enabling this novel architecture paradigm for database managementsystems. In this paper we present PAllocator; a fail-safe persistent SCM allocator whosedesign emphasizes high concurrency and capacity scalability. Contrary to previous works;PAllocator thoroughly addresses the important challenge of persistent memoryfragmentation by implementing an efficient defragmentation algorithm. We show that …,Proceedings of the VLDB Endowment,2017,2
Generating what-if scenarios for time series data,Lars Kegel; Martin Hahmann; Wolfgang Lehner,Abstract Time series data has become a ubiquitous and important data source in manyapplication domains. Most companies and organizations strongly rely on this data for criticaltasks like decision-making; planning; predictions; and analytics in general. While all thesetasks generally focus on actual data representing organization and business processes; it isalso desirable to apply them to alternative scenarios in order to prepare for developmentsthat diverge from expectations or assess the robustness of current strategies. When it comesto the construction of such what-if scenarios; existing tools either focus on scalar data or theyaddress highly specific scenarios. In this work; we propose a generally applicable and easy-to-use method for the generation of what-if scenarios on time series data. Our approachextracts descriptive features of a data set and allows the construction of an alternate …,Proceedings of the 29th International Conference on Scientific and Statistical Database Management,2017,2
Table identification and reconstruction in spreadsheets,Elvis Koci; Maik Thiele; Oscar Romero; Wolfgang Lehner,Abstract Spreadsheets are one of the most successful content generation tools; used inalmost every enterprise to perform data transformation; visualization; and analysis. The highdegree of freedom provided by these tools results in very complex sheets; intermingling theactual data with formatting; formulas; layout artifacts; and textual metadata. To unlock thewealth of data contained in spreadsheets; a human analyst will often have to understandand transform the data manually. To overcome this cumbersome process; we propose aframework that is able to automatically infer the structure and extract the data from thesedocuments in a canonical form. In this paper; we describe our heuristics-based method fordiscovering tables in spreadsheets; given that each cell is classified as either header;attribute; metadata; data; or derived. Experimental results on a real-world dataset of 439 …,International Conference on Advanced Information Systems Engineering,2017,2
Insights into the Comparative Evaluation of Lightweight Data Compression Algorithms.,Patrick Damme; Dirk Habich; Juliana Hildebrandt; Wolfgang Lehner,ABSTRACT Lightweight data compression is frequently applied in inmemory databasesystems to tackle the growing gap between processor speed and main memory bandwidth.In recent years; the number of available compression algorithms has grown considerably.Since the correct choice of one of these algorithms requires understanding of theirperformance behavior; we systematically evaluated several stateof-the-art compressionalgorithms on a multitude of different data characteristics. In this demonstration; the attendeewill learn our findings in an interactive tour through our obtained measurements. The mostimportant insight is that there is no single-best algorithm; but that the choice depends on thedata characteristics and is non-trivial.,EDBT,2017,2
SAP HANA–The Evolution of an In-Memory DBMS from Pure OLAP Processing Towards Mixed Workloads,Norman May; Alexander Böhm; Wolfgang Lehner,The journey of SAP HANA started as an in-memory appliance for complex; analyticalapplications. The success of the system quickly motivated SAP to broaden the scope fromthe OLAP workloads the system was initially architected for to also handle transactionalworkloads; in particular to support its Business Suite flagship product. In this paper; wehighlight some of the core design changes to evolve an in-memory column store systemtowards handling OLTP workloads. We also discuss the challenges of running mixedworkloads with low-latency OLTP queries and complex analytical queries in the context ofthe same database management system and give an outlook on the future databaseinteraction patterns of modern business applications we see emerging currently.,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,2
Putting web tables into context,Katrin Braunschweig; Maik Thiele; Elvis Koci; Wolfgang Lehner,Google; Inc. (search). SIGN IN SIGN UP. Putting Web Tables into Context.Authors: Katrin Braunschweig; Technische Universität Dresden. MaikThiele; Technische Universität Dresden. Elvis Koci.,Proceedings of the International Joint Conference on Knowledge Discovery; Knowledge Engineering and Knowledge Management,2016,2
Optimization of parallelization of user-defined functions with flexible partitioning,*,Technologies are disclosed for generating query execution plans optimized for parallelexecution for programs having both core database relational functions and user-definedfunctions. A variety of optimization strategies can be employed to improve performance in aparallel execution scenarios. A flexible range of permitted partition arrangements can bespecified as acceptable to parallelized instances of the user-defined function. The optimizercan leverage such information when constructing an optimized query execution plan.Partitioning arrangements or other properties can be leveraged to avoid additional orunnecessary processing.,*,2016,2
Topology-aware optimization of big sparse matrices and matrix multiplications on main-memory systems,David Kernert; Wolfgang Lehner; Frank Köhler,Since data sizes of analytical applications are continuously growing; many data scientistsare switching from customized micro-solutions to scalable alternatives; such as statisticaland scientific databases. However; many algorithms in data mining and science areexpressed in terms of linear algebra; which is barely supported by major database vendorsand big data solutions. On the other side; conventional linear algebra algorithms and legacymatrix representations are often not suitable for very large matrices. We propose a strategyfor large matrix processing on modern multicore systems that is based on a novel; adaptivetile matrix representation (AT MATRIX). Our solution utilizes multiple techniques inspiredfrom database technology; such as multidimensional data partitioning; cardinality estimation;indexing; dynamic rewrites; and many more in order to optimize the execution time …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,2
InVerDa-co-existing schema versions made foolproof,Kai Herrmann; Hannes Voigt; Thorsten Seyschab; Wolfgang Lehner,In modern software landscapes multiple applications usually share one database as theirsingle point of truth. All these applications will evolve over time by their very nature. Oftenformer versions need to stay available; so database developers find themselves maintainingco-existing schema version of multiple applications in multiple versions. This is highly error-prone and accounts for significant costs in software projects; as developers realize thetranslation of data accesses between schema versions with hand-written delta code. In thisdemo; we showcase INVERDA; a tool for integrated; robust; and easy to use databaseversioning. We rethink the way of specifying the evolution to new schema versions. Usingthe richer semantics of a descriptive database evolution language; we generate all requiredartifacts automatically and make database versioning foolproof.,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,2
Modularization of lightweight data compression algorithms,Juliana Hildebrandt; Dirk Habich; Patrick Damme; Wolfgang Lehner,Abstract Modern database systems are very often in the position to store their entire data inmain memory. Aside from increased main memory capacities; a further driver for in-memorydatabase systems was the shift to a column-oriented storage format in combination withlightweight data compression techniques. Using both mentioned software concepts; largedatasets can be held and processed in main memory with a low memory footprint. In recentyears; a lot of lightweight data compression algorithms have been developed to efficientlysupport different data characteristics. In our research work; we have investigated a largenumber of those algorithms to determine similarities and differences between the algorithms.Based on this survey; we developed a novel modularization concept which can be used todescribe and to implement a wide range of lightweight compression algorithms in a …,*,2015,2
Exploiting big data in time series forecasting: A cross-sectional approach,Claudio Hartmann; Martin Hahmann; Wolfgang Lehner; Frank Rosenthal,Forecasting time series data is an integral component for management; planning anddecision making. Following the Big Data trend; large amounts of time series data areavailable from many heterogeneous data sources in more and more applications domains.The highly dynamic and often fluctuating character of these domains in combination with thelogistic problems of collecting such data from a variety of sources; imposes new challengesto forecasting. Traditional approaches heavily rely on extensive and complete historical datato build time series models and are thus no longer applicable if time series are short or; evenmore important; intermittent. In addition; large numbers of time series have to be forecastedon different aggregation levels with preferably low latency; while forecast accuracy shouldremain high. This is almost impossible; when keeping the traditional focus on creating …,Data Science and Advanced Analytics (DSAA); 2015. 36678 2015. IEEE International Conference on,2015,2
GraphVista: Interactive Exploration Of Large Graphs,Marcus Paradies; Michael Rudolf; Wolfgang Lehner,Abstract: The potential to gain business insights from graph-structured data through graphanalytics is increasingly attracting companies from a variety of industries; ranging from webcompanies to traditional enterprise businesses. To analyze a graph; a user often executesisolated graph queries using a dedicated interface---a procedural graph programminginterface or a declarative graph query language. The results are then returned anddisplayed using a specific visualization technique. This follows the classical ad-hoc Query$\rightarrow $ Result interaction paradigm and often requires multiple query iterations untilan interesting aspect in the graph data is identified. This is caused on the one hand by theschema flexibility of graph data and on the other hand by the intricacies of declarative graphquery languages. To lower the burden for the user to explore an unknown graph without …,arXiv preprint arXiv:1506.00394,2015,2
Optimierung der Anfrageverarbeitung mittels Kompression der Zwischenergebnisse,Dirk Habich; Patrick Damme; Wolfgang Lehner,In Hauptspeicher-zentrischen Architekturansätzen für Datenbanksysteme müssen für dieAnfrageverarbeitung sowohl die Basisrelationen als auch die Zwischenergebnisse imHauptspeicher gehalten werden. Des Weiteren ist der Aufwand; um Zwischenergebnisse zugenerieren; äquivalent zum Aufwand; um Änderungen an den Basisrelationendurchzuführen. Daher sollten zur effizienten Anfrageverarbeitung die Zwischenergebnisseso organisiert werden; dass eine effiziente Verarbeitung im Anfrageplan möglich ist. Fürdiesen Bereich schlagen wir den durchgängigen Einsatz leichtgewichtigerKompressionsverfahren für Zwischenergebnisse vor und haben das Ziel; eineausgewogene Anfrageverarbeitung auf Basis komprimierter Zwischenergebnisse zuentwickeln. Dieser Artikel gibt einen Überblick über unser Konzept und die …,Datenbanksysteme für Business; Technologie und Web (BTW 2015),2015,2
Systematical evaluation of solar energy supply forecasts,Robert Ulbricht; Martin Hahmann; Hilko Donker; Wolfgang Lehner,Abstract The capacity of renewable energy sources constantly increases world-wide andchallenges the maintenance of the electric balance between power demand and supply. Toallow for a better integration of solar energy supply into the power grids; a lot of researchwas dedicated to the development of precise forecasting approaches. However; there is stillno straightforward and easy-to-use recommendation for a standardized forecasting strategy.In this paper; a classification of solar forecasting solutions proposed in the literature isprovided for both weather-and energy forecast models. Subsequently; we describe our ideaof a standardized forecasting process and the typical parameters possibly influencing theselection of a specific model. We discuss model combination as an optimization option andevaluate this approach comparing different statistical algorithms against flexible hybrid …,International Workshop on Data Analytics for Renewable Energy Integration,2014,2
Flexible relational data model–a common ground for schema-flexible database systems,Hannes Voigt; Wolfgang Lehner,Abstract An increasing number of application fields represent dynamic and open discoursescharacterized by high mutability; variety; and pluralism in data. Data in dynamic and opendiscourses typically exhibits an irregular schema. Such data cannot be directly representedin the traditional relational data model. Mapping strategies allow representation but increasedevelopment and maintenance costs. Likewise; NoSQL systems offer the required schemaflexibility but introduce new costs by not being directly compatible with relational systemsthat still dominate enterprise information systems. With the Flexible Relational Data Model(FRDM) we propose a third way. It allows the direct representation of data with irregularschemas. It combines tuple-oriented data representation with relation-oriented dataprocessing. So that; FRDM is still relational; in contrast to other flexible data models …,East European Conference on Advances in Databases and Information Systems,2014,2
Cinderella—Adaptive online partitioning of irregularly structured data,Kai Herrmann; Hannes Voigt; Wolfgang Lehner,In an increasing number of use cases; databases face the challenge of managing irregularlystructured data. Irregularly structured data is characterized by a quickly evolving variety ofentities without a common set of attributes. These entities do not show enough regularity tobe captured in a traditional database schema. A common solution is to centralize the diverseentities in a universal table. Usually; this leads to a very sparse table. Although today'stechniques allow efficient storage of sparse universal tables; query efficiency is still aproblem. Queries that reference only a subset of attributes have to read the whole universaltable including many irrelevant entities. One possible solution is to use a partitioning of thetable; which allows pruning partitions of irrelevant entities before they are touched. Creatingand maintaining such a partitioning manually is very laborious or even infeasible; due to …,Data Engineering Workshops (ICDEW); 2014 IEEE 30th International Conference on,2014,2
pEDM: Online-forecasting for smart energy analytics,Lars Dannecker; Philipp Rösch; Ulrike Fischer; Gordon Gaumnitz; Wolfgang Lehner; Gregor Hackenbroich,Abstract Continuous balancing of energy demand and supply is a fundamental prerequisitefor the stability of energy grids and requires accurate forecasts of electricity consumption andproduction at any point in time. Today's Energy Data Management (EDM) systems alreadyprovide accurate predictions; but typically employ a very time-consuming and inflexibleforecasting process. However; emerging trends such as intra-day trading and an increasingshare of renewable energy sources need a higher forecasting efficiency. Additionally; thewide variety of applications in the energy domain pose different requirements with respect toruntime and accuracy and thus; require flexible control of the forecasting process. To solvethis issue; we introduce our novel online forecasting process as part of our EDM systemcalled pEDM. The online forecasting process rapidly provides forecasting results and …,Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,2013,2
Pack Indexing for Time-Constrained In-Memory Query Processing.,Tobias Jaekel; Hannes Voigt; Thomas Kissinger; Wolfgang Lehner,Abstract: Main memory databases management systems are used more often and in a widespread of application scenarios. To take significant advantage of the main memory readperformance; most techniques known from traditional disk-centric database systems have tobe adapted and re-designed. In the field of indexing; many mainmemory-optimized indexstructures have been proposed. Most of these works aim at primary indexing. Secondaryindexes are rarely considered in the context of main memory databases. Either queryperformance is sufficiently good without secondary indexing or main memory is a resourcetoo scarce to invest in huge secondary indexes. A more subtle trade between benefit andcosts of secondary indexing has not been considered so far. In this paper we present PackIndexing; a secondary indexing technique for main memory databases that allows a …,BTW,2013,2
Rethinking Energy Data Management: Trends and Challenges in Today's Transforming Markets.,Robert Ulbricht; Ulrike Fischer; Wolfgang Lehner; Hilko Donker,Abstract: The energy market domain is subject to a continuous transformation process;mostly driven by governmental regulations. To efficiently handle the large amounts of dataand the communication processes between market participants; specialized databaseapplications have been developed. In this paper; we present the energy data managementsystem (EDMS) as a standard software solution; describing its core components and typicalsystem integration aspects. However; current market topics like smart metering; energysaving; forecasting for renewable energy sources; mobile consumption and smart grids leadto new database challenges. We provide an overview of these trends and discuss theirimpact on existing information systems; focusing on the technical challenges of dataintegration; data storage; data analytics and scalability. As energy data management has …,BTW,2013,2
SMIX Live--A Self-Managing Index Infrastructure for Dynamic Workloads,Thomas Kissinger; Hannes Voigt; Wolfgang Lehner,As databases accumulate growing amounts of data at an increasing rate; adaptive indexingbecomes more and more important. At the same time; applications and their use get moreagile and flexible; resulting in less steady and less predictable workload characteristics.Being inert and coarse-grained; state-of-the-art index tuning techniques become less usefulin such environments. Especially the full-column indexing paradigm results in lot of indexedbut never queried data and prohibitively high memory and maintenance costs. In ourdemonstration; we present Self-Managing Indexes; a novel; adaptive; fine-grained;autonomous indexing infrastructure. In its core; our approach builds on a novel access paththat automatically collects useful index information; discards useless index information; andcompetes with its kind for resources to host its index information. Compared to existing …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,2
Touch it; Mine it; View it; Shape it.,Martin Hahmann; Dirk Habich; Wolfgang Lehner,Abstract: To benefit from the large amounts of data; gathered in more and more applicationdomains; analysis techniques like clustering have become a necessity. As their applicationexpands; a lot of unacquainted users come into contact with these techniques.Unfortunately; most clustering approaches are complex and/or scenario specific; whichmakes clustering a challenging domain to access. In this demonstration; we want to presenta clustering process; that can be used in a hands-on way.,BTW,2011,2
Approximate query answering and result refinement on xml data,Katja Seidler; Eric Peukert; Gregor Hackenbroich; Wolfgang Lehner,Abstract Today; many economic decisions are based on the fast analysis of XML data. Yet;the time to process analytical XML queries is typically high. Although current XMLtechniques focus on the optimization of query processing; none of these support earlyapproximate feedback as possible in relational Online Aggregation systems. In this paper;we introduce a system that provides fast estimates to XML aggregation queries. Whileprocessing; these estimates and the assigned confidence bounds are constantly improving.In our evaluation; we show that without significantly increasing the overall execution time oursystem returns accurate guesses of the final answer long before traditional systems are ableto produce output.,International Conference on Scientific and Statistical Database Management,2010,2
Multi-process Optimization Via Horizontal Message Queue Partitioning.,Matthias Böhm; Dirk Habich; Wolfgang Lehner,Page 1. ICEIS 2010 Databases and Information Systems Integration Multi-Process Optimizationvia Horizontal Message Queue Partitioning Matthias Böhm; Dirk Habich; Wolfgang Lehner DresdenUniversity of Technology © Prof. Dr.-Ing. Wolfgang Lehner | 10.06.2010 Page 2. > MotivationInformation S P id Systems Pyramid ▪ Horizontal Integration ▪ Data synchronization by many smallmessages → EAI servers; MOM systems ▪ Vertical Integration ▪ Trend towards Operational BI;direct data propagation to the DWH → (near) real-time ETL High Performance Requirements ▪Many independent instances of integration flows → Optimization is required Matthias Böhm |Multi-Process Optimization via Horizontal Message Queue Partitioning | 2 y p g ▪ Temporalconsistency between distributed systems Page 3. > System Architecture Reference SystemArchitecture for integration flows ▪ Inbound / outbound adapters …,ICEIS (1),2010,2
Serviceorientierte Architektur im Precision Dairy Farming aus der Perspektive der Informatik.,Dirk Habich; Wolfgang Lehner,Abstract: Als Basis für ein effizientes Informationsmanagement im Umfeld des PrecisionDairy Farming (PDF) bietet sich der Einsatz einer serviceorientierten Architektur (SOA) an.Der konsequente Einsatz einer serviceorientierten Architektur bringt jedoch nicht nurVorteile mit sich; sondern verursacht auch einen erheblichen organisatorischen Aufwand. Indiesem Artikel zeigen wir die Vorteile für den Einsatz einer SOA innerhalb des PDF auf undpräsentieren einen Ansatz; um den organisatorischen Aufwand effizient von Beginn an zuminimieren.,GI Jahrestagung (1),2010,2
Query Processing on Prefix Trees,Matthias Boehm; Patrick Lehmann; Peter Benjamin Volk; Wolfgang Lehner,*,*,2010,2
Merging OLTP and OLAP–Back to the Future,Wolfgang Lehner,Introduction When the terms “Data Warehousing” and “Online Analytical Processing” werecoined in the 1990s by Kimball; Codd; and others; there was an obvious need for separatingdata and workload for operational transactional-style processing and decision-makingimplying complex analytical queries over large and historic data sets. Large data warehouseinfrastructures have been set up to cope with the special requirements of analytical queryanswering for multiple reasons: For example; analytical thinking heavily relies on predefinednavigation paths to guide the user through the data set and to provide different views ondifferent aggregation levels. Multi-dimensional queries exploiting hierarchically structureddimensions lead to complex star queries at a relational backend; which could hardly behandled by classical relational systems.,International Workshop on Business Intelligence for the Real-Time Enterprise,2009,2
Invisible Deployment of Integration Processes,Matthias Boehm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Abstract Due to the changing scope of data management towards the management ofheterogeneous and distributed systems and applications; integration processes gain inimportance. This is particularly true for those processes used as abstractions of workflow-based integration tasks; these are widely applied in practice. In such scenarios; a typical ITinfrastructure comprises multiple integration systems with overlapping functionalities. Themajor problems in this area are high development effort; low portability and inefficiency.Therefore; in this paper; we introduce the vision of invisible deployment that addresses thevirtualization of multiple; heterogeneous; physical integration systems into a single logicalintegration system. This vision comprises several challenging issues in the fields ofdeployment aspects as well as runtime aspects. Here; we describe those challenges …,International Conference on Enterprise Information Systems,2009,2
Model driven development of complex and data intensive integration processes,Matthias Bohm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Abstract. Due to the changing scope of data management from centrally stored data towardsthe management of distributed and heterogeneous systems; the integration takes place ondifferent levels. The lack of standards for information integration as well as applicationintegration resulted in a large number of different integration models and proprietarysolutions. With the aim of a high degree of portability and the reduction of developmentefforts; the model-driven development—following the Model-Driven Architecture (MDA)—isadvantageous in this context as well. Hence; in the GCIP project (Generation of ComplexIntegration Processes); we focus on the model-driven generation and optimization ofintegration tasks using a process-based approach. In this paper; we contribute detailedgeneration aspects and finally discuss open issues and further challenges.,Model-Based Software and Data Integration: First International Workshop; MBSDI 2008; Berlin; Germany; April 1-3; 2008; Proceedings,2008,2
Poster session: Constrained dynamic physical database design,Hannes Voigt; Wolfgang Lehner; Kenneth Salem,Physical design has always been an important part of database administration. Today'scommercial database management systems offer physical design tools; which recommend aphysical design for a given workload. However; these tools work only with static workloadsand ignore the fact that workloads; and physical designs; may change over time. Researchhas now begun to focus on dynamic physical design; which can account for time-varyingworkloads. In this paper; we consider a dynamic but constrained approach to physicaldesign. The goal is to recommend dynamic physical designs that reflect major workloadtrends but that are not tailored too closely to the details of the input workloads. To achievethis; we constrain the number of changes that are permitted in the recommended design. Inthis paper we present our definition of the constrained dynamic physical design problem …,Data Engineering Workshop; 2008. ICDEW 2008. IEEE 24th International Conference on,2008,2
Quality of service and predictability in dbms,Kai-Uwe Sattler; Wolfgang Lehner,Abstract DBMS are a ubiquitous building block of the software stack in many complexapplications. Middleware technologies; application servers and mapping approaches hidethe core database technologies just like power; networking infrastructure and operatingsystem services. Furthermore; many enterprise-critical applications demand a certaindegree of quality of service (QoS) or guarantees; eg wrt. response time; transactionthroughput; latency but also completeness or more generally quality of results. Examples ofsuch applications are billing systems in telecommunication; where each telephone call hasto be monitored and registered in a database; Ecommerce applications where orders haveto be accepted even in times of heavy load and the waiting time of customers should notexceed a few seconds; ERP systems processing a large number of transactions in …,Proceedings of the 11th international conference on Extending database technology: Advances in database technology,2008,2
Robust data management,Wolfgang Lehner,Abstract Mobility gains more and more importance from a technological as well as socialperspective. On the one hand; mobility is required from the personal and professionalenvironment in order to keep pace with the developments in a global world. On the otherhand; the existence of wireless networks and the success of cell-phones enable a wideusage of mobile communication infrastructure. While mobile devices (especially cell-phone)are becoming more and more a general vehicle to perform a wide spectrum of applicationslike internet browsing; etc. many; many issues are still unsolved in order to provide atechnologically solid and well accepted mobile infrastructure. In the following; we focus onthe term of robustness as a mean to achieve this goal. No only the general possibility tocommunicate via mobile devices using wireless networks is the question; but the reliable …,Dagstuhl Seminar Proceedings,2005,2
GignoMDA-modellgetriebene Entwicklung von Datenbankanwendungen.,Sebastian Richly; Wolfgang Lehner; Daniel Schaller,Anfang 2000 veröffentlichte die Object Management Group (OMG) einen Vorschlag zurEinführung der Model Driven Architecture; womit ein zentrales Problem derSoftwareentwicklung gelöst werden sollte. Die Technik von Rechnersystemen unterliegtimmer kleiner werdenden Änderungszyklen. Das betrifft sowohl Hardware; als auchBetriebssysteme und (GUI)-Bibliotheken. Geschäftsprozesse hingegen sind meist statisch.Mögliche Änderungen verändern den Prozess nicht fundamental. Beide Beobachtungenführen in der Praxis häufig dazu; dass bei einem Technologiewechsel die Fachlogik zumeistkomplett reimplementiert wird; obwohl sich diese kaum verändert hat. Um dieses Problemzu lösen; bediente sich die OMG einiger „alter “Ideen zur modellbasierten und generativenSoftware-Entwicklung.(siehe auch [1]),Datenbank-Spektrum,2005,2
Data management support for notification services,Wolfgang Lehner,Abstract Database management systems are highly specialized to efficiently organize andprocess huge amounts of data in a transactional manner. During the last years; however;database management systems have been evolving as a central hub for the integration ofmostly heterogeneous and autonomous data sources to provide homogenized data access.The next step in pushing database technology forward to play the role of an informationmarketplace is to actively notify registered users about incoming messages or changes inthe underlying data set. Therefore; notification services may be seen as a generic term forsubscription systems or; more general; data stream systems which both enable processingof standing queries over transient data. This article gives a comprehensive introduction intothe context of notification services by outlining their differences to the classical query …,*,2005,2
Semiautomatische Quellenanbindung mittels XML und Plug-ins in SCINTRA,Lutz Schlesinger; Wolfgang Lehner; Martin Seel; Michael Hübner,*,Arbeitsberichte des Instituts für Informatik,2002,2
Using Semantics for Query Derivability in Data Warehouse Applications,Jens Albrecht; W Hämmer; Wolfgang Lehner; Lutz Schlesinger,Abstract Materialized summary tables and cached query results are frequently used for theoptimization of aggregate queries in a data warehouse. Query rewriting techniques areincorporated into database systems to use those materialized views and thus avoidaccessing the possibly huge raw data. A rewriting is only possible if the query is derivablefrom these views. Several approaches can be found in the literature to check the derivabilityand find query rewritings. However; most algorithms either find rewritings only in veryrestricted cases or in complex cases which rarely occur in data warehouse environments.The specific application scenario of a data warehouse with its multidimensional perspectiveallows the consideration of much more semantic information; eg structural dependencieswithin the dimension hierarchies and different characteristics of measures. The motivation …,*,2001,2
Adaptive Präaggregation in multidimensionalen Datenbanksystemen,Jens Albrecht; Wolfgang Huemmer; Wolfgang Lehner; Lutz Schlesinger,Zusammenfassung Das Konzept des 'Online Analytical Processing'umfaßt die interaktiveAnalyse multidimensionaler Daten. Die dabei zu verarbeitenden Datenmengen sind in derRegel sehr groß. Deshalb müssen geeignete Optimierungsverfahren für die Gewährleistungder Interaktivität eingesetzt werden. Das durch die multidimensionalen Datenstrukturengeführte Benutzerverhalten im Anwendungsfeld OLAP bildet die Grundlage für das indiesem Beitrag vorgestellte redundanzbasierte Optimierungsverfahren. Die Grundideedieses adaptiven Verfahrens besteht darin; bereits berechnete Ergebnisse undTeilergebnisse in einem reservierten Speicherbereich fester Größe zu puffem; um neueAnfrageergebnisse daraus abzuleiten. Notwendig für einen solchen multidimensionalenAggregatpuffer ist zum einen ein Algorithmus; um konstruktiv die Ableitbarkeit eines …,*,1999,2
Application-specific architectures for energy-efficient database query processing and optimization,Sebastian Haas; Stefan Scholze; Sebastian Höppner; Annett Ungethüm; Christian Mayr; René Schüffny; Wolfgang Lehner; Gerhard Fettweis,Abstract Data processing on a continuously growing volume of data and the increasingpower restrictions have become an ubiquitous challenge in our world today. Besidesparallel computing; a promising approach to improve the energy efficiency of currentsystems is to integrate specialized hardware. This paper presents two application-specificarchitectures to accelerate basic database operators frequently used in modern databasesystems: an extended instruction set based on a given Cadence Tensilica processor (ASIP)and a comparable application-specific integrated circuit (ASIC). The ASIP is implemented ina system-on-chip and manufactured in a 28 nm CMOS technology to realize measurementsof performance and power consumption. Furthermore; the comparison with the ASIC blocksallows to quantify the results with the ASIP approach in terms of throughput; area; and …,Microprocessors and Microsystems,2017,1
Rethinking DRAM Caching for LSMs in an NVRAM Environment,Lucas Lersch; Ismail Oukid; Ivan Schreter; Wolfgang Lehner,Abstract The rise of NVRAM technologies promises to change the way we think aboutsystem architectures. In order to fully exploit its advantages; it is required to develop systemsspecially tailored for NVRAM devices. Not only this imposes great challenges; but alsodeveloping full system architectures from scratch is undesirable in many scenarios due toprohibitive development costs. Instead; we analyze in this paper the behavior of an existinglog-structured persistent key-value store; namely LevelDB; when run on top of an emulatedNVRAM device. We investigate initial opportunities for improvement when adapting asystem tailored for HDD/SSDs to run on top of an NVRAM environment. Furthermore; weanalyze the behavior of the DRAM caching components of LevelDB and whether moresuitable caching policies are required.,Advances in Databases and Information Systems,2017,1
Partitioning Strategy Selection for In-Memory Graph Pattern Matching on Multiprocessor Systems,Alexander Krause; Thomas Kissinger; Dirk Habich; Hannes Voigt; Wolfgang Lehner,Abstract Pattern matching on large graphs is the foundation for a variety of applicationdomains. The continuously increasing size of the underlying graphs requires highly parallelin-memory graph processing engines that need to consider non-uniform memory access(NUMA) and concurrency issues to scale up on modern multiprocessor systems. To tacklethese aspects; a fine-grained graph partitioning becomes increasingly important. Hence; wepresent a classification of graph partitioning strategies and evaluate representativealgorithms on medium and large-scale NUMA systems in this paper. As a scalable patternmatching processing infrastructure; we leverage a data-oriented architecture that preservesdata locality and minimizes concurrency-related bottlenecks on NUMA systems. Our in-depth evaluation reveals that the optimal partitioning strategy depends on a variety of …,European Conference on Parallel Processing,2017,1
The data center under your desk: how disruptive is modern hardware for DB system design?,Wolfgang Lehner,Abstract While we are already used to see more than 1;000 cores within a single machine;the next processing platforms for database engines will be heterogeneous with built-in GPU-style processors as well as specialized FPGAs or chips with domain-specific instruction sets.Moreover; the traditional volatile as well as the upcoming non-volatile RAM with capacitiesin the 100s of TBytes per machine will provide great opportunities for storage engines butalso call for radical changes on the architecture of such systems. Finally; the emergence ofeconomically affordable; high-speed/low-latency interconnects as a basis for rack-scalecomputing is questioning long-standing folklore algorithmic assumptions but will certainlyplay an important role in the big picture of building modern data management platforms. Inthis talk; we will try to classify and review existing approaches from a performance …,Proceedings of the VLDB Endowment,2017,1
Adaptive tile matrix representation and multiplication,*,According to some embodiments; matrix A data may be loaded into a temporary; unorderedstarting representation that contains coordinates and values for each element of matrix A. Z-curve ordering of matrix A may be performed to create a two-dimensional density map ofmatrix A by counting matrix elements that are contained in logical two-dimensional blockcells of a given size. A quad-tree recursion may be executed on the two-dimensional densitymap structure in reduced Z-space to identify areas of different densities in the twodimensional matrix space. An adaptive tile matrix representation of input matrix A may thenbe created. According to some embodiments; an adaptive tile matrix multiplication operationmay perform dynamic tile-granular optimization based on density estimates and a costmodel.,*,2017,1
Big data causing big (TLB) problems: taming random memory accesses on the GPU,Tomas Karnagel; Tal Ben-Nun; Matthias Werner; Dirk Habich; Wolfgang Lehner,Abstract GPUs are increasingly adopted for large-scale database processing; where dataaccesses represent the major part of the computation. If the data accesses are irregular; likehash table accesses or random sampling; the GPU performance can suffer. Especially whenscaling such accesses beyond 2GB of data; a performance decrease of an order ofmagnitude is encountered. This paper analyzes the source of the slowdown throughextensive micro-benchmarking; attributing the root cause to the Translation Lookaside Buffer(TLB). Using the micro-benchmarks; the TLB hierarchy and structure are fully analyzed ontwo different GPU architectures; identifying never-before-published TLB sizes that can beused for efficient large-scale application tuning. Based on the gained knowledge; wepropose a TLB-conscious approach to mitigate the slowdown for algorithms with irregular …,Proceedings of the 13th International Workshop on Data Management on New Hardware,2017,1
An analysis of LSM caching in NVRAM,Lucas Lersch; Ismail Oukid; Wolfgang Lehner; Ivan Schreter,Abstract The rise of NVRAM technologies promises to change the way we think aboutsystem architectures. In order to fully exploit its advantages; it is required to develop systemsspecially tailored for NVRAM devices. Not only this imposes great challenges; butdeveloping full system architectures from scratch is undesirable in many scenarios due toprohibitive development costs. Instead; we analyze in this paper the behavior of an existinglog-structured persistent key-value store; namely LevelDB; when run on top of an emulatedNVRAM device. We investigate initial opportunities for improvement when adapting asystem tailored for HDD/SSDs to run on top of an NVRAM environment. Furthermore; weanalyze the behavior of the legacy DRAM caching component of LevelDB and whether moresuitable caching policies are required.,Proceedings of the 13th International Workshop on Data Management on New Hardware,2017,1
Feature-driven time series generation,Lars Kegel; Martin Hahmann; Wolfgang Lehner,ABSTRACT Time series data are an ubiquitous and important data source in many domains.Most companies and organizations rely on this data for critical tasks like decision-making;planning; and analytics in general. Usually; all these tasks focus on actual data representingorganization and business processes. In order to assess the robustness of current systemsand methods; it is also desirable to focus on time-series scenarios which represent specifictime-series features. This work presents a generally applicable and easy-to-use method forthe feature-driven generation of time series data. Our approach extracts descriptive featuresof a data set and allows the construction of a specific version by means of the modification ofthese features.,29th GI-Workshop on Foundations of Databases,2017,1
SPARQLytics: multidimensional analytics for RDF,Michael Rudolf; Hannes Voigt; Wolfgang Lehner,With the rapid growth of open RDF data in recent years; being able to performmultidimensional analytics with it has become more and more important; in particular for thedata analyst performing explorative business intelligence tasks. Existing analyticapproaches are often not flexible enough to address the needs of data analysts andenthusiasts with iterative exploratory workflows. In this paper we propose SPARQLytics; atool that exposes the concepts of multidimensional graph analytics by offering standardOLAP cube operations and generating SPARQL queries. Our evaluation shows thatSPARQLytics unburdens data analysts from writing many lines of SPARQL code in iterativedata explorations and at the same time it does not impose any overhead to query execution.SPARQLytics fits well with interactive computing tools; such as Jupyter; providing data …,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,1
Updating database schemas in a zero-downtime environment,*,A system is described for processing schema updated in a zero-downtime environment. Atechnique includes establishing an application session to access a database; receiving aschema update; converting the database to an updated database according to the schemaupdate after establishing the application session; generating a temporary compensationview from the schema update; the temporary compensation view containing compensationlogic to locate database objects belonging to the database; receiving a database transactionfrom the application session to access a database object in the database; and processingthe compensation logic to locate the database object.,*,2016,1
Extending graph traversals with application logic,*,Traversal hooks are based on an event-oriented programming model and provide anexpressive mechanism to extend a graph traversal operator with domain-specific coding. Atraversal operator can visit (ie; traversal events) vertices and edges of a graph in an orderedmanner. Related apparatus; systems; techniques and articles are also described.,*,2016,1
Special issue: Modern hardware,Peter Boncz; Wolfgang Lehner; Thomas Neumann,While database systems have long enjoyed a “free ride” with ever-increasing clock cycles ofthe CPU; in the last decade this increase stalled. On the computational side; we have seenan ever-increasing number of cores as well as the advent of specialized computing unitsranging from GPUs via FPGA to chips with specific extensions. On the memory side; we notonly observe a significant growth of the capacity of main memory; but a continued largeperformance impact of RAM latency on data access cost; recently aggravated by increasingNUMA effects. Storage-wise we have witnessed the introduction of NAND devices (eg;SSDs) impacting the established role of magnetic disk drive. These advances taken togetherimpact current database architectures and ask for adjustments; extensions or even acomplete re-write in order to establish a scalable; affordable; and flexible foundation for …,The VLDB Journal,2016,1
ResilientStore: A Heuristic-Based Data Format Selector for Intermediate Results,Rana Faisal Munir; Oscar Romero; Alberto Abelló; Besim Bilalli; Maik Thiele; Wolfgang Lehner,Abstract Large-scale data analysis is an important activity in many organizations thattypically requires the deployment of data-intensive workflows. As data is processed theseworkflows generate large intermediate results; which are typically pipelined from oneoperator to the following. However; if materialized; these results become reusable; hence;subsequent workflows need not recompute them. There are already many solutions thatmaterialize intermediate results but all of them assume a fixed data format. A fixed format;however; may not be the optimal one for every situation. For example; it is well-known thatdifferent data fragmentation strategies (eg; horizontal and vertical) behave better or worseaccording to the access patterns of the subsequent operations. In this paper; we presentResilientStore; which assists on selecting the most appropriate data format for …,International Conference on Model and Data Engineering,2016,1
Processing diff-queries on property graphs,*,A system; computer-implemented method; and a computer-readable medium fordetermining why a query returns a null set in a database management system. When thedatabase management system receives a query graph; database management systemcompares the query graph to the data graph. Based on the comparison; databasemanagement system identifies a discovered component of the query graph and a missingcomponent of the query graph; wherein the missing query component indicates a reason forthe null set.,*,2016,1
Architecture of a multi-domain processing and storage engine,Johannes Luong; Dirk Habich; Thomas Kissinger; Wolfgang Lehner,Google; Inc. (search). SIGN IN SIGN UP. Architecture of a Multi-domain Processing and StorageEngine. Authors: Johannes Luong; Technische Universität Dresden. Dirk Habich; TechnischeUniversität Dresden. Thomas Kissinger; Technische Universität Dresden. Wolfgang Lehner;Technische Universität Dresden. 2016 Article. Bibliometrics. &.,Proceedings of the 5th International Conference on Data Management Technologies and Applications,2016,1
Web-based Benchmarks for Forecasting Systems: The ECAST Platform,Robert Ulbricht; Claudio Hartmann; Martin Hahmann; Hilko Donker; Wolfgang Lehner,Abstract The role of precise forecasts in the energy domain has changed dramatically. Newsupply forecasting methods are developed to better address this challenge; but meaningfulbenchmarks are rare and time-intensive. We propose the ECAST online platform in order tosolve that problem. The system's capability is demonstrated on a real-world use case bycomparing the performance of different prediction tools.,Proceedings of the 2016 International Conference on Management of Data,2016,1
Multidimensional graph analytics,*,Disclosed herein are system; method; and computer program product embodiments forperforming ad-hoc analytical queries of graph data. An embodiment operates by receiving agraph pattern for a subgraph of interest. The facts of interest are then selected from graphdata based on the received graph pattern. Dimensions are then defined based on adimension seed pattern and a set of level expressions; and measures are defined based ona computation function and an aggregation function. A graph cube is formed based on theselected facts and the defined dimensions and measures. Because the facts; dimensions;and measures of interest are defined at the time of an analytical query; a user does not haveto define such facts; dimensions; and measures; or know which analytical queries will be ofinterest; at the time of data collection.,*,2016,1
Multi-GPU approximation methods for silent data corruption of AN codes,Matthias Werner; Till Kolditz; Tomas Karnagel; Dirk Habich; Wolfgang Lehner,Abstract Multi-bit flip rates are assumed to increase dramatically with future transistortechnologies; especially in computer DRAM. We previously showed that employing ANcodes in main-memory database systems is a feasible choice in terms of performanceoverhead and memory consumption [1]. The silent data corruption probability of a code isdetermined by its distance distribution; whose computational complexity is exponential fornon-linear codes like AN coding. We provide exact and approximation algorithms forcomputing the distance distribution on GPUs for AN codes.,Proc. 12th Int’l Workshop Boolean Problems; ser. IWSBP,2016,1
Annotations for parallelization of user-defined functions with flexible partitioning,*,Annotations can be placed in source code to indicate properties for user-defined functions. Awide variety of properties can be implemented to provide information that can be leveragedwhen constructing a query execution plan for the user-defined function and associated coredatabase relational operations. A flexible range of permitted partition arrangements can bespecified via the annotations. Other supported properties include expected sorting andgrouping arrangements; ensured post-conditions; and behavior of the user-defined function.,*,2015,1
Robust cardinality estimation for subgraph isomorphism queries on property graphs,Marcus Paradies; Elena Vasilyeva; Adrian Mocan; Wolfgang Lehner,Abstract With an increasing popularity of graph data and graph processing systems; theneed of efficient graph processing and graph query optimization becomes more important.Subgraph isomorphism queries; one of the fundamental graph query types; rely on anaccurate cardinality estimation of a single edge of a pattern for efficient query processing.State of the art approaches do not consider two important aspects for cardinality estimationof graph queries on property graphs: the existence of nodes with a high outdegree andfunctional dependencies between attributes. In this paper we focus on these two challengesand integrate the detection of high-outdegree nodes and functional dependency analysisinto the cardinality estimation. We evaluate our approach on two real data sets and compareit against a state-of-the-art query optimizer for property graphs as implemented in Neo4j.,*,2015,1
Database Evolution for Software Product Lines,Kai Herrmann; Jan Reimann; Hannes Voigt; Birgit Demuth; Stefan Fromm; Robert Stelzmann; Wolfgang Lehner,Google; Inc. (search). SIGN IN SIGN UP. Database Evolution for Software ProductLines. Authors: Kai Herrmann; Technische Universität Dresden. Jan Reimann;Technische Universität Dresden. Hannes Voigt; Technische Universität Dresden. BirgitDemuth; Technische Universität Dresden. Stefan Fromm; Dresden-Informatik GmbH.Robert Stelzmann; iSAX GmbH & Co. KG. Wolfgang Lehner.,Proceedings of 4th International Conference on Data Management Technologies and Applications,2015,1
Needles in the haystack—tackling bit flips in lightweight compressed data,Till Kolditz; Dirk Habich; Dmitrii Kuvaiskii; Wolfgang Lehner; Christof Fetzer,Abstract Modern database systems are very often in the position to store their entire data inmain memory. Aside from increased main emory capacities; a further driver for in-memorydatabase system has been the shift to a column-oriented storage format in combination withlightweight data compression techniques. Using both mentioned software concepts; largedatasets can be held and efficiently processed in main memory with a low memory footprint.Unfortunately; hardware becomes more and more vulnerable to random faults; so that eg;the probability rate for bit flips in main memory increases; and this rate is likely to escalate infuture dynamic random-access memory (DRAM) modules. Since the data is highlycompressed by the lightweight compression algorithms; multi bit flips will have an extremeimpact on the reliability of database systems. To tackle this reliability issue; we introduce …,International Conference on Data Management Technologies and Applications,2015,1
From Static to Agile-Interactive Particle Physics Analysis in the SAP HANA DB.,David Kernert; Norman May; Michael Hladik; Klaus Werner; Wolfgang Lehner,Abstract: In order to confirm their theoretical assumptions; physicists employ Monte-Carlogenerators to produce millions of simulated particle collision events and compare them withthe results of the detector experiments. The traditional; static analysis workflow of physicistsinvolves creating and compiling a C++ program for each study; and loading large data filesfor every run of their program. To make this process more interactive and agile; we createdan application that loads the data into the relational in-memory column store DBMS SAPHANA; exposes raw particle data as database views and offers an interactive web interfaceto explore this data. We expressed common particle physics analysis algorithms using SQLqueries to benefit from the inherent scalability and parallelization of the DBMS. In this paperwe compare the two approaches; ie manual analysis with C++ programs and interactive …,DATA,2015,1
Transparent Forecasting Strategies in Database Management Systems,Ulrike Fischer; Wolfgang Lehner,Abstract Whereas traditional data warehouse systems assume that data is complete or hasbeen carefully preprocessed; increasingly more data is imprecise; incomplete; andinconsistent. This is especially true in the context of big data; where massive amount of dataarrives continuously in real-time from vast data sources. Nevertheless; modern data analysisinvolves sophisticated statistical algorithm that go well beyond traditional BI and;additionally; is increasingly performed by non-expert users. Both trends require transparentdata mining techniques that efficiently handle missing data and present a complete view ofthe database to the user. Time series forecasting estimates future; not yet available; data of atime series and represents one way of dealing with missing data. Moreover; it enablesqueries that retrieve a view of the database at any point in time—past; present; and future …,European Business Intelligence Summer School,2013,1
Report on the first international workshop on energy data management (endm 2012),Torben Bach Pedersen; Wolfgang Lehner; Gregor Hackenbroich,The energy sector is one of the most active application domains being forced to re-think thecurrent practice and apply data-management based IT solutions to provide a scalable andsustainable supply and distribution of energy. Challenges range from energy production byseamlessly incorporating renewable energy resources over energy distribution andmonitoring to controlling energy consumption. Decisions are based on huge amounts ofempirically collected data from smart meters; new energy sources (increasingly RES-renewable energy sources such as wind; solar; hydro; thermal; etc); new distributionsmechanisms (Smart Grid); and new types of consumers and devices; eg; electric cars.Energy is at the top of the worldwide political agenda; eg; due to global warming concernsand recent nuclear accidents. Ambitious goals for reductions of energy consumption and …,ACM SIGMOD Record,2013,1
Virtualization for Data Management Services,Wolfgang Lehner; Kai-Uwe Sattler,Abstract Virtualization is the key concept to provide a scalable and flexible computingenvironment in general. In this chapter; we focus on virtualization concepts in the context ofdata management tasks. We review existing concepts and technologies spanning multiplesoftware layers.,*,2013,1
Overview and Comparison of Current Database-as-a-Service Systems,Wolfgang Lehner; Kai-Uwe Sattler,Abstract As described in Chap. 1 the different services that can be obtained from a cloud aretypically classified by the categories Infrastructure-as-a-Service (IaaS); Platform-as-a-Service (PaaS); and Software-as-a-Service (SaaS). The three different layers are oftendisplayed as a stack; indicating that the services of one cloud layer can be built based on theservices offered by the lower layers. For example; a Software-as-a-Service application canexploit the scalability and robustness offered by the services on the Platform-as-a-Servicelayer. That way the Software-as-a-Service application is in the position to achieve goodscalability and availability properties by itself.,*,2013,1
DrillBeyond: Open-World SQL Queries Using Web Tables.,Julian Eberius; Maik Thiele; Katrin Braunschweig; Wolfgang Lehner,Abstract: The Web consists of a huge number of documents; but also large amountsstructured information; for example in the form of HTML tables containing relationalstyledata. One typical usage scenario for this kind of data is their integration into a database ordata warehouse in order to apply data analytics. However; in today's business intelligencetools there is an evident lack of support for so-called situational or ad-hoc data integration. Inthis demonstration we will therefore present DrillBeyond; a novel database and informationretrieval engine which allows users to query a local database as well as the web datasets ina seamless and integrated way with standard SQL. The audience will be able to posequeries to our DrillBeyond system which will be answered partly from local data in thedatabase and partly from datasets that originate from the Web of Data. We will …,BTW,2013,1
Efficient integration of external information into forecast models from the energy domain,Lars Dannecker; Elena Vasilyeva; Matthias Boehm; Wolfgang Lehner; Gregor Hackenbroich,Abstract Forecasting is an important analysis technique to support decisions andfunctionalities in many application domains. While the employed statistical models oftenprovide a sufficient accuracy; recent developments pose new challenges to the forecastingprocess. Typically the available time for estimating the forecast models and providingaccurate predictions is significantly decreasing. This is especially an issue in the energydomain; where forecast models often consider external influences to provide a highaccuracy. As a result; these models exhibit a higher number of parameters; resulting inincreased estimation efforts. Also; in the energy domain new measurements are constantlyappended to the time series; requiring a continuous adaptation of the models to newdevelopments. This typically involves a parameter re-estimation; which is often almost as …,East European Conference on Advances in Databases and Information Systems,2012,1
Technology Time Machine 2012-Paving the path for the future technology developments [includes 9 white papers],Wolfgang Lehner; Gerhard Fettweis,The IEEE Technology Time Machine (TTM) is a unique event for industry leaders;academics; and decision making government officials who direct R&D activities; planresearch pro grams or manage portfolios of research activities. This report covers the maintopics of the 2nd Symposium of future technologies. The Symposium brought together worldrenowned experts to discuss the evolutionary and revolutionary advances in technologylandscapes as we look towards 2020 and beyond. TTM facilitated informal discussionsamong the participants and speakers thus pro vi ding an excellent opportunity for informalinteraction between attendees; senior business leaders; world-renowned innovators; andthe press. The goal of the Symposium is to discover key critical innovations acrosstechnologies which will alter the research and application space of the future. Topics …,Technology Time Machine Symposium (TTM); 2012 IEEE,2012,1
Explorative Multi-View Clustering Using Frequent-Groupings.,Martin Hahmann; Markus Dumat; Dirk Habich; Wolfgang Lehner; Dresden Germany,Abstract In this paper; we present our novel clustering technique called frequent-groupingsas combination of alternative and ensemble clustering. Our new approach combines thebenefit of both underlying concepts in an integrated way and allows the creation of robustclustering alternatives. In detail; we present (1) different approaches for the automaticcreation of alternatives;(2) a user-guided exploratory extraction process and (3) anevaluation method to compare extracted alternatives with regards to their similarities.,MultiClust@ SDM,2012,1
Echtzeit-Data-Warehouse-Systeme,Maik Thiele; Wolfgang Lehner,Zusammenfassung Die stets zentraler werdende Rolle der Data Warehouses; in allenEntscheidungsebenen eines Unternehmens; führt zu der Forderung nach hochaktuellenDaten bzw. echtzeitfähigen Data-Warehouses-Systemen. Dieser Artikel stellt die Frageinwieweit mit bestehenden Data-Warehouse-Architekturen eine Informationsversorgung inEchtzeit zu gewährleisten ist; deckt die Schwächen dieser Architekturen auf und diskutiertverschiedene Lösungsansätze.,Datenbank-Spektrum,2011,1
A domain-specific language for do-it-yourself analytical mashups,Julian Eberius; Maik Thiele; Wolfgang Lehner,Abstract The increasing amount and variety of data available in the web leads to newpossibilities in end-user focused data analysis. While the classic data base technologies fordata integration and analysis (ETL and BI) are too complex for the needs of end users;newer technologies like web mashups are not optimal for data analysis. To make productiveuse of the data available on the web; end users need easy ways to find; join and visualize it.We propose a domain specific language (DSL) for querying a repository of heterogeneousweb data. In contrast to query languages such as SQL; this DSL describes the visualizationof the queried data in addition to the selection; filtering and aggregation of the data. Theresulting data mashup can be made interactive by leaving parts of the query variable. Wealso describe an abstraction layer above this DSL that uses a recommendation-driven …,International Conference on Web Engineering,2011,1
Resiliency-Aware Data Management,Matthias Boehm; Wolfgang Lehner; Christof Fetzer,ABSTRACT Computing architectures change towards massively parallel environments withincreasing numbers of heterogeneous components. The large scale in combination withdecreasing feature sizes leads to dramatically increasing error rates. The heterogeneityfurther leads to new error types. Techniques for ensuring resiliency in terms of robustnessregarding these errors are typically applied at hardware abstraction and operating systemlevels. However; as errors become the normal case; we observe increasing costs in terms ofcomputation overhead for ensuring robustness. In this paper; we argue that ensuringresiliency on the data management level can reduce the required overhead by exploitingcontext knowledge of query processing and data storage. Apart from reacting on alreadydetected errors; this was mostly neglected in database research so far. We therefore give …,Proceedings of the VLDB Endowment,2011,1
A sample advisor for approximate query processing,Philipp Rösch; Wolfgang Lehner,Abstract The rapid growth of current data warehouse systems makes random sampling acrucial component of modern data management systems. Although there is a large body ofwork on database sampling; the problem of automatic sample selection remained (almost)unaddressed. In this paper; we tackle the problem with a sample advisor. We propose a costmodel to evaluate a sample for a given query. Based on this; our sample advisor determinesthe optimal set of samples for a given set of queries specified by an expert. We furtherpropose an extension to utilize recorded workload information. In this case; the sampleadvisor takes the set of queries and a given memory bound into account for the computationof a sample advice. Additionally; we consider the merge of samples in case of overlappingsample advice and present both an exact and a heuristic solution. Within our evaluation …,East European Conference on Advances in Databases and Information Systems,2010,1
Sample Footprints für Data-Warehouse-Datenbanken,Philipp Rösch; Wolfgang Lehner,Zusammenfassung Durch stetig wachsende Datenmengen in aktuellen Data-Warehouse-Datenbanken erlangen Stichproben eine immer größer werdende Bedeutung. Insbesondereinteraktive Analysen können von den signifikant kürzeren Antwortzeiten der approximativenAnfrageverarbeitung erheblich profitieren. Linked-Bernoulli-Synopsen bieten in diesemSzenario speichereffiziente; schemaweite Synopsen; dh Synopsen mit Stichproben jeder imSchema enthaltenen Tabelle bei minimalem Mehraufwand für die Erhaltung derreferenziellen Integrität innerhalb der Synopse. Dies ermöglicht eine effizienteUnterstützung der näherungsweisen Beantwortung von Anfragen mit beliebigenFremdschlüsselverbundoperationen. In diesem Artikel wird der Einsatz von Linked-Bernoulli-Synopsen in Data-Warehouse-Umgebungen detaillierter analysiert. Dies beinhaltet zum …,Computer Science-Research and Development,2010,1
Ein begriffsbasierter Ansatz zur semantischen Extraktion von Datenbankschemata.,Henri Mühle; Hannes Voigt; Wolfgang Lehner,Abstract: Die durch das rasante Anwachsen digitaler Datenbestände in Volumen und Vielfaltnotwendig gewordene effiziente Verwaltung der erhobenen Datenbestände; bringtherkömmliche Datenbankmethoden an ihre Grenzen. Ein modelliertes Datenbankschemazur Grundstrukturierung der Datenbank kann längst nicht mehr statisch rigide modelliertwerden. Vielmehr werden schemaflexible Datenbanken benötigt; die ihr Schemaentsprechend an Anderungen im Datenbestand anpassen können. Da dasDatenbankschema basierend auf einer konzeptuellen Datenbanksicht modelliert wird;präsentieren wir einen Ansatz; der die Formale Begriffsanalyse als Modellierungsmethodeeinsetzt. Die Formale Begriffsanalyse greift genau diese begriffsorientierte Weltsicht auf.Damit können wir Schemaextraktion und weiterführende Problemstellungen mit wohl …,Software Engineering (Workshops),2010,1
Quality and performance optimization of sensor data stream processing,Anja Klein; Wolfgang Lehner,Abstract—Intelligent sensor devices together with data stream management systems allowthe automatic recording and processing of huge data volumes to guide any kind of processcontrol or business decision. However; a crucial problem is posed by data qualitydeficiencies due to imprecise sensors; environmental influences; transfer failures; etc. If nothandled carefully; they misguide decisions and lead to inappropriate reactions. In this paper;we present the quality-driven optimization of stream processing that improves the resultingquality of data and service. After an introduction to data quality management in data streams;we define the targeted optimization problem comprising the optimization objectives andparameters that configure the required stream processing operators. Based on the genericoptimization framework; we discuss and evaluate the optimization execution in batch and …,International Journal on Advances in Networks and Services Volume 3; Number 1 & 2; 2010,2010,1
Drift-Aware Ensemble Regression,Peter Benjamin Volk FrankRosenthal; Martin Hahmann; Dirk Habich; Wolfgang Lehner,Abstract. Regression models are often required for controlling production processes bypredicting parameter values. However; the implicit assumption of standard regressiontechniques that the data set used for parameter estimation comes from a stationary jointdistribution may not hold in this context because manufacturing processes are subject tophysical changes like wear and aging; denoted as process drift. Thiscan cause theestimated model to deviate signiﬁcantly from the current state of the modeled system. In thispaper; we discuss the problem of estimating regression models from drifting processes andwe present ensemble regression; an approach that maintains a set of regression models—estimated from diﬀerent ranges of the data set—according to their predictive performance.We extensively evaluate our approach on synthetic and real-world data.,Machine Learning and Data Mining in Pattern Recognition: 6th International Conference; MLDM 2009; Leipzig; Germany; July 23-25; 2009; Proceedings,2009,1
Standing Processes in Service-Oriented Environments,Steffen Preißler; Dirk Habich; Wolfgang Lehner,Current realization techniques for service-oriented architectures (SOA) and businessprocess management (BPM) cannot be efficiently applied to any kind of applicationscenario. For example; an important requirement in the finance sector is the continuousevaluation of stock prices to automatically trigger business processes--eg the buying orselling of stocks--with regard to several strategies. In this paper; we address the continuousevaluation of message streams within BPM to establish a common environment for stream-based message processing and traditional business processes. In detail; we propose thenotion of standing processes as (i) a process-centric concept for the interpretation ofmessage streams; and (ii) a trigger element for subsequent business processes. Thedemonstration system focuses on the execution of standing processes and the smooth …,Services-I; 2009 World Conference on,2009,1
Streaming Web Services and Standing Processes.,Steffen Preißler; Hannes Voigt; Dirk Habich; Wolfgang Lehner,Abstract: Today; service orientation is a well established concept in modern ITinfrastructures. Web services and WS-BPEL as the two key technologies handle largestructured data sets very inefficiently because they process the whole data set at once. Inthis demo; we present a framework to build standing business processes. Standing businessprocesses rely on item-wise data set processing; exploit pipeline parallelism and show asignificantly higher throughput than the traditional WS-BPEL approach.,BTW,2009,1
Message Indexing for Document-Oriented Integration Processes.,Matthias Böhm; Uwe Wloka; Dirk Habich; Wolfgang Lehner,ABSTRACT The integration of heterogeneous systems is still an evolving research area.Due to the complexity of integration processes; there are challenges for the optimization ofintegration processes. Message-based integration systems; like EAI servers and workflowprocess engines; are mostly documentoriented using XML technologies in order to achievesuitable data independence from the different and particular proprietary datarepresentations of the supported external systems. However; such an approach causeslarge costs for single-value evaluations within the integration processes. At this point;message indexing; adopting extended database technologies; could be applied in order toreach better performance. In this paper; we introduce our message indexing structure MIXand discuss and evaluate immediate as well as deferred indexing concepts. Further; we …,ICEIS (1),2008,1
Improving Data Independence; Efficiency and Functional Flexibility of Integration Platforms.,Matthias Böhm; Jürgen Bittner; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Abstract. The concept of Enterprise Application Integration (EAI) is widely used forintegrating heterogeneous applications and systems via message-based communication.Typically; EAI servers provide a huge set of specific inbound and outbound adapters usedfor interacting with the external systems and for converting proprietary message formats.However; the main problems in currently available products are the monolithic design ofthese adapters and performance deficits caused by the need for data independence. First;we classify and discuss these open problems. Second; we introduce our model-drivenDIEFOS (data independence; efficiency and functional flexibility using feature-orientedsoftware engineering) approach and show how the feature-based generation of dynamicadapters can improve data independence; efficiency and functional flexibility. Finally; we …,CAiSE Forum,2008,1
Hybride Modellierung operativer und analytischer Daten; dargestellt am Beispiel des Precision Dairy Farming,Christian Schulze; P Wagner; W Lehner,Abgeleitet aus den kurz skizzierten Defiziten im Datenmanagement und derDatenverfügbarkeit ist es das Ziel der Arbeit; einen Vorschlag zur Datenmodellierung alswesentlichen Bestandteil des Datenmanagements und damit die Basis betrieblicherInformationssysteme für den Bereich der Milcherzeugung zu erarbeiten. BesonderesAugenmerk ist hierbei auf die simultane Beachtung sowohl operativer Daten zurkurzfristigen als auch analytischer Daten zur strategischen Entscheidungsunterstützung zulegen. Basierend auf einer Analyse des Informationsbedarfs sind relevante Daten zuidentifizieren und entsprechend der Nutzungsrichtung im Datenmodell in geeigneter Weiseabzubilden. Der Fokus ist dabei nicht nur auf unternehmensinterne;produktionsprozessorientierte Daten zu legen; sondern auch auf den Informationsbedarf …,Martin-Luther-Universität Halle-Wittenberg,2008,1
Quality of service-driven stream mining,Marcel Karnstedt; Kai-Uwe Sattler; Dirk Habich; Wolfgang Lehner,Abstract Scalable stream processing systems have to continuously manage changingresources efficiently; which is usually achieved by applying best-effort approaches on thelevel of processing operations. Thus; several authors have recently dealt with the problem ofresource-aware stream processing; proposing methods and techniques capable of adaptingto changing resources; both on the system and operator level. In this paper; we argue thatQuality-of-Service (QoS) requirements are as mandatory as resource awareness whencreating stream processing systems applicable in a wide and general range of data streamapplications. We discuss QoS requirements and QoS-driven stream mining techniques asbuilding blocks of the proposed framework for resource-and quality-aware streamprocessing systems. We exemplify the applicability of our approach by presenting and …,Proceedings of the 2nd International Workshop on Data Stream Mining and Management in conjunction with the 7th IEEE International Conference on Data Mining,2007,1
Der Einfluss der Datenverteilung auf die Performanz eines Data Warehouse.,Thomas Legler; Wolfgang Lehner; Andrew Ross,Abstract: Dieses Papier befasst sich mit einer Studie über die Optimierungsmöglich- keiten vonAnfragen auf verteilten Data Warehouse Architekturen mittels verschieden- artiger Verteilungsstrategiender beteiligten Tabellen am Beispiel SAP NetWeaver BI … Dank wachsender Vernetzung unddurch automatisierende Methoden sammeln Unterneh- men mehr Daten als je zuvor. Damithaben sie die Möglichkeit; interne Abläufe und Wirtschaftsdaten genau zu analysieren. Die Datensind jedoch im Regelfall uneinheitlich und zu granular um persistiert zu werden; wodurch solcheInformationen oft in vorver- arbeiteter Form in einem Data Warehouse abgelegt werden. Eineweit verbreitete Dar- stellungsform eines Data Warehouse ist das Sternschema [CCS;KRT+98] ; eine zen- tral angeordnete Faktentabelle mit Belegeinträgen und Verweisen aufDimensionstabel- len; welche weiterführende Informationen enthalten. Derartig …,BTW,2007,1
Working Group Report on Managing and Integrating Data in P2P Databases,Peter A Boncz; Angela Bonifati; Peter Janacik; Birgitta Konig-Ries; Arantza Illarramendi; Wolfgang Lehner; Wolfgang May; Aris Ouksel; Kay Romer; Brahmananda Sapkota; Kai-Uwe Sattler; Heinz Schweppe; Rita Steinmetz; Can Turker,Page 1. Working Group Report on Managing and Integrating Data in P2P Databases PeterA. Boncz CWI; The Netherlands Angela Bonifati ∗ National Research Council; Italy PeterJanacik University of Padeborn; Germany Birgitta K¨onig-Ries Jena University; GermanyArantza Illarramendi Basque Country University; Spain Wolfgang Lehner TU Dresden;Germany Pedro J. Marr ˙on University of Stuttgart; Germany Wolfgang May G¨ottingenUniversity; Germany Aris Ouksel University of Illinois at Chicago; USA Kay R¨omer ETH Zurich;Switzerland Brahmananda Sapkota DERI Research Center; Ireland Kai-Uwe Sattler TUIlmenau; Germany Heinz Schweppe Freie Universitaet Berlin; Germany Rita SteinmetzUniversity of Padeborn; Germany Can T¨urker ETH Zurich; Switzerland …,Proc. of the conference on Scalable Data Management in Evolving Networks,2007,1
A message transformation model for data-centric service integration processes,Matthias Böhm; Uwe Wloka; Jürgen Bittner; Dirk Habich; Wolfgang Lehner,ABSTRACT The horizontal integration of systems by message-based communication viamiddleware products is a widespread method of application integration to ensure anadequate loose coupling of participating systems and applications. For the description ofsuch service integration processes; the use of functionally oriented process descriptionlanguages; like WSBPEL; is gaining in importance. However; these languages revealdeficits when describing data-centric application scenarios. Due to these deficits and thelack of a model for service integration processes; this paper contributes to the systematicmodeling of complex message transformations in data-centric integration processes. Thepracticability of the model is shown with a prototypical implementation within the serviceintegration platform TransConnectR© of the SQL GmbH Dresden.,33rd International Very Large Data Bases Conference,2007,1
Zusammenführung der konzeptuellen Modelle für operative und analytische Daten in einem logischen Modell; dargestellt am Beispiel des Precision Dairy Farming.,Christian Schulze; Joachim Spilke; Wolfgang Lehner,Abstract: Die als Ergebnis der konzeptuellen Datenmodellierung im Rahmen des PrecisionDairy Farming vorliegenden getrennten Modelle für operative und analytische Daten sind inein logisches Modell zu überführen. Wir propagieren dabei den Entwurf einesgemeinsamen relationalen Modells für beide Datensichten. Nur so kann Redundanz undInkonsistenz auf Modell-und Datenebene vermieden werden. Anhand eines Beispiels wirddie Vorgehensweise erläutert.,GIL Jahrestagung,2006,1
Data management in a connected world: essays dedicated to Hartmut Wedekind on the occasion of his 70th birthday,Hartmut Wedekind; Theo Härder; Wolfgang Lehner,Data management systems play the most crucial role in building large application s-tems.Since modern applications are no longer single monolithic software blocks but highlyflexible and configurable collections of cooperative services; the data mana-ment layer alsohas to adapt to these new requirements. Therefore; within recent years; data managementsystems have faced a tremendous shift from the central management of individual records ina transactional way to a platform for data integration; fede-tion; search services; and dataanalysis. This book addresses these new issues in the area of data management frommultiple perspectives; in the form of individual contributions; and it outlines future challengesin the context of data management. These contributions are dedicated to Prof. em. Dr. Dr.-Ing. E. h. Hartmut Wedekind on the occasion of his 70th birthday; and were (co-) authored …,*,2005,1
XPath-Aware Chunking of XML-Documents.,Wolfgang Lehner; Florian Irmert,Abstract Dissemination systems are used to route information received from manypublishers individually to multiple subscribers. The core of a dissemination system consistsof an efficient filtering engine deciding what part of an incoming message goes to whichrecipient. Within this paper we are proposing a chunking framework of XML documents tospeed up the filtering process for a set of registered subscriptions based on XPathexpressions. The problem which will be leveraged by the proposed chunking scheme isbased on the observation that the execution time of XPath expressions increases with thesize of the underlying XML document. The proposed chunking strategy is based on the ideaof sharing XPath prefixes among the query set additionally extended by individually selectednodes to be able to handle XPath-filter expressions. Extensive tests showed substantial …,BTW,2003,1
Konsistenzquantifizierung in grid-datenbanksystemen,Lutz Schlesinger; Wolfgang Lehner,Kurzfassung: Das Konzept des Grid-Computing gewinnt insbesondere bei verteiltenDatenbankanwendungen immer mehr an Bedeutung. Ein Grid-Datenbanksystem bestehtdabei aus lokal autonomen Systemen; die für globale Datenbankanfragen zur Ausführungkomplexer Analysen zusammengeschaltet werden. Eine Datenbankanfrage wird durch eineAuftragsvergabekomponente an ein Mitglied des Grids weitergeleitet. Um dieAnfragegeschwindigkeit zu erhöhen; halten im skizzierten Architekturmodell Knoten dieSnapshots von Datenquellen anderer Mitglieder lokal vor. Ist aus Benutzersicht ein Zugriffauf teilweise veraltete Zustände von Datenquellen akzeptierbar; so kann dieAuftragsvergabe neben systemtechnischen Eigenschaften (Lastbalancierung;Kommunikationsaufwand) auch inhaltliche Aspekte berücksichtigen; wobei als …,Datenbanksysteme für Business; Technologie und Web (BTW),2003,1
Erweiterungsmechanismen in kommerziellen Datenbanksystemen,Lutz Schlesinger; Wolfgang Lehner; Michael Hübner; Martin Seel,*,Heterogene Informationssysteme; Arbeitsberichte des Instituts für Informatik,2002,1
Ausgewählte Konzepte der Realisierung von PubScribe,M Redert; C Reinhard; W Hümmer; W Lehner,Kurzfassung Modellierungsaspekte erfordern stets eine adäquate Realisierung. In diesemBeitrag werden eine Vielzahl unterschiedlicher Aspekte der Realisierung von PubScribeaufgegriffen und ausführlich diskutiert. Dies erfolgt in drei Schritten; Im ersten Schritt wirdbeschrieben; wie eine eingehende Subskription den globalen Anfragegraph modifiziert undRestrukturierungen impliziert. Im zweiten Schritt wird erläutert; welche dynamischeVorgänge bei der Registrierung eines Produzenten und der Aufnahme einer produziertenNachricht ablaufen. Der Hauptteil dieses Beitrags widmet sich der Thematik deranfrageübergreifenden Optimierung und Restrukturierung. Prinzipielle Abläufe in diesemZusammenhang; notwendige Voraussetzungen und die Erzeugung von Kompensationenwerden aufgeführt. Die Technik im speziellen und die Realisierung im allgemeinen wird …,*,2001,1
Diversity of Processing Units,Wolfgang Lehner; Annett Ungethüm; Dirk Habich,Abstract Recent hardware developments are providing a plethora of alternatives to well-known general-purpose processing units. This development reaches into all majordirections; ie; into high-speed and low latency communications systems; novel memorycomponents as well as a zoo of different processing units in addition to the traditional CPU-style processors. While all developments have great impact on the design of databasesystems; we will try—in the context of this Kurz Erklärt—to categorize recent advances in thecontext of processing units and comment on the impact on database systems.,Datenbank-Spektrum,2018,*
Frequent patterns in ETL workflows: An empirical approach,Vasileios Theodorou; Alberto Abelló; Maik Thiele; Wolfgang Lehner,Abstract The complexity of Business Intelligence activities has driven the proposal of severalapproaches for the effective modeling of Extract-Transform-Load (ETL) processes; based onthe conceptual abstraction of their operations. Apart from fostering automation andmaintainability; such modeling also provides the building blocks to identify and representfrequently recurring patterns. Despite some existing work on classifying ETL componentsand functionality archetypes; the issue of systematically mining such patterns and theirconnection to quality attributes such as performance has not yet been addressed. In thiswork; we propose a methodology for the identification of ETL structural patterns. We logicallymodel the ETL workflows using labeled graphs and employ graph algorithms to identifycandidate patterns and to recognize them on different workflows. We showcase our …,Data & Knowledge Engineering,2017,*
The Dresden Database Systems Group,Wolfgang Lehner,Abstract The Dresden Database Systems Group focuses on the advancement of datamanagement techniques from a system level as well as information managementperspective. With more than 15 PhD students the research group is involved in a variety oflarger research projects ranging from activities to exploit modern hardware for scalablestorage engines to advancing statistical methods for large-scale time series management.The group is visible at an international level as well as actively involved in cooperations withnational and regional research partners,ACM SIGMOD Record,2017,*
Balancing Performance and Energy for Lightweight Data Compression Algorithms,Annett Ungethüm; Patrick Damme; Johannes Pietrzyk; Alexander Krause; Dirk Habich; Wolfgang Lehner,Abstract Energy consumption becomes more and more a critical design factor; wherebyperformance is still an important requirement. Thus; a balance between performance andenergy has to be established. To tackle that issue for database systems; we proposed theconcept of work-energy profiles. However; generating such profiles requires extensivebenchmarking. To overcome that; we propose to approximate work-energy-profiles forcomplex operations based on the profiles of low-level operations in this paper. To show thefeasibility of our approach; we use lightweight data compression algorithms as complexoperations; since compression as well as decompression are heavily used in in-memorydatabase systems; where data is always managed in a compressed representation.Furthermore; we evaluate our approach on a concrete hardware system.,Advances in Databases and Information Systems,2017,*
Asynchronous Graph Pattern Matching on Multiprocessor Systems,Alexander Krause; Annett Ungethüm; Thomas Kissinger; Dirk Habich; Wolfgang Lehner,Abstract Pattern matching on large graphs is the foundation for a variety of applicationdomains. Strict latency requirements and continuously increasing graph sizes demand theusage of highly parallel in-memory graph processing engines that need to consider non-uniform memory access (NUMA) and concurrency issues to scale up on modernmultiprocessor systems. To tackle these aspects; graph partitioning becomes increasinglyimportant. Hence; we present a technique to process graph pattern matching on NUMAsystems in this paper. As a scalable pattern matching processing infrastructure; we leveragea data-oriented architecture that preserves data locality and minimizes concurrency-relatedbottlenecks on NUMA systems. We show in detail; how graph pattern matching can beasynchronously processed on a multiprocessor system.,Advances in Databases and Information Systems,2017,*
Balancing Performance and Energy for Lightweight Data Compression Algorithms,Dirk Habich; Wolfgang Lehner,Abstract. Energy consumption becomes more and more a critical design factor; wherebyperformance is still an important requirement. Thus; a balance between performance andenergy has to be established. To tackle that issue for database systems; we proposed theconcept of work-energy profiles. However; generating such profiles requires extensivebenchmarking. To overcome that; we propose to approximate workenergy-profiles forcomplex operations based on the profiles of low-level operations in this paper. To show thefeasibility of our approach; we use lightweight data compression algorithms as complexoperations; since compression as well as decompression are heavily used in in-memorydatabase systems; where data is always managed in a compressed representation.Furthermore; we evaluate our approach on a concrete hardware system.,New Trends in Databases and Information Systems: ADBIS 2017 Short Papers and Workshops; AMSD; BigNovelTI; DAS; SW4CH; DC; Nicosia; Cyprus; September 24–27; 2017; Proceedings,2017,*
Asynchronous Graph Pattern Matching on Multiprocessor Systems,Wolfgang Lehner,Abstract. Pattern matching on large graphs is the foundation for a variety of applicationdomains. Strict latency requirements and continuously increasing graph sizes demand theusage of highly parallel in-memory graph processing engines that need to consider non-uniform memory access (NUMA) and concurrency issues to scale up on modernmultiprocessor systems. To tackle these aspects; graph partitioning becomes increasinglyimportant. Hence; we present a technique to process graph pattern matching on NUMAsystems in this paper. As a scalable pattern matching processing infrastructure; we leveragea data-oriented architecture that preserves data locality and minimizes concurrency-relatedbottlenecks on NUMA systems. We show in detail; how graph pattern matching can beasynchronously processed on a multiprocessor system.,New Trends in Databases and Information Systems: ADBIS 2017 Short Papers and Workshops; AMSD; BigNovelTI; DAS; SW4CH; DC; Nicosia; Cyprus; September 24–27; 2017; Proceedings,2017,*
AL: unified analytics in domain specific terms,Johannes Luong; Dirk Habich; Wolfgang Lehner,Abstract Data driven organizations gather information on various aspects of theirendeavours and analyze that information to gain valuable insights or to increaseautomatization. Today; these organizations can choose from a wealth of specializedanalytical libraries and platforms to meet their functional and non-functional requirements.Indeed; many common application scenarios involve the combination of multiple suchlibraries and platforms in order to provide a holistic perspective. Due to the scatteredlandscape of specialized analytical tools; this integration can result in complex and hard toevolve applications. In addition; the necessary movement of data between tools and formatscan introduce a serious performance penalty. In this article we present a unifiedprogramming environment for analytical applications. The environment includes AL; a …,Proceedings of The 16th International Symposium on Database Programming Languages,2017,*
Adaptive recovery for SCM-enabled databases,*,A system includes determination of a plurality of secondary data structures of a database tobe rebuilt; determination; for each of the plurality of secondary data structures; of a currentranking based on a pre-crash workload; a crash-time workload; the post-crash workload;and a rebuild time of the secondary data structure; determination to rebuild one of theplurality of secondary data structures based on the determined rankings; and rebuilding ofthe one of the plurality of secondary data structures in a dynamic random access memorybased on primary data of a database stored in non-volatile random access memory.,*,2017,*
Context Similarity for Retrieval-Based Imputation,Ahmad Ahmadov; Maik Thiele; Wolfgang Lehner; Robert Wrembel,Abstract Completeness as one of the four major dimensions of data quality is a pervasiveissue in modern databases. Although data imputation has been studied extensively in theliterature; most of the research is focused on inference-based approach. We propose toharness Web tables as an external data source to effectively and efficiently retrieve missingdata while taking into account the inherent uncertainty and lack of veracity that they contain.Existing approaches mostly rely on standard retrieval techniques and out-of-the-boxmatching methods which result in a very low precision; especially when dealing withnumerical data. We; therefore; propose a novel data imputation approach by applyingnumerical context similarity measures which results in a significant increase in the precisionof the imputation procedure; by ensuring that the imputed values are of the same domain …,Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017,2017,*
Hardware Based Databases,Wolfgang Lehner,Recent development on the hardware side will have a massive impact on the software stackof a computer system. While the software side enjoyed a “free ride” with ever increasingclock cycles for many decades; the time has finally come to adjust principles of softwarearchitecture in order to exploit the opportunities provided by modern hardware components.Within this special issue of it–Information Technology; we are perceiving data managementsystems as one of the most performance hungry software applications giving evidence of thelongstanding strive to directly control the hardware layer. The collection of articles addressesthis topic from multiple directions–advances on the memory as well as on the processorside; low-level considerations versus a more global perspective in hybrid cloudenvironments as well as academic perspective versus industrial contributions from SAP …,it-Information Technology,2017,*
An Analysis of the Feasibility of Graph Compression Techniques for Indexing Regular Path Queries,Frank Tetzel; Hannes Voigt; Marcus Paradies; Wolfgang Lehner,Abstract Regular path queries (RPQs) are a fundamental part of recent graph querylanguages like SPARQL and PGQL. They allow the definition of recursive path structuresthrough regular expressions in a declarative pattern matching environment. We study theuse of the K2-tree graph compression technique to materialize RPQ results with low memoryconsumption for indexing. Compact index representations enable the efficient storage ofmultiple indexes for varying RPQs.,Proceedings of the Fifth International Workshop on Graph Data-management Experiences & Systems,2017,*
Special Section on the International Conference on Data Engineering 2015,Wolfgang Lehner; Johannes Gehrke; Kyuseok Shim,THIS special section covers extended versions of top- ranked papers of the 31st InternationalConference on Data Engineering that was held in Seoul; Korea; on April 13- 17; 2015. The ICDE2015 conference received 645 submissions within the research track. With an acceptance rateof 13 per- cent—which reflects the goal of ICDE to keep the highest quality standards—84 researchpapers were accepted. Within the extremely well structured reviewing process; every paper wasat least reviewed by three members of the program committee (PC). The PC consisted of 142members; this year; special emphasis was laid on a balanced structure of the committee by appointingexperienced senior research- ers as well as young researchers starting out in a career inacademia. Every PC member's review workload was about eight and nine papers. Inaddition; every paper was assigned to a topic area. ICDE 2015 was structured into 15 …,IEEE Transactions on Knowledge and Data Engineering,2017,*
Dealing with Uncertainty: An Empirical Study on the Relevance of Renewable Energy Forecasting Methods,Wolfgang Lehner,Abstract. The increasing share of fluctuating renewable energy sources on the world-wideenergy production leads to a rising public interest in dedicated forecasting methods. Asdifferent scientific communities are dedicated to that topic; many solutions are proposed butnot all are suited for users from utility companies. We describe an empirical approach toanalyze the scientific relevance of renewable energy forecasting methods in literature. Then;we conduct a survey amongst forecasting software providers and users from the energydomain and compare the outcomes of both studies.,Data Analytics for Renewable Energy Integration: 4th ECML PKDD Workshop; DARE 2016; Riva del Garda; Italy; September 23; 2016; Revised Selected Papers,2017,*
Graph traversal operator and extensible framework inside a column store,*,A system; computer-implemented method; and a computer-readable storage medium for atraversal of a property graph; are provided. The edge table of the property graph is dividedinto a plurality of fragments. A first fragment is selected for traversal using a set of selectedvertices; where the traversal identifies a set of edges. Based on the set of edges; a set ofadjacent vertices is determined during the traversal. A set of discovered vertices in theproperty graph is determined based on the set of selected vertices and the set of adjacentvertices.,*,2017,*
Interactive exploration of large graphs,*,Disclosed herein are system; method; and computer program product embodiments forinteractive exploration of graph data. An embodiment operates by compiling a data query inany database-readable language based upon an input parameter. A data query of a graphdata set is then conducted but is halted when a breakpoint generated from the inputparameter is reached. Halting the data query includes partitioning a data subset from thegraph data set. The data subsets is then displayed.,*,2017,*
Parallel programming of in memory database utilizing extensible skeletons,*,An execution framework allows developers to write sequential computational logic;constrained for the runtime system to efficiently parallelize execution of custom businesslogic. The framework can be leveraged to overcome limitations in executing low levelprocedural code; by empowering the system runtime environment to parallelize this code.Embodiments employ algorithmic skeletons in the realm of optimizing/executing data flowgraphs of database management systems. By providing an extensible set of algorithmicskeletons the developer of custom logic can select the skeleton appropriate for new customlogic; and then fill in the corresponding computation logic according to the structuraltemplate of the skeleton. The skeleton provides a set of constraints known to the executionenvironment; that can be leveraged by the optimizer and the execution environment to …,*,2017,*
Energy Elasticity on Heterogeneous Hardware using Adaptive Resource Reconfiguration,Annett Ungethüm; Thomas Kissinger; Willi-Wolfram Mentzel; Eric Mier; Dirk Habich; Wolfgang Lehner,Energy awareness of database systems has emerged as a critical research topic; becauseenergy consumption is becoming a major factor. Recent energy-related hardwaredevelopments tend towards o ering more and more configuration opportunities for thesoftware to control its own energy-based behavior. Existing research within the DBcommunity so far mainly focused on leveraging this configuration spectrum to identify themost energy-efficient configuration for specific operators or entire queries. In [Un16]; weintroduced the concept of energy elasticity and proposed the energy-control loop as animplementation of this concept. Energy elasticity refers to the ability of software to behaveenergy-proportional and energy-e cient at the same time while maintaining a certain qualityof service.,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,*
InVerDa–The Liquid Database,Kai Herrmann; Hannes Voigt; Thorsten Seyschab; Wolfgang Lehner,Multiple applications; which share one common database; will evolve over time by their verynature. Often; former versions need to stay available; so database developers findthemselves maintaining co-existing schema versions of multiple applications in multipleversions—usually with handwritten delta code—which is highly error-prone and explainssignificant costs in software projects. We showcase IVD; a tool using the richer semantics ofa bidirectional database evolution language to generate all the delta code automatically;easily providing co-existing schema versions within one database. IVD automaticallydecides on an optimized physical database schema serving all schema versions totransparently optimize the performance for the current workload.,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,*
Overview on Hardware Optimizations for Database Engines,Annett Ungethüm; Dirk Habich; Tomas Karnagel; Sebastian Haas; Eric Mier; Gerhard Fettweis; Wolfgang Lehner,The key objective of database systems is to e ciently manage an always increasing amountof data. Thereby; a high query throughput and a low query latency are core requirements. Tosatisfy these requirements; database engines are highly adapted to the given hardware byusing all features of modern processors. Apart from this software optimization; even tailor-made processing circuits running on FGPAs are built to run mostly stateless query plans witha high throughput. A similar approach; which was already investigated three decades ago; isto build customized hardware like a database processor. Tailor-made hardware allows toachieve performance numbers that cannot be reached with software running on general-purpose CPUs; while at the same time; addressing the dark silicon problem. The maindisadvantage of custom hardware is the high development cost that comes with …,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,*
Exploratory Ad-Hoc Analytics for Big Data,Julian Eberius; Maik Thiele; Wolfgang Lehner,Abstract In a traditional relational database management system; queries can only bedefined over attributes defined in the schema; but are guaranteed to give single; definitiveanswer structured exactly as specified in the query. In contrast; an information retrievalsystem allows the user to pose queries without knowledge of a schema; but the result will bea top-k list of possible answers; with no guarantees about the structure or content of theretrieved documents. In this chapter; we present Drill Beyond; a novel IR/RDBMS hybridsystem; in which the user seamlessly queries a relational database together with a largecorpus of tables extracted from a web crawl. The system allows full SQL queries over arelational database; but additionally enables the user to use arbitrary additional attributes inthe query that need not to be defined in the schema. The system then processes this semi …,*,2017,*
1 „Query-time Data Integration “von Julian Eberius; TU Dresden; Dez. 2015,Wolfgang Lehner,In Zeiten von Big Data ist eine klassische Datenintegration auf ein globales Zielschemamittels schwergewichtiger und statischer ETL-Prozesse oft nicht mehr möglich. LoseDatensammlungen wie Data Lakes; aber auch Open-Data-Plattformen; sind das Ergebnisdieser Entwicklung. Jedoch sind aktuelle Datenmanagementlösungen nicht in der Lage;solche Datenbestände ad hoc für ein bestimmtes Informationsbedürfnis zu integrieren.Stattdessen sind meist teure; manuelle Prozesse erforderlich; bevor die eigentlicheAnalysearbeit beginnen kann. In dieser Arbeit wird daher das Paradigma derDatenintegration zur Anfragezeit als alternatives Konzept vorgestellt. Es zielt darauf ab;Nutzern die Möglichkeit zu geben; sogenannte offene Anfragen zu stellen. Dabei handelt essich um Anfragen; die über das definierte Schema einer bestehenden Datenbank …,*,2017,*
Penalized graph partitioning based allocation strategy for database-as-a-service systems,Tim Kiefer; Dirk Habich; Wolfgang Lehner,Databases as a service (DBaaS) transfer the advantages of cloud computing to datamanagement systems; which is important for the big data era. The allocation in a DBaaSsystem; ie; the mapping from databases to nodes of the infrastructure; influencesperformance; utilization; and cost-effectiveness of the system. Modeling databases and theunderlying infrastructure as weighted graphs and using graph partitioning and mappingalgorithms yields an allocation strategy. However; graph partitioning assumes that individualvertex weights add up (linearly) to partition weights. In reality; performance does usually notscale linearly with the amount of work due to contention on the hardware; on operatingsystem resources; or on DBMS components. To overcome this issue; we propose anallocation strategy based on penalized graph partitioning in this paper. We show how …,Big Data Computing Applications and Technologies (BDCAT); 2016 IEEE/ACM 3rd International Conference on,2016,*
Dealing with Uncertainty: An Empirical Study on the Relevance of Renewable Energy Forecasting Methods,Robert Ulbricht; Anna Thoß; Hilko Donker; Gunter Gräfe; Wolfgang Lehner,Abstract The increasing share of fluctuating renewable energy sources on the world-wideenergy production leads to a rising public interest in dedicated forecasting methods. Asdifferent scientific communities are dedicated to that topic; many solutions are proposed butnot all are suited for users from utility companies. We describe an empirical approach toanalyze the scientific relevance of renewable energy forecasting methods in literature. Then;we conduct a survey amongst forecasting software providers and users from the energydomain and compare the outcomes of both studies.,International Workshop on Data Analytics for Renewable Energy Integration,2016,*
ResilientStore: A Heuristic-Based Data Format Selector for Intermediate Results,Maik Thiele; Wolfgang Lehner,Abstract. Large-scale data analysis is an important activity in many organizations thattypically requires the deployment of data-intensive workflows. As data is processed theseworkflows generate large intermediate results; which are typically pipelined from oneoperator to the following. However; if materialized; these results become reusable; hence;subsequent workflows need not recompute them. There are already many solutions thatmaterialize intermediate results but all of them assume a fixed data format. A fixed format;however; may not be the optimal one for every situation. For example; it is well-known thatdifferent data fragmentation strategies (eg; horizontal and vertical) behave better or worseaccording to the access patterns of the subsequent operations. In this paper; we presentResilientStore; which assists on selecting the most appropriate data format for …,Model and Data Engineering: 6th International Conference; MEDI 2016; Almería; Spain; September 21-23; 2016; Proceedings,2016,*
Big by blocks: modular analytics,Martin Hahmann; Claudio Hartmann; Lars Kegel; Dirk Habich; Wolfgang Lehner,Abstract Big Data and Big Data analytics have attracted major interest in research andindustry and continue to do so. The high demand for capable and scalable analytics incombination with the ever increasing number and volume of application scenarios and datahas lead to a large and intransparent landscape full of versions; variants and individualalgorithms. As this zoo of methods lacks a systematic way of description; understanding isalmost impossible which severely hinders effective application and efficient development ofanalytic algorithms. To solve this issue we propose our concept of modular analytics thatabstracts the essentials of an analytic domain and turns them into a set of universal buildingblocks. As arbitrary algorithms can be created from the same set of blocks; understanding iseased and development benefits from reusability.,it-Information Technology,2016,*
Penalized Graph Partitioning for Static and Dynamic Load Balancing,Tim Kiefer; Dirk Habich; Wolfgang Lehner,Abstract With ubiquitous parallel architectures; the importance of optimally distributed andthereby balanced work is unprecedented. To tackle this challenge; graph partitioningalgorithms have been successfully applied in various application areas. However; there is amismatch between solutions found by classic graph partitioning and the behavior of manyreal hardware systems. Graph partitioning assumes that individual vertex weights add up topartition weights (here; referred to as linear graph partitioning). This implies thatperformance scales linearly with the number of tasks. In reality; performance does usuallynot scale linearly with the amount of work due to contention on various resources. Weaddress this mismatch with our novel penalized graph partitioning approach in this paper.Furthermore; we experimentally evaluate the applicability and scalability of our method.,European Conference on Parallel Processing,2016,*
Logical Data Independence in the 21st Century--Co-Existing Schema Versions with InVerDa,Kai Herrmann; Hannes Voigt; Andreas Behrend; Jonas Rausch; Wolfgang Lehner,Abstract: We present InVerDa; a tool for end-to-end support of co-existing schema versionswithin one database. While it is state of the art to run multiple versions of a continuouslydeveloped application concurrently; the same is hard for databases. In order to keepmultiple co-existing schema versions alive; that all access the same data set; developersusually employ handwritten delta code (eg views and triggers in SQL). This delta code ishard to write and hard to maintain: if a database administrator decides to adapt the physicaltable schema; all handwritten delta code needs to be adapted as well; which is expensiveand error-prone in practice. With InVerDa; developers use a simple bidirectional databaseevolution language in the first place that carries enough information to generate all the deltacode automatically. Without additional effort; new schema versions become immediately …,arXiv preprint arXiv:1608.05564,2016,*
Towards Efficient Multi-domain Data Processing,Johannes Luong; Dirk Habich; Thomas Kissinger; Wolfgang Lehner,Abstract Economy and research increasingly depend on the timely analysis of large datasetsto guide decision making. Complex analysis often involve a rich variety of data types andspecial purpose processing models. We believe; the database system of the future will usecompilation techniques to translate specialized and abstract high level programming modelsinto scalable low level operations on efficient physical data formats. We currently envisionoptimized relational and linear algebra languages; a flexible data flow language (Alanguage inspired by the programming models of popular data flow engines like ApacheSpark (spark. apache. org) or Apache Flink (flink. apache. org).) and scaleable physicaloperators and formats for relational and array data types. In this article; we propose adatabase system architecture that is designed around these ideas and we introduce our …,International Conference on Data Management Technologies and Applications,2016,*
Challenges for Context-Driven Time Series Forecasting,Robert Ulbricht; Hilko Donker; Claudio Hartmann; Martin Hahmann; Wolfgang Lehner,Abstract Predicting time series is a crucial task for organizations; since decisions are oftenbased on uncertain information. Many forecasting models are designed from a genericstatistical point of view. However; each real-world application requires domain-specificadaptations to obtain high-quality results. All such specifics are summarized by the term ofcontext. In contrast to current approaches; we want to integrate context as the primary driverin the forecasting process. We introduce context-driven time series forecasting focusing ontwo exemplary domains: renewable energy and sparse sales data. In view of this; wediscuss the challenge of context integration in the individual process steps.,Journal of Data and Information Quality (JDIQ),2016,*
Multi-GPU Approximation Methods for Silent Data Corruption of AN-Coding,Matthias Werner; Till Kolditz; Tomas Karnagel; Dirk Habich; Wolfgang Lehner; Till Kolditz; Dirk Habich; Dmitrii Kuvaiskii; Wolfgang Lehner; Christof Fetzer; Till Kolditz; Dirk Habich; Patrick Damme; Wolfgang Lehner; Dmitrii Kuvaiskii; Christof Fetzer; Till Kolditz; Benjamin Schlegel; Dirk Habich; Wolfgang Lehner; Till Kolditz; Thomas Kissinger; Benjamin Schlegel; Dirk Habich; Wolfgang Lehner,Background The key objective of database systems is to reliably manage data; while highquery throughput and low query latency are core requirements [1]. To satisfy theserequirements for a constantly increasing amount of data; database systems constantly adaptto new hardware features [2; 3; 4; 5; 6; 7]; for instance: new instruction sets; increasing corecounts; changing core/cache topologies; increasing DRAM bandwidths; or new persistencetechnologies (nvRAM)[8; 9; 10]. These advances come with a backdraw; though: for a longtime it has been known that hardware is subject to soft and hard errors [11; 12; 13]. Softerrors are also called bit flips; which may occur due to cosmic rays; heat; hardware aging; orelectrical crosstalk; which; in turn; is due to the ongoing miniaturization of integrated curcuits[11; 14]. Hardware aging even leads to increasing error rates during a system's run-time …,12th International Workshop on Boolean Problems; IWSBP 2016; Freiberg; Germany,2016,*
Graph databases provide schema-flexible storage and support complex; expressive queries. However; the flexibility and expressiveness in these queries come at ad...,Elena Vasilyeva; Maik Thiele; Christof Bornhövd; Wolfgang Lehner,We propose TEDI; an indexing for solving shortest path; and k Nearest Neighbors (kNN)problems. TEDI is based on the tree decomposition methodology. The graph is firstdecomposed into a tree in which the node contains vertices. The shortest paths are stored insuch nodes. These local shortest paths together with the tree structure constitute the index ofthe graph. Based on this index; algorithms...,Journal of Computer and System Sciences,2016,*
Model-Driven Integration of Compression Algorithms in Column-Store Database Systems.,Juliana Hildebrandt; Dirk Habich; Wolfgang Lehner,Abstract. Modern database systems are very often in the position to store their entire data inmain memory. Aside from increased main memory capacities; a further driver for in-memorydatabase systems was the shift to a decomposition storage model in combination withlightweight data compression algorithms. Using both mentioned storage design concepts;large datasets can be held and processed in main memory with a low memory footprint. Inrecent years; a large corpus of lightweight data compression algorithms has beendeveloped to efficiently support different data characteristics. In this paper; we present ournovel model-driven concept to integrate this large and evolving corpus of lightweight datacompression algorithms in column-store database systems. Core components of ourconcept are (i) a unified conceptual model for lightweight compression algorithms;(ii) …,LWDA,2016,*
HUGS-A Lightweight Graph Partitioning Approach.,Alexander Krause; Hannes Voigt; Wolfgang Lehner,ABSTRACT The growing interest in graph data lead to increasingly more research in thefield of graph data management and graph analytics. Nowadays; even large graphs of uptoa size of billions of vertices and edges fit into main memory of big modern multisocketmachines; making them a first-grade platform for graph management and graph analytics.High performance data management solutions have to be aware of the NUMA properties ofsuch big machines. A dataoriented architecture (DORA) is a particular solution to that.However; it requires partitioning the data in a way such that inter-partition communicationcan be avoided. Graph partitioning is a long studied problem and stateof-the-art solutions;such as multilevel k-way partitioning and recursive bisection achieve good results in feasibletime. Integrating such solution is a rather difficult task; though. In this paper; we present a …,GvD,2016,*
Model Kit for Lightweight Data Compression Algorithms.,Juliana Hildebrandt; Dirk Habich; Patrick Damme; Wolfgang Lehner,ABSTRACT Modern database systems are very often in the position to store and efficientlyprocess their entire data in main memory. Aside from increased main memory capacities; afurther driver for in-memory database systems has been the shift to a column-orientedstorage format in combination with lightweight data compression techniques. In recent years;a lot of lightweight data compression algorithms have been developed to efficiently supportdifferent data characteristics. Therefore; database systems should include a large number ofthese algorithms. To enable this; we introduce our novel modularization concept includingour model kit implementation for lightweight data compression algorithms.,EDBT,2016,*
Considering User Intention in Differential Graph Queries,Elena Vasilyeva; Maik Thiele; Christof Bornhövd; Wolfgang Lehner,Abstract Empty answers are a major problem by processing pattern matching queries ingraph databases. Especially; there can be multiple reasons why a query failed. To supportusers in such situations; differential queries can be used that deliver missing parts of a graphquery. Multiple heuristics are proposed for differential queries; which reduce the searchspace. Although they are successful in increasing the performance; they can discard querysubgraphs relevant to a user. To address this issue; the authors extend the concept ofdifferential queries and introduce top-k differential queries that calculate the ranking basedon users' preferences and significantly support the users' understanding of query databasemanagement systems. A user assigns relevance weights to elements of a graph query thatsteer the search and are used for the ranking. In this paper the authors propose different …,Journal of Database Management (JDM),2015,*
Parallel programming of in memory database utilizing extensible skeletons,*,An execution framework allows developers to write sequential computational logic;constrained for the runtime system to efficiently parallelize execution of custom businesslogic. The framework can be leveraged to overcome limitations in executing low levelprocedural code; by empowering the system runtime environment to parallelize this code.Embodiments employ algorithmic skeletons in the realm of optimizing/executing data flowgraphs of database management systems. By providing an extensible set of algorithmicskeletons the developer of custom logic can select the skeleton appropriate for new customlogic; and then fill in the corresponding computation logic according to the structuraltemplate of the skeleton. The skeleton provides a set of constraints known to the executionenvironment; that can be leveraged by the optimizer and the execution environment to …,*,2015,*
SCIT: A Schema Change Interpretation Tool for Dynamic-Schema Data Warehouses,Rihan Hai; Vasileios Theodorou; Maik Thiele; Wolfgang Lehner,Abstract Data Warehouses (DW) have to continuously adapt to evolving businessrequirements; which implies structure modification (schema changes) and data migrationrequirements in the system design. However; it is challenging for designers to control theperformance and cost overhead of different schema change implementations. In this paper;we demonstrate SCIT; a tool for DW designers to test and implement different logical designalternatives in a two-fold manner. As a main functionality; SCIT translates common DWschema modifications into directly executable SQL scripts for relational database systems;facilitating design and testing automation. At the same time; SCIT assesses changes andrecommends alternative design decisions to help designers improve logical designs andavoid common dimensional modeling pitfalls and mistakes. This paper serves as a walk …,Australasian Database Conference,2015,*
SynopSys: Foundations for Multidimensional Graph Analytics,Wolfgang Lehner,Abstract. The past few years have seen a tremendous increase in often irregularly structureddata that can be represented most naturally and efficiently in the form of graphs. Makingsense of incessantly growing graphs is not only a key requirement in applications like socialmedia analysis or fraud detection but also a necessity in many traditional enterprisescenarios. Thus; a flexible approach for multidimensional analysis of graph data is needed.Whereas many existing technologies require up-front modelling of analytical scenarios andare difficult to adapt to changes; our approach allows for ad-hoc analytical queries of graphdata. Extending our previous work on graph summarization; in this position paper we lay thefoundation for large graph analytics to enable business intelligence on graph-structureddata.,Enabling Real-Time Business Intelligence: International Workshops; BIRTE 2013; Riva del Garda; Italy; August 26; 2013; and BIRTE 2014; Hangzhou; China; September 1; 2014; Revised Selected Papers,2015,*
Message from the ICDE 2015 Program Committee and general chairs,Johannes Gehrke; Wolfgang Lehner; Kyuseok Shim; Sang Kyun Cha; Guy Lohman,Since its inception in 1984; the IEEE International Conference on Data Engineering (ICDE)has become a premier forum for the exchange and dissemination of data managementresearch results among researchers; users; practitioners; and developers. Continuing thislong-standing tradition; the 31st ICDE will be hosted this year in Seoul; South Korea; fromApril 13 to April 17; 2015. It is our great pleasure to welcome you to ICDE 2015 and topresent its proceedings to you.,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,*
Enjoy FRDM-play with a schema-flexible RDBMS,Hannes Voigt; Patrick Damme; Wolfgang Lehner,Relational database management systems build on the closed world assumption requiringupfront modeling of a usually stable schema. However; a growing number of today'sdatabase applications are characterized by self-descriptive data. The schema of self-descriptive data is very dynamic and prone to frequent changes; a situation which is alwaystroublesome to handle in relational systems. This demo presents the relational databasemanagement system FRDM. With flexible relational tables FRDM greatly simplifies themanagement of self-descriptive data in a relational database system. Self-descriptive datacan reside directly next to traditionally modeled data and both can be queried together usingSQL. This demo presents the various features of FRDM and provides first-hand experienceof the newly gained freedom in relational database systems.,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,*
Datenbanksysteme für Business; Technologie und Web (BTW 2015)-Workshopband,Norbert Ritter; Andreas Henrich; Wolfgang Lehner; Andreas Thor; Steffen Friedrich; Wolfram Wingerath,*,*,2015,*
Resiliency-aware Data Compression for In-memory Database Systems.,Till Kolditz; Dirk Habich; Patrick Damme; Wolfgang Lehner; Dmitrii Kuvaiskii; Oleksii Oleksenko; Christof Fetzer,Abstract: Nowadays; database systems pursuit a main memory-centric architecture; wherethe entire business-related data is stored and processed in a compressed form in mainmemory. In this case; the performance gain is massive because database operations canbenefit from its higher bandwidth and lower latency. However; current main memory-centricdatabase systems utilize general-purpose error detection and correction solutions toaddress the emerging problem of increasing dynamic error rate of main memory. The costsof these generalpurpose methods dramatically increases with increasing error rates. Toreduce these costs; we have to exploit context knowledge of database systems for resiliency.Therefore; we introduce our vision of resiliency-aware data compression in this paper;where we want to exploit the benefits of both fields in an integrated approach with low …,DATA,2015,*
Big Data-Zentren-Vorstellung und Panel,Volker Markl; Erhard Rahm; Wolfgang Lehner; Michael Beigl; Thomas Seidl,Zur Erforschung der verschiedenen Facetten von Big Data “wurden jüngst drei Zen-” trengegründet. Hierbei handelt es sich um die vom Bundesministerium für Bildung undForschung (BMBF) geförderten Kompetenzzentren BBDC (Berlin Big Data Center; LeitungTU Berlin) und ScaDS (Competence Center for Scalable Data Services and Solutions;Leitung TU Dresden und Uni Leipzig) sowie das in Zusammenarbeit von Industrie undForschung eingerichtete SDIL (Smart Data Innovation Lab; Leitung KIT). Diese drei Zentrenwerden zunächst in Kurzvorträgen vorgestellt. Eine sich anschließende Panel-Diskussionarbeitet Gemeinsamkeiten; spezifische Ansprüche und Kooperationsmöglichkeiten heraus.,Datenbanksysteme für Business; Technologie und Web (BTW 2015),2015,*
Towards Programmability of a NUMA-Aware Storage Engine.,Dirk Habich; Johannes Schad; Thomas Kissinger; Wolfgang Lehner,Abstract. The SQL database language was originally intended for application programmers.However; after more than 20 years of language extensions; SQL can only be generated bysoftware components and is no longer suitable for an increasing user base like knowledgeworkers or data scientists; who want to work with data in an interactive fashion. The originalidea of declarative query languages; telling the system what information to retrieve and nothow to retrieve it; is still relevant. However; procedural elements are extremely worthwhileand have to be part of a next generation database programming language withoutcompromising performance and scalability. To tackle this challenge; we are going to presentour overall approach consisting of a highly-scalable NUMA-aware storage engine ERIS anda novel appropriate procedural programming approach on top of ERIS in this paper.,LWA,2015,*
Modularisierung leichtgewichtiger Kompressionsalgorithmen.,Juliana Hildebrandt; Dirk Habich; Patrick Damme; Wolfgang Lehner,ABSTRACT Im Kontext von In-Memory Datenbanksystemen nehmen leichtgewichtigeKompressionsalgorithmen eine entscheidende Rolle ein; um eine effiziente Speicherungund Verarbeitung großer Datenmengen im Hauptspeicher zu realisieren. Verglichen mitklassischen Komprimierungstechniken wie zB Huffman erzielen leichtgewichtigeKompressionsalgorithmen vergleichbare Kompressionsraten aufgrund der Einbeziehungvon Kontextwissen und erlauben eine schnellere Kompression und Dekompression. DieVielfalt der leichtgewichtigen Kompressionsalgorithmen hat in den letzten Jahrenzugenommen; da ein großes Optimierungspotential über die Einbeziehung desKontextwissens besteht. Um diese Vielfalt zu bewältigen; haben wir uns mit derModularisierung von leichtgewichtigen Kompressionsalgorithmen beschäftigt und ein …,GvD,2015,*
Online bit flip detection for in-memory B-trees live!,Till Kolditz; Benjamin Schlegel; Dirk Habich; Wolfgang Lehner,Hardware vendors constantly decrease the feature sizes of integrated circuits to obtainhigher performance and energy efficiency. As a side-effect; integrated circuits-like CPUs andmain memory-become more and more vulnerable to external influences and thus unreliable;which results in increasing numbers of (multi-) bit flips. From a database perspective bit fliperrors in main memory will become a major challenge for modern in-memory databasesystems; which keep all their enterprise data in volatile; unreliable main memory. Existinghardware error control techniques like ECC-DRAM are able to detect and correct memoryerrors; but their detection and correction capabilities are limited and come along with severaldownsides. To underline this we heat up RAM live on-site to show possible error rates offuture hardware. We previously presented various techniques for the B-Tree-as a wide …,Datenbanksysteme für Business; Technologie und Web (BTW 2015),2015,*
Towards optimal execution of density-based clustering on heterogeneous hardware,Dirk Habich; Stefanie Gahrig; Wolfgang Lehner,Abstract Data Clustering is an important and highly utilized data mining technique in variousapplication domains. With ever increasing data volumes in the era of big data; the efficientexecution of clustering algorithms is a fundamental prerequisite to gain understanding andacquire novel; previously unknown knowledge from data. To establish an efficient execution;the clustering algorithms have to be re-engineered to fully exploit the provided hardwarecapabilities. Shared-memory multiprocessor systems like graphics processing units (GPUs)provide extremely high parallelism combined with a high bandwidth transfer at low cost. Theavailability of such computing units increases with upcoming processors; where a commonCPU and various computing units; like GPU; are tightly coupled using a unified sharedmemory hierarchy. In this paper; we consider density-based clustering for such …,Proceedings of the 3rd International Workshop on Big Data; Streams and Heterogeneous Source Mining: Algorithms; Systems; Programming Models and Applications,2014,*
Report on the second international workshop on energy data management (EnDM 2013),Torben Bach Pedersen; Wolfgang Lehner,The energy sector is in transition–being forced to rethink the current practice and apply data-management based IT solutions to provide a scalable and sustainable supply anddistribution of energy. Novel challenges range from renewable energy production overenergy distribution and monitoring to controlling and moving energy consumption. Hugeamounts of “Big Energy Data;” ie; data from smart meters; new renewable energy sources(RES–such as wind; solar; hydro; thermal; etc); novel distributions mechanisms (Smart Grid);and novel types of consumers and devices; eg; electric cars; are being collected and mustbe managed and analyzed to yield their potential. Energy is at the top of the worldwidepolitical agenda. For example; The European Union has stated the “20-20-20 goals”(20%renewable energy; 20% better energy efficiency; and 20% CO2 reduction by 2020). Even …,ACM SIGMOD Record,2014,*
Efficient forecasting for hierarchical time series,Lars Dannecker; Robert Lorenz; Philipp Rösch; Wolfgang Lehner; Gregor Hackenbroich,Abstract Forecasting is used as the basis for business planning in many application areassuch as energy; sales and traffic management. Time series data used in these areas is oftenhierarchically organized and thus; aggregated along the hierarchy levels based on theirdimensional features. Calculating forecasts in these environments is very time consuming;due to ensuring forecasting consistency between hierarchy levels. To increase theforecasting efficiency for hierarchically organized time series; we introduce a novelforecasting approach that takes advantage of the hierarchical organization. There; we reusethe forecast models maintained on the lowest level of the hierarchy to almost instantly createalready estimated forecast models on higher hierarchical levels. In addition; we define ahierarchical communication framework; increasing the communication flexibility and …,Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,2013,*
Optimizing Sample Design for Approximate Query Processing,Philipp Rösch; Wolfgang Lehner,Abstract The rapid increase of data volumes makes sampling a crucial component ofmodern data management systems. Although there is a large body of work on databasesampling; the problem of automatically determine the optimal sample for a given queryremained (almost) unaddressed. To tackle this problem the authors propose a sampleadvisor based on a novel cost model. Primarily designed for advising samples of a fewqueries specified by an expert; the authors additionally propose two extensions of thesample advisor. The first extension enhances the applicability by utilizing recorded workloadinformation and taking memory bounds into account. The second extension increases theeffectiveness by merging samples in case of overlapping pieces of sample advice. For bothextensions; the authors present exact and heuristic solutions. Within their evaluation; the …,International Journal of Knowledge-Based Organizations (IJKBO),2013,*
Query processing on prefix trees live,Thomas Kissinger; Benjamin Schlegel; Dirk Habich; Wolfgang Lehner,Abstract Modern database systems have to process huge amounts of data and shouldprovide results with low latency at the same time. To achieve this; data is nowadays typicallyhold completely in main memory; to benefit of its high bandwidth and low access latency thatcould never be reached with disks. Current in-memory databases are usually column-storesthat exchange columns or vectors between operators and suffer from a high tuplereconstruction overhead. In this demonstration proposal; we present DexterDB; whichimplements our novel prefix tree-based processing model that makes indexes the first-classcitizen of the database system. The core idea is that each operator takes a set of indexes asinput and builds a new index as output that is indexed on the attribute requested by thesuccessive operator. With that; we are able to build composed operators; like the multi …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,*
Research challenges for energy data management (panel),Torben Bach Pedersen; Wolfgang Lehner,Abstract This panel paper aims at initiating discussion at the Second International Workshopon Energy Data Management (EnDM 2013) about the important research challenges withinEnergy Data Management. The authors are the panel organizers; extra panelists will berecruited before the workshop.,Proceedings of the Joint EDBT/ICDT 2013 Workshops,2013,*
Über Aufbau und Auswertung,Wolfgang Lehner; Michael Teschke; Hartmut Wedekind,Kurzfassung Der Aufbau multidimensionaler Daten in extensionalen und intensionalenZusam-menhängen wird mit Hilfe der Generalisierungshierarchie von Smith/Smithbeschrieben. Es zeigt sich; daß Auswertungen multidimensionaler Daten sowohlklassifikationsbezogen; dh extensional; als auch eigenschaftsbezogen; dh inten-sional;orientiert sind. Ein adäquater Auswertungsprozeß wird auf relationaler Basis beschriebenund ein dazu notwendiger Substitutionsoperator definiert. Insbesondere durch dieintensionale Analyse werden Datenwürfel sehr groß; was eine volle Auswertung sehr zeit-und speicheraufwendig macht und deshalb gegenüber'Adhoc"-Auswertungenzurückzustellen ist.,Datenbanksysteme in Büro; Technik und Wissenschaft: GI-Fachtagung. Ulm; 5.-7. März 1997,2013,*
Dr. Dean Jacobs,Alfons Kemper; Wolfgang Lehner,Dean Jacobs hinterlässt seine Frau Gudrun sowie die zwei Söhne Nicolas (18) und Willem(12); die bei München leben. Nicht zuletzt seiner Familie zuliebe hat Dean seineamerikanische Heimat verlassen und sich auf das „Abenteuer “Deutschland eingelassen.Seine offene Art hat ihn innerhalb kürzester Zeit zu einem respektierten und allseitsbeliebten Mitglied der deutschen Datenbänkler werden lassen. Als Mentor für Doktorandeninsbesondere der TUM und des HPI hat er wesentliche Anstöße für innovative undpraxisrelevante Arbeiten gegeben.,Datenbank-Spektrum,2013,*
Special issue on best papers of VLDB 2011,Wolfgang Lehner; Sunita Sarawagi,The selection covers a wide spectrum of database systemrelated topics; ranging from queryoptimization to information retrieval and model management. Also; the papers come fromresearch groups all over the world: Redmond; Waterloo; Oxford; Berlin; and Bombay; a trulyinternational mix! All contributions are significantly extended and improved with respect tothe original conference version. We hope you enjoy these “Best of VLDB 2011” papers—may the papers spark novel research ideas!!! The first paper “Efficiently Adapting GraphicalModels for Selectivity Estimation” of this special issue focuses on a classical problem withindatabase query optimization: how to increase the accuracy of a cost model? Sincetraditional query optimization techniques are based on the independence assumption ofindividual columns; even small correlations in the database may results in significant …,The VLDB Journal,2013,*
Supporting database schema evolution represents a long-standing challenge of practical and theoretical importance for modern information systems. In this paper; w...,Wolfgang Lehner; Sunita Sarawagi; Carlo Curino; Hyun Jin Moon; Alin Deutsch; Carlo Zaniolo,In this paper; we present a technique for building a high-availability (HA) databasemanagement system (DBMS). The proposed technique can be applied to any DBMS withlittle or no customization; and with reasonable performance overhead. Our approach isbased on Remus; a commodity HA solution implemented in the virtualization layer; that usesasynchronous virtual machine state replication to provide...,The VLDB Journal,2013,*
Cloud-Specific Services for Data Management,Wolfgang Lehner; Kai-Uwe Sattler,Abstract Cloud-based data management poses several challenges which go beyondtraditional database technologies. Outsourcing the operation of database applications to aprovider who; on the one hand; takes responsibility not only for providing the infrastructurebut also for maintaining the system and; on the other hand; can pool resources and operatethem in a cost-efficient and dynamic way promise cost savings and elasticity in usage.However; most customers are willing to move their on-premise setup to a hostedenvironment only if their data are kept securely and privately as well as non-functionalproperties such as availability or performance are guaranteed.,*,2013,*
Web-Scale Analytics for BIG Data,Wolfgang Lehner; Kai-Uwe Sattler,Abstract Virtualization is the key concept to provide a scalable and flexible computingenvironment in general. In this chapter; we focus on virtualization concepts in the context ofdata management tasks. We review existing concepts and technologies spanning multiplesoftware layers.,*,2013,*
1 Separating Key Concerns in Query Processing—Set Orientation; Physical Data Independence; and Parallelism von Sebastian Bächle TU Kaiserslautern; Dezemb...,Wolfgang Lehner,Declarative query languages are the most convenient and most productive abstraction forinteracting with complex data management systems. While the developer can focus on theapplication logic; the compiler takes care of translating and optimizing a query for efficientexecution. Today; applications increasingly call for declarative data management for manynovel storage designs and system architectures. The realization of a query processingsystem for every new kind of storage; language; or data model is a complex and time-consuming task. It requires considerable effort to design; implement; test; and optimize acompiler; which utilizes the system optimally. Thereby; a large part of the work is devoted toporting and adapting proven algorithms and optimizations from existing solutions. Thisthesis studies the design of a compiler and runtime infrastructure for consolidating this …,*,2013,*
Summary and Outlook,Wolfgang Lehner; Kai-Uwe Sattler,Abstract The overall goal of this book is to give an in-depth introduction into the context ofdata management services running in the cloud. Since the “as-a-Service”-philosophy isconsidered one of the extremely relevant developments in computer and informationtechnology and a huge number of different variants of services–ranging from infrastructuralservices to services providing access to complex software components–is already existing; itis time to provide a comprehensive overview and list of challenges for “Data-as-a-Service”(DaaS). The DaaS approach is not only relevant because data management is oneof the ubiquitous core tasks in many service and application stacks and the market fordatabase technologies is a very big and growing market. We also believe that this kind ofservice will change the way of how we will work with data and databases in the near …,*,2013,*
Data Cloudification,Wolfgang Lehner; Kai-Uwe Sattler,Although we already live in a world of data; we just see the tip of an iceberg. Data is everywhereand decisions based on large data sets are driving not only business related but more and morepersonal decisions. The challenges are enormous and range from technical questions on howto setup and run an efficient and cost-effective data management platform to security and privacyconcerns to prevent the loss of personal self-determination. Within this section; we present thegeneral setup of the “-as-a-Service”-paradigm and discuss certain facets of data managementsolutions to cope with the existing and upcoming challenges … Over the last years the worldcollected an astonishing amount of digital information. This particularly applies to data collectedand generated within modern web applications which leads to unprecedented challenges indata and information management. Search engines such as Google; Bing or Yahoo …,*,2013,*
Special section on large-scale analytics,Wolfgang Lehner; Michael J Franklin,Big Data is no longer exclusively the domain of big organizations. Companies;collaborations; and organizations of all types and sizes are increasingly faced with the needto analyze and make sense of large and growing collections of data. Solving the challengeof large-scale analytics requires innovation across the spectrum of data management: Largevolumes of data have to be acquired; processed; stored; and eventually reclaimed. Complexstatistical procedures must be applied to those large data sets. Transactional guarantees arerequired to provide a consistent picture with operational systems. Metadata must bemaintained to provide the context of the underlying raw data for later analysis. Thesechallenges must be faced independently and together in order to establish a scalable;affordable; and flexible large-scale analytics infrastructure. This special section focuses …,The VLDB Journal,2012,*
Die Datenbankforschungsgruppe der Technischen Universität Dresden stellt sich vor,Wolfgang Lehner,Zentral in diesem Bereich sind aktuell Arbeiten einer durch den Europäischen Sozialfondgeförderten Nachwuchsforschergruppe zur Entwicklung von Konzepten und Werkzeugenzur situativen Bereitstellung; Integration und Analyse externer Datenquellen. Das Wesen dersituativen Datenanalyse („Do-it-Yourself Analytics “) besteht grundsätzlich darin; dass die zuverwendenden Datenquellen a priori nicht bekannt sind und trotzdem zeitnah integriertwerden müssen. Um diesen Prozess zu unterstützen; wurde das „DrillBeyond “-Konzeptentwickelt; das die Stärken eines strukturierten DBMS sowie die Einfachheit,Datenbank-Spektrum,2012,*
Frontiers in Crowdsourced Data Integration,Katrin Braunschweig; Julian Eberius; Maik Thiele; Wolfgang Lehner,Abstract There is an ever-increasing amount and variety of open web data available that isinsufficiently examined or not considered at all in decision making processes. This isbecause of the lack of end-user friendly tools that help to reuse this public data and to createknowledge out of it. Therefore; we propose a schema-optional data repository that providesthe flexibility necessary to store and gradually integrate heterogeneous web data. Based onthis repository; we propose a semi-automatic schema enrichment approach that efficientlyaugments the data in a “pay-as-you-go” fashion. Due to the inherently appearingambiguities we further propose a crowd-based verification component that is able to resolvesuch conflicts in a scalable manner. Zusammenfassung Die stetig wachsende Zahl offenverfügbarer Webdaten findet momentan viel zu wenig oder gar keine Berücksichtigung in …,it-Information Technology,2012,*
A flexible graph-based data model supporting incremental schema design and evolution,Katrin Braunschweig; Maik Thiele; Wolfgang Lehner,Abstract Web data is characterized by a great structural diversity as well as frequentchanges; which poses a great challenge for web applications based on that data. We wantto address this problem by developing a schema-optional and flexible data model thatsupports the integration of heterogenous and volatile web data. Therefore; we want to relyon graph-based models that allow to incrementally extend the schema by variousinformation and constraints. Inspired by the on-going web 2.0 trend; we want users toparticipate in the design and management of the schema. By incrementally adding structuralinformation; users can enhance the schema to meet their very specific requirements.,International Conference on Web Engineering,2011,*
Deklarative Verarbeitung von Datenströmen in Sensornetzwerken.,Daniel Klan,Zusammenfassung Sensoren finden sich heutzutage in vielen Teilen des täglichen Lebens.Sie dienen dabei der Erfassung und Uberführung von physikalischen oder chemischenEigenschaften in digital auswertbare Größen. Drahtlose Sensornetzwerke als Mittel zurgroßflächigen; weitestgehend autarken Uberwachung von Regionen oder Gebäuden sindTeil dieser Brücke und halten immer stärker Einzug in den industriellen Einsatz. DieEntwicklung von geeigneten Systemen ist mit einer Vielzahl von Herausforderungenverbunden. Aktuelle Lösungen werden oftmals gezielt für eine spezielle Aufgabe entworfen;welche sich nur bedingt für den Einsatz in anderen Umgebungen eignen. Die sichwiederholende Neuentwicklung entsprechender verteilter Systeme sowohl aufHardwareebene als auch auf Softwareebene; zählt zu den wesentlichen Gründen …,*,2011,*
Proceedings of the VLDB Endowment Volume 4 Issue 5,José Blakeley; Joseph M Hellerstein; Nick Koudas; Wolfgang Lehner; Sunita Sarawagi; Uwe Röhm,@article{1952377; author = {Parameswaran; Aditya and Sarma; Anish Das and Garcia-Molina;Hector and Polyzotis; Neoklis and Widom; Jennifer}; title = {Human-assisted graph search:it's okay to ask questions}; journal = {Proc. VLDB Endow.}; volume = {4}; number = {5}; year ={2011}; issn = {2150-8097}; pages = {267--278}; publisher = {VLDB Endowment}; }@article{1952378; author = {Yakout; Mohamed and Elmagarmid; Ahmed K. and Neville;Jennifer and Ouzzani; Mourad and Ilyas; Ihab F.}; title = {Guided data repair}; journal = {Proc.VLDB Endow.}; volume = {4}; number = {5}; year = {2011}; issn = {2150-8097}; pages ={279--289}; publisher = {VLDB Endowment}; } @article{1952379; author = {Venetis; Petrosand Gonzalez; Hector and Jensen; Christian S. and Halevy; Alon}; title = {Hyper-local;directions-based ranking of places}; journal = {Proc …,*,2011,*
Sprachraumerstellung als Bestandteil der Geschäftsprozessmodellierung in SOA.,Franziska Gietl; Joachim Spilke; Dirk Habich; Wolfgang Lehner,Abstract: Für die Implementierung der annotationsbasierten Geschäftsmodellierung ist esnotwendig; die erforderlichen Bedingungen nach bestimmten Kriterien zu erstellen.Grundlage dafür ist ein Sprachraum; aus welchem die in den Bedingungssätzenenthaltenen Begriffe entnommen werden. Dieser Sprachraum besteht im Wesentlichen ausKonzepten und Klassen; die diese Konzepte repräsentieren; sowie Fakttypen. DieVorgehensweise der Erstellung eines Sprachraums für das Precision Dairy Farming istGegenstand dieses Artikels.,GIL Jahrestagung,2011,*
Large-Scale Data Analytics Using Ensemble Clustering,Martin Hahmann; Dirk Habich; Wolfgang Lehner,Abstract Data clustering is a highly used analysis technique in many application domains.From the end user's perspective; the wide variety of available algorithms and their technicalparameterization bring major difficulties in the determination of a user-satisfying clusteringresult. To overcome this issue in the context of large-scale analysis; we developed a novelfeedback-driven clustering process. Aside from presenting the theoretical concepts; we alsodescribe our developed infrastructure to efficiently handle the still increasing data volumes;within our process.,*,2011,*
Listen to the customer: model-driven database design,Hannes Voigt; Kai Herrmann; Tim Kiefer; Wolfgang Lehner,Abstract In modern IT landscapes; databases are subject to a major role change. Especiallyin Service-Oriented Architectures; databases are more and more frequently dedicated to asingle application. Therefore; it is even more important to reflect the applicationrequirements in their design. Software developers and application experts formulateapplication requirements in software models. Hence; we obviously need to bridge the gap tothe software world and directly derive a database design from the software models used inapplication development and maintenance. We introduce this concept as model-drivendatabase design. In this paper; we present the architecture principles of a model-drivendatabase design tool and details on the enumeration and evaluation of logical databasedesigns.,Proceedings of the Fourteenth International Database Engineering & Applications Symposium,2010,*
An XML-Based Streaming Concept for Business Process Execution,Steffen Preissler; Dirk Habich; Wolfgang Lehner,Abstract Service-oriented environments are central backbone of todays enterpriseworkflows. These workflow includes traditional process types like travel booking or orderprocessing as well as data-intensive integration processes like operational businessintelligence and data analytics. For the latter process types; current execution semantics andconcepts do not scale very well in terms of performance and resource consumption. In thispaper; we present a concept for data streaming in business processes that is inspired by thetypical execution semantics in data management environments. Therefore; we present aconceptual process and execution model that leverages the idea of stream-based serviceinvocation for a scalable and efficient process execution. In selected results of the evaluationwe show; that it outperforms the execution model of current process engines.,International Conference on Enterprise Information Systems,2010,*
One Clustering Process Fits All-A Visually Guided Ensemble Approach.,Martin Hahmann; Dirk Habich; Maik Thiele; Wolfgang Lehner,Abstract Looking back on the past decade of research on clustering algorithms; we witnesstwo major and apparent trends: 1) The already vast amount of existing clustering algorithms;is continuously broadened and 2) clustering algorithms in general; are becoming more andmore adapted to specific application domains with very particular assumptions. As a result;algorithms have grown complicated and/or very scenariodependent; which made clusteringa hardly accessible domain for non-expert users. This is an especially critical development;since; due to increasing data gathering; the need for analysis techniques like clusteringemerges in many application domains. In this paper; we oppose the current focus onspecialization; by proposing our vision of a usable; guided and universally applicableclustering process. In detail; we are going to describe our already conducted work and …,LWA,2010,*
Global Slope Change Synopses for Measurement Maps,Frank Rosenthal; Ulrike Fischer; Peter B Volk; Wolfgang Lehner,Quality control using scalar quality measures is standard practice in manufacturing.However; there are also quality measures that are determined at a large number of positionson a product; since the spatial distribution is important. We denote such a mapping of localcoordinates on the product to values of a measure as a measurement map. In this paper; weexamine how measurement maps can be clustered according to a novel notion of similarity—mapscape similarity—that considers the overall course of the measure on the map. Wepresent a class of synopses called global slope change that uses the profile of the measurealong several lines from a reference point to different points on the borders to represent ameasurement map. We conduct an evaluation of global slope change using a real-worlddata set from manufacturing and demonstrate its superiority over other synopses.,Data Mining; 2009. ICDM'09. Ninth IEEE International Conference on,2009,*
Design of the SAP Netweaver BWA Extensible Data Analytics Platform,F Färber; B Jäcksch; J Wöhler; G Radestock; W Lehner; Hasso Plattner; Alexander Zeier,ABSTRACT For the last decade; BI applications usually have been running in parallel tooperational applications. Now; however; we are faced with the challenge of a tightintegration of both worlds–decision making has become part of the operational activities;and therefore; all BI-related activities are expected to lie within the same applicationenvironment and based–as much as possible–on the same set of data. Within this extendedabstract; we want to outline SAP's strategy towards a data analytics platform that providesboth scalability (through the use of parallelism) and main-memory orientation as its coreprinciples as well as support for domain-specific applications by providing an easy-to-usebut tightly integrated extensibility framework of the query processor. Therefore; we firstsketch the overall architecture and then give some insight into the principles of providing …,*,2009,*
Anfragegetriebene Indizierung räumlicher Daten.,Hannes Voigt; Steffen Preißler; Matthias Böhm; Wolfgang Lehner,Abstract: Mit der zunehmenden Verbreitung von GPS-und internetfähigen Smartphoneswerden ortsbezogene Informationsdienste immer beliebter. Zur Sicherung einer hohenDienstqualität werden die zugrundeliegenden Ortsinformationen indiziert. BekannteIndexstrukturen für räumliche Daten teilen diese gemäß ihrer Verteilung auf; wodurch alleAnfragen gleich behandelt werden. Möchte man häufige Anfrage durch eine genauereIndizierung besonders unterstützen; so muss sich die Aufteilung der Daten nicht an derDatenverteilung; sondern an der Anfrageverteilung orientieren. In diesem Papier stellen wirdas QD-Grid vor; eine räumliche Indexstruktur; deren Indizierung sich inkrementell mit dengestellten Anfragen aufbaut. Zusätzlich präsentieren wir Evaluationsergebnisse.,GI Jahrestagung,2009,*
Pre analysis and clustering of uncertain data from manufacturing processes.,Peter Benjamin Volk; Martin Hahmann; Dirk Habich; Wolfgang Lehner,Abstract With increasing complexity of manufacturing processes; the volume of data that hasto be evaluated rises accordingly. The complexity and data volume make any kind of manualdata analysis infeasable. At this point; data mining techniques become interesting. Theapplication of current techniques is of complex nature because most of the data is capturedby sensor measurement tools. Therefore; every measured value contains a specific error. Inthis paper; we propose an erroraware extension of the density-based algorithm DBSCAN.Furthermore; we discuss some quality measures that could be utilized for furtherinterpretations of the determined clustering results. Additionally; we introduce the concept ofpre-analysis during a necessary data integration step for the proposed algorithm. With thisconcept; the runtime of the error-aware clustering algorithm can be optimized and the …,LWA,2008,*
Euro-Par 2006 Parallel Processing,Wolfgang E Nagel; Wolfgang V Walter; Wolfgang Lehner,*,*,2007,*
06431 Working Group Report on Managing and Integrating Data in P2P Databases,Peter A Boncz; Angela Bonifati; Arantza Illarramendi; Peter Janacik; Birgitta König-Ries; Wolfgang Lehner; Pedro Jose Marrón; Wolfgang May; Aris Ouksel; Kay Römer; Brahmananda Sapkota; Kai-Uwe Sattler; Heinz Schweppe; Rita Steinmetz; Can Türker,Abstract In this report; to our best recollection; we provide a summary of the working groupĆ¢ ā ‚¬ Ė Managing and Integrating Data in P2P DatabasesĆ¢ ā ‚¬ ā „¢ of the DagstuhlSeminar nr. 6431 on Ć¢ ā ‚¬ Ė Scalable Data Management in Evolving NeworksĆ¢ ā ‚¬ ā „¢;held on October 23-27 in Dagstuhl (Germany).,Dagstuhl Seminar Proceedings,2007,*
Shrinked Data Marts Enabled for Negative Caching,Maik Thiele; Wolfgang Lehner,Data marts storing pre-aggregated data; prepared for further roll-ups; play an essential rolein data warehouse environments and lead to significant performance gains in the queryevaluation. However; in order to ensure the completeness of query results on the data martwithout to access the underlying data warehouse; null values need to be stored explicitly;this process is denoted as negative caching. Such null values typically occur inmultidimensional data sets; which are naturally very sparse. To our knowledge; there is nowork on shrinking the null tuples in a multi-dimensional data set within ROLAP. For thesetuples; we propose a lossless compression technique; leading to a dramatic reduction insize of the data mart. Queries depending on null value information can be answered with100% precision by partially inflating the shrunken data mart. We complement our …,Database Engineering and Applications Symposium; 2006. IDEAS'06. 10th International,2006,*
Introduction,Patrick Valduriez; Wolfgang Lehner; Domenico Talia; Paul Watson,Abstract Managing and efficiently analysing the vast amounts of data produced by a hugevariety of data sources is one of the big challenges in computer science. The developmentand implementation of algorithms and applications that can extract information diamondsfrom these ultra-large; and often distributed; databases is a key challenge for the design offuture data management infrastructures. Today's data-intensive applications often suffer fromperformance problems and an inability to scale to high numbers of distributed data sources.Therefore; distributed and parallel databases have a key part to play in overcoming resourcebottlenecks; achieving guaranteed quality of service and providing system scalability. Theincreased availability of distributed architectures; clusters; Grids and P2P systems;supported by high performance networks and intelligent middleware provides parallel …,Proceedings of the 12th international conference on Parallel Processing,2006,*
Euro-Par 2006 Parallel Processing: 12th International Euro-Par Conference; Dresden; Germany; 28.08-01.09. 2006: Proceedings,Wolfgang Lehner; Wolfgang E Nagel; Wolfgang V Walter,*,*,2006,*
Euro-Par 2006: Parallel Processing Workshops: CoreGRID 2006; UNICORE Summit 2006; Petascale Computational Biology and Bioinformatics; Dresden; Germany;...,Wolfgang Lehner; Norbert Meyer; Achim Streit; Craig Stewart,No description available for this conference proceedings.,Conference proceedings Euro-Par,2006,*
DW2004–Data Warehousing und EAI,Robert Winter; Joachim Schelp; Dipl-Wirt-Inf Ronny Fischer; Jörg Becker; Peter Chamoni; Barry Devlin; Dublin Dr Barbara Dinter; Stefan Eicker; Thomas Fuhrer; Winterthur Versicherungen; Winterthur Prof Dr Roland Gabriel; PD Dr Peter Gluchowski; Holger Günzel; PD Dr Roland Holten; M Prof Dr Reinhard Jung; Hans-Georg Kemper; Hermann Krallmann; Wolfgang Lehner; Peter Loos; Erhard Rahm; Ing Bodo Rieger; Michael Rohloff; Michael Rosemann; Elmar J Sinz; Bernhard Thalheim,Autorenverzeichnis Dipl.-Ing. Stephan Aier Technische Universität Berlin EAI Competence CenterSekr. FR 6-7 Franklinstr. 28/29 D-10587 Berlin E-Mail: stephan. aier@ tu-berlin. de URL:http://www. sysedv. tu-berlin. de Dr. Carsten Bange Business Application Research Center(BARC) Wiesenweg 31 D-97082 Würzburg E-Mail: bange@ barc. de URL: http://www. barc. deDr. Andreas Bauer T-Systems Äußere Bayreuther Str. 100 D-90491 Nürnberg E-Mail:andreas. bauer@ t-systems. com URL: http://www. t-systems. com Prof. Dr. Jörg Becker EuropeanResearch Center for Information Systems (ERCIS) Wirtschaftsinformatik und InformationsmanagementLeonardo-Campus 3 D-48149 Münster E-Mail: becker@ ercis. de URL: http://www. ercis. deDipl.-Inf.-Wiss. Dipl.-Betriebswirt (FH) Joachim J … 504 Autorenverzeichnis Thomas EckertBundesamt für Migration und Flüchtlinge Frankenstraße 210 D-90461 Nürnberg E-Mail …,Auf dem Weg zur Integration Factory: Proceedings der DW2004-Data Warehousing und EAI,2005,*
Hierarchisches gruppenbasiertes Sampling,Rainer Gemulla; Henrike Berthold; Wolfgang Lehner,Abstract In Zeiten wachsender Datenbankgrößen ist es unumgänglich; Anfragennäherungsweise auszuwerten um schnelle Antworten zu erhalten. Dieser Artikel stelltverschiedene Methoden vor; dieses Ziel zu erreichen; und wendet sich anschließend demSampling zu; welches mit Hilfe einer Stichprobe schnell zu adäquaten Ergebnissen führt.Enthalten Datenbankanfragen Verbund-oder Gruppierungsoperationen; so sinkt dieGenauigkeit vieler Sampling-Verfahren sehr stark; insbesondere werden vor allem kleineGruppen nicht erkannt. Dieser Artikel befasst sich mit hierarchischen gruppenbasiertemSampling; welches Sampling; Gruppierung und Verbundoperationen kombiniert.,Informatik-Forschung und Entwicklung,2005,*
SPEAKY,Wolfgang Bergthaler; Roland Galler; Robert Strohmaier,Es gibt eine Unzahl an Literatur bezüglich Vortragstechnik und Rhetorik; doch es bietet sichnur schwer die Möglichkeit für einen Redner; sich selbst zu analysieren und damitAuffälligkeiten zu entdecken. SPEAKY ist ein Software-Tool; das die Analyse einesVortragenden anhand von objektiv messbaren Daten wie Position; Bewegung; Lautstärke;Redepausen; Folienanzahl und Wörter per Folie ermöglicht. Es sammelt während einerPräsentation die entsprechenden Daten; filtert sie und bereitet sie so auf; dass aufKnopfdruck ein aufgezeichneter Vortrag analysiert werden kann. Das Tool stellt primär einHilfsmittel für einen Vortragenden dar; sich selbst zu bewerten. Anhand der visualisiertenDaten werden Auffälligkeiten gezielt angesteuert.,Informatik-Forschung und Entwicklung,2005,*
04441 Working Group--Research Issues in Mobile Querying,Martin Breunig; Christian S Jensen,Abstract This document reports on key aspects of the discussions conducted within theworking group. In particular; the document aims to offer a structured and somewhat digestedsummary of the group's discussions. The document first offers concepts that enablecharacterization of" mobile queries''as well as the types of systems that enable such queries.It explores the notion of context in mobile queries. The document ends with a fewobservations; mainly regarding challenges.,Dagstuhl Seminar Proceedings,2005,*
Wenn Ausnahmen zum Normalfall werden: Modellierung struktureller Ausnahmen in Datenbanken,Wolfgang LEHNER; Sven SCHMIDT; Daniel SCHALLER,*,Wissenschaftliche Zeitschrift der Technischen Universität Dresden,2004,*
Research Issues in Mobile Querying,M Breunig; Christian Søndergaard Jensen; M Klein; A Zeitz; G Koloniari; J Grünbauer; PJ Marrón; C Panieyiotoa; S Boll; Simonas Saltenis; K-U Sattler; M Hauswirth; W Lehner; O Wolfson,Abstract: This document reports on key aspects of the discussions conducted within theworking group. In particular; the document aims to offer a structured and somewhat digestedsummary of the group's discussions. The document first offers concepts that enablecharacterization of" mobile queries" as well as the types of systems that enable suchqueries. It explores the notion of context in mobile queries. The document ends with a fewobservations; mainly regarding challenges.,Dagstuhl Seminar Proceedings,2004,*
Verknüpfung schwach-konsistenter Datenbankzustände in Grid-organisierten Rechnerstrukturen.,Wolfgang Lehner; Lutz Schlesinger; Verena Sauerborn,*,Datenbank-Spektrum,2004,*
Feingranulare Verarbeitung von XML-Strömen.,Sven Schmidt; Dirk Habich; Wolfgang Lehner,Abstract: Ausgehend von einer Vielzahl von Quellen haben Daten oft einen transientenCharakter und werden in Form von Datenströmen disseminiert. Zur adäquaten Verarbeitungexistieren sogenannte Datenstrom-Managementsysteme (DSMS); die in der Lage sind; instrombasierter Art und Weise vom Benutzer spezifizierte Anfragen bzgl. der Datenströmeauszuwerten und die Ergebnisse kontinuierlich auszugeben. In diesem Beitrag wird gezeigt;dass anspruchsvolle Verarbeitungsoperationen auf hierarchisch strukturiertenDatenströmen realisiert werden können. Dabei wird die Verwendung von XML im Kontextder Datenstrom-Managementsysteme motiviert und ein angepasstes Verarbeitungsmodellbasierend auf XML-Creeks skizziert.,GI Jahrestagung (1),2004,*
Data Mining und Data Warehousing.,DA Keim; W Lehner,DA Keim and W. Lehner. Data Mining und Data Warehousing. Datenbank-Spektrum;dpunkt.verlag; 9():5; 2004 … DA Keim and N. Koudas. Introduction to special issue with bestpapers from KDD 2002. Information Systems; Elsevier Science; 29(4):271-272; 2004 … DAKeim; M. Heczko; A. Hinneburg and M. Wawryniuk:. Multi-Resolution Similarity Search in ImageDatabases. ACM/Springer Multimedia Systems Journal; 10(1):28-40; 2004 … DA Keim; T. Munznerand SC North. Guest Editor's Introduction: Special Section on InfoVis. IEEE Transactions on Visualizationand Computer Graphics (TVCG); 10(4):446; 2004 … DA Keim and T. Ertl. WissenschaftlicheVisualisierung. it - Information Technology; Oldenbourg; 46(3):109-110; 2004 … T. Ertl andDA Keim. Wissenschaftliche Visualisierung - Ausgewählte Forschungsprojekte. it - InformationTechnology; Oldenbourg; 46(3):148-153; 2004 … DA Keim; SC North and C. Panse …,IEEE Transactions on Visualization and Computer Graphics (TVCG),2004,*
Nutzung von Datenbankdiensten in Data-Warehouse-Anwendungen (Connecting Data Warehouse Applications with Database Services),Lutz Schlesinger; Wolfgang Lehner; Wolfgang Hümmer; Andreas Bauer,Zusammenfassung Zentral für eine effiziente Analyse der in Data-Warehouse-Systemengespeicherten Daten ist das Zusammenspiel zwischen Anwendung und Datenbanksystem.Der vorliegende Artikel klassifiziert und diskutiert unterschiedliche Wege; Data-Warehouse-Anwendungen mit dem Datenbanksystem zu koppeln; um komplexe OLAP-Szenarien zurBerechnung dem Datenbankdienst zu überlassen. Dabei werden vier unterschiedlicheKategorien; die Spracherweiterung (SQL); die anwendungsspezifischeSprachneuentwicklung (MDX); die Nutzung spezifischer Objektmodelle (JOLAP) undschließlich der Rückgriff auf XML-basierte WebServices (XCube) im einzelnen diskutiert undvergleichend gegenübergestellt. Summary The connection of the applications and theunderlying database system is crucial for performing analyses efficiently within a data …,it-Information Technology,2003,*
Entwurf und Betrieb von Data-Warehouse-Systemen,Wolfgang Lehner,Data-Warehouse-Systeme haben sich in den vergangenen Jahren als eine zweite Säuleder Informationsverarbeitung in Organisationen etabliert. Neben den klassischentransaktional ausgerichteten Systemen adressieren Data-Warehouse-Systeme imWesentlichen die analytisch orientierte Auswertung unterschiedlichster Datenbestände ineiner Organisation; wobei (meist relational organisierte) Datenbanksysteme densystemtechnischen Kern eines derartigen Informationssystems dabei bilden. Bevor jedochüber Herausforderungen und unterschiedliche Technologien im Umfeld der Data-Warehouse-Systeme berichtet wird; ist eine begriffliche Eingrenzung derartigerInformationssysteme angebracht. Im Kontext dieses Themenheftes wird ein Data-Warehouse-System dabei als eine Sammlung von Systemkomponenten und …,it-Information Technology,2003,*
Schwerpunktthema: Entwurf und Betrieb von Data-Warehouse-Systemen,Wolfgang Lehner,*,*,2003,*
Consistency BASED Snapshot MANAGEMENT IN Data Grids,Lutz Schlesinger; Wolfgang Lehner,Abstract Almost over the last 20 years grid technology has been developed to exploitunutilized computing capacity around the world. The two major application areas are solvingcomputing intensive problems having less data (computational grid) or operating on largevolumes of data (data grid). From a database perspective it is advisable to replicate datasets at different nodes in the grid before executing a single query. The query processorassigns a query to those nodes; which do not operate at full capacity and which have storedthe appropriate replica. As the replica may be outdated usually a synchronizationmechanism is necessary. This may be extremely expensive if full consistency betweenoriginal data and replica is required. To avoid synchronization we follow the approach tostore multiple versions of static snapshots. New versions of local data sets are distributed …,tc,2003,*
On the problem of generating common predecessors,Wolfgang Lehner; Wolfgang Hümmer; Lutz Schlesinger; Andreas Bauer,Abstract Using common subexpressions to speed up a set of queries is a well known andlong studied problem. However; due to the isolation requirement; operating a database inthe classic transactional way does not offer many applications to exploit the benefits ofsimultaneously computing a set of queries. In the opposite; many applications can beidentified in the context of data warehousing; eg optimizing the incremental maintenanceprocess of multiple dependent materialized views or the generation of application specificdata marts. In the paper we discuss the problem whether it is always advisable to generatethe most complete common predecessor for a given set of queries or to restrict apredecessor to a subset of all possible base tables. As we will see; this question cannot beanswered without having knowledge about the cardinality of queries after aggregation …,Proceedings of the 5th ACM international workshop on Data Warehousing and OLAP,2002,*
Data warehousing: warehouse administration,Wolfgang Lehner,Abstract Running an organizational-wide reliable data warehouse requires a hugeadministration effort. This article outlines data warehouse-specific administrational issuesfrom the two perspectives of administration roles and administration tasks. The roles of adata administrator; application administrator; and systems administrator are identified andoutlined in the first part. Subsequently; different administrational tasks; such as activities fromdata acquisition to publication; are described and assigned to one or more administrationalroles. This article stresses the fact that the task of data administration in the context of a datawarehouse is of high complexity and embraces a mixture of technical system administrationtasks and of administrational tasks in new areas; such as quality assurance and security.,Handbook of data mining and knowledge discovery,2002,*
Modeling of Census Data in a Multidimensional Environment Holger Giinzel1; Wolfgang Lehner1; Stein Eriksen2; Jon Folkedal2'Department of Database Systems;...,Holger Giinzel; Wolfgang Lehner; Stein Eriksen; Jon Folkedal,*,Advances in Databases and Information Systems: Second East European Symposium; ADBIS'98; Poznan; Poland; September 7-10; 1998; Proceedings,1998,*
Modeling of census data in a multidimensional environment,Holger Günzel; Wolfgang Lehner; Stein Eriksen; Jon Folkedal,Abstract The general aim of the KOSTRA project; initiated by Statistics Norway; is to set up adata reporting chain from the norwegian municipalities to a central database at StatisticsNorway. In this paper; we present an innovative data model for supporting a data analysisprocess consisting of two sequential data production phases using two conceptionaldatabase schemes. A first data schema must provide a sound basis for an efficient analysisreflecting a multidimensional view on data. Another schema must cover all structuralinformation; which is essential for supporting the generation of electronic forms as well as forperforming consistency checks of the gathered in-formation. The resulting modelingapproach provides a seamless solution for both proposed challenges. Based on therelational model; both schemes are powerful to cover the heterogeneity of the data source …,East European Symposium on Advances in Databases and Information Systems,1998,*
Part I: Databases and Information Systems Integration,Luis Fernando Orleans; Geraldo Zimbräo; Luiz Andre P Paes Lerne; Marco A Casanova; Karin K Breitman; Antonio L Furtado; Louis Raymond; Anne-Marie Croteau; Francois Bergeron; Matthias Boehm; Dirk Habich; Steffen Preissler; Wolfgang Lehner; Uwe Wloka; Jürgen Müller; Jens Krüger; Sebastian Enderlein; Marco Helmich; Alexander Zeier; Sascha Hunold; Björn Krellner; Thomas Rauber; Thomas Reichel; Gudula Rünger; Angelo Di Iorio; Michele Schirinzi; Fabio Vitali; Carlo Marchetti; Jose Paulo Leal; Ricardo Queirös; David Morgan; Jai W Kang; James M Kang,Page 1. Table of Contents Part I: Databases and Information Systems Integration MIDAS: AMiddleware for Information Systems with QoS Concerns .... 3 Luis Fernando Orleans andGeraldo Zimbräo Instance-Based OWL Schema Matching 14 Luiz Andre P. Paes Lerne; MarcoA. Casanova; Karin K. Breitman; and Antonio L. Furtado The Integrative Role of IT in Productand Process Innovation: Growth and Productivity Outcomes for Manufacturing 27 LouisRaymond; Anne-Marie Croteau; and Francois Bergeron Vectorizing Instance-Based IntegrationProcesses 40 Matthias Boehm; Dirk Habich; Steffen Preissler; Wolfgang Lehner; and UweWloka Invisible Deployment of Integration Processes 53 Matthias Boehm; Dirk Habich;Wolfgang Lehner; and Uwe Wloka Customizing Enterprise Software as a Service Applications:Back-End Extension in a Multi-tenancy Environment 66 …,*,*,*
Metamodeling Lightweight Data Compression Algorithms and its Application Scenarios,Juliana Hildebrandt; Dirk Habich; Thomas Kühn; Patrick Damme; Wolfgang Lehner,Abstract. Lossless lightweight data compression is a very important optimization techniquein various application domains like database systems; information retrieval or machinelearning. Despite this importance; currently; there exists no comprehensive and non-technical abstraction. To overcome this issue; we have developed a systematic approachusing metamodeling that focuses on the non-technical concepts of these algorithms. In thispaper; we describe COLLATE; the metamodel we developed; and show that each algorithmcan be described as a model conforming with COLLATE. Furthermore; we use COLLATE tospecify a compression algorithm language COALA; so that lightweight data compressionalgorithms can be specified and modified in a descriptive and abstract way. Additionally; wepresent an approach to transform such descriptive algorithms into executable code. As …,*,*,*
Entwicklung und Analyse einer Systembeschreibungssprache für Datenbank-Systeme auf Mehrkernarchitekturen,Dipl-Inf Tomas Karnagel; Wolfgang Lehner,In dieser Studienarbeit soll eine Systembeschreibungssprache für eine Mehrkernarchitekturuntersucht werden. Hierbei wird sowohl das Laden der Daten als auch die eigentlicheBerechnung betrachtet. Im ersten Schritt werden mehrere ausgewählte DatenbankOperatoren analysiert. Diese werden im nächsten Schritt durch eine neu zu entwickelndeDatenbank-Beschreibungssprache spezifiziert. Hierbei wird auf die Limitierung derArchitektur geachtet; zum Bsp. durch die Prozessoren und deren Abarbeitung auf lokalemon-Chip Speicher. Eine Beschleunigung ist z. B. durch eine Parallelisierung als auch durchdas explizite Vorladen von Daten möglich. Für eine Parallelisierung stehen mehrereMöglichkeiten zur Verfügung. Im abschließenden Teil der Studienarbeit erfolgt eineZusammensetzung mehrere Datenbank Operatoren zu einer komplexen Anfrage.,*,*,*
Efficient Online Analytical Processing,J Albrecht; A Bauer; O Deyerling; H Günzel; W Hümmer; W Lehner; L Schlesinger,*,*,*,*
Johann-Christoph Freytag; Rico Bergmann; and Lukas Dölle,Morten Middelfart; Badrish Chandramouli; Qiming Chen; Meichun Hsu; Malu Castellanos; Yannis Sotiropoulos; Damianos Chatziantoniou; Adriana Matei; Kuo-Ming Chao; Nick Godwin; Oliver Kennedy; Ying Yang; Jan Chomicki; Ronny Fehling; Zhen Hua Liu; Dieter Gawlick; Dilshod Ibragimov; Katja Hose; Torben Bach Pedersen; Esteban Zimányi; Ahmed Khan Leghari; Martin Wolf; Yongluan Zhou; Masahiro Oke; Hideyuki Kawashima; Michael Rudolf; Hannes Voigt; Christof Bornhövd; Wolfgang Lehner; Karsten Schmidt; Sebastian Bächle; Philipp Scholl; Georg Nold,Query Adaptation and Privacy for Real-Time Business Intelligence (Keynote Paper) . . . . . . . .… Johann-Christoph Freytag; Rico Bergmann; and LukasDölle … The Inverted Data Warehouse Based on TARGIT Xbone: How the Biggest of Data CanBe Mined by “The Little Guy” (Invited Paper) … Building Engines and Platformsfor the Big Data Age (Invited Paper) … Backtrack-Based and Window-Oriented OptimisticFailure Recovery in Distributed Stream Processing (Invited Paper) …Qiming Chen; Meichun Hsu; and Malu Castellanos … LinkViews: An Integration Frameworkfor Relational and Stream Systems … Adriana Matei; Kuo-Ming Chao; and Nick Godwin …Oliver Kennedy; Ying Yang; Jan Chomicki; Ronny Fehling; Zhen Hua Liu; and Dieter Gawlick… Towards Exploratory OLAP Over Linked Open Data – A Case Study. . . . . . 114 …,*,*,*
Datenbanksysteme in Business; Technologie und Web (BTW),Johann-Christoph Freytag; Thomas Ruf; Wolfgang Lehner; Gottfried Vossen,Alle zwei Jahre findet die BTW-Konferenz der Gesellschaft für Informatik (GI) als dienationale Datenbankkonferenz an einem ausgezeichneten Ort in Deutschland statt. So istdie Westfälischen-Wilhelms Universität in Münster vom 4. bis 6. März 2009 Gastgeberin die13. BTW-Konferenz. Münster mit seiner mehr als 1200-jährigen Geschichte im ZentrumWestfalens gilt vielen als Ort der Tradition und der Kultur; hier wurde 1648 der WestfälischeFrieden geschlossen; der das Ende des 30-jährigen Krieges in Europa markierte. Heutesind die Stadt; traditionell Sitz von Bildungs-und Verwaltungseinrichtungen sowieFinanzdienstleistern; und das umliegende Münsterland eine Region mit wachsenderWirtschafts-und Innovationskraft; die sie zum großen Teil der hiesigen Universität und ihrenangegliederten Instituten verdankt. Die BTW-Tagung ist seit über 20 Jahren das zentrale …,*,*,*
ADBIS 2009,Matthias Boehm; Dirk Habich; Steffen Preissler1 Matthias Boehm; Dirk Habich Steffen Preissler; Wolfgang Lehner; Uwe Wloka,Page 1. ADBIS 2009 Cost-Based Vectorization of Instance-Based Integration Processes MatthiasBoehm1;2 Dirk Habich1 Steffen Preissler1 Matthias Boehm ; Dirk Habich Steffen PreisslerWolfgang Lehner1 Uwe Wloka2 1 TU Dresden; Database Technology Group; Germany 2 HTWDresden; Database Group; Germany Page 2. Problem Description • Context – Integrationprocesses Receive o1 SAP R/3 Integration processes • Workflow-based integration tasks • Dataexchange between heterogeneous systems and applications 1 Translation o2 SAP R/3 systemsand applications – Eg; EAI servers; MOM; WfMS – Typically; instance-based process executionInvoke o3 JDBC • Problem – Efficiency of the integration systems has high impact C it fdtbt dit ib td t • Consistency of data between distributed systems • Performance of the source systems(synchronous execution) → Optimization strongly needed …,*,*,*
BOOK-CHAPTERS,HaHL11 Martin Hahmann; Dirk Habich; Wolfgang Lehner; KHSL13 Tomas Karnagel; Benjamin Schlegel; MKHL13 Lukas M Maas; Thomas Kissinger,Hab08 Dirk Habich: Komplexe Datenanalyseprozesse in serviceorientierten Umgebungen.(PhD-Thesis); Technische Universität Dresden; Fakultät Informatik; Institut fürSystemarchitektur; Lehrstuhl Datenbanken … ThHa07 Maik Thiele; Dirk Habich: Orchestrierungdatenintensiver Prozes- se - Einsatz von BPEL in der Genexpressionsanalyse.; VDM Ver- lagDr. Müller; Saarbrücken; Germany ISBN: 978-3-8364-0239-2; 2007 … HaHL11 MartinHahmann; Dirk Habich; Wolfgang Lehner: Large-Scale Data Analytics using EnsembleClustering; In: Handbook of Data Intensive Computing; Editors: Borko Furht and ArmandoEscalante; ISBN 978-1- 4614-1415-5; pages 285-322; 2011 … KHSL13 Tomas Karnagel; DirkHabich; Benjamin Schlegel; Wolfgang Lehner: The HELLS-join: a heterogeneous stream joinfor extremely large win- dows; In: Proceedings of the Ninth International Workshop on …,*,*,*
2006 10th International Database Engineering and Applications Symposium,A Chakraborty; MK Garg; AK Majumdar; S Sural; Dimitrios Katsaros; Nikos Dimokas; Yannis Manolopoulos,In this paper; we consider the problem of recovery from committed malicious transactions indistributed databases. We define several useful dependency relations among transactionsand based on them present an online recovery scheme for restoring the consistency of adatabase,*,*,*
Next-Generation Hardware for Data Management-more a Blessing than a Curse?,Wolfgang Lehner,ABSTRACT Recent hardware developments have touched almost all components of acomputing system: the existence of many and potentially heterogeneous cores; theavailability of volatile and non-volatile main memories with an ever growing capacity; andthe emergence of economically affordable; highspeed/low-latency interconnects are only afew prominent examples. Every single development as well as their combination has amassive impact on the design of modern computing systems. However; it is still an openquestion; if; how; and at which level of detail; a database system has to explicitly be aware ofthose developments and exploit them using specifically designed algorithms and datastructures. Within the talk I will try to give an answer to this question and argue for a clearroadmap of HW/SW-DB-CoDesign especially providing an outlook to upcoming …,Grundlagen von Datenbanken,*,*
STRUKTURELLE UND OPERATIONELLE MODELLIERUNGASPEKTE IN PUBSCRIBE,M Redert; W Lehner; W Hümmer,Kurzfassung Wesentlich für den Bereich des' Personalized Information Delivery'ist dieEigenschaft; dass Informationen in Form von Nachrichten sequentiell in einSubskriptionssystem eingehen; dort verarbeitet und wiederum sequentiell dem Benutzerzugestellt werden. Diese Eigenschaft schlägt sich ebenfalls auf Modellierungsebene imPubScribe-System nieder. So wird im Rahmen dieses Beitrags der Modellierungsaspekt ausstruktureller und operationeller Perspektive eruiert. Die strukturelle Einheit bildet dabei dieDatenstruktur einer" Queue". Basierend auf diesen Einheiten werden in diesem Beitrag eineVielzahl von unären und binären Operatoren definiert; die im Kontext dieser Datenstrukturabgeschlossen sind. Neben der fundamental wichtigen Filteroperation finden sich Skalar-;Verbund-und Vereinigungsoperatoren. Als zentrale Erweiterung gegenüber anderen …,*,*,*
System ubergreifende Kostennormalisierung f ur Integrationsprozesse,Matthias Böhm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Zusammenfassung Auf Grundlage der Vielzahl proprietärer Integrationssysteme istzunehmend die Entwicklung von Ansätzen zur modellbasierten Generierung vonIntegrationsprozessen zu beobachten. Eine derartige Generierung bietet weiterhin dieMöglichkeit der Auswahl des optimalen Integrationssystems; welche ein hohesOptimierungspotenzial in sich birgt. Die Grundlage für eine solche Entscheidung ist jedocheine integrationssystemübergreifende Kostennormalisierung; um die Vergleichbarkeit vonVerarbeitungsstatistiken zu ermöglichen. Basierend auf einem plattformunabhängigenKostenmodell und der systemübergreifenden Kostennormalisierung kann einekostenbasierte Optimalitätsentscheidung hinsichtlich des effizientesten Integrationssystemsgetroffen werden. Folglich können hierbei veränderliche Workload-Charakteristika in die …,Gesellschaft für Informatik (GI) publishes this series in order to make available to a broad public recent findings in informatics (ie computer science and informa-tion systems); to document conferences that are organized in co-operation with GI and to publish the annual GI Award dissertation.,*,*
ERIS: A NUMA-AWARE IN-MEMORY STORAGE ENGINE FOR TERA-SCALE ANALYTICAL WORKLOAD,Benjamin Schlegel; Dirk Habich; Daniel Molka; Wolfgang Lehner,Page 1. © Prof. Dr.-Ing. Wolfgang Lehner | Thomas Kissinger 2014/09/01 Tim Kiefer ADMS 2014Benjamin Schlegel Hangzhou; China Dirk Habich Daniel Molka Wolfgang Lehner ERIS: ANUMA-AWARE IN-MEMORY STORAGE ENGINE FOR TERA-SCALE ANALYTICAL WORKLOADPage 2. | 2 Motivation ERIS: A NUMA-Aware In-Memory Storage Engine for Tera-Scale AnalyticalWorkload Databases in the many-core era 0 0.5 1 1.5 2 0 0.5 1 1.5 2 2.5 3 3.5 0 64 128 192256 320 384 448 Scan Th rough p u t [TiB/s] Loo ku p Thro u gh p u t [Billio n /s] #Cores SharedLookup ERIS Lookup Shared Scan ERIS Scan Page 3. | 3 NUMA Systems ERIS: A NUMA-AwareIn-Memory Storage Engine for Tera-Scale Analytical Workload 85 196 16.4 1.8 0 50 100 150200 250 local remote 0 5 10 15 20 latency (ns) bandwidth (GB/s) AMD 8 nodes 64 cores 64GBs max 2 hops Page 4. | 4 NUMA Systems …,*,*,*
THE REVOLUTION AHEAD: PUBLISH/SUBSCRIBE MEETS DATABASE SYSTEMS,Wolfgang Lehner; Wolfgang Hümmer,Abstract The traditional way of user interaction with a database system follows theclassical'request/response'query paradigm where the user is issuing a query and retrievingthe result as fast as possible. The primary issues are efficiency and consistency. Driven byhuge numbers of concurrent users accessing large data sets especially in the context ofData Warehouse Systems; the'request/response'paradigm looks no longer feasible. There iscommonly no doubt that the novel query paradigm of'publish/subscribe'may be one way todiminish the before mentioned problems. In the'publish/subscribe'context; a user registers asubscription (also called a standing query) once at a subscription management system andperiodically or aperiodically receives notifications; ie the result of the query with regard to thecurrently valid state of the underlying database. In this paper; we give an overview of the' …,*,*,*
GI-Edition,Gunter Saake; Andreas Henrich; Wolfgang Lehner; Thomas Neumann; Veit Köppen,In den letzten Jahren hat es auf dem Gebiet des Datenmanagements große Veränderungengegeben. Dabei muss sich die Datenbankforschungsgemeinschaft insbesondere denHerausforderungen von „Big Data “stellen; welches die Analyse von riesigen Datenmengenunterschiedlicher Struktur mit kurzen Antwortzeiten erfordert. Neben klassisch strukturiertenDaten müssen moderne Datenbanksysteme und Anwendungen ebenfalls semistrukturierte;textuelle und andere multi-modale Daten sowie Datenströme in völlig neuenGrößenordnungen verwalten. Gleichzeitig müssen die Verarbeitungssysteme die Korrektheitund Konsistenz der Daten sicherstellen.,*,*,*
Demo Program Committee,Sihem Amer-Yahia; Arvind Arasu; Sunil Arvindam; Magdalena Balazinska; Fabio Casati; Malu Castellanos; Mariano Cilia; Brian F Cooper; Adina Crainiceanu; Abhinandan Das; Alin Dobra; Pablo Guerrero; Christian Konig; Georgia Koutrika; Wolfgang Lehner; Feifei Li; Ashwin Machanavajjhala; Thomas Neumann; Dan Olteanu; Carlos Ordonez; Peter Pietzuch; Adam Silberstein; Alkis Simitsis,Sihem Amer-Yahia; Qatar Computing Research Institute Arvind Arasu; Microsoft Research SunilArvindam; SAP Research; India Magdalena Balazinska; University of Washington FabioCasati; University of Trento; Italy Malu Castellanos; HP Labs; USA Mariano Cilia; IntelCorporation; Argentina Brian F Cooper; Google Adina Crainiceanu; US Naval Academy AbhinandanDas; Google Alin Dobra; University of Florida Javier Garcia-Garcia; UNAM University; MexicoPablo Guerrero; TU Darmstadt; Germany Melanie Herschel; Tubingen University ChristianKonig; Microsoft Research Georgia Koutrika; IBM Almaden Research Center WolfgangLehner; TU Dresden; Germany Feifei Li; Florida State University Ashwin Machanavajjhala; YahooResearch Thomas Neumann; TU Munchen Dan Olteanu; University of Oxford CarlosOrdonez; University of Houston Peter Pietzuch; Imperial College London Lin Qiao; IBM …,*,*,*
Message Indexing for Document-Oriented Integration Processes,Dirk Habich; Wolfgang Lehner,*,*,*,*
TECHNISCHE BERICHTE,Philipp Große; Norman May; Wolfgang Lehner,ABSTRACT Large-scale data analysis relies on custom code both for preparing the data foranalysis as well as for the core analysis algorithms. The map-reduce framework offers asimple model to parallelize custom code; but it does not integrate well with relationaldatabases. Likewise; the literature on optimizing queries in relational databases has largelyignored user-defined functions (UDFs). In this paper; we discuss annotations for user-defined functions that facilitate optimizations that both consider relational operators andUDFs. We believe this to be the superior approach compared to just linking map-reduceevaluation to a relational database because it enables a broader range of optimizations. Inthis paper we focus on optimizations that enable the parallel execution of relationaloperators and UDFs for a number of typical patterns. A study on real-world data …,*,*,*
GignoMDA-MDA Approach for Applications in the Database Domain,Sebastian Richly; Dirk Habich; Wolfgang Lehner,Abstract Database Systems are often used as persistent layer for applications. This impliesthat database schemas are generated out of transient programming class descriptions. Thebasic idea of the MDA approach generalizes this principle by providing a framework togenerate program code (and database schemas) for different programming platforms. Withinour GignoMDA-project [1]; we extended the classic concept of the MDA. That means; ourapproach provides a single point of information describing all aspects of databaseapplications (eg database schema; user interfaces; business logic and projectdocumentation;...) with a great potential of cross-layer optimization. In addition to the fullautomatic generation of an complete multi-tier database applications we implemented in theGignoMDA-project; our new cross-layer optimization hints are a novel way for the …,*,*,*
ABBILDUNG RELATIONALER DATENBANKEN AUF DAS UNIX-DATEISYSTEM,Jens Albrecht; Wolfgang Lehner,Dennoch erfolgt der Datenaustausch zwischen verschiedenen Datenbanksystemen beikleineren Warehouse-Szenarien sowie bei unzähligen ähnlichen Anwendungen übereinfache Textdateien; die eine festgelegte Struktur haben. Jedes kommerzielleDatenbanksystem bietet die Möglichkeit; Textdateien zu importieren und zu exportieren.Einige Hersteller haben für diesen Zweck spezielle Werkzeuge (zB der SQL* Loader vonOracle [Orac95]); die mit Hilfe von Skripten den Datenaustausch halbautomatischdurchführen können. Dennoch muß oft sehr viel Zeit investiert werden; um Informationenaus einem Datenbanksystem in ein anderes zu übertragen. Die Metainformationen zu denRelationen; wie Datentypen und Constraints; gehen bei der Übertragung in der Regelverloren und müssen von Hand nachgetragen werden; sofern nicht durch das …,*,*,*
Hybrid Database Architectures Using the Example of the New SAP In-Memory Technology,Franz Färber; Bernhard Jäcksch; Christian Lemke; Philipp Große; Wolfgang Lehner,*,*,*,*
Query Processing on Prefix Trees Revisited,Thomas Kissinger; Matthias Boehm; Patrick Lehmann; Wolfgang Lehner,Abstract There is a trend towards operational or Live BI (Business Intelligence) that requiresimmediate synchronization between the operational source systems and the datawarehouse infrastructure in order to achieve high up-to-dateness for analytical query results.The high performance requirements imposed by many ad-hoc queries are typicallyaddressed with read-optimized column stores and in-memory data management. However;Live BI additionally requires transactional and update-friendly in-memory indexing due tohigh update rates of propagated data. For example; in-memory indexing with prefix treesexhibits a well-balanced read/write performance because no index reorganization isrequired. The vision of this project is to use the underlying in-memory index structures; in theform of prefix trees; for query processing as well. Prefix trees are used as intermediate …,*,*,*
Derivability of OLAP Function Sequences in Relational Database Systems,Wolfgang Lehner; Lutz Schlesinger,Abstract: OLAP functions are a novel technique to specify sequence oriented queries inSQL. Based on the global grouping mechanism; OLAP functions provide a columnbasedordering; partitioning; and windowing functionality. In this paper we are investigating theissue on how derivability of sliding window queries from materialized views exhibiting OLAPfunctions may be achieved. Materialized views reflect a core technology to speed upaggregation queries mostly used in a data warehouse environment.,*,*,*
PUBSCRIBE MIKRO-UND MAKROARCHITEKTUR,M Redert; C Reinhard; W Lehner; W Hümmer,Kurzfassung In dem folgenden Beitrag wird die Architektur des PubScribe-Systems aus zweiunterschiedlichen Perspektiven beleuchtet. Auf der einen Seite wird die Makroarchitekturvorgestellt; die sich aus einem Verbund von Vermittlungskomponenten mitunterschiedlichen Strategien der verteilten Abarbeitung einzelnerSubskriptionsanforderungen ergibt. Auf der anderen Seite wird die Architekturmikroskopisch untersucht; Es wird gezeigt; aus welchen Modulen sich jeweils eine einzelneVermittlungskomponente zusammensetzt. Aus operationeller Sicht wird dabei insbesonderedie Tatsache des Rollenmodells herausgearbeitet.,*,*,*
PUBLISH/SUBSCRIBE-SYSTEME IM DATA-WAREHOUSING: MEHR ALS NUR EINE RENAISSANCE DER BATCH-VERARBEITUNG,W Lehner; W Hümmer; M Redert; C Reinhard,Kurzfassung Der Erfolg von Data-Warehouse-Systemen weckt neue Anforderungen; die inkünftigen Data-Warehouse-Architekturen berücksichtigt werden müssen. Dieses Papiermotiviert die Erweiterung der klassischen Data-Warehouse-Architektur um eine'Publish/Subscribe'-Komponente; um eine nachrichtenzentrierte Informationsversorgungund eine angebotsgetriebene Informationsbereitstellung im Kontext eines integriertenInformationssystems zu ermöglichen. Um diesen Anspruch zu konkretisieren; wird alsBeispiel eines 'Publish/Subscribe'-Systems das PubScribe-Projekt vorgestellt. Dabei wirdauf die logische Architektur; das Verarbeitungsmodell und die prototypische Realisierungeingegangen. Es zeigt sich; dass der PubScribe-Ansatz; basierend auf einem striktenRollenkonzept in Kombination mit einer komplexen Verarbeitungslogik; als solider …,*,*,*
REVEALING REAL PROBLEMS IN REAL DATA WAREHOUSE APPLICATIONS,Wolfgang Lehner; Thomas Ruf,Abstract Data warehouse systems offer novel techniques and services for data-richapplications both from a data modeling and a data processing point of view. In this paper;we investigate how well state-of-the-art concepts fulfill the requirements of real-world datawarehouse applications. Referring to a concrete example from the market research area;key problems with current approaches are identified in the areas of dimensional modeling;aggregation management; metric definitions; versioning and duality of both master andtracking data; context-sensitive fact calculations; derived attributes; heterogeneous reports;and data security. For some of these problems; solutions are shown both within and beyondthe framework of the data warehouse platform used for building GfK's market research datawarehouse system. The paper concludes with a list of requirements for extensions of …,*,*,*
Parameterfreies Clustering durch Mehrstufigen Ansatz,Dirk Habich; Martin Hahmann; Wolfgang Lehner,Abstract Das Ziel des Clusterings besteht darin; eine (semi-) automatische Partitionierungeiner Datenmenge in Gruppen durchzuführen; so dass Objekte in einer Gruppe möglichstähnlich und Objekte verschiedener Gruppen möglichst unähnlich zueinander sind. In allenAnwendungsgebieten muss nach der Festlegung eines Ahnlichkeitsmaßes ein geeignetesVerfahren aus der Menge der vorhandenen ausgewählt werden. Diese Auswahl des ambesten geeigneten Verfahrens ist sehr komplex und muss heutzutage durch einen Expertenerledigt werden. Des Weiteren ist die Bestimmung der optimalen Parameterwerte für diesesVerfahren bezüglich der betrachteten Daten oftmals nicht trivial. Um ein gutesPartitionierungsergebnis zu erhalten; wird dieses Verfahren iterativ mit angepasstenParameterwerten auf die Daten angewandt. Um diese Komplexität zu reduzieren …,LWA 2007 Lernen–Wissen–Adaption,*,*
COMBI-GROUP (): Eine relationaler Operator zur Unterstützung von Data Mining Anwendungen,Dirk Habich; MLU Halle; A Hinneburg; W Lehner,Zusammenfassung Die Anwendung von Data Mining Algorithmen werden durch dieZunahme der Daten in Datenbanksystemen immer interessanter. Die Implementierung derData Mining Algorithmen kann entweder ausserhalb oder innerhalb des Datenbanksystemvorgenommen werden. Client Programme müssen die notwendigen Daten aus demDatenbanksystem laden und degradieren dieses damit zu einem reinen Speichersystemohne Berücksichtigung und Benutzung der effizienten Operatoren innerhalb desDatenbanksystemes. Die Implementierung der Data Mining Algorithmen innerhalb desDatenbanksystemes erfordert komplexe Mechanismen. Als dritte Alternative bietet sich dieImplementierung eines spezifischen Anwendungsoperators im Datenbanksystem an; der fürbesonders datenintensive Aufgaben durch die Data Mining Anwendung benutzt werden …,group,*,*
ÜBER ANBAHNUNG UND AUSHANDLUNG,H Wedekind; W Hümmer; W Lehner,Zusammenfassung. Die modell-und systemtechnische Unterstützung des elektonischenHandels; überlicherweise als eBusiness bezeichnet; muss als zentrale Herausforderungsowohl im Bereich der Forschung als auch aus kommerzieller Perspektive gesehen werden.Dieser Aufsatz fokussiert die beiden Phasen der Anbahnung und Aushandlung vonVerträgen. Es wird diskutiert; wie aus einem N: M-Verhältnis in der Phase derPartnerfindung nahtlos eine 1: 1-Beziehung zur konkreten Aushandlung eines Vertrageshergestellt werden kann und die Inhalte des Vertrages nicht verloren gehen. Im Kontext derMereologie (Logik der Teil-Ganze-Relation) wird die Rekonstruktion eines Vertragswerkesvorgenommen. Geschachtelt strukturierte Teile des zukünftigen Vertragswerkes aus derAnbahnungsphase werden als Gegenstand eines dialogischen Aushandelns aufgefasst …,*,*,*
