Data exchange: semantics and query answering,Ronald Fagin; Phokion G Kolaitis; Renée J Miller; Lucian Popa,Abstract Data exchange is the problem of taking data structured under a source schema andcreating an instance of a target schema that reflects the source data as accurately aspossible. In this paper; we address foundational and algorithmic issues related to thesemantics of data exchange and to the query answering problem in the context of dataexchange. These issues arise because; given a source instance; there may be many targetinstances that satisfy the constraints of the data exchange problem. We give an algebraicspecification that selects; among all solutions to the data exchange problem; a special classof solutions that we call universal. We show that a universal solution has no more and noless data than required for data exchange and that it represents the entire space of possiblesolutions. We then identify fairly general; yet practical; conditions that guarantee the …,Theoretical Computer Science,2005,1223
Translating web data,Lucian Popa; Yannis Velegrakis; Renée J Miller; Mauricio A Hernández; Ronald Fagin,An important issue in modern information systems and e-commerce applications is providingsupport for interoperability of independent data sources. A broad variety of data is availableon the Web in distinct heterogeneous sources; stored under different formats: databaseformats (for example; relational model); semistructured formats (for example; DTDs; SGML orXML Schema); scientific formats; etc. Integration of such data is an increasingly importantproblem. Nonetheless; the effort involved in such integration; in practice; is considerable:translation of data from one format (or schema) to another requires writing and managingcomplex data; transformation programs or queries. The chapter presents a novel frameworkfor mapping between any combination of XML and relational schemas; in which a high-level;user specified mapping is translated into semantically meaningful queries that transform …,*,2002,673
The Clio project: managing heterogeneity,Renée J Miller; Mauricio A Hernández; Laura M Haas; Ling-Ling Yan; CT Howard Ho; Ronald Fagin; Lucian Popa,Abstract Clio is a system for managing and facilitating the complex tasks of heterogeneousdata transformation and integration. In Clio; we have collected together a powerful set ofdata management techniques that have proven invaluable in tackling these difficultproblems. In this paper; we present the underlying themes of our approach and present abrief case study.,SIgMOD Record,2001,419
Data exchange: getting to the core,Ronald Fagin; Phokion G Kolaitis; Lucian Popa,Abstract Data exchange is the problem of taking data structured under a source schema andcreating an instance of a target schema that reflects the source data as accurately aspossible. Given a source instance; there may be many solutions to the data exchangeproblem; that is; many target instances that satisfy the constraints of the data exchangeproblem. In an earlier article; we identified a special class of solutions that we call universal.A universal solution has homomorphisms into every possible solution; and hence is a “mostgeneral possible” solution. Nonetheless; given a source instance; there may be manyuniversal solutions. This naturally raises the question of whether there is a “best” universalsolution; and hence a best solution for data exchange. We answer this question byconsidering the well-known notion of the core of a structure; a notion that was first studied …,ACM Transactions on Database Systems (TODS),2005,388
Composing schema mappings: Second-order dependencies to the rescue,Ronald Fagin; Phokion G Kolaitis; Lucian Popa; Wang-Chiew Tan,Abstract A schema mapping is a specification that describes how data structured under oneschema (the source schema) is to be transformed into data structured under a differentschema (the target schema). A fundamental problem is composing schema mappings: giventwo successive schema mappings; derive a schema mapping between the source schemaof the first and the target schema of the second that has the same effect as applyingsuccessively the two schema mappings. In this article; we give a rigorous semantics to thecomposition of schema mappings and investigate the definability and computationalcomplexity of the composition of two schema mappings. We first study the important case ofschema mappings in which the specification is given by a finite set of source-to-target tuple-generating dependencies (source-to-target tgds). We show that the composition of a finite …,ACM Transactions on Database Systems (TODS),2005,352
Clio grows up: from research prototype to industrial tool,Laura M Haas; Mauricio A Hernández; Howard Ho; Lucian Popa; Mary Roth,Abstract Clio; the IBM Research system for expressing declarative schema mappings; hasprogressed in the past few years from a research prototype into a technology that is behindsome of IBM's mapping technology. Clio provides a declarative way of specifying schemamappings between either XML or relational schemas. Mappings are compiled into anabstract query graph representation that captures the transformation semantics of themappings. The query graph can then be serialized into different query languages;depending on the kind of schemas and systems involved in the mapping. Clio currentlyproduces XQuery; XSLT; SQL; and SQL/XML queries. In this paper; we revisit thearchitecture and algorithms behind Clio. We then discuss some implementation issues;optimizations needed for scalability; and general lessons learned in the road towards …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,317
Constraint-based XML query rewriting for data integration,Cong Yu; Lucian Popa,Abstract We study the problem of answering queries through a target schema; given a set ofmappings between one or more source schemas and this target schema; and given that thedata is at the sources. The schemas can be any combination of relational or XML schemas;and can be independently designed. In addition to the source-to-target mappings; weconsider as part of the mapping scenario a set of target constraints specifying additionalproperties on the target schema. This becomes particularly important when integrating datafrom multiple data sources with overlapping data and when such constraints can expressdata merging rules at the target. We define the semantics of query answering in such anintegration scenario; and design two novel algorithms; basic query rewrite and queryresolution; to implement the semantics. The basic query rewrite algorithm reformulates …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,247
Clio: Schema mapping creation and data exchange,Ronald Fagin; Laura M Haas; Mauricio Hernández; Renée J Miller; Lucian Popa; Yannis Velegrakis,Abstract The Clio project provides tools that vastly simplify information integration.Information integration requires data conversions to bring data in different representationsinto a common form. Key contributions of Clio are the definition of non-procedural schemamappings to describe the relationship between data in heterogeneous schemas; a newparadigm in which we view the mapping creation process as one of query discovery; andalgorithms for automatically generating queries for data transformation from the mappings.Clio provides algorithms to address the needs of two major information integration problems;namely; data integration and data exchange. In this chapter; we present our algorithms forboth schema mapping creation via query discovery; and for query generation for dataexchange. These algorithms can be used in pure relational; pure XML; nested relational …,*,2009,186
Semantic adaptation of schema mappings when schemas evolve,Cong Yu; Lucian Popa,Abstract Schemas evolve over time to accommodate the changes in the information theyrepresent. Such evolution causes invalidation of various artifacts depending on theschemas; such as schema mappings. In a heterogenous environment; where cooperationamong data sources depends essentially upon them; schema mappings must be adapted toreflect schema evolution. In this study; we explore the mapping composition approach foraddressing this mapping adaptation problem. We study the semantics of mappingcomposition in the context of mapping adaptation and compare our approach with theincremental approach of Velegrakis et al [21]. We show that our method is superior in termsof capturing the semantics of both the original mappings and the evolution. We design andimplement a mapping adaptation system based on mapping composition as well as …,Proceedings of the 31st international conference on Very large data bases,2005,162
-Mapping Adaptation under Evolving Schemas,Yannis Velegrakis; Renée J Miller; Lucian Popa,This chapter identifies the problem of mapping adaptation in dynamic environments withevolving schemas. To achieve interoperability; modem information systems and e-commerce applications use mappings to translate data from one representation to another.In dynamic environments like the Web; data sources may change not only their data but alsotheir schemas; their semantics; and their query capabilities. Such changes must be reflectedin the mappings. The chapter motivates the need for an automated system to adaptmappings and describes several areas in which the solutions can be applied. This chapterpresents a novel framework and a tool; Toronto Mapping Adaptation System (ToMAS); thatautomatically maintains the consistency of the mappings as schemas evolve. The approachis unique in many ways. It considers and manages a very general class of mappings …,*,2003,154
Nested mappings: schema mapping reloaded,Ariel Fuxman; Mauricio A Hernandez; Howard Ho; Renee J Miller; Paolo Papotti; Lucian Popa,Abstract Many problems in information integration rely on specifications; called schemamappings; that model the relationships between schemas. Schema mappings for bothrelational and nested data are well-known. In this work; we present a new formalism forschema mapping that extends these existing formalisms in two significant ways. First; ournested mappings allow for nesting and correlation of mappings. This results in a naturalprogramming paradigm that often yields more accurate specifications. In particular; we showthat nested mappings can naturally preserve correlations among data that existing mappingformalisms cannot. We also show that using nested mappings for purposes of exchangingdata from a source to a target will result in less redundancy in the target data. The secondextension to the mapping formalism is the ability to express; in a declarative way …,Proceedings of the 32nd international conference on Very large data bases,2006,151
Method for schema mapping and data transformation,*,A computer program product is provided that uses data examples as a basis forunderstanding and refining declarative schema mappings. The system of the presentinvention identifies a set of intuitive operators for manipulating examples includingestablishing value correspondences; data linking; data trimming; data walking; and datachasing. These operators allow a user to follow and refine an example by walking through adata source. In addition; these operators can identify a large class of schema mappings anddistinguish effectively between alternative schema mappings. With these operators; a user isable to quickly and intuitively build and refine complex data transformation queries that mapone data source into another while continuously verifying that the mapping is accurate andappropriate.,*,2006,104
Physical data independence; constraints and optimization with universal plans,Alin Deutsch; Lucian Popa; Val Tannen,Abstract We present an optimization method and al gorithm designed for three objectives:physi cal data independence; semantic optimization; and generalized tableau minimization.The method relies on generalized forms of chase and" backchase" with constraints(dependen cies). By using dictionaries (finite functions) in physical schemas we can capturewith con straints useful access structures such as indexes; materialized views; sourcecapabilities; access support relations; gmaps; etc. The search space for query plans isdefined and enumerated in a novel manner: the chase phase rewrites the original query intoa" universal" plan that integrates all the access structures and alternative pathways that areallowed by appli cable constraints. Then; the backchase phase produces optimal plans byeliminating various combinations of redundancies; again according to constraints. This …,Database Research Group (CIS),1999,104
Query reformulation with constraints,Alin Deutsch; Lucian Popa; Val Tannen,Abstract Let Σ 1; Σ 2 be two schemas; which may overlap; C be a set of constraints on thejoint schema Σ 1∪ Σ 2; and q 1 be a Σ 1-query. An (equivalent) reformulation of q 1 in thepresence of C is a Σ 2-query; q 2; such that q 2 gives the same answers as q 1 on any Σ 1∪Σ 2-database instance that satisfies C. In general; there may exist multiple suchreformulations and choosing among them may require; for example; a cost model.,ACM SIGMOD Record,2006,103
Quasi-inverses of schema mappings,Ronald Fagin; Phokion G Kolaitis; Lucian Popa; Wang-Chiew Tan,Abstract Schema mappings are high-level specifications that describe the relationshipbetween two database schemas. Two operators on schema mappings; namely thecomposition operator and the inverse operator; are regarded as especially important.Progress on the study of the inverse operator was not made until very recently; as evenfinding the exact semantics of this operator turned out to be a fairly delicate task.Furthermore; this notion is rather restrictive; since it is rare that a schema mappingpossesses an inverse. In this article; we introduce and study the notion of a quasi-inverse ofa schema mapping. This notion is a principled relaxation of the notion of an inverse of aschema mapping; intuitively; it is obtained from the notion of an inverse by not differentiatingbetween instances that are equivalent for data-exchange purposes. For schema …,ACM Transactions on Database Systems (TODS),2008,102
Preserving mapping consistency under schema changes,Yannis Velegrakis; J Miller; Lucian Popa,Abstract In dynamic environments like the Web; data sources may change not only their databut also their schemas; their semantics; and their query capabilities. When a mapping is leftinconsistent by a schema change; it has to be detected and updated. We present a novelframework and a tool (ToMAS) for automatically adapting (rewriting) mappings as schemasevolve. Our approach considers not only local changes to a schema but also changes thatmay affect and transform many components of a schema. Our algorithm detects mappingsaffected by structural or constraint changes and generates all the rewritings that areconsistent with the semantics of the changed schemas. Our approach explicitly modelsmapping choices made by a user and maintains these choices; whenever possible; as theschemas and mappings evolve. When there is more than one candidate rewriting; the …,The VLDB Journal—The International Journal on Very Large Data Bases,2004,100
Mapping XML and relational schemas with Clio,Lucian Popa; Mauricio A Hernandez; Yannis Velegrakis; Renée J Miller; Felix Naumann; Howard Ho,Merging and coalescing data from multiple and diverse sources into different data formatscontinues to be an important problem in modern information systems. Schema matching (theprocess of matching elements of a source schema with elements of a target schema) andschema mapping (the process of creating a query that maps between two disparateschemas) are at the heart of data integration systems. We demonstrate Clio; a semi-automatic schema mapping tool developed at the IBM Almaden Research Center. In thispaper; we showcase Clio's mapping engine which allows mapping to and from relationaland XML schemas; and takes advantage of data constraints in order to preserve dataassociations.,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,79
Towards a theory of schema-mapping optimization,Ronald Fagin; Phokion G Kolaitis; Alan Nash; Lucian Popa,Abstract A schema mapping is a high-level specification that describes the relationshipbetween two database schemas. As schema mappings constitute the essential buildingblocks of data exchange and data integration; an extensive investigation of the foundationsof schema mappings has been carried out in recent years. Even though several differentaspects of schema mappings have been explored in considerable depth; the study ofschema-mapping optimization remains largely uncharted territory to date. In this paper; welay the foundation for the development of a theory of schema-mapping optimization. Sinceschema mappings are constructs that live at the logical level of information integrationsystems; the first step is to introduce concepts and to develop techniques for transformingschema mappings to" equivalent" ones that are more manageable from the standpoint of …,Proceedings of the twenty-seventh ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2008,75
An Equational Chase for Path-Conjunctive Queries; Constraints; Views,Lucian Popa; Val Tannen,Abstract We consider the class of path-conjunctive queries and constraints (dependencies)defined over complex values with dictionaries. This class includes the relational conjunctivequeries and embedded dependencies; as well as many interesting examples of complexvalue and oodb queries and integrity constraints. We show that some important classicalresults on containment; dependency implication; and chasing extend and generalize to thisclass.,International Conference on Database Theory,1999,73
Systematic approach to query optimization,*,The present invention demonstrates the development and application of thechase/backchase (“C&B”) technique to systematically optimize generating alternative queryplans; aimed at multiple disparate targets. It further provides a first optimization prototypethat uses path-conjunctive query graphs internally. The methods; systems; apparatus andtechniques of the present invention capture and extend many aspects of semanticoptimizations; physical data independence; use of materialized views and cached queries;as well as generalized tableau-like minimization. Moreover; using a uniform representationwith constraints; the techniques make these disparate optimization principles highlycooperative. This present invention provides a new class of optimization opportunities; suchas the non-trivial use of indexes and materialized views enabled only by the presence of …,*,2003,71
Extracting; linking and integrating data from public sources: A financial case study,Douglas Burdick; Sanjiv Das; Mauricio Hernandez; Howard Ho; Georgia Koutrika; Rajasekar Krishnamurthy; Lucian Popa; Ioana Stanoi; Shivakumar Vaithyanathan,*,IEEE Data Engineering Bulletin,2011,70
Interactive generation of integrated schemas,Laura Chiticariu; Phokion G Kolaitis; Lucian Popa,Abstract Schema integration is the problem of creating a unified target schema based on aset of existing source schemas that relate to each other via specified correspondences. Theunified schema gives a standard representation of the data; thus offering a way to deal withthe heterogeneity in the sources. In this paper; we develop a method and a design tool thatprovide: 1) adaptive enumeration of multiple interesting integrated schemas; and 2) easy-to-use capabilities for refining the enumerated schemas via user interaction. Our method is adeparture from previous approaches to schema integration; which do not offer a systematicexploration of the possible integrated schemas. The method operates at a logical level;where we recast each source schema into a graph of concepts with Has-A relationships. Wethen identify matching concepts in different graphs by taking into account the …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,66
Reverse data exchange: coping with nulls,Ronald Fagin; Phokion G Kolaitis; Lucian Popa; Wang-Chiew Tan,Abstract An inverse of a schema mapping M is intended to undo what M does; thus providinga way to perform reverse data exchange. In recent years; three different formalizations of thisconcept have been introduced and studied; namely the notions of an inverse of a schemamapping; a quasi-inverse of a schema mapping; and a maximum recovery of a schemamapping. The study of these notions has been carried out in the context in which sourceinstances are restricted to consist entirely of constants; while target instances may containboth constants and labeled nulls. This restriction on source instances is crucial for obtainingsome of the main technical results about these three notions; but; at the same time; limitstheir usefulness; since reverse data exchange naturally leads to source instances that maycontain both constants and labeled nulls. We develop a new framework for reverse data …,ACM Transactions on Database Systems (TODS),2011,61
A chase too far?,Lucian Popa; Alin Deutsch; Arnaud Sahuguet; Val Tannen,Abstract In a previous paper we proposed a novel method for generating alternative queryplans that uses chasing (and back-chasing) with logical constraints. The method bringstogether use of indexes; use of materialized views; semantic optimization and joinelimination (minimization). Each of these techniques is known separately to be beneficial toquery optimization. The novelty of our approach is in allowing these techniques to interactsystematically; eg. non-trivial use of indexes and materialized views may be enabled only bysemantic constraints. We have implemented our method for a variety of schemas andqueries. We examine how far we can push the method in term of complexity of both schemasand queries. We propose a technique for reducing the size of the search space by“stratifying” the sets of constraints used in the (back) chase. The experimental results …,ACM SIGMOD Record,2000,61
Clio: Schema Mapping Creation and Data Exchange; Conceptual Modeling: Foundations and Applications: Essays in Honor of John Mylopoulos,Ronald Fagin; Laura M Haas; Mauricio Hernández; Renée J Miller; Lucian Popa; Yannis Velegrakis,*,*,2009,51
Semi-automatic schema integration in clio,Laura Chiticariu; Mauricio A Hernández; Phokion G Kolaitis; Lucian Popa,Abstract Schema integration is the problem of finding a unified representation; called theintegrated schema; from a set of source schemas that are related to each other. Therelationships between the source schemas can be represented via correspondencesbetween schema elements or via some other forms of schema mappings such as constraintsor views. The integrated schema can be viewed as a means for dealing with theheterogeneity in the source schemas; by providing a standard representation of the data.Schema integration has received much of attention in the research literature [1; 2; 6; 8; 10]and still remains a challenge in practice. Existing approaches require substantial amount ofhuman feedback during the integration process and moreover; the outcome of theseapproaches is a single integrated schema. In general; however; there can be multiple …,Proceedings of the 33rd international conference on Very large data bases,2007,50
Schema mapping evolution through composition and inversion,Ronald Fagin; Phokion G Kolaitis; Lucian Popa; Wang-Chiew Tan,Abstract Mappings between different representations of data are the essential buildingblocks for many information integration tasks. A schema mapping is a high-levelspecification of the relationship between two schemas; and represents a useful abstractionthat specifies how the data from a source format can be transformed into a target format. Thedevelopment of schema mappings is laborious and time consuming; even in the presence oftools that facilitate this development. At the same time; schema evolution inevitably causesthe invalidation of the existing schema mappings (since their schemas change). Providingtools and methods that can facilitate the adaptation and reuse of the existing schemamappings in the context of the new schemas is an important research problem. In thischapter; we show how two fundamental operators on schema mappings; namely …,*,2011,48
Mapping-driven XML transformation,Haifeng Jiang; Howard Ho; Lucian Popa; Wook-Shin Han,Abstract Clio is an existing schema-mapping tool that provides user-friendly means tomanage and facilitate the complex task of transformation and integration of heterogeneousdata such as XML over the Web or in XML databases. By means of mappings from source totarget schemas; Clio can help users conveniently establish the precise semantics of datatransformation and integration. In this paper we study the problem of how to efficientlyimplement such data transformation (ie; generating target data from the source data basedon schema mappings). We present a three-phase framework for high-performance XML-to-XML transformation based on schema mappings; and discuss methodologies andalgorithms for implementing these phases. In particular; we elaborate on novel techniquessuch as streamed extraction of mapped source values and scalable disk-based merging …,Proceedings of the 16th international conference on World Wide Web,2007,47
Object/relational query optimization with chase and backchase,Lucian Popa,Abstract Traditionally; query optimizers assume a direct mapping from the logical entitiesmodeling the data (eg relations) and the physical entities storing the data (eg indexes); eachphysical entity corresponding precisely to one logical entity. This assumption is no longertrue in non-traditional applications (object-oriented and semi-structured databases; dataintegration); which often exhibit a mismatch between the logical view and the actual storageof data. In addition; there is an increased amount of redundancy; even at the logical level;that can greatly enhance optimization opportunities; if exploited. To deal with all this; wepropose a novel architecture for query optimization; in which physical optimization isleveraged at the level of query rewriting. As a consequence; the other important aspect ofquery optimization; semantic optimization (that takes advantage of the redundancy at the …,*,2001,38
XML mapping technology: Making connections in an XML-centric world,Mary Roth; Mauricio A Hernández; Phil Coulthard; L Yan; Lucian Popa; HC-T Ho; CC Salter,Extensible Markup Language (XML) has grown rapidly over the last decade to become thede facto standard for heterogeneous data exchange. Its popularity is due in large part to theease with which diverse kinds of information can be represented as a result of the self-describing nature and extensibility of XML itself. The ease and speed with which informationcan be represented does not extend; however; to exchanging such information betweenautonomous sources. In the absence of controlling standards; such sources will typicallychoose differing XML representations for the same concept; and the actual exchange ofinformation between them requires that the representation produced by one source betransformed into a representation understood by the other. Creating this informationexchange “glue” is a tedious and error-prone process; whether expressed as Extensible …,IBM Systems Journal,2006,37
HIL: a high-level scripting language for entity integration,Mauricio Hernández; Georgia Koutrika; Rajasekar Krishnamurthy; Lucian Popa; Ryan Wisnesky,Abstract We introduce HIL; a high-level scripting language for entity resolution andintegration. HIL aims at providing the core logic for complex data processing flows thataggregate facts from large collections of structured or unstructured data into clean; unifiedentities. Such flows typically include many stages of processing that start from the outcomeof information extraction and continue with entity resolution; mapping and fusion. A HILprogram captures the overall integration flow through a combination of SQL-like rules thatlink; map; fuse and aggregate entities. A salient feature of HIL is the use of logical indexes inits data model to facilitate the modular construction and aggregation of complex entities.Another feature is the presence of a flexible; open type system that allows HIL to handleinput data that is irregular; sparse or partially known. As a result; HIL can accurately …,Proceedings of the 16th international conference on extending database technology,2013,36
Top-k generation of integrated schemas based on directed and weighted correspondences,Ahmed Radwan; Lucian Popa; Ioana R Stanoi; Akmal Younis,Abstract Schema integration is the problem of creating a unified target schema based on aset of existing source schemas and based on a set of correspondences that are the result ofmatching the source schemas. Previous methods for schema integration rely on theexploration; implicit or explicit; of the multiple design choices that are possible for theintegrated schema. Such exploration relies heavily on user interaction; thus; it is timeconsuming and labor intensive. Furthermore; previous methods have ignored the additionalinformation that typically results from the schema matching process; that is; the weights andin some cases the directions that are associated with the correspondences. In this paper; wepropose a more automatic approach to schema integration that is based on the use ofdirected and weighted correspondences between the concepts that appear in the source …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,33
Constraint-based XML query rewriting for data integration,*,A system and method for data integration by querying multiple extensible markup language(XML) source schemas through a common XML target schema; wherein the systemcomprises XML mapping connections between the XML source schemas and the XML targetschema; wherein the XML mapping connections accommodate XML mappings; the XMLsource schemas comprise data; and the XML target schema comprise a set of constraints;which comprise data merging rules for integrating the data from multiple source schemascomprising overlapping information; a target query associated with the target schema; and aquery rewriter adapted to reformulate the target query in terms of the source schemas basedon the mappings; and to integrate the data based on the set of constraints. The queryrewriter is adapted to rewrite the target query into a set of source queries comprising the …,*,2009,32
MapMerge: correlating independent schema mappings,Bogdan Alexe; Mauricio Hernández; Lucian Popa; Wang-Chiew Tan,Abstract One of the main steps toward integration or exchange of data is to design themappings that describe the (often complex) relationships between the source schemas orformats and the desired target schema. In this paper; we introduce a new operator; calledMapMerge; that can be used to correlate multiple; independently designed schemamappings of smaller scope into larger schema mappings. This allows a more modularconstruction of complex mappings from various types of smaller mappings such as schemacorrespondences produced by a schema matcher or pre-existing mappings that weredesigned by either a human user or via mapping tools. In particular; the new operator alsoenables a new" divide-and-merge" paradigm for mapping creation; where the design isdivided (on purpose) into smaller components that are easier to create and understand …,The VLDB Journal—The International Journal on Very Large Data Bases,2012,29
Discovering linkage points over web data,Oktie Hassanzadeh; Ken Q Pu; Soheil Hassas Yeganeh; Renée J Miller; Lucian Popa; Mauricio A Hernández; Howard Ho,Abstract A basic step in integration is the identification of linkage points; ie; finding attributesthat are shared (or related) between data sources; and that can be used to match records orentities across sources. This is usually performed using a match operator; that associatesattributes of one database to another. However; the massive growth in the amount andvariety of unstructured and semi-structured data on the Web has created new challenges forthis task. Such data sources often do not have a fixed pre-defined schema and contain largenumbers of diverse attributes. Furthermore; the end goal is not schema alignment as theseschemas may be too heterogeneous (and dynamic) to meaningfully align. Rather; the goal isto align any overlapping data shared by these sources. We will show that even attributeswith different meanings (that would not qualify as schema matches) can sometimes be …,Proceedings of the VLDB Endowment,2013,27
Midas: integrating public financial data,Sreeram Balakrishnan; Vivian Chu; Mauricio A Hernández; Howard Ho; Rajasekar Krishnamurthy; Shi Xia Liu; Jan H Pieper; Jeffrey S Pierce; Lucian Popa; Christine M Robson; Lei Shi; Ioana R Stanoi; Edison L Ting; Shivakumar Vaithyanathan; Huahai Yang,Abstract The primary goal of the Midas project is to build a system that enables easy andscalable integration of unstructured and semi-structured information present across multipledata sources. As a first step in this direction; we have built a system that extracts andintegrates information from regulatory filings submitted to the US Securities and ExchangeCommission (SEC) and the Federal Deposit Insurance Corporation (FDIC). Midas creates arepository of entities; events; and relationships by extracting; conceptualizing; integrating;and aggregating data from unstructured and semi-structured documents. This repositoryenables applications to use the extracted and integrated data in a variety of ways includingmashups with other public data and complex risk analysis.,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,26
Tomas: A system for adapting mappings while schemas evolve,Yannis Velegrakis; Renée J Miller; Lucian Popa; John Mylopoulos,We demonstrate the Toronto Mapping Adaptation System (ToMAS); a tool for automaticallydetecting and adapting mappings that have become invalid or inconsistent due to changesin either data semantics or schemas. Due to its modular architecture and its stand-alonenature; ToMAS can easily be applied to numerous scenarios and can interoperate withmany other tools. To the best of our knowledge; no other tool can correctly maintain theconsistency of the mappings under schema changes at the level of complexity supported byToMAS.,Data Engineering; 2004. Proceedings. 20th International Conference on,2004,25
IBM UFO repository: Object-oriented data integration,Michael N Gubanov; Lucian Popa; Howard Ho; Hamid Pirahesh; Jeng-Yih Chang; Shr-Chang Chen,Abstract Currently; WWW; large enterprises; and desktop users suffer from an inability toefficiently access and manage differently structured data. The same data objects (egProduct) stored by different databases; repositories; distributed web storage systems; etc arenamed; referenced; and combined internally into schemas or data structures differently. Thisleads to structural mismatch of data that often consists of the same semantic objects (egEBay and Yahoo! online auction offers).,Proceedings of the VLDB Endowment,2009,21
Schema management,Periklis Andritsos; Ronald Fagin; Ariel Fuxman; Laura M Haas; Mauricio A Hernández; Howard Ho; Anastasios Kementsietsidis; Phokion G Kolaitis; Renée J Miller; Felix Naumann; Lucian Popa; Yannis Velegrakis,Abstract Clio is a management system for heterogeneous data that couples a traditionaldatabase management engine with additional tools for managing schemas (models of data)and mappings between schemas. In this article; we provide a brief overview of Clio andplace our solutions in the context of the rich research literature on data integration andtransformation. Clio is the result of an on-going collaboration between the University ofToronto and IBM Almaden Research Center in which we are addressing both foundationaland systems issues related to heterogeneous data; schema; and integration management.,*,2002,19
Schema mapping specification framework,*,A method; system and program product for specifying; in a schema mapping framework; amapping between a source schema and a target schema. The source and target schemasare schemas included in respective groups of registered; heterogeneous schemas. Thesource and target schemas may be of different types. Serialized versions of the source andtarget schemas include source objects and target objects; respectively. A mapping model isserialized into mapping objects that include logical references representing the sourceobjects and logical references representing the target objects. The logical references areresolved to the source objects and target objects; thereby storing pointers to the sourceobjects and to the target objects. After resolving the logical references; the mapping modelincludes the logical references and the pointers to the source and target objects.,*,2011,17
Creating nested mappings with Clio,Mauricio A Hernández; Howard Ho; Lucian Popa; Ariel Fuxman; Renee J Miller; Takeshi Fukuda; Paolo Papotti,Schema mappings play a central role in many data integration and data exchangescenarios. In those applications; users need to quickly and correctly specify how datarepresented in one format is converted into a different format. Clio (L. Popa et al.; 2002) is ajoint research project between IBM and the University of Toronto studying the creation;maintenance; and use of schema mappings. There have always been two goals in our workin Clio: 1) the automatic creation of logical assertions that capture the way one or moresource schemas are mapped into a target schema; and 2) the generation of transformationqueries or programs that transform a source data instance into a target data instance.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,16
A structure-based approach to querying semi-structured data,Mary Fernandez; Lucian Popa; Dan Suciu,Abstract Several researchers have considered integrating multiple unstructured; semi-structured; and structured data sources by modeling all sources as edge labeled graphs.Data in this model is self-describing and dynamically typed; and captures both schema anddata information. The labels are arbitrary atomic values; such as strings; integers; reals; etc.;and the integrated data graph is stored in a unique data repository; as a relation of edges.The relation is dynamically typed; ie each edge label is tagged with its type. Although theunique; labeled graph repository is flexible; it looses all static type information; and results insevere efficiency penalties compared to querying structured databases; such as relational orobject-oriented databases. In this paper we propose an alternative method of storing andquerying semi-structured data; using storage schemas; which are closely related to …,International Workshop on Database Programming Languages,1997,14
Scalable algorithms for mapping-based xml transformation,*,A computer-implemented method for use with an extensible markup language (XML)document includes inputting a high-level mapping specification for a schema mapping; andgenerating a target XML document based on the mapping. The method may performschema mapping-based XML transformation as a three-phase process comprising tupleextraction; XML-fragment generation; and data merging. The tuple extraction phase may beadapted to handle streamed XML data (as well as stored/indexed XML data). The datamerging phase may use a hybrid method that can dynamically switch between main memory-based and disk-based algorithms based on the size of the XML data to be merged.,*,2008,13
Scalable algorithms for mapping-based XML transformation,*,A computer-implemented method for use with an extensible markup language (XML)document includes inputting a high-level mapping specification for a schema mapping; andgenerating a target XML document based on the mapping. The method may performschema mapping-based XML transformation as a three-phase process comprising tupleextraction; XML-fragment generation; and data merging. The tuple extraction phase may beadapted to handle streamed XML data (as well as stored/indexed XML data). The datamerging phase may use a hybrid method that can dynamically switch between main memory-based and disk-based algorithms based on the size of the XML data to be merged.,*,2014,11
Clio: A schema mapping tool for information integration,Mauricio Hernandez; Howard Ho; Felix Naumann; Lucian Popa,The summary form only given. Information integration typically requires the construction ofcomplex artifacts like federated databases; ETL scripts; data warehouses; applications foraccessing multiple data sources; and applications that ingest or publish XML. For manycompanies; it is one of the most complicated IT tasks they face today. To reduce the overallcost; intelligent tools are needed to simplify this difficult task. Clio is a semi-automatic tool forschema mapping and data integration developed at IBM Almaden Research Center over thepast few years. It takes source and target schemas as input; which may describe relational orXML data models. Via a graphical Schema Viewer; a user can then interactively specifyattribute correspondences between the source and target schemas. An AttributeMatchercomponent helps suggest such correspondences; based on the similarity of both attribute …,Parallel Architectures; Algorithms and Networks; 2005. ISPAN 2005. Proceedings. 8th International Symposium on,2005,11
Querying an object-oriented database using CPL,Susan B Davidson; Carmem Hara; Lucian Popa,Abstract The Collection Programming Language is based on a complex value model of dataand has successfully been used for querying transforming and integrating data from a widevariety of structured data sources-relational; ACeDB; and ASN. 1 among others. However;since there is no notion of objects and classes in CPL; it cannot adequately model recursivetypes or inheritance; and hence cannot be used to query object-oriented databases(OODBs). By adding a reference type and four operations to CPL-dereference; methodinvocation; identity test and class type cast-it is possible to express a large class ofinteresting" safe" queries against OODBs. As an example of how the extended CPL can beused to query an OODB; we will describe how the extended language has been used as aquery interface to Shore databases.,Technical Reports (CIS),1997,11
A Declarative Framework for Linking Entities,Douglas Burdick; Ronald Fagin; Phokion G Kolaitis; Lucian Popa; Wang-Chiew Tan,Abstract We introduce and develop a declarative framework for entity linking and; inparticular; for entity resolution. As in some earlier approaches; our framework is based on asystematic use of constraints. However; the constraints we adopt are link-to-sourceconstraints; unlike in earlier approaches where source-to-link constraints were used todictate how to generate links. Our approach makes it possible to focus entirely on theintended properties of the outcome of entity linking; thus separating the constraints from anyprocedure of how to achieve that outcome. The core language consists of link-to-sourceconstraints that specify the desired properties of a link relation in terms of source relationsand built-in predicates such as similarity measures. A key feature of the link-to-sourceconstraints is that they employ disjunction; which enables the declarative listing of all the …,18th International Conference on Database Theory (ICDT 2015),2015,9
Methods and systems for discovery of linkage points between data sources,*,Data records are linked across a plurality of datasets. Each dataset contains at least onedata record; and each data record is associated with an entity and includes one or moreattributes of that entity and a value for each attribute. Values associated with attributes arecompared across datasets; and matching attributes having values that satisfy apredetermined similarity threshold are identified. In addition; linkage points between pairs ofdatasets are identified. Each linkage point links one or more pairs of data records. Each datarecord in the pair of data records is contained in one of a given pair of datasets; and eachpair of data records is associated with a common entity having matching attributes in thegiven pair of datasets. Data records associated with the common entities are linked acrossdatasets using the identified linkage points.,*,2017,8
Method and sytsem for generating nested mapping specifications in a schema mapping formalism and for generating transformation queries based thereon,*,A method and system for generating nested mapping specifications and transformationqueries based thereon. Basic mappings are generated based on source and target schemasand correspondences between elements of the schemas. A directed acyclic graph (DAG) isconstructed whose edges represent ways in which each basic mapping is nestable underany of the other basic mappings. Any transitively implied edges are removed from the DAG.Root mappings of the DAG are identified. Trees of mappings are automatically extractedfrom the DAG; where each tree of mappings is rooted at a root mapping and expresses anested mapping specification. A transformation query is generated from the nested mappingspecification by generating a first query for transforming source data into flat views of thetarget and a second query for nesting flat view data according to the target format …,*,2008,8
Simplifying information integration: Object-based flow-of-mappings framework for integration,Bogdan Alexe; Michael Gubanov; Mauricio A Hernández; Howard Ho; Jen-Wei Huang; Yannis Katsis; Lucian Popa; Barna Saha; Ioana Stanoi,Abstract The Clio project at IBM Almaden investigates foundational aspects of datatransformation; with particular emphasis on the design and execution of schema mappings.We now use Clio as part of a broader data-flow framework in which mappings are just onecomponent. These data-flows express complex transformations between several source andtarget schemas and require multiple mappings to be specified. This paper describesresearch issues we have encountered as we try to create and run these mapping-baseddata-flows. In particular; we describe how we use Unified Famous Objects (UFOs); a schemaabstraction similar to business objects; as our data model; how we reason about flows ofmappings over UFOs; and how we create and deploy transformations into different run-timeengines.,International Workshop on Business Intelligence for the Real-Time Enterprise,2008,8
Data science challenges in real estate asset and capital markets,Douglas Burdick; Michael Franklin; Paulo Issler; Rajasekar Krishnamurthy; Lucian Popa; Louiqa Raschid; Richard Stanton; Nancy Wallace,Abstract The real estate financial markets are complex supply chains. Understanding theirbehavior is limited by a lack of data that would capture the richly interconnected networks offinancial institutions and complex financial products; eg; asset backed securities. This lack oftransparency is further compounded by limited knowledge of the contractual rules thatcontrol the flow of funds from mortgage pools to securities; as well as the financial eventsthat regulate these flows. In this project; we will use the IBM Midas framework and tools toextract entities; relationships; events; contractual rules and risk profiles for financialinstitutions. Our source of information will be the MBS prospectus documents that are publicand are filed with the Securities and Exchange Commission. We will describe the datamanagement needs of the Haas Real Estate and Financial Markets (REFM) Lab and …,Proceedings of the international workshop on data science for macro-modeling,2014,5
Sequential composition of schema mappings,*,A method for generating a schema mapping. A provided mapping M12 relates schema S1 toschema S2. A provided mapping M23 relates schema S2 to schema S3. A mapping M13 isgenerated from schema S1 to schema S3 as a composition of mappings M12 and M23.Mappings M12; M23; and M13 are each expressed in terms of at least one second-ordernested tuple-generating dependency (SO nested tgd). Mapping M13 does not expresslyrecite any element of schema S2. At least one schema of the schemas S1 and S2 maycomprise at least one complex type expression nested inside another complex typeexpression. Mapping M13 may define the composition of the mappings M12 and M23 withrespect to a relationship semantics or a transformation semantics.,*,2010,5
Mapping polymorphism,Ryan Wisnesky; Mauricio A Hernández; Lucian Popa,Abstract We examine schema mappings from a type-theoretic perspective and aim tofacilitate and formalize the reuse of mappings. Starting with the mapping language of Clio;we present a type-checking algorithm such that typable mappings are necessarilysatisfiable. We add type variables to the schema language and present a theory ofpolymorphism; including a sound and complete type inference algorithm and a semanticnotion of a principal type of a mapping. Principal types; which intuitively correspond to theminimum amount of schema structure required by the mappings; have an importantapplication for mapping reuse. Concretely; we show that mappings can be reused; with thesame semantics; on any schemas as long as these schemas are expansions (ie; subtypes)of the principal types.,Proceedings of the 13th International Conference on Database Theory,2010,5
Mapping-based query generation with duplicate elimination and minimal union,*,A method and system for generating a query implementing a schema mapping. A mappingM is provided from a schema S to a schema T; where M relates S to T; and M includes aplurality of constraints. Schemas S and T each include one or more elements; and Tincludes at least one set type element. Mapping M is expressed in terms of at least onenested tuple-generating dependency. A query Q is generated where Q is capable ofapplying M to an input instance I to result in an output instance J; where I conforms to S; Jconforms to T; and I and J satisfy the plurality of constraints. Instance J is in partitionednormal form (ie; satisfies minimal union semantics) and includes no duplicate elementinstances.,*,2009,5
Chase and axioms for PC queries and dependencies,Lucian Popa; Val Tannen,This report is the extended version of PT99] and while they are some overlapping parts(mainly the results) we don't repeat here some of the examples given in PT99]; even thoughwe frequently refer to them. The overview of this report is as follows. In section 2 we presentsome aspects of our internal framework; called CoDi 1. This is a language and equationaltheory that combines a treatment of dictionaries with our previous work BBW92; BNTW95;LT97] on collections and aggregates using the theory of monads. In addition we usedictionaries (nite functions); which allow us to represent oodb schemas and queries. Whilehere we focus on set-related queries; we show elsewhere2 that the (full) CoDi collection;aggregation and dictionary primitives su ce for implementing the quasi-totality ofODMG/ODL/OQL Cat96]. Using boolean aggregates; CoDi can represent constraints as …,*,1999,5
Entity integration using high-level scripting languages,*,Embodiments of the present invention relate to a new method of entity integration using high-level scripting languages. In one embodiment; a method of and computer product for entityintegration is provided. An entity declaration is read from a machine readable medium. Theentity declaration describes an entity including at least one nested entity. An indexdeclaration is read from a machine readable medium. The index declaration describes anindex of nested entities. An entity population rule is read from a machine readable medium.The entity population rule describes a mapping from an input schema to an output schema.The output schema conforms to the entity declaration. A plurality of input records is read froma first data store. The input records conform to the input schema. The entity population ruleapplies to the plurality of records to create a plurality of output records complying with the …,*,2017,4
Financial analytics from public data,Doug Burdick; Alexandre Evfimievski; Rajasekar Krishnamurthy; Neal Lewis; Lucian Popa; Scott Rickards; Peter Williams,Abstract There is a significant amount of" public" unstructured content available that iscentered around the financial performance and counterparty relationships of a wide range oforganizations; including private and public companies; governments; and public utilities.However; the wealth of structured entity and relationship information is buried inside a largeamount of unstructured data. Converting such data into a structured format is essential forbuilding novel analytics applications including counterparty and credit risk monitoring;investment decision making; development of new financial indices; systemic risk analysis;etc. In this paper; we illustrate three different application use cases where large-scaleextraction and integration from the relevant public data enabled the creation of newapplications to perform novel modeling and analysis. We discuss the main technological …,Proceedings of the International Workshop on Data Science for Macro-Modeling,2014,4
Biofederator: A data federation system for bioinformatics on the web,Ahmed Radwan; Akmal Younis; MA Hernández; H Ho; L Popa; S Shivaji; S Khuri,Abstract A problem facing many bioinformatics researchers today is the aggregation andanalysis of vast amounts of data produced by large scale projects from various laboratoriesaround the world. Depositing such data into centralized web-based repositories (eg NCBI;UCSC Genome Browser) is the common approach. However; the distributed nature of thedata; its growth rate; and increased collaborative needs represent real challenges calling fornovel decentralized web architectures. The BioFederator is a web services-based datafederation architecture for bioinformatics applications. Based on collaborations withbioinformatics researchers; several domainspecific data federation challenges and needsare identified. The BioFederator addresses such challenges and provides an architecturethat incorporates a series of utility services. These address issues like automatic workflow …,IIWeb Workshop,2007,4
Adapting mappings in frequently changing environments,Yannis Velegrakis; Renée J Miller; Lucian Popa,Abstract To achieve interoperability; modern information systems and e-commerceapplications use mappings to translate data from one representation to another. In dynamicenvironments like the Web; data sources may change not only their data but also theirschemas; their semantics; and their query capabilities. Such changes must be reflected inthe mappings. Mappings left inconsistent by a schema change have to be detected andupdated. As large; complicated schemas become more prevalent; and as data is reused inmore and more applications; manually maintaining mappings (even simple mappings likeview definitions) is becoming impractical. We present a novel framework and tool forautomatically adapting mappings as schemas evolve. Our approach considers not only localchanges to a schema; but also changes that may affect and transform many components …,Int. Conf of Very Large Databases (VLDB)(September 2003),2003,4
High-level rules for integration and analysis of data: New challenges,Bogdan Alexe; Douglas Burdick; Mauricio A Hernández; Georgia Koutrika; Rajasekar Krishnamurthy; Lucian Popa; Ioana R Stanoi; Ryan Wisnesky,Abstract Data integration remains a perenially difficult task. The need to access; integrateand make sense of large amounts of data has; in fact; accentuated in recent years. There arenow many publicly available sources of data that can provide valuable information in variousdomains. Concrete examples of public data sources include: bibliographic repositories(DBLP; Cora; Citeseer); online movie databases (IMDB); knowledge bases (Wikipedia;DBpedia; Freebase); social media data (Facebook and Twitter; blogs). Additionally; anumber of more specialized public data repositories are starting to play an increasinglyimportant role. These repositories include; for example; the US federal government data;congress and census data; as well as financial reports archived by the US Securities andExchange Commission (SEC).,*,2013,3
Method for generating nested mapping specifications in a schema mapping formalism,*,A method for generating nested mapping specifications and transformation queries basedthereon. Basic mappings are generated based on source and target schemas andcorrespondences between elements of the schemas. A directed acyclic graph (DAG) isconstructed whose edges represent ways in which each basic mapping is nestable underany of the other basic mappings. Any transitively implied edges are removed from the DAG.Root mappings of the DAG are identified. Trees of mappings are automatically extractedfrom the DAG; where each tree of mappings is rooted at a root mapping and expresses anested mapping specification.,*,2008,3
Chase & Backchase: A Method for Query Optimization with Materialized Views and Integrity Constraints,Alin Deutsch; Lucian Popa; Val Tannen,Abstract We have previously proposed chase and backchase as a novel method for usingmaterialized views and integrity constraints in query optimization. In this paper; we show thatthe method is usable in realistic optimizers by extending it to bag and mixed (ie bag-set)semantics as well as to grouping views and by showing how to integrate it with standard cost-based optimization. We understand materialized views broadly; including user-definedviews; cached queries and physical access structures (such as join indexes; access supportrelations; and gmaps). Moreover; our internal query representation supports object featureshence the method applies to OQL and (extended) SQL: 1999 queries. Chase andbackchase supports a very general class of integrity constraints; thus being able to findexecution plans using views that do not fall in the scope of other methods. In fact; we …,Technical Reports (CIS),2001,3
Optimization for physical independence in information integration components,Alin Deutsch; Lucian Popa; Val Tannen,Abstract We present an optimization method and algorithm motivated by the need forphysical independence in mediator-like components. Such components can store physicalaccess structures that facilitate joins or path navigation across the sources they cover. Usinghigh-level representations with the data structure of dictionaries (nite functions) we are ableto represent the physical access structures as views over the integrated logical schema ofthe sources. Dictionaries re ect directly the e ciency of the representation and facilitate costestimation. We de ne and enumerate in a novel manner a search space for query plans thatincorporate the physical access structures as views on the logical schema. We capture theviews through constraints (dependencies) and we use our previous work on the\equational"chase to rewrite with these constraints. This makes our method easy to integrate in the …,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES,1999,3
Correlating independent schema mappings,*,Embodiments of the invention relate to correlating schema mappings. In one embodiment; aset of schema mappings over a source schema and a target schema are received. Each ofthe schema mappings is decomposed into a basic schema mapping. A first set and secondset of relations re determined for the source schema and the target schema; respectively.Each relation in the first set of relations is paired to at least one relation in the second set ofrelations. The pairing forms multiple relation pairs between the first set and second ofrelations in the form of (T; T′); where T is a source portion of a relation pair and T′ is atarget portion of the relation pair. A set of basic schema mappings is identified that matchesthe relation pair. Each basic schema mapping is merged into a single schema mapping.,*,2013,2
iller RJ; et a1. Translating web data,L Popa; YM Velegrakis,*,VLDB Journal,2002,2
Towards high-precision and reusable entity resolution algorithms over sparse financial datasets,Douglas Burdick; Lucian Popa; Rajasekar Krishnamurthy,Abstract We describe our approach to the FEIII Data Challenge; which requires matchingentities across multiple financial datasets (FFIEC; SEC and LEI). By making use of a high-level language (HIL) that includes constructs for expressing both the matching logic and thepolicies to avoid or reduce the ambiguities among the matches; we are able to producehighly-accurate results in a sparse context; with only name and location attributes. As part ofthe high-level specification; we also make use of a Smart-Term Generation (STG)component; which provides us with a sophisticated subroutine for normalizing companynames. The high-level specification is reusable; in the sense that the same HIL specification(modulo changing the attribute names) is uniformly applicable not only between FFIEC andSEC; but also between FFIEC and LEI; and between LEI and SEC. Our approach used …,Proceedings of the Second International Workshop on Data Science for Macro-Modeling,2016,1
Entity resolution between datasets,*,Embodiments relate to entity resolution. One aspect includes creating a deterministic modelby defining an entity to be resolved; selecting two datasets for comparison; definingmatching predicates for attributes of the datasets to select a set of candidate matches; anddefining a precedence rule for the candidate matches to select a subset of the candidatematches. An aspect further includes running the deterministic model on the two datasets.Running the deterministic model includes applying the matching predicates and theprecedence rule to data in the datasets that correspond to the attributes. An aspect alsoincludes applying a cardinality rule to results of the running; and outputting the matchingcandidates for which the cardinality rule is satisfied.,*,2016,1
Mapping polymorphism-proofs,Ryan Wisnesky; Mauricio A Hernández; Lucian Popa,This technical report contains the proofs for the paper Mapping Polymorphism; by RyanWisnesky; Mauricio A. Hernandez; and Lucian Popa; which appears in the proceedings ofthe 13th International Conference on Database Theory; held March 22–25; 2010; inLausanne; Switzerland (ICDT'10).,*,2009,1
System and method for translating data from a source schema to a target schema,*,*,*,2003,1
The Clio Project: Managing Heterogeneity,CT Howard Ho; Ronald Fagin; Lucian Popa,Abstract Clio is a system for managing and facilitating the complex tasks of heterogeneousdata transformation and integration. In Clio; we have collected together a powerful set ofdata management techniques that have proven invaluable in tackling these difficultproblems. In this paper; we present the underlying themes of our approach and present abrief case study.,*,*,1
Active Learning for Large-Scale Entity Resolution,Kun Qian; Lucian Popa; Prithviraj Sen,Abstract Entity resolution (ER) is the task of identifying different representations of the samereal-world object across datasets. Designing and tuning ER algorithms is an error-prone;labor-intensive process; which can significantly benefit from data-driven; automated learningmethods. Our focus is on" big data''scenarios where the primary challenges include 1)identifying; out of a potentially massive set; a small subset of informative examples to belabeled by the user; 2) using the labeled examples to efficiently learn ER algorithms thatachieve both high precision and high recall; and 3) executing the learned algorithm todetermine duplicates at scale. Recent work on learning ER algorithms has employed activelearning to partially address the above challenges by aiming to learn ER rules in the form ofconjunctions of matching predicates; under precision guarantees. While successful in …,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,2017,*
Incremental information integration using a declarative framework,*,Embodiments of the present invention relate to a declarative framework for efficientincremental information integration. In one embodiment; a method of and computer programproduct for information integration is provided. An integration rule is received. A first data setis accessed. A first representation of the first data is generated set based on the plurality ofintegration rules. The first representation is flat and includes a plurality of records. At leastone index is generated. The index encodes at least one hierarchical relationship among theplurality of records. A second representation is generated of the first representation basedon the at least one index. The second representation comprising nested data.,*,2017,*
Expressive Power of Entity-Linking Frameworks,Douglas Burdick; Ronald Fagin; Phokion G Kolaitis; Lucian Popa; Wang-Chiew Tan,Abstract We develop a unifying approach to declarative entity linking by introducing thenotion of an entity linking framework and an accompanying notion of the certain links in sucha framework. In an entity linking framework; logic-based constraints are used to expressproperties of the desired link relations in terms of source relations and; possibly; in terms ofother link relations. The definition of the certain links in such a framework makes use ofweighted repairs and consistent answers in inconsistent databases. We demonstrate themodeling capabilities of this approach by showing that numerous concrete entity linkingscenarios can be cast as such entity linking frameworks for suitable choices of constraintsand weights. By using the certain links as a measure of expressive power; we investigate therelative expressive power of several entity linking frameworks and obtain sharp …,LIPIcs-Leibniz International Proceedings in Informatics,2017,*
Proceedings of the 18th International Conference on Extending Database Technology; Brussels; Belgium; March 23-27; 2015,Gustavo Alonso; Floris Geerts; Lucian Popa; Pablo Barcelo; Jens Teubner; Martin Ugarte; Jan van den Bussche; Jan Paredaens,Publication. Title; Proceedings of the 18th International Conference on ExtendingDatabase Technology; Brussels; Belgium; March 23-27; 2015 …,*,2015,*
Data Exchange,Lucian Popa,realized using such logical structures. For example; in tree based data acquisition protocols;a collection tree is built that is rooted at the data collection center such as the sink node [8].The dissemination of the data requests from the participating nodes and collection of datafrom the sensor nodes are accomplished using this tree. A cluster based data acquisitionmechanism has been proposed in [3]. As shown in Fig. 1; nodes are organized into a fixednumber of clusters; and nodes within each cluster dynamically elect a cluster head. The dataacquisition is carried out in two phases. In the first phase; cluster heads collect data fromtheir cluster nodes. In the second phase; cluster heads send collected data to the nodes thathave subscribed to the data. The cluster heads are re-elected to balance energyconsumption among the nodes in the cluster. Zhang et al.[13] have proposed an adaptive …,*,2009,*
A PC Chase,Lucian Popa; Val Tannen,Abstract PC stands for path-conjunctive; the name of a class of queries and dependenciesthat we define over complex values with dictionaries. This class includes the relationalconjunctive queries and embedded dependencies; as well as many interesting examples ofcomplex value and oodb queries and integrity constraints. We show that some importantclassical results on containment; dependency implication; and chasing extend andgeneralize to this class.,*,1998,*
Input/output: algorithms and architectures,Lucian Popa,Abstract Modern processors are improving their speed at a very fast rate. Memory designtechnologies try to keep pace with rapid evolution of processors; and combined with cachingtechniques; they partly succeed. However; disk speed is not improving at the same rate. Onthe contrary; seek time; latency and bandwidth are almost the same as ten years ago; andthere is no much hope for the future. To prevent I/O to become a signi cant slowdown incomputer systems; alternative solutions must be found. The paper presents an overview ofsoftware and architectural techniques that try to overcome the I/O problem: disk arrays(RAID); external-memory algorithms; interconnection networks; parallel I/O; in their relationwith uniprocessor and multiprocessor architectures.,*,1998,*
Data Exchange: Semantics and Query Answering,Lucian Popa,Abstract Data exchange is the problem of taking data structured under a source schema andcreating an instance of a target schema that reflects the source data as accurately aspossible. In this paper; we address foundational and algorithmic issues related to thesemantics of data exchange and to the query answering problem in the context of dataexchange. These issues arise because; given a source instance; there may be many targetinstances that satisfy the constraints of the data exchange problem. We give an algebraicspecification that selects; among all solutions to the data exchange problem; a special classof solutions that we call universal. We show that a universal solution has no more and noless data than required for data exchange and that it represents the entire space of possiblesolutions. We then identify fairly general; and practical; conditions that guarantee the …,*,*,*
Data Engineering,Jens Bleiholder; Melanie Herschel; Felix Naumann; Lukasz Golab; Flip Korn; Divesh Srivastava; Arvind Arasu; Surajit Chaudhuri; Zhimin Chen; Kris Ganjam; Raghav Kaushik; Vivek Narasayya; Doug Burdick; Mauricio Hernandez; Howard Ho; Georgia Koutrika; Rajasekar Krishnamurthy; Lucian Popa; Ioana R Stanoi; Shivakumar Vaithyanathan; Sanjiv Das,The Data Engineering Bulletin The Bulletin of the Technical Committee on Data Engineeringis published quarterly and is distributed to all TC members. Its scope includes the design;implementation; modelling; theory and application of database systems and theirtechnology. Letters; conference information; and news should be sent to the Editor-in-Chief.Papers for each issue are solicited by and should be sent to the Associate Editorresponsible for the issue. Opinions expressed in contributions are those of the authors anddo not necessarily reflect the positions of the TC on Data Engineering; the IEEE ComputerSociety; or the authors' organizations. The Data Engineering Bulletin web site is at http://tab.computer. org/tcde/bull_about. html.,*,*,*
Mapping Generation and Data Translation of Heterogeneous,L Popa; Y Velegrakis; RJ Miller; M Hernéandez; R Fagin,*,*,*,*
Eighth International Symposium of Parallel Architectures; Algorithms and Networks,V Shestak; H Siegel; A Maciejewski; S Ali; K Iwama; M Hernandez; H Ho; F Naumann; L Popa; S Hsieh; C Lin; Y Chung; C Tang; N Thibault; C Laforest; L Higham; J Kawash; P Ha; M Papatriantafilou; P Tsigas; P Shareghi; H Sarbazi-Azad; Y Kikuchi; T Araki; M Yang; T Li; Y Jiang; Y Yang,Page 1. v Table of Contents Eighth International Symposium of Parallel Architectures;Algorithms and Networks I-SPAN 2005 Message from the General Chair _____ xiConference _____xii Invited Talk I Robust Resource Allocations in Parallel ComputingSystems: Model and Heuristics _____ 2 V. Shestak; H. Siegel; A. Maciejewski; and S. AliInvited Talk II Classic and Quantum Network Coding _____ 10 K. Iwama Invited Talk IIIClio: A Schema Mapping Tool for Information Integration_____ 11 M. Hernandez; H. Ho;F. Naumann; and L. Popa Technical Sessions Session 1A: Algorithms I …,*,*,*
Data Engineering,Ronald Fagin; Ariel Fuxman; Laura M Haas; Mauricio A Hernández; Howard Ho; Anastasios Kementsietsidis; Renée J Miller; Felix Nauman; Lucian Popa; Yannis Velegrakis; Charlotte Vilarem; Ling-Ling Yan; Andrea Calı; Diego Calvanese; Giuseppe De Giacomo; Maurizio Lenzerini,Bulletin of the Technical Committee on Data Engineering September 2002 Vol. 25 No. 3 IEEEComputer Society Letters Letter from the Editor-in-Chief...................................................... DavidLomet 1 Letter from the Special Issue Editor.................................................. Renée J. Miller 2 SpecialIssue on Integration Management Data Integration: Where Does the Time Go?...... LenSeligman; Arnon Rosenthal; Paul Lehner; Angela Smith 3 Integration Through a Practitioner'sEye.... Srinivasa Narayanan; Subbu N. Subramanian and the Tavant Team 11 Data … EditorialBoard Editor-in-Chief David B. Lomet Microsoft Research One Microsoft Way; Bldg. 9 RedmondWA 98052-6399 lomet@ microsoft. com Associate Editors Umeshwar Dayal Hewlett-PackardLaboratories 1501 Page Mill Road; MS 1142 Palo Alto; CA 94304 Johannes Gehrke Departmentof Computer Science Cornell University Ithaca; NY 14853 Christian S. Jensen …,Urbana,*,*
Data Exchange: Semantics and Query Answering,Ronald Fagin Phokion G Kolaitis; Renée J Miller; Lucian Popa,Abstract Data exchange is the problem of taking data structured under a source schema andcreating an instance of a target schema that reflects the source data as accurately aspossible. In this paper; we address foundational and algorithmic issues related to thesemantics of data exchange and to the query answering problem in the context of dataexchange. These issues arise because; given a source instance; there may be many targetinstances that satisfy the constraints of the data exchange problem. We give an algebraicspecification that selects; among all solutions to the data exchange problem; a special classof solutions that we call universal. We show that a universal solution has no more and noless data than required for data exchange and that it represents the entire space of possiblesolutions. We then identify fairly general; yet practical; conditions that guarantee the …,*,*,*
EII and ETL Unification---It’s about Time!,Howard Ho; Joint Mauricio Hern; Lucian Popa,Integration; transformation and exchange of data are well known and fast growing problems.Solving the integration problem typically requires:(1) understanding and reconciling thedifferences between various schemas; and (2) generation of mappings between theseschemas; which will then drive any subsequent processing of data. The combination of steps(1) and (2) is a labor-intensive and error-prone process. Our research group has years ofexpertise building the Clio schema mapping system [1; 2] that semi-automatically guidesusers towards the creation and deployment of schema mappings. We investigated andimplemented methods for generating transformation queries given a schema mapping (forboth relational and XML schemas) and methods for discovering such mappings (schemamatching). We also studied the foundational aspects that underlie schema mappings and …,*,*,*
Mappings as Building Blocks for Complex Integration,Lucian Popa,Page 1. Bertinoro; INFINT Workshop; Sep 30 - Oct 4 2007 1 Mappings as Building Blocks forComplex Integration Lucian Popa IBM Almaden Research Center Page 2. 2 Outline Schemamapping research: what can we do now ∎ Brief description of Clio What could be next: ∎ Flowsof mappings and transformations ∎ How can we increase automation in creating such flows ? ∎Mappings as reusable objects ∎ Repository of common mappings and types Disclaimer: ∎ thisis a very “mapping-biased” look at integration ☺ ∎ there are plenty of important pieces left out …Result of several discussions with: L. Haas; M. Hernandez; H. Ho; P.Kolaitis; R. Miller; M. Gubanov;H. Pirahesh; IRStanoi; … Page 3. 3 Schema Mappings (Recap) A schema mapping is a usefulabstraction that specifies a relationship between a source schema and a target schema ∎ Canbe used for: data transformation; query answering; to represent …,*,*,*
