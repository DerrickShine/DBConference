Putting lipstick on pig: Enabling database-style workflow provenance,Yael Amsterdamer; Susan B Davidson; Daniel Deutch; Tova Milo; Julia Stoyanovich; Val Tannen,Abstract Workflow provenance typically assumes that each module is a" black-box"; so thateach output depends on all inputs (coarse-grained dependencies). Furthermore; it does notmodel the internal state of a module; which can change between repeated executions. Inpractice; however; an output may depend on only a small subset of the inputs (fine-graineddependencies) as well as on the internal state of the module. We present a novelprovenance framework that marries database-style and workflow-style provenance; by usingPig Latin to expose the functionality of modules; thus capturing internal state and fine-grained dependencies. A critical ingredient in our solution is the use of a novel form ofprovenance graph that models module invocations and yields a compact representation offine-grained workflow provenance. It also enables a number of novel graph …,Proceedings of the VLDB Endowment,2011,115
Provenance for aggregate queries,Yael Amsterdamer; Daniel Deutch; Val Tannen,Abstract We study in this paper provenance information for queries with aggregation.Provenance information was studied in the context of various query languages that do notallow for aggregation; and recent work has suggested to capture provenance by annotatingthe different database tuples with elements of a commutative semiring and propagating theannotations through query evaluation. We show that aggregate queries pose novelchallenges rendering this approach inapplicable. Consequently; we propose a newapproach; where we annotate with provenance information not just tuples but also theindividual values within tuples; using provenance to describe the values computation. Werealize this approach in a concrete construction; first for" simple" queries where theaggregation operator is the last one applied; and then for arbitrary (positive) relational …,Proceedings of the thirtieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2011,90
Circuits for Datalog Provenance.,Daniel Deutch; Tova Milo; Sudeepa Roy; Val Tannen,ABSTRACT The annotation of the results of database queries with provenance informationhas many applications. This paper studies provenance for datalog queries. We start byconsidering provenance representation by (positive) Boolean expressions; as pioneered inthe theories of incomplete and probabilistic databases. We show that even for linear datalogprograms the representation of provenance using Boolean expressions incurs a super-polynomial size blowup in data complexity. We address this with an approach that is novel inprovenance studies; showing that we can construct in PTIME poly-size (data complexity)provenance representations as Boolean circuits. Then we present optimization techniquesthat embed the construction of circuits into seminaive datalog evaluation; and further reducethe size of the circuits. We also illustrate the usefulness of our approach in multiple …,ICDT,2014,32
On provenance minimization,Yael Amsterdamer; Daniel Deutch; Tova Milo; Val Tannen,Abstract Provenance information has been proved to be very effective in capturing thecomputational process performed by queries; and has been used extensively as the input tomany advanced data management tools (eg; view maintenance; trust assessment; or queryanswering in probabilistic databases). We observe here that while different (set-) equivalentqueries may admit different provenance expressions when evaluated on the samedatabase; there is always some part of these expressions that is common to all. We refer tothis part as the core provenance. In addition to being informative; the core provenance isalso useful as a compact input to the aforementioned data management tools. We formallydefine the notion of core provenance. We study algorithms that; given a query; compute anequivalent (called p-minimal) query that for every input database; the provenance of …,ACM Transactions on Database Systems (TODS),2012,32
On the Limitations of Provenance for Queries with Difference.,Yael Amsterdamer; Daniel Deutch; Val Tannen,Abstract The annotation of the results of database transformations was shown to be veryeffective for various applications. Until recently; most works in this context focused onpositive query languages. The provenance semirings is a particular approach that wasproven effective for these languages; and it was shown that when propagating provenancewith semirings; the expected equivalence axioms of the corresponding query languages aresatisfied. There have been several attempts to extend the framework to account for relationalalgebra queries with difference. We show here that these suggestions fail to satisfy someexpected equivalence axioms (that in particular hold for queries on “standard” set and bagdatabases). Interestingly; we show that this is not a pitfall of these particular attempts; butrather every such attempt is bound to fail in satisfying these axioms; for some semirings …,TaPP,2011,29
Querying structural and behavioral properties of business processes,Daniel Deutch; Tova Milo,Abstract BPQL is a novel query language for querying business process specifications;introduced recently in [5; 6]. It is based on an intuitive model of business processes asrewriting systems; an abstraction of the emerging BPEL (Business Process ExecutionLanguage) standard [7]. BPQL allows users to query business processes visually; in amanner very analogous to the language used to specify the processes. The goal of thepresent paper is to study the formal model underlying BPQL and investigate its properties aswell as the complexity of query evaluation. We also study its relationship to previouslysuggested formalisms for process modeling and querying. In particular we propose a queryevaluation algorithm of polynomial data complexity that can be applied uniformly to querieson the structure of the process specification as well as on the potential behavior of the …,International Symposium on Database Programming Languages,2007,28
Caravan: Provisioning for What-If Analysis.,Daniel Deutch; Zachary G Ives; Tova Milo; Val Tannen,ABSTRACT Problems of what-if analysis (such as hypothetical deletions; insertions; andmodifications) over complex analysis queries are increasingly commonplace; eg; in forminga business strategy or looking for causal relationships in science. Here; data analysts aretypically interested only in task-specific views of the data; and they expect to be able tointeractively manipulate the data in a natural and seamless way—possibly on a phone ortablet; and possibly via a spreadsheet or similar interface without having to carry the fullmachinery of a DBMS. The Caravan system enables what-if analysis: fast; lightweight;interactive exploration of alternative answers; within views computed over large-scaledistributed data sources. Our novel approach is based on creating dedicated provisionedautonomous representations; or PARs. PARs are compiled out of the data; initial analysis …,CIDR,2013,24
On probabilistic fixpoint and markov chain query languages,Daniel Deutch; Christoph Koch; Tova Milo,Abstract We study highly expressive query languages such as datalog; fixpoint; and while-languages on probabilistic databases. We generalize these languages such thatcomputation steps (eg datalog rules) can fire probabilistically. We define two possiblesemantics for such query languages; namely inflationary semantics where the results ofeach computation step are added to the current database and noninflationary queries thatinduce a random walk in-between database instances. We then study the complexity ofexact and approximate query evaluation under these semantics.,Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2010,23
Type inference and type checking for queries on execution traces,Daniel Deutch; Tova Milo,Abstract This paper studies; for the first time; the management of type information for animportant class of semi-structured data: nested DAGs (Directed Acyclic Graphs) thatdescribe execution traces of business processes (BPs for short). Specifically; we considerhere type inference and type checking for queries over BP execution traces. The queries thatwe consider select portions of the traces that are of interest to the user; the types describethe possible shape of the execution traces in the input/output of the query. We formallydefine and characterize here three common classes of BP execution traces and theirrespective notions of type inference and type checking. We study the complexity of the twoproblems for query languages of varying expressive power and present efficient typeinference/checking algorithms whenever possible. Our analysis offers a nearly complete …,Proceedings of the VLDB Endowment,2008,22
Evaluating top-k queries over business processes,Daniel Deutch; Tova Milo,A Business Process (BP) consists of some business activities undertaken by one or moreorganizations in pursuit of some business goal. Tools for querying and analyzing BPspecifications are extremely valuable for companies as they allow to optimize the BP;identify potential problems; and reduce operational costs. In particular; given a BPspecification; identifying the top-k execution flows that are most likely to occur in practice outof those satisfying the query criteria; is crucial for various applications. To address this need;we introduce in this paper the notion of {\em likelihood} for BP execution flows; and study top-k query evaluation (finding the $ k $ most likely matches) for queries over BP specifications.We analyze the complexity of query evaluation in this context and present novel algorithmsfor computing top-k query results. To our knowledge; this is the first paper that studies …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,19
Top-k projection queries for probabilistic business processes,Daniel Deutch; Tova Milo,Abstract A Business Process (BP) consists of some business activities undertaken by one ormore organizations in pursuit of some business goal. Tools for querying and analyzing BPspecifications are extremely valuable for companies. In particular; given a BP specification;identifying the top-k flows that are most likely to occur in practice; out of those satisfying agiven query criteria; is crucial for various applications such as personalized advertizementand BP web-site design. This paper studies; for the first time; top-k query evaluation forqueries with projection in this context. We analyze the complexity of the problem for differentclasses of distribution functions for the flows likelihood; and provide efficient (PTIME)algorithms whenever possible. Furthermore; we show an interesting application of ouralgorithms to the analysis of BP execution traces (logs); for recovering missing …,Proceedings of the 12th International Conference on Database Theory,2009,18
Declarative platform for data sourcing games,Daniel Deutch; Ohad Greenshpan; Boris Kostenko; Tova Milo,Abstract Harnessing a crowd of users for the collection of mass data (data sourcing) hasrecently become a wide-spread practice. One effective technique is based on games as atool that attracts the crowd to contribute useful facts. We focus here on the data managementlayer of such games; and observe that the development of this layer involves challengessuch as dealing with probabilistic data; combined with recursive manipulation of this data.These challenges are difficult to address using current declarative data managementframework works; and we thus propose here a novel such framework; and demonstrate itsusefulness in expressing different aspects in the data management of Trivia-like games. Wehave implemented a system prototype with our novel data management framework at itscore; and we highlight key issues in the system design; as well as our experimentations …,Proceedings of the 21st international conference on World Wide Web,2012,16
A quest for beauty and wealth (or; business processes for database researchers),Daniel Deutch; Tova Milo,Abstract While classic data management focuses on the data itself; research on BusinessProcesses considers also the context in which this data is generated and manipulated;namely the processes; the users; and the goals that this data serves. This allows theanalysts a better perspective of the organizational needs centered around the data. As such;this research is of fundamental importance. Much of the success of database systems in thelast decade is due to the beauty and elegance of the relational model and its declarativequery languages; combined with a rich spectrum of underlying evaluation and optimizationtechniques; and efficient implementations. This; in turn; has lead to an economic wealth forboth the users and vendors of database systems. Similar beauty and wealth are sought for inthe context of Business Processes. Much like the case for traditional database research …,Proceedings of the thirtieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2011,16
Using markov chain monte carlo to play trivia,Daniel Deutch; Ohad Greenshpan; Boris Kostenko; Tova Milo,We introduce in this Demonstration a system called Trivia Masster that generates a verylarge Database of facts in a variety of topics; and uses it for question answering. The factsare collected from human users (the “crowd”); the system motivates users to contribute to theDatabase by using a Trivia Game; where users gain points based on their contribution. Akey challenge here is to provide a suitable Data Cleaning mechanism that allows to identifywhich of the facts (answers to Trivia questions) submitted by users are indeedcorrect/reliable; and consequently how many points to grant users; how to answer questionsbased on the collected data; and which questions to present to the Trivia players; in order toimprove the data quality. As no existing single Data Cleaning technique provides asatisfactory solution to this challenge; we propose here a novel approach; based on a …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,13
A Model for Fine-Grained Data Citation.,Susan B Davidson; Daniel Deutch; Tova Milo; Gianmaria Silvello,ABSTRACT An increasing amount of information is being collected in structured; evolving;curated databases; driving the question of how information extracted from such datasets viaqueries should be cited. Unlike traditional research products; such books and journals;which have a fixed granularity; data citation is a challenge because the granularity varies.Different portions of the database; with varying granularity; may have different citations.Furthermore; there are an infinite number of queries over a database; each accessing andgenerating different subsets of the database; so we cannot hope to explicitly attach a citationto every possible result set and/or query. We present the novel problem of automaticallygenerating citations for general queries over a relational database; and explore a solutionbased on a set of citation views; each of which attaches a citation to a view of the …,CIDR,2017,11
Deduction with contradictions in datalog,Serge Abiteboul; Daniel Deutch; Victor Vianu,We study deduction in the presence of inconsistencies. Following previous works; wecapture deduction via datalog programs and inconsistencies through violations of functionaldependencies (FDs). We study and compare two semantics for datalog with FDs: the first; ofa logical nature; is based on inferring facts one at a time; while never violating the FDs; thesecond; of an operational nature; consists in a fixpoint computation in which maximal sets offacts consistent with the FDs are inferred at each stage. Both semantics arenondeterministic; yielding sets of possible worlds. We introduce a PTIME (in the size of theextensional data) algorithm; that given a datalog program; a set of FDs and an inputinstance; produces a c-table representation of the set of possible worlds. Then; we proposeto quantify nondeterminism with probabilities; by means of a probabilistic semantics. We …,International Conference on Database Theory,2014,11
Optimal top-k query evaluation for weighted business processes,Daniel Deutch; Tova Milo; Neoklis Polyzotis; Tom Yam,Abstract A Business Process (BP for short) consists of a set of activities that achieve somebusiness goal when combined in a flow. Among all the (maybe infinitely many) possibleexecution flows of a BP; analysts are often interested in identifying flows that are" mostimportant"; according to some weight metric. This paper studies the following problem: givena specification of such a BP; a weighting function over BP execution flows; a query; and anumber k; identify the k flows with the highest weight among those satisfying the query. Weprovide here; for the first time; a provably optimal algorithm for identifying the top-k weightedflows of a given BP; and use it for efficient top-k query evaluation.,Proceedings of the VLDB Endowment,2010,11
A provenance framework for data-dependent process analysis,Daniel Deutch; Yuval Moskovitch; Val Tannen,Abstract A data-dependent process (DDP) models an application whose control flow isguided by a finite state machine; as well as by the state of an underlying database. DDPsare commonly found eg; in e-commerce. In this paper we develop a framework supportingthe use of provenance in static (temporal) analysis of possible DDP executions. Usingprovenance support; analysts can interactively test and explore the effect of hypotheticalmodifications to a DDP's state machine and/or to the underlying database. They can alsoextend the analysis to incorporate the propagation of annotations from meta-domains ofinterest; eg; cost or access privileges. Toward this goal we note that the framework ofsemiring-based provenance was proven highly effective in fulfilling similar needs in thecontext of database queries. In this paper we consider novel constructions that generalize …,Proceedings of the VLDB Endowment,2014,10
Approximated summarization of data provenance,Eleanor Ainy; Pierre Bourhis; Susan B Davidson; Daniel Deutch; Tova Milo,Abstract Many modern applications involve collecting large amounts of data from multiplesources; and then aggregating and manipulating it in intricate ways. The complexity of suchapplications; combined with the size of the collected data; makes it difficult to understandhow the resulting information was derived. Data provenance has proven helpful in thisrespect; however; maintaining and presenting the full and exact provenance informationmay be infeasible due to its size and complexity. We therefore introduce the notion ofapproximated summarized provenance; which provides a compact representation of theprovenance at the possible cost of information loss. Based on this notion; we present a novelprovenance summarization algorithm which; based on the semantics of the underlying dataand the intended use of provenance; outputs a summary of the input provenance …,Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,2015,9
Finding optimal probabilistic generators for XML collections,Serge Abiteboul; Yael Amsterdamer; Daniel Deutch; Tova Milo; Pierre Senellart,Abstract We study the problem of; given a corpus of XML documents and its schema; findingan optimal (generative) probabilistic model; where optimality here means maximizing thelikelihood of the particular corpus to be generated. Focusing first on the structure ofdocuments; we present an efficient algorithm for finding the best generative probabilisticmodel; in the absence of constraints. We further study the problem in the presence ofintegrity constraints; namely key; inclusion; and domain constraints. We study in this casetwo different kinds of generators. First; we consider a continuation-test generator thatperforms; while generating documents; tests of schema satisfiability; these tests prevent fromgenerating a document violating the constraints but; as we will see; they are computationallyexpensive. We also study a restart generator that may generate an invalid document and …,Proceedings of the 15th International Conference on Database Theory,2012,9
Navigating in complex mashed-up applications,Daniel Deutch; Ohad Greenshpan; Tova Milo,Abstract Mashups integrate a set of Web-services and data sources; often referred to asmashlets. We study in this paper a common scenario where these mashlets are componentsof larger Web-Applications. In this case; integration of mash-lets yields a set of inter-connected applications; referred to as Mashed-up Applications (abbr. MashAPP). Whileinteractions between the mashlets enrich the individual applications; they also rendernavigation within them more intricate for the user; as actions in one application may affectothers. To assist users in their navigation through MashAPPs we provide a solution basedon a simple; generic model for MashAPPs and navigation flows within them. Queries overthe model allow users to describe navigation flows of interest; and an effective queryevaluation algorithm provides users with recommendations on how to navigate within the …,Proceedings of the VLDB Endowment,2010,9
Goal-oriented web-site navigation for on-line shoppers,Daniel Deutch; Tova Milo; Tom Yam,Abstract Web-sites for on-line shopping typically offer a vast number of product options andcombinations thereof. While this is very useful; it often makes the navigation in the site andthe identification of the" ideal" purchase (where the notion of ideal differs among users) aconfusing; non-trivial experience. This demonstration presents ShopIT (ShoppIng assitanT);a system that assists on-line shoppers by suggesting the most effective navigation paths fortheir specified criteria and preferences. The suggestions are continually adapted tochoices/decisions taken by the users while navigating. ShopITis based on a set of novel;adaptive; provably optimal algorithms for TOP-K query evaluation.,Proceedings of the VLDB Endowment,2009,9
Selective provenance for datalog programs using top-k queries,Daniel Deutch; Amir Gilad; Yuval Moskovitch,Abstract Highly expressive declarative languages; such as datalog; are now commonly usedto model the operational logic of data-intensive applications. The typical complexity of suchdatalog programs; and the large volume of data that they process; call for result explanation.Results may be explained through the tracking and presentation of data provenance; andhere we focus on a detailed form of provenance (how-provenance); defining it as the set ofderivation trees of a given fact. While informative; the size of such full provenanceinformation is typically too large and complex (even when compactly represented) to allowdisplaying it to the user. To this end; we propose a novel top-k query language for queryingdatalog provenance; supporting selection criteria based on tree patterns and ranking basedon the rules and database facts used in derivation. We propose an efficient novel …,Proceedings of the VLDB Endowment,2015,8
A structural/temporal query language for business processes,Daniel Deutch; Tova Milo,Abstract A Business Process consists of multiple business activities; which; when combinedin a flow; achieve some particular goal. These processes usually operate in a distributedenvironment and the software implementing them is fairly complex. Thus; effective tools foranalysis of the possible executions of such processes are extremely important forcompanies (Beeri et al.; 2006; 2007 [4; 5]);(Deutch and Milo; 2008 [13]); these tools canallow to debug and optimize the processes; and to make an optimal use of them. The goal ofthe present paper is to consider a formal model underlying Business Processes and studyquery languages over such processes. We study in details the relationship of the proposedmodel with previously suggested formalisms for processes modeling and querying. Inparticular we propose a query evaluation algorithm of polynomial data complexity that …,Journal of Computer and System Sciences,2012,8
Provenance-based analysis of data-centric processes,Daniel Deutch; Yuval Moskovitch; Val Tannen,Abstract We consider in this paper static analysis of the possible executions of data-dependent applications; namely applications whose control flow is guided by a finite-statemachine; as well as by the state of an underlying database. We note that previous work inthis context has not addressed two important features of such analysis; namely analysisunder hypothetical scenarios; such as changes to the application's state machine and/or tothe underlying database; and the consideration of meta-data; such as cost or accessprivileges. Observing that semiring-based provenance has been proven highly effective insupporting these two features for database queries; we develop in this paper a semiring-based provenance framework for the analysis of data-dependent processes; accounting forhypothetical reasoning and meta-data. The development addresses two interacting new …,The VLDB Journal,2015,7
Business Processes: A Database Perspective,Daniel Deutch; Tova Milo,Abstract While classic data management focuses on the data itself; research on BusinessProcesses also considers the context in which this data is generated and manipulated;namely the processes; users; and goals that this data serves. This provides the analysts abetter perspective of the organizational needs centered around the data. As such; thisresearch is of fundamental importance. Much of the success of database systems in the lastdecade is due to the beauty and elegance of the relational model and its declarative querylanguages; combined with a rich spectrum of underlying evaluation and optimizationtechniques; and efficient implementations. Much like the case for traditional databaseresearch; elegant modeling and rich underlying technology are likely to be highlybeneficiary for the Business Process owners and their users; both can benefit from easy …,Synthesis Lectures on Data Management,2012,5
Mob data sourcing,Daniel Deutch; Tova Milo,Abstract Crowdsourcing is an emerging paradigm that harnesses a mass of users to performvarious types of tasks. We focus in this tutorial on a particular form of crowdsourcing; namelycrowd (or mob) datasourcing whose goal is to obtain; aggregate or process data. Weoverview crowd datasourcing solutions in various contexts; explain the need for a principledsolution; describe advances towards achieving such a solution; and highlight remaininggaps.,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,5
Navigating through mashed-up applications with compass,Daniel Deutch; Ohad Greenshpan; Tova Milo,Mashups integrate a set of complementary Web-services and data sources; often referred toas mashlets. We consider here a common scenario where the integrated mashlets are partof larger Web-applications; and their integration yields a set of inter-connected applications.We refer to them as Mashed-up Applications (abbr. MashAPP). The inter-connectionsbetween the mashlets enrich the individual Web-applications; but at the same time make theuser navigation within them more intricate as actions in one application may affect others. Toaddress this difficulty; we present COMPASS; a system that assists users in their navigationthrough MashAPPs. The system employs a novel top-k algorithm to propose users the mosteffective navigation paths for their specified goals. The suggestions are continually adaptedto choices taken by the users while navigating.,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,5
Querying and monitoring distributed business processes,Tova Milo; Daniel Deutch,Abstract A business process (BP for short) consists of a group of business activitiesundertaken by one or more organizations in pursuit of some particular goal. It usuallyoperates in a cross-organization; distributed environment and the software implementing it isfairly complex. Standards facilitate the design; deployment; and execution of BPs. Inparticular; the recent BPEL standard (Business Process Execution Language); provides anXML-based language to describe the interface between the participants in a process; as wellas the full operational logic of the process and its execution flow. BPEL specifications areautomatically compiled into executable code that implements the described BP and runs ona BPEL application server. Processes execution is traced; and their run-time behavior canbe recorded in standard XML formats.,Proceedings of the VLDB Endowment,2008,5
Possible and certain answers for queries over order-incomplete data,Antoine Amarilli; Mouhamadou Lamine Ba; Daniel Deutch; Pierre Senellart,Abstract: To combine and query ordered data from multiple sources; one needs to handleuncertainty about the possible orderings. Examples of such" order-incomplete" data includeintegrated event sequences such as log entries; lists of properties (eg; hotels andrestaurants) ranked by an unknown function reflecting relevance or customer ratings; anddocuments edited concurrently with an uncertain order on edits. This paper introduces aquery language for order-incomplete data; based on the positive relational algebra withorder-aware accumulation. We use partial orders to represent order-incomplete data; andstudy possible and certain answers for queries in this context. We show that these problemsare respectively NP-complete and coNP-complete; but identify many tractable casesdepending on the query operators or input partial orders. Comments: 55 pages; 5 figures …,arXiv preprint arXiv:1707.07222,2017,4
Qplain: Query by explanation,Daniel Deutch; Amir Gilad,To assist non-specialists in formulating database queries; multiple frameworks thatautomatically infer queries from a set of input and output examples have been proposed.While highly useful; a shortcoming of the approach is that if users can only provide a smallset of examples; many inherently different queries may qualify. We observe that additionalinformation about the examples; in the form of their explanations; is useful in significantlyfocusing the set of qualifying queries. We propose to demonstrate QPlain; a system thatlearns conjunctive queries from examples and their explanations. We capture explanationsof different levels of granularity and detail; by leveraging recently developed models for dataprovenance. Explanations are fed through an intuitive interface; are compiled to theappropriate provenance model; and are then used to derive proposed queries. We will …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,4
selp: Selective tracking and presentation of data provenance,Daniel Deutch; Amir Gilad; Yuval Moskovitch,Highly expressive declarative languages; such as Datalog; are now commonly used tomodel the operational logic of data-intensive applications. The typical complexity of suchDatalog programs; and the large volume of data that they process; call for the tracking andpresentation of data provenance. Provenance information is crucial for explaining andjustifying the Datalog program results. However; the size of full provenance information is inmany cases too large (and its concise representations are too complex) to allow itspresentation to the user. To this end; we propose a demonstration of selP; a system thatallows the selective presentation of provenance; based on user-specified top-k queries. Wewill demonstrate the usefulness of selP using a real-life program and data; in the context ofInformation Extraction.,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,4
Querying probabilistic business processes for sub-flows,Daniel Deutch,Abstract A Business Process (BP for short) consists of a set of activities which; combined in aflow; achieve some business goal. A given BP may have a large; possibly infinite; number ofpossible execution flows (EX-flows for short); each having some probability to occur at runtime. This paper studies query evaluation over such probabilistic BPs. We focus on twoimportant classes of queries; namely boolean queries that compute the probability that arandom EX-flow of a BP satisfies a given property; and projection queries focusing onportions of EX-flows that are of interest to the user. For the latter queries the answer consistsof the top-k instances of these portions that are most likely to occur at run-time. We study thecomplexity of query evaluation for both kinds of queries; showing in particular that projectionqueries may be harder to evaluate than boolean queries. We present a picture of which …,Theory of Computing Systems,2013,4
Type inference and type checking for queries over execution traces,Daniel Deutch; Tova Milo,Abstract We study here Type Inference and Type Checking for queries over the executiontraces of Business Processes. We define formal models for such execution traces; allowingto capture various realistic scenarios of partial information about these traces. We thendefine corresponding notions of types; and the problems of type inference and type checkingin this context. We further provide a comprehensive study of the decidability and complexityof these problems; in various cases; and suggest efficient algorithms where possible.,The VLDB Journal—The International Journal on Very Large Data Bases,2012,4
On models and query languages for probabilistic processes,Daniel Deutch; Tova Milo,Abstract Probabilistic processes appear naturally in various contexts; with applications toBusiness Processes; XML data management and more. Many models for specifying andquerying such processes exist in the literature; a main goal of research in this area is todesign models that are expressive enough to capture real-life processes and analysis tasks;but at the same time allow for efficient query evaluation. We depict the model established in[13; 16; 17; 18]; and claim that it achieves a good balance between expressivity and queryevaluation complexity. We compare and contrast the model with other common models forprobabilistic processes; highlighting the different choices made in models design and theireffect on expressivity and incurred complexity.,ACM SIGMOD Record,2010,4
Analyzing data-centric applications: Why; what-if; and how-to,Pierre Bourhis; Daniel Deutch; Yuval Moskovitch,We consider in this paper the analysis of complex applications that query and update anunderlying database in their operation. We focus on three classes of analytical questionsthat are important for application owners and users alike: Why was a result generated? Whatwould be the result if the application logic or database is modified in a particular way? Howcan one interact with the application to achieve a particular goal? Answering thesequestions efficiently is a fundamental step towards optimizing the application and its use.Noting that provenance was a key component in answering similar questions in the contextof database queries; we develop a provenance-based model and efficient algorithms forthese problems in the context of data-centric applications. Novel challenges here include thedynamic update of data; combined with the possibly complex workflows allowed by …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,3
Approximated Provenance for Complex Applications.,Eleanor Ainy; Susan B Davidson; Daniel Deutch; Tova Milo,Abstract Many applications now involve the collection of large amounts of data from multipleusers; and then aggregating and manipulating it in intricate ways. The complexity of suchapplications; combined with the size of the collected data; makes it difficult to understandhow information was derived; and consequently difficult to asses its credibility; to optimizeand debug its derivation; etc. Provenance has been helpful in achieving such goals indifferent contexts; and we illustrate its potential for novel complex applications such as thoseperforming crowd-sourcing. Maintaining (and presenting) the full and exact provenanceinformation may be infeasible for such applications; due to the size of the provenance and itscomplex structure. We propose some initial directions towards addressing this challenge;through the notion of approximated provenance.,TAPP,2014,3
PROPOLIS: provisioned analysis of data-centric processes,Daniel Deutch; Yuval Moskovitch; Val Tannen,Abstract We consider in this demonstration the (static) analysis of data-centric process-based applications; namely applications that depend on an underlying database and whosecontrol is guided by a finite state transition system. We observe that analysts of suchapplications often want to do more than analyze a specific instance of the application'sprocess control and database. In particular they want to interactively test and explore theeffect on analysis results of different hypothetical modifications applied to the application'stransition system and to the underlying database. To that end; we propose a demonstrationof PROPOLIS; a system for PROvisioned PrOcess anaLysIS; namely analysis of data-centricprocesses under hypothetical modification scenarios. Our solution is based on the notion ofa provisioned expression (which in turn is based on the notion of data provenance) …,Proceedings of the VLDB Endowment,2013,3
Top-k queries over web applications,Daniel Deutch; Tova Milo; Neoklis Polyzotis,Abstract The core logic of web applications that suggest some particular service; such asonline shopping; e-commerce etc.; is typically captured by Business Processes (BPs).Among all the (maybe infinitely many) possible execution flows of a BP; analysts are ofteninterested in identifying flows that are “most important”; according to some weight metric. Thegoal of the present paper is to provide efficient algorithms for top-k query evaluation over thepossible executions of Business Processes; under some given weight function. Uniquedifficulties in top-k analysis in this settings stem from (1) the fact that the number of possibleexecution flows of a given BP is typically very large; or even infinite in presence of recursionand (2) that the weights (eg; likelihood; monetary cost; etc.) induced by actions performedduring the execution (eg; product purchase) may be inter-dependent (due to probabilistic …,The VLDB Journal,2013,3
Provenance for Web 2.0 data,Meghyn Bienvenu; Daniel Deutch; Fabian M Suchanek,Abstract In this paper; we look at Web data that comes from multiple sources; as in the Web2.0. We argue that Web data is more than just its content. Rather; a piece of Web datacarries along different facets; such the transformations that data underwent; the differentperspectives that users have on the content; and the context in which a statement is made.We put forward the idea that provenance; ie the tracing of where data comes from; can helpus model these phenomena. We study how far existing approaches address the issue ofprovenance for Web data; and identify gaps and open problems.,Workshop on Secure Data Management,2012,3
Dealing with the Deep Web and all its Quirks.,Meghyn Bienvenu; Daniel Deutch; Davide Martinenghi; Pierre Senellart; Fabian M Suchanek,ABSTRACT Several approaches harvest; query; or combine Deep Web sources. Yet; inaddition to well-studied aspects of the problem such as query answering using views;access limitations; or top-k querying; the Deep Web exhibits a number of peculiarities thatare often neglected. First; the services usually deliver not all results; but only the top-nresults according to some ranking function. This function may not be compatible with theordering specified in a user's query. Subsequent results have to be obtained by paging; ormay not even be accessible. Second; the services may deliver results in a granularity that isincompatible with the query or joinable services (eg; months vs. exact dates). Moreover; theservices may perform selections or ranking over attributes that are not exposed in theresults: this poses an incompleteness problem. Additional challenges come from …,VLDS,2012,3
Querying web-based applications under models of uncertainty,Daniel Deutch; Tova Milo,Abstract Many businesses offer their services to customers via Web-based applicationinterfaces. Reasoning about execution flows of such applications is extremely valuable forcompanies. Such reasoning must often operate under terms of uncertainty and partialinformation; due to partial tracing; effects of unknown external parameters; and more. Theobjectives of this research are (1) to define models for capturing Web application executions;with partial information and uncertainly of various flavors;(2) to design algorithms that allowfor efficient reasoning over applications/execution traces under these models; and (3) toprovide practical implementations that exploit these sound theoretical foundations foreffective optimization of Web applications. We identify a restricted class of models thatcapture realistic scenarios; while allowing for an efficient query-based applications …,Proceedings of the VLDB Endowment,2008,3
Provenance for natural language queries,Daniel Deutch; Nave Frost; Amir Gilad,Abstract Multiple lines of research have developed Natural Language (NL) interfaces forformulating database queries. We build upon this work; but focus on presenting a highlydetailed form of the answers in NL. The answers that we present are importantly based onthe provenance of tuples in the query result; detailing not only the results but also theirexplanations. We develop a novel method for transforming provenance information to NL; byleveraging the original NL query structure. Furthermore; since provenance information istypically large and complex; we present two solutions for its effective presentation as NL text:one that is based on provenance factorization; with novel desiderata relevant to the NL case;and one that is based on summarization. We have implemented our solution in an end-to-end system supporting questions; answers and provenance; all expressed in NL. Our …,Proceedings of the VLDB Endowment,2017,2
Nlprov: Natural language provenance,Daniel Deutch; Nave Frost; Amir Gilad,Abstract We propose to present NLProv: an end-to-end Natural Language (NL) interface fordatabase queries. Previous work has focused on interfaces for specifying NL questions;which are then compiled into queries in a formal language (eg SQL). We build upon thiswork; but focus on presenting a detailed form of the answers in Natural Language. Theanswers that we present are importantly based on the provenance of tuples in the queryresult; detailing not only which are the results but also their explanations. We develop anovel method for transforming provenance information to NL; by leveraging the original NLquestion structure. Furthermore; since provenance information is typically large; we presenttwo solutions for its effective presentation as NL text: one that is based on provenancefactorization with novel desiderata relevant to the NL case; and one that is based on …,Proceedings of the VLDB Endowment,2016,2
Learning queries from examples and their explanations,Daniel Deutch; Amir Gilad,ABSTRACT To assist non-specialists in formulating database queries; multiple frameworksthat automatically infer queries from a set of examples have been proposed. While highlyuseful; a shortcoming of the approach is that if users can only provide a small set ofexamples; many inherently different queries may qualify; and only some of these actuallymatch the user intentions. Our main observation is that if users further explain theirexamples; the set of qualifying queries may be significantly more focused. To captureexplanations; we leverage previously developed models of data provenance; and inparticular their “embedding" in the model of provenance semirings. An important advantageis that the obtained problem definition is generic and allows plugging-in explanation modelsof different levels of detail and granularity. We highlight several modeling and …,CoRR; abs/1602.03819,2016,2
Provenance for nondeterministic order-aware queries,Antoine Amarilli; LM Ba; Daniel Deutch; Pierre Senellart,ABSTRACT Data transformations that involve (partial) ordering; and consolidate data inpresence of uncertainty; are common in the context of various applications. The complexityof such transformations; in addition to the possible presence of meta-data; call forprovenance support. We introduce; for the first time; a framework that accounts for theconjunction of these needs. To this end; we enrich the positive relational algebra with order-aware operators; some of which are non-deterministic; accounting for uncertainty. We studythe expressive power and the complexity of deciding possibility for the obtained language.We then equip the language with (semiring-based) provenance tracking and highlight theunique challenges in supporting provenance for the order-aware operations. We explainhow to overcome these challenges; designing a new provenance structure and a …,Preprint: http://a3nm. net/publications/amarilli2014provenance. pdf,2014,2
Propolis: Provisioned analysis of data-centric processes (demo,Daniel Deutch; Yuval Moskovitch; Val Tannen,Abstract We consider in this demonstration the (static) analysis of data-centric process-based applications; namely applications that depend on an underlying database and whosecontrol is guided by a finite state transition system. We observe that analysts of suchapplications often want to do more than analyze a specific instance of the application's pro-cess control and database. In particular they want to in-teractively test and explore the effecton analysis results of different hypothetical modifications applied to the appli-cation'stransition system and to the underlying database. To that end; we propose a demonstrationof PROPOLIS; a system for PROvisioned PrOcess anaLysIS; namely analy-sis of data-centric processes under hypothetical modification scenarios. Our solution is based on thenotion of a provi-sioned expression (which in turn is based on the notion of data …,In VLDB,2013,2
Querying probabilistic execution traces,Daniel Deutch; Tova Milo,ABSTRACT Many businesses offer their services to customers via webbased applicationinterfaces. Since different users may interact with a given application in different ways; theexecution flow of the underlying business process may vary between executions.Knowledge about common execution flows is extremely valuable for companies: it can beused to optimize business processes; employ targeted advertisement; reduce operationalcosts; and ultimately increase competitiveness. We present and study in this paper a novelprobabilistic model that allows to describe the likelihood of the possible execution flows of agiven business process. Based on this model; we suggest efficient algorithms to identify thetopk most common execution flows. We also propose a query language that allows users torestrict attention to execution flows that satisfy certain selection criteria and focus on …,*,2008,2
POLYTICS: provenance-based analytics of data-centric applications,Pierre Bourhis; Daniel Deutch; Yuval Moskovitch,We consider in this demonstration the analysis of complex data-intensive applications. Wefocus on three classes of analytical questions that are important for application owners andusers alike: Why was a result obtained? What would be the result if the application logic ordatabase is modified in a particular way? How can one interact with the application toachieve a particular goal? Answering these questions efficiently is a fundamental steptowards optimizing the application and its use. Noting that provenance was a keycomponent in answering similar questions in the context of database queries; we havedeveloped POLYTICS; a system that employs novel provenance-based solutions for theseanalytic questions for data-centric applications. We propose to demonstrate POLYTICSusing an online bicycle shop application as an example; letting participants play the role …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,1
Representing and Querying Order-Incomplete Data,Antoine Amarilli; Mouhamadou Lamine Ba; Daniel Deutch; Pierre Senellart,*,Draft: http://pierre. senellart. com/publications/amarilli2016representing. pdf,2015,1
Towards web-scale how-provenance,Daniel Deutch; Amir Gilad; Yuval Moskovitch,The annotation of data with meta-data; and its propagation through data-intensivecomputation in a way that follows the transformations that the data undergoes (“how-provenance”); has many applications; including explanation of the computation results;assessing their trustworthiness and proving their correctness; evaluation in presence ofincomplete or probabilistic information; view maintenance; etc. As data gets bigger; itstransformations become more complex; and both are being relegated to the cloud; the roleof provenance in these applications is even more crucial. But at the same time; the overheadincurred due to provenance computation; in terms of time; space and communication; maylimit the scalability of how-provenance management systems. We envision an approach foraddressing this complex problem; through allowing selective tracking of how-provenance …,Data Engineering Workshops (ICDEW); 2015 31st IEEE International Conference on,2015,1
Deduction in the presence of distribution and contradictions,Serge Abiteboul; Meghyn Bienvenu; Daniel Deutch,We study deduction; captured by datalog-style rules; in the presence of contradictions;captured by functional depen-dency (FD) violation. We propose a simple non-deterministicsemantics for datalog with FDs based on inferring facts one at a time; never violating theFDs. We present a novel proof theory for this semantics. We also discuss a set-at-a-timesemantics; where at each iteration; all facts that can be inferred are added to the database;and then choices are made between contradicting facts. We then build upon a distributeddatalog idiom; namely Webdamlog; to define a semantics for the distributed setting. Observethat contra-dictions naturally arise in such a setting; with different peers having conflictinginformation or opinions. We study differ-ent semantics for this setting.,WebDB,2012,1
Optimal probabilistic generators for XML corpora,Serge Abiteboul; Yael Amsterdamer; Daniel Deutch; Tova Milo; Pierre Senellart,Abstract We study the problem of; given a corpus of XML documents and its schema; findingan optimal probabilistic model (optimality meaning maximizing the likelihood of the corpus tobe generated). We present an efficient algorithm for finding the best probabilistic model; inabsence of constraints. We further study the problem in presence of integrity constraints(key; inclusion; and domain constraints) and consider in this case two different kinds ofgenerators: a continuation-test generator that performs; while generating; some tests ofschema satisfiability; these tests allow avoiding the violation of constraints (but as we show;are costly to implement); and a restart generator that may generate an invalid document andthen restart and try again.,BDA (Bases de données avancées),2011,1
On the optimality of top-k algorithms for Interactive Web Applications.,Yael Amsterdamer; Daniel Deutch; Tova Milo,ABSTRACT In an interactive Web application; the application state changes according touser choices/actions. To assist users in their interaction with such applications; there is aneed to provide them with recommendations for the top-k (according to some ranking metric)interactions. These recommendations must be continually updated; as the user interacts withthe application; to be consistent with the actual choices she makes. Efficiency of computationis critical here to provide fast response time and a pleasant user experience. This paperestablishes formal foundations for measuring the optimality of top-k algorithms of theaforementioned type; ie how well they perform relative to other algorithms; with respect to allpossible input instances. We define several intuitive notions of optimality in this setting;analyze the fundamental difficulties in obtaining optimal algorithms; and identify …,WebDB,2011,1
Querying Future and Past in Business Processes.,Daniel Deutch; Tova Milo,Abstract A business process (BP for short) consists of a group of business activitiesundertaken in pursuit of some particular goal. Analysis of BPs bears two main flavors;namely analysis of future and past executions. We intuitively explain these analysis goalsand the models and algorithms employed to achieve them.,IEEE Data Eng. Bull.,2009,1
Querying order-incomplete data,Antoine Amarilli; LM Ba; Daniel Deutch; Pierre Senellart,ABSTRACT To combine ordered data originating from multiple sources; one needs aframework that can represent uncertainty about the possible orderings or; as we call it; order-incomplete data. Examples of such data are lists of properties (such as hotels andrestaurants) ranked by an unknown function reflecting relevance or customer ratings;documents edited concurrently with uncertainty on the order of contributions; and the resultof integrating event sequences such as sensor readouts or log entries. Our work extends thepositive relational algebra to ordered and order-incomplete data; and introduces a set ofaxioms to guide the design of a bag semantics for the language. We introduce two simplesuch semantics; one of which is shown to be the most general for our set of axioms. We nextdesign a strong representation system for them; based on partial orders interpreted …,Preprint: http://a3nm. net/publications/amarilli2015querying. pdf,*,1
Computing Possible and Certain Answers over Order-Incomplete Data,Antoine Amarilli; Mouhamadou Lamine Ba; Daniel Deutch; Pierre Senellart,Abstract: This paper studies the complexity of query evaluation for databases whoserelations are partially ordered; the problem commonly arises when combining ordered datafrom multiple sources. We focus on queries in a useful fragment of SQL; namely positiverelational algebra with aggregates; whose bag semantics we extend to the partially orderedsetting. Our semantics leads to the study of two main computational problems; namely thepossibility and certainty of query answers. We show that these problems are respectively NP-complete and coNP-complete; but identify tractable cases depending on the query operatorsor input partial orders. We further introduce a duplicate elimination operator and study itseffect on the complexity results.,arXiv preprint arXiv:1801.06396,2018,*
Efficient provenance tracking for datalog using top-k queries,Daniel Deutch; Amir Gilad; Yuval Moskovitch,Abstract Highly expressive declarative languages; such as datalog; are now commonly usedto model the operational logic of data-intensive applications. The typical complexity of suchdatalog programs; and the large volume of data that they process; call for result explanation.Results may be explained through the tracking and presentation of data provenance;defined here as the set of derivation trees of a given fact. While informative; the size of suchfull provenance information is typically too large and complex (even when compactlyrepresented) to allow displaying it to the user. To this end; we propose a novel top-k querylanguage for querying datalog provenance; supporting selection criteria based on treepatterns and ranking based on the rules and database facts used in derivation. We proposean efficient novel algorithm that computes in polynomial data complexity a compact …,The VLDB Journal,2018,*
PROX: Approximated Summarization of Data Provenance,Eleanor Ainy; Pierre Bourhis; Susan B Davidson; Daniel Deutch; Tova Milo,Abstract Many modern applications involve collecting large amounts of data from multiplesources; and then aggregating and manipulating it in intricate ways. The complexity of suchapplications; combined with the size of the collected data; makes it difficult to understand theapplication logic and how information was derived. Data provenance has been provenhelpful in this respect in different contexts; however; maintaining and presenting the full andexact provenance may be infeasible; due to its size and complex structure. For that reason;we introduce the notion of approximated summarized provenance; where we seek acompact representation of the provenance at the possible cost of information loss. Based onthis notion; we have developed PROX; a system for the management; presentation and useof data provenance for complex applications. We propose to demonstrate PROX in the …,Advances in database technology: proceedings. International Conference on Extending Database Technology,2016,*
Query By Provenance,Daniel Deutch; Amir Gilad,Abstract: To assist non-specialists in formulating database queries; multiple frameworks thatautomatically infer queries from a set of examples have been proposed. While highly useful;a shortcoming of the approach is that if users can only provide a small set of examples;many inherently different queries may qualify; and only some of these actually match theuser intentions. Our main observation is that if users further explain their examples; the set ofqualifying queries may be significantly more focused. We develop a novel framework whereusers explain example tuples by choosing input tuples that are intuitively the" cause" for theirexamples. Their explanations are automatically" compiled" into a formal model forexplanations; based on previously developed models of data provenance. Then; our novelalgorithms infer conjunctive queries from the examples and their explanations. We prove …,arXiv preprint arXiv:1602.03819,2016,*
Optimal Probabilistic Generation of XML Documents,Serge Abiteboul; Yael Amsterdamer; Daniel Deutch; Tova Milo; Pierre Senellart,Abstract We study the problem of; given a corpus of XML documents and its schema; findingan optimal (generative) probabilistic model; where optimality here means maximizing thelikelihood of the particular corpus to be generated. Focusing first on the structure ofdocuments; we present an efficient algorithm for finding the best generative probabilisticmodel; in the absence of constraints. We further study the problem in the presence ofintegrity constraints; namely key; inclusion; and domain constraints. We study in this casetwo different kinds of generators. First; we consider a continuation-test generator thatperforms; while generating documents; tests of schema satisfiability; these tests prevent fromgenerating a document violating the constraints but; as we will see; they are computationallyexpensive. We also study a restart generator that may generate an invalid document and …,Theory of Computing Systems,2015,*
Approximated Summarization of Data Provenance,Ainy Eleanor; Pierre Bourhis; Susan Davidson; Daniel Deutch; Tova Milo,Many modern applications involve collecting large amounts of data from multiple sources;and then aggregating and manipulating it in intricate ways. The complexity of suchapplications; combined with the size of the collected data; makes it difficult to understandhow the resulting information was derived. Data provenance has proven helpful in thisrespect; however; maintaining and presenting the full and exact provenance informationmay be infeasible due to its size and complexity. We therefore introduce the notion ofapproximated summarized provenance; which provides a compact representation of theprovenance at the possible cost of information loss. Based on this notion; we present a novelprovenance summarization algorithm which; based on the semantics of the underlying dataand the intended use of provenance; outputs a summary of the input provenance …,CIKM,2015,*
Requêtes sur des données à ordre incomplet,Antoine Amarilli,Page 1 …,*,2015,*
PRINCIPLES OF DATABASE SYSTEMS (PODS 2014),Martin Grohe; Serge Abiteboul; Pablo Barcelo; Jan van den Bussche; Andrea Cali; Sara Cohen; Dario Colazzo; Claire David; Daniel Deutch; Thomas Eiter; Alexandre Evfimievski; Roberto Grossi; Sudipto Guha; André Hernich; Kristian Kersting; Jure Leskovec; Sebastian Maneth; Gabriele Puppis; Dan Suciu; Tony Tan; Wang-Chiew Tan; Yufei Tao; Ke Yi; Richard Rick Hull,Topics that fit the interests of the symposium include the following: design; semantics; andoptimization of query and database languages; data modeling; data structures andalgorithms for data management; dynamic aspects of databases (updates; views); querylanguages for semi-structured data (including XML and RDF); search query languages(including techniques from information retrieval); web services; automatic verification ofdatabase-driven systems; incompleteness; inconsistency; and uncertainty in databases;constraints (specification; reasoning; mining; constraint databases); domain-specificdatabases (multi-media; scientific; spatial; temporal; text); schema and query extraction;mining and learning of data models and queries; data integration; data exchange;provenance; workflows; metadata management; meta-querying; semantic; linked …,*,*,*
Querying and Analyzing Business Processes Under Models of Uncertainty,Daniel Deutch; Tova Milo,*,*,*,*
Managing Uncertainty and Conflicts in a Distributed World,Serge Abiteboul; Daniel Deutch,We consider a distributed setting where peers in a network exchange information; and applyreasoning to derive further information. We note that uncertainty is common in such setting.Peers may have disagreements and state or infer conflicting facts. Peers can settle conflictsby choosing between contradicting base or inferred facts; which introduces a first cause ofuncertainty. Then; there is an inherent uncertainty introduced by the asynchronousenvironment: the order in which messages are sent and received; as well as the order ofapplying reasoning steps; are both uncertain. In this short paper; we consider the problem ofmodeling the dynamics of such networks; accounting for uncertainty. We briefly recall aproposal for the management of uncertainty; namely datalogfd [2]. We consider extending itto the distributed Datalog dialect Webdamlog introduced in [1]. We mention preliminary …,*,*,*
Question Answering for Large Scale Data Sets Generated by the Web Crowd,Daniel Deutch; Ohad Greenshpan; Boris Kostenko; Tova Milo,ABSTRACT Trivia Masster is a system that collects a very large number of facts from humanusers; and then utilizes this Database for question answering. Users contribute to theDatabase by playing a Trivia game; and gain points based upon the correctness of theircontribution. The obtained database is comprised of many facts; contributed by manydifferent users; as a consequence; it contains erroneous data. The key challenge in thesystem development thus lies in the design of effective methods to evaluate queries oversuch dirty Databases. We found that no single existing technique provides a satisfactorysolution to this challenge; in all cases; consequently; we have employed a novel approach;based on a declarative framework for defining recursive and probabilistic Data Cleaningrules. Our solution employs an algorithm that is based on Markov Chain Monte Carlo …,*,*,*
559 On making directed graphs transitive,Mathias Weller; Christian Komusiewicz; Rolf Niedermeier; Johannes Uhlmann; Pedro Lara; Fábio Borges; Renato Portugal; Nadia Nedjah; Daniel Deutch; Tova Milo; Zehui Shao; Fei Deng; Meilian Liang; Xiaodong Xu; Jesper Jansson; Kunihiko Sadakane; Wing-Kin Sung; Raphael Yuster; Andrei A Bulatov; Víctor Dalmau; Martin Grohe; Dániel Marx; Jonathan Katz; Philip MacKenzie; Gelareh Taban; Virgil Gligor; Ken-ichi Kawarabayashi; Yusuke Kobayashi,*,*,*,*
Provenance for Web 2.0 Data,Daniel Deutch; Fabian M Suchanek,Abstract. In this paper; we look at Web data that comes from multiple sources; as in the Web2.0. We argue that Web data is more than just its content. Rather; a piece of Web datacarries along different facets; such the transformations that data underwent; the differentperspectives that users have on the content; and the context in which a statement is made.We put forward the idea that provenance; ie the tracing of where data comes from; can helpus model these phenomena. We study how far existing approaches address the issue ofprovenance for Web data; and identify gaps and open problems.,*,*,*
