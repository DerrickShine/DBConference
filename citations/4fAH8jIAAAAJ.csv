Roadrunner: Towards automatic data extraction from large web sites,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,Abstract The paper investigates techniques for extracting data from HTML sites through theuse of automatically generated wrappers. To automate the wrapper generation and the dataextraction process; the paper develops a novel technique to compare HTML pages andgenerate a wrapper based on their similarities and differences. Experimental results on real-life data-intensive Web sites confirm the feasibility of the approach.,Proceedings of the International Conference on Very Large Databases,2001,1371
To weave the web,Paolo Atzeni; Giansalvatore Mecca; Paolo Merialdo,Abstract The paper discusses the issue of views in the Web context. We introduce a set oftools and languages for managing and restructuring data coming from the World Wide Web.We present a speci c data model; called the Araneus Data Model; inspired to the structurestypically present in Web sites. The model allows us to describe the scheme of a Webhypertext; in the spirit of databases. Based on the data model; we develop two languages tosupport a sophisticate view de nition process: the rst; called Ulixes; is used to build databaseviews of the Web; which can then be analyzed and integrated using database techniques;the second; called Penelope; allows the de nition of derived Web hypertexts from relationalviews. This can be used to generate hypertextual views over the Web.,Proceedings of the International Conference on Very Large Data Bases,1997,322
Design and maintenance of data-intensive web sites,Paolo Atzeni; Giansalvatore Mecca; Paolo Merialdo,Abstract A methodology for designing and maintaining large Web sites is introduced. Itwould be especially useful if data to be published in the site are managed using a DBMS.The design process is composed of two intertwined activities: database design andhypertext design. Each of these is further divided in a conceptual phase and a logical phase;based on specific data models; proposed in our project. The methodology strongly supportssite maintenance: in fact; the various models provide a concise description of the sitestructure; they allow to reason about the overall organization of pages in the site andpossibly to restructure it.,International Conference on Extending Database Technology,1998,260
Automatic information extraction from large websites,Valter Crescenzi; Giansalvatore Mecca,Abstract Information extraction from websites is nowadays a relevant problem; usuallyperformed by software modules called wrappers. A key requirement is that the wrappergeneration process should be automated to the largest extent; in order to allow for large-scale extraction tasks even in presence of changes in the underlying sites. So far; however;only semi-automatic proposals have appeared in the literature. We present a novelapproach to information extraction from websites; which reconciles recent proposals forsupervised wrapper induction with the more traditional field of grammar inference. Grammarinference provides a promising theoretical framework for the study of unsupervised---that is;fully automatic---wrapper generation algorithms. However; due to some unrealisticassumptions on the input; these algorithms are not practically applicable to Web …,Journal of the ACM (JACM),2004,212
Grammars have exceptions,Valter Crescenzi; Giansalvatore Mecca,Abstract Extending database-like techniques to semi-structured and Web data sources isbecoming a prominent research field. These data sources are essentially collections oftextual documents. Hence; in this context; one of the key tasks consists in wrappingdocuments to build database abstractions of their content that can be manipulated usinghigh-level tools. However; the degree of heterogeneity and the lack of structure makestandard grammar parsers excessively rigid; and often unable to capture the richness ofconstructs in these documents. This paper presents Minerva; a formalism for writingwrappers around Web sites and other textual data sources. The key feature of Minerva is theattempt to couple the benefits of a declarative; grammar-based approach; with the flexibilityof procedural programming. This is done by enriching regular grammars with an explicit …,Information Systems,1998,184
Automatic annotation of data extracted from large Web sites.,Luigi Arlotta; Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,ABSTRACT Data extraction from web pages is performed by software modules calledwrappers. Recently; some systems for the automatic generation of wrappers have beenproposed in the literature. These systems are based on unsupervised inference techniques:taking as input a small set of sample pages; they can produce a common wrapper to extractrelevant data. However; due to the automatic nature of the approach; the data extracted bythese wrappers have anonymous names. In the framework of our ongoing projectRoadRunner; we have developed a prototype; called Labeller; that automatically annotatesdata extracted by automatically generated wrappers. Although Labeller has been developedas a companion system to our wrapper generator; its underlying approach has a generalvalidity and therefore it can be applied together with other wrapper generator systems …,WebDB,2003,170
Semistructured and structured data in the web: Going back and forth,Paolo Atzeni; Giansalvatore Mecca; Paolo Merialdo,As a consequence of the explosion of the World Wide Web 11]; an increasing amount ofinformation is stored in repositories organized according to loose structures; usually ashypertextual documents; and data access is based on browsing and information retrievaltechniques. Due to their intuitive nature; browsing and searching present severe limitations8]. Also; they o er little or no support to a global view of information in the Web; nor to theactual extraction and manipulation of data: speci ce ort is required to use Web data as inputto subsequent computations or to correlate values in Web pages. In this paper; we presentthe approach to the management of Web data as attacked in the Araneus project carried outby the database group at Universit a di Roma Tre. Our approach is based on ageneralization of the notion of view to the Web framework. In fact; in traditional databases …,SIGMOD Record,1997,170
The Araneus Web-based management system,Giansalvatore Mecca; Paolo Atzeni; Alessandro Masci; Giuseppe Sindoni; Paolo Merialdo,The paper describes the ARANEUS Wel-Base Management System [l; 5; 4; 61; a systemdeveloped at Universitb di Roma Tre; which represents a proposal towards the definition ofa new kind of data-repository; designed to manage Web data in the database style. We calla Web-Base a collection of data of heterogeneous nature; and more specifically:(i) highlystructured data; such as the ones typically stored in relational or objectoriented databasesystems;(G) semistructured data; in the Web style. We can simplify by saying that itincorporates both databases and Web sites. A Web-Base Management System (WBMS) is asystem for managing such Web-bases. More specifically; it should provide functionalities forboth database and Web site management. It is natural to think of it as an evolution ofordinary DBMSs; in the sense that it will play in future generation Web-based Information …,ACM SIGMOD Record,1998,162
Cut and paste,Paolo Atzeni; Giansalvatore Mecca,Abstract The paper develops EDITOR; a language for manipulating semi-structureddocuments; such as the ones typically avai-lable on the Web. EDITOR programs allow tosearch and restructure a document. They are based on two simple ideas; taken from texteditors: Search” instructions are used to select regions of interest in a document; and “cut.! Ypaste” to restructure them. We study the expressive power and the complexity of theseprograms. We show that they are computationally complete; in the sense that anycomputable document restructuring can be expressed in EDITOR. We also study thecomplexity of a safe subclass of programs; showing that it captures exactly the class ofpolynomial-time restruc-turings. The language has been implemented in Java; and is usedin the ARANEUS project to build database views over Web sites.,Proceedings of the sixteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1997,154
A new algorithm for clustering search results,Giansalvatore Mecca; Salvatore Raunich; Alessandro Pappalardo,Abstract We develop a new algorithm for clustering search results. Differently from manyother clustering systems that have been recently proposed as a post-processing step forWeb search engines; our system is not based on phrase analysis inside snippets; butinstead uses latent semantic indexing on the whole document content. A main contributionof the paper is a novel strategy–called dynamic SVD clustering–to discover the optimalnumber of singular values to be used for clustering purposes. Moreover; the algorithm issuch that the SVD computation step has in practice good performance; which makes itfeasible to perform clustering when term vectors are available. We show that the algorithmhas very good classification performance; and that it can be effectively used to cluster resultsof a search engine to make them easier to browse by users. The algorithm has being …,Data & Knowledge Engineering,2007,125
The (Short) Araneus Guide to Web-Site Development.,V Crescenzi G Mecca; P Merialdo; P Atzeni,*,ACM SIGMOD Workshop on The Web and Databases (WebDB'99),1999,95
Design and development of data-intensive web sites: The Araneus approach,Paolo Merialdo; Paolo Atzeni; Giansalvatore Mecca,Abstract Data-intensive Web sites are large sites based on a back-end database; with afairly complex hypertext structure. The paper develops two main contributions:(a) a specificdesign methodology for data-intensive Web sites; composed of a set of steps and designtransformations that lead from a conceptual specification of the domain of interest to theactual implementation of the site;(b) a tool called H omer; conceived to support the sitedesign and implementation process; by allowing the designer to move through the varioussteps of the methodology; and to automate the generation of the code needed to implementthe actual site. Our approach to site design is based on a clear separation between severaldesign activities; namely database design; hypertext design; and presentation design. Allthese activities are carried on by using high-level models; all subsumed by an extension …,ACM Transactions on Internet Technology (TOIT),2003,93
RoadRunner: automatic data extraction from data-intensive web sites,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,Data extraction from HTML pages is performed by software modules; usually calledwrappers. Roughly speaking; a wrapper identifies and extracts relevant pieces of text insidea web page; and reorganizes them in a more structured format. In the literature there is anumber of systems to (semi-) automatically generate wrappers for HTML pages [1]. We haverecently investigated for original approaches that aims at pushing further the level ofautomation of the wrapper generation process. Our main intuition is that; in a dataintensiveweb site; pages can be classified in a small number of classes; such that pages belonging tothe same class share a rather tight structure. Based on this observation; we have studied annovel technique; we call the matching technique [2]; that automatically generates a commonwrapper by exploiting similarities and differences among pages of the same class. In …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,89
Schema mapping verification: the spicy way,Angela Bonifati; Giansalvatore Mecca; Alessandro Pappalardo; Salvatore Raunich; Gianvito Summa,Abstract Schema mapping algorithms rely on value correspondences-ie; correspondencesamong semantically related attributes-to produce complex transformations among datasources. These correspondences are either manually specified or suggested by separatemodules called schema matchers. The quality of mappings produced by a mappinggeneration tool strongly depends on the quality of the input correspondences. In this paper;we introduce the Spicy system; a novel approach to the problem of verifying the quality ofmappings. Spicy is based on a three-layer architecture; in which a schema matching moduleis used to provide input to a mapping generation module. Then; a third module; the mappingverification module; is used to check candidate mappings and choose the ones thatrepresent better transformations of the source into the target. At the core of the system …,Proceedings of the 11th international conference on Extending database technology: Advances in database technology,2008,86
The LLUNATIC data-cleaning framework,Floris Geerts; Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,Abstract Data-cleaning (or data-repairing) is considered a crucial problem in many database-related tasks. It consists in making a database consistent with respect to a set of givenconstraints. In recent years; repairing methods have been proposed for several classes ofconstraints. However; these methods rely on ad hoc decisions and tend to hard-code thestrategy to repair conflicting values. As a consequence; there is currently no generalalgorithm to solve database repairing problems that involve different kinds of constraints anddifferent strategies to select preferred values. In this paper we develop a uniform frameworkto solve this problem. We propose a new semantics for repairs; and a chase-basedalgorithm to compute minimal solutions. We implemented the framework in a DBMS-basedprototype; and we report experimental results that confirm its good scalability and …,Proceedings of the VLDB Endowment,2013,83
Araneus in the Era of XML.,Giansalvatore Mecca; Paolo Merialdo; Paolo Atzeni,A large body of research has been recently motivated by the attempt to extend databasemanipulation techniques to data on the Web (see [13] for a survey). Most of these researchefforts–which range from the definition of Web query languages and the relatedoptimizations; to systems for Web site development and management; and to integrationtechniques–started before XML was introduced; and therefore have strived for a long time tohandle the highly heterogeneous nature of HTML pages. In the meanwhile; Web datasources have evolved from small; home-made collections of HTML pages into complexplatforms for distributed data access and application development; and XML promises toimpose itself as a more appropriate format for this new breed of Web sites. XML brings dataon the Web closer to databases; since; differently from HTML; it is based on a clean …,IEEE Data Eng. Bull.,1999,82
In search of the lost schema,Stéphane Grumbach; Giansalvatore Mecca,Abstract We study the problem of rediscovering the schema of nested relations that havebeen encoded as strings for storage purposes. We consider various classes of encodingfunctions; and consider the mark-up encodings; which allow to find the schema withoutknowledge of the encoding function; under reasonable assumptions on the input data.Depending upon the encoding of empty sets; we propose two polynomial on-line algorithms(with different buffer size) solving the schema finding problem. We also prove that with ahigh probability; both algorithms find the schema after examining a fixed number of tuples;thus leading in practice to a linear time behavior with respect to the database size forwrapping the data. Finally; we show that the proposed techniques are well-suited forpractical applications; such as structuring and wrapping HTML pages and Web sites.,International Conference on Database Theory,1999,71
++ Spicy: an Open-Source Tool for Second-Generation Schema Mapping and Data Exchange,Bruno Marnette; Giansalvatore Mecca; Paolo Papotti; Salvatore Raunich; Donatello Santoro,ABSTRACT Recent results in schema-mapping and data-exchange research may beconsidered the starting point for a new generation of systems; capable of dealing with asignificantly larger class of applications. In this paper we demonstrate the first of thesesecond-generation systems; called++ SPiCY. We introduce a number of scenarios from avariety of data management tasks; such as data fusion; data cleaning; and ETL; and showhow; based on the system; schema mappings and data exchange techniques can be veryeffectively applied to these contexts. We compare++ SPiCY to the previous generations oftools; to show that this is much-needed advancement in the field.,Proceedings of the VLDB Endowment,2011,62
Core schema mappings,Giansalvatore Mecca; Paolo Papotti; Salvatore Raunich,Abstract Research has investigated mappings among data sources under two perspectives.On one side; there are studies of practical tools for schema mapping generation; these focuson algorithms to generate mappings based on visual specifications provided by users. Onthe other side; we have theoretical researches about data exchange. These study how togenerate a solution-ie; a target instance-given a set of mappings usually specified as tuplegenerating dependencies. However; despite the fact that the notion of a core of a dataexchange solution has been formally identified as an optimal solution; there are yet nomapping systems that support core computations. In this paper we introduce several newalgorithms that contribute to bridge the gap between the practice of mapping generation andthe theory of data exchange. We show how; given a mapping scenario; it is possible to …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,58
Scalable data exchange with functional dependencies,Bruno Marnette; Giansalvatore Mecca; Paolo Papotti,Abstract The recent literature has provided a solid theoretical foundation for the use ofschema mappings in data-exchange applications. Following this formalization; newalgorithms have been developed to generate optimal solutions for mapping scenarios in ahighly scalable way; by relying on SQL. However; these algorithms suffer from a seriousdrawback: they are not able to handle key constraints and functional dependencies on thetarget; ie; equality generating dependencies (egds). While egds play a crucial role in thegeneration of optimal solutions; handling them with first-order languages is a difficultproblem. In fact; we start from a negative result: it is not always possible to compute solutionsfor scenarios with egds using an SQL script. Then; we identify many practical cases in whichthis is possible; and develop a best-effort algorithm to do this. Experimental results show …,Proceedings of the VLDB Endowment,2010,50
Sequences; Datalog and transducers,Giansalvatore Mecca; Anthony J Bonner,Abstract This paper develops a query language for sequence databases; such as genomedatabases and text databases. The language; called Sequence Dataiog; extends classicalDatalog with interpreted function symbols for manipulating sequences. It has both a clearoperational and declarative semantics; based on a new notion called the extended activedomain of a database. The extended domain contains all the sequences in the databaseand all their subsequences. This idea leads to a clear distinction between safe and unsaferecursion over sequences: safe recursion stays inside the extended active domain; whileunsafe recursion does not. By carefully limiting the amount of unsafe recursion; the paperdevelops a safe and expressive subset of Sequence Dat slog. As part of the development; anew type of transducer is introduced; called a genera~ izect sequence transducer. Unsafe …,Proceedings of the Fourteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,1995,50
Efficient queries over web views,Giansalvatore Mecca; Alberto Mendelzon; Paolo Merialdo,Abstract Large web sites are becoming repositories of structured information that can benefitfrom being viewed and queried as relational databases. However; querying these viewsefficiently requires new techniques. Data usually resides at a remote site and is organizedas a set of related HTML documents; with network access being a primary cost factor inquery evaluation. This cost can be reduced by exploiting the redundancy often found in sitedesign. We use a simple data model; a subset of the Araneus data model; to describe thestructure of a web site. We augment the model with link and inclusion constraints thatcapture the redundancies in the site. We map relational views of a site to a navigationalalgebra and show how to use the constraints to rewrite algebraic expressions; reducing thenumber of network accesses.,International Conference on Extending Database Technology - EDBT'98,1998,44
Data-intensive web sites: design and maintenance,Paolo Atzeni; Paolo Merialdo; Giansalvatore Mecca,Abstract A methodology for designing and maintaining data–intensive Web sites isintroduced. Leveraging on ideas well established in the database field; the approach heavilyrelies on the use of models for the description of Web sites. The design process is composedof two intertwined activities: database design and hypertext design. Each of these is furtherdivided in a conceptual phase and a logical phase; based on specific data models. Themethodology strongly supports site maintenance: in fact; the various models provide aconcise description of the site structure; they allow to reason about the overall organizationof pages in the site and possibly to restructure it.,World Wide Web,2001,40
Automatic Web Information Extraction in the Road R unner System,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,Abstract This paper presents Road Runner; a research project that aims at developingsolutions for automatically extracting data from large HTML data sources. The target of ourresearch are data-intensive Web sites; ie; HTML-based sites with a fairly complex structure;that publish large amounts of data. The paper describes the top-level software architectureof the Road Runner System; and the novel research challenges posed by the attempt toautomate the information extraction process.,International Conference on Conceptual Modeling,2001,38
Wrapping-oriented classification of web pages,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,Abstract Data extraction from HTML Web pages is performed by software programs calledwrapper. Writing wrappers is a costly and labor intensive task; recently several proposalhave attacked the problem of automatically generating wrappers. In this paper; we study aproblem related to the automation of the wrapping generation process: given a portion of aWeb site to wrap; we develop techniques to cluster its HTML pages into page classes withhomogeneous organization and layout; these classes can become the input to the wrappergeneration process. Also; once a wrapper library has been generated for a bunch of Websites; our techniques can be used in order to select; for any new page downloaded fromthese site; the right wrapper in the library. Based on the proposed techniques we havedeveloped a software prototype; and conducted several experiments on HTML pages …,Proceedings of the 2002 ACM symposium on Applied computing,2002,37
Cut and paste,Giansalvatore Mecca; Paolo Atzeni,Abstract The paper develops Editor; a language for manipulating semistructured documents;such as those typically available on the Web. Editor programs are based on two simpleideas; taken from text editors:“search” instructions are used to select regions of interest in adocument; and “cut & paste” instructions to restructure them. We study the expressive powerand the complexity of these programs. We show that they are computationally complete; inthe sense that any computable document restructuring can be expressed in Editor. We alsostudy the complexity of a safe subclass of programs; showing that it captures exactly theclass of polynomial-time restructurings. The language has been implemented in Java and iscurrently used in the Araneus project as a basis for a wrapper-generation toolkit.,Journal of Computer and System Sciences,1999,37
From Databases to Web-Bases: The ARANEUS Experience,G Mecca; P Atzeni; P Merialdo; A Masci; G Sindoni,Abstract The ARANEUS project aims at developing tools for data-management on the WorldWide Web. Web-based information systems deal with data of heterogeneous nature; mainlydatabase data and HTML documents. We have implemented a system; called a Web-baseManagement system; for managing such repositories. The system is designed to supportseveral classes of applications:(i) high-level access to data in the Web;(ii) design;implementation and maintenance of Web sites;(iii) cooperative applications on the Web. Wediscuss the lessons learned from our experiences with the system; ranging from database-style query interfaces to popular Web sites; to the design and implementation of severalsites; among which an integrated Web museum; which correlates data coming from severalvirtual museums on the Web.,In Technical Report 34-1998. Dipartimento,1998,37
Mapping and cleaning,Floris Geerts; Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,We address the challenging and open problem of bringing together two crucial activities indata integration and data quality; ie; transforming data using schema mappings; and fixingconflicts and inconsistencies using data repairing. This problem is made complex by severalfactors. First; schema mappings and data repairing have traditionally been considered asseparate activities; and research has progressed in a largely independent way in the twofields. Second; the elegant formalizations and the algorithms that have been proposed forboth tasks have had mixed fortune in scaling to large databases. In the paper; we introducea very general notion of a mapping and cleaning scenario that incorporates a wide variety offeatures; like; for example; user interventions. We develop a new semantics for thesescenarios that represents a conservative extension of previous semantics for schema …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,35
Concise and expressive mappings with+ Spicy,Giansalvatore Mecca; Paolo Papotti; Salvatore Raunich; Marcello Buoncristiano,Abstract We introduce the+ Spicy mapping system. The system is based on a number ofnovel algorithms that contribute to increase the quality and expressiveness of mappings.+Spicy integrates the computation of core solutions in the mapping generation process in ahighly efficient way; based on a natural rewriting of the given mappings. This allows for anefficient implementation of core computations using common runtime languages like SQL orXQuery and guarantees very good performances; orders of magnitude better than those ofprevious algorithms. The rewriting algorithm can be applied both to mappings generated bythe system; or to pre-defined mappings provided as part of the input. To do this; the systemwas enriched with a set of expressive primitives; so that+ Spicy is the first mapping systemthat brings together a sophisticate and expressive mapping generation algorithm with an …,Proceedings of the VLDB Endowment,2009,31
Sequences; datalog; and transducers,Anthony Bonner; Giansalvatore Mecca,Abstract This paper develops a query language for sequence databases; such as genomedatabases and text databases. The language; calledSequence Datalog; extends classicalDatalog with interpreted function symbols for manipulating sequences. It has both a clearoperational and declarative semantics; based on a new notion called theextended activedomainof a database. The extended domain contains all the sequences in the database andall their subsequences. This idea leads to a clear distinction between safe and unsaferecursion over sequences: safe recursion stays inside the extended active domain; whileunsafe recursion does not. By carefully limiting the amount of unsafe recursion; the paperdevelops a safe and expressive subset of Sequence Datalog. As part of the development; anew type of transducer is introduced; called ageneralized sequence transducer. Unsafe …,Journal of Computer and System Sciences,1998,31
An automatic data grabber for large web sites,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo; Paolo Missier,Abstract We demonstrate a system to automatically grab data from data intensive web sites.The system first infers a model that describes at the intensional level the web site as acollection of classes; each class represents a set of structurally homogeneous pages; and itis associated with a small set of representative pages. Based on the model a library ofwrappers; one per class; is then inferred; with the help an external wrapper generator. Themodel; together with the library of wrappers; can thus be used to navigate the site andextract the data.,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,27
Core Schema Mappings: Scalable Core Computations in Data Exchange,Giansalvatore Mecca; Paolo Papotti; Salvatore Raunich,Abstract Research has investigated mappings among data sources under two perspectives.On the one side; there are studies of practical tools for schema mapping generation; thesefocus on algorithms to generate mappings based on visual specifications provided by users.On the other side; we have theoretical researches about data exchange. These study how togenerate a solution–ie; a target instance–given a set of mappings usually specified as tuplegenerating dependencies. Since the notion of a core solution has been formally identified asan optimal solution; it is very important to efficiently support core computations in mappingsystems. In this paper; we introduce several new algorithms that contribute to bridge the gapbetween the practice of mapping generation and the theory of data exchange. We showhow; given a mapping scenario; it is possible to generate an executable script that …,Information Systems,2012,22
Ulixes: Building relational views over the web,Paolo Atzeni; Alessandro Masci; Giansalvatore Mecca; Paolo Merialdo; Elena Tabet,We consider structured Web sites; those sites in which structures are so tight and regularthat we can assimilate the site; from the logical viewpoint; to a conventional database. Wehave argued that; with respect to structured Web servers; it is possible to apply ideas fromtraditional database techniques; specifically with respect to design; query; and update [2].Here we focus on the querying process; which consists in associating a scheme with aserver and then use this scheme to pose queries in a high level query language. Todescribe the scheme; we use a specific data model; called the ARANEUS Data Model(ADM). We say that ADM is a page oriented model; in the sense that the main construct ofthe model is that of page scheme; used to describe the structure of sets of homogeneouspages in the server. ADM schemes are then offered to the user; who can query them by …,icde,1997,18
The Spicy system: towards a notion of mapping quality,Angela Bonifati; Giansalvatore Mecca; Alessandro Pappalardo; Salvatore Raunich; Gianvito Summa,Abstract We introduce the Spicy system; a novel approach to the problem of automaticallyselecting the best mappings among two data sources. Known schema mapping algorithmsrely on value correspondences--ie correspondences among semantically related attributes--to produce complex transformations among data sources. Spicy brings together schemamatching and mapping generation tools to further automate this process. A key observation;here; is that the quality of the mappings is strongly influenced by the quality of the inputcorrespondences. To address this problem; Spicy adopts a three-layer architecture; in whicha schema matching module is used to provide input to a mapping generation module. Then;a third module; the mapping verification module; is used to check candidate mappings andchoose the ones that represent better transformations of the source into the target. At the …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,17
That's all folks!: llunatic goes open source,Floris Geerts; Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,Abstract It is widely recognized that whenever different data sources need to be integratedinto a single target database errors and inconsistencies may arise; so that there is a strongneed to apply data-cleaning techniques to repair the data. Despite this need; databaseresearch has so far investigated mappings and data repairing essentially in isolation.Unfortunately; schema-mappings and data quality rules interact with each other; so thatapplying existing algorithms in a pipelined way--ie; first exchange then data; then repair theresult--does not lead to solutions even in simple settings. We present the Llunatic mappingand cleaning system; the first comprehensive proposal to handle schema mappings anddata repairing in a uniform way. Llunatic is based on the intuition that transforming andcleaning data are different facets of the same problem; unified by their declarative nature …,Proceedings of the VLDB Endowment,2014,16
The LOGIDATA+ language and semantics,Paolo Atzeni; Luca Cabibbo; Giansalvatore Mecca; Letizia Tanca,Abstract A language for the LOGIDATA+ model is presented. The language is rule-basedand allows for the management of complex structures; classes; hierarchies and datafunctions. Negation is allowed in the body of rules. The semantics is based on a fixpointoperator and represents an extension of ordinary declarative-language semantics. The mainissue is the management of oid-invention; that is; the creation and manipulation of objectsbelonging to the classes of a scheme. In order to correctly deal with negation andmultivalued data functions; an extended notion of stratification is introduced.,*,1993,15
Discovery and correctness of schema mapping transformations,Angela Bonifati; Giansalvatore Mecca; Paolo Papotti; Yannis Velegrakis,Abstract Schema mapping is becoming pervasive in all data transformation; exchange; andintegration tasks. It brings to the surface the problem of differences and mismatches betweenheterogeneous formats and models; respectively; used in source and target databases to bemapped one to another. In this chapter; we start by describing the problem of schemamapping; its background; and technical implications. Then; we outline the early schemamapping systems; along with the new generation of schema mapping tools. Moving from theformer to the latter entailed a dramatic change in the performance of mapping generationalgorithms. Finally; we conclude the chapter by revisiting the query answering techniquesallowed by the mappings; and by discussing useful applications and future and currentdevelopments of schema mapping tools.,*,2011,14
Managing Web-based data-database models and transformations,Paolo Atzeni; Giansalvatore Mecca; Paolo Merialdo,The paper considers the Araneus data model which employs database techniques andwrappers to extract data from and generate Web sites. The project features a logical modelthat abstracts physical aspects of Web sites. Araneus provides high-level descriptions ofpages that let us both extract data from the Web and generate Web sites from databases.,IEEE Internet Computing,2002,14
IsaLog: A declarative language for complex objects with hierarchies,Paolo Atzeni; Luca Cabibbo; Giansalvatore Mecca,The IsaLog model and language are presented. The model has complex objects withclasses; relations; and is a hierarchies. The language is strongly types and declarative. Themain issue is the definition of the semantics of the language; given in three different waysthat are shown to be equivalent: a model-theoretic semantics; a reduction to logicprogramming with function symbols; and a fixpoint semantics. Each of the semanticspresents new aspects with respect to existing proposals because of the interaction of oid-invention with general is a hierarchies. The solutions are based on the explicit Skolemfunctors; which provide a powerful tool for manipulating object-identifiers.,Data Engineering; 1993. Proceedings. Ninth International Conference on,1993,13
Database challenges for exploratory computing,Marcello Buoncristiano; Giansalvatore Mecca; Elisa Quintarelli; Manuel Roveri; Donatello Santoro; Letizia Tanca,Helping users to make sense of very big datasets is nowadays considered an importantresearch topic. However; the tools that are available for data analysis purposes typicallyaddress professional data scientists; who; besides a deep knowledge of the domain ofinterest; master one or more of the following disciplines: mathematics; statistics; computerscience; computer engineering; and programming. On the contrary; in our vision it is vital tosupport also different kinds of users who; for various reasons; may want to analyze the dataand obtain new insight from them. Examples of these data enthusiasts [4; 9] are journalists;investors; or politicians: non-technical users who can draw great advantage from exploringthe data; achieving new and essential knowledge; instead of reading query results with tonsof records. The term data exploration generally refers to a data user being able to find her …,ACM SIGMOD Record,2015,12
What is the IQ of your Data Transformation System?,Giansalvatore Mecca; Paolo Papotti; Salvatore Raunich; Donatello Santoro,Abstract Mapping and translating data across different representations is a crucial problemin information systems. Many formalisms and tools are currently used for this purpose; to thepoint that developers typically face a difficult question:" what is the right tool for mytranslation task?" In this paper; we introduce several techniques that contribute to answerthis question. Among these; a fairly general definition of a data transformation system; a newand very efficient similarity measure to evaluate the outputs produced by such a system; anda metric to estimate user efforts. Based on these techniques; we are able to compare a widerange of systems on many translation tasks; to gain interesting insights about theireffectiveness; and; ultimately; about their" intelligence".,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,12
Ontology-based mappings,Giansalvatore Mecca; Guillem Rull; Donatello Santoro; Ernest Teniente,Abstract Data translation consists of the task of moving data from a source database to atarget database. This task is usually performed by developing mappings; ie executabletransformations from the source to the target schema. However; a richer description of thetarget database semantics may be available in the form of an ontology. This is typicallydefined as a set of views over the base tables that provides a unified conceptual view of theunderlying data. We investigate how the mapping process changes when such a richconceptualization of the target database is available. We develop a translation algorithm thatautomatically rewrites a mapping from the source schema to the target ontology into anequivalent mapping from the source to the target databases. Then; we show how to handlethis problem when an ontology is available also for the source. Differently from previous …,Data & Knowledge Engineering,2015,11
Querying sequence databases with Transducers,Anthony J Bonner; Giansalvatore Mecca,Abstract. This paper develops a database query language called Transducer Datalogmotivated by the needs of a new and emerging class of database applications. In theseapplications; such as text databases and genome databases; the storage and manipulationof long character sequences is a crucial feature. The issues involved in managing this kindof data are not addressed by traditional database systems; either in theory or in practice. Toaddress these issues; we recently introduced a new machine model called a generalizedsequence transducer. These generalized transducers extend ordinary transducers byallowing them to invoke other transducers as “subroutines.” This paper establishes thecomputational properties of Transducer Datalog; a query language based on this newmachine model. In the process; we develop a hierarchy of time-complexity classes based …,Acta informatica,2000,11
Structures in the Web.,Paolo Atzeni; Alessandro Masci; Giansalvatore Mecca; Paolo Merialdo; Elena Tabet,Abstract. The paper discusses the issue of views in the Web context. We introduce a set oftools and languages for managing and restructuring data coming from the World Wide Web.We present a specific data model; called the Araneus Data Model; inspired to the structurestypically present in Web sites. The model allows us to describe the scheme of a Webhypertext; in the spirit of databases. Based on the data model; we develop the Araneus Viewlanguage; to support a sophisticate view definition process; the language has two maincomponents: the first; called Ulixes; is used to build database views of the Web; which canthen be analyzed and integrated using database techniques; the second; called Penelope;allows the definition of derived Web hypertexts from relational views. This can be used togenerate hypertextual views over the Web.,SEBD,1997,11
HOMER: a model-based CASE tool for data-intensive Web sites,Paolo Merialdo; Paolo Atzeni; Marco Magnante; Giansalvatore Mecca; Marco Pecorone,We present HOMER; a CASE tool for building and maintaining complex; data-intensive Websites. In HOMER the processes of creation and maintenance of a Web site are completelybased on the adoption of suitable models; to describe the various aspects of the site(content~ navigation structure; presentation). The development of a site does not require anycode writing activity: based on the results of the design process; the system automaticallycreates programs to implement the site; statically and/or dynamically; as needed; also; thesystem does not depend on any specific tool or language: it has a modular architecture;which integrates external servers for specific tasks; finally; the system supports siteadministrators for several maintenance activities; which can involve changes over the site atdifferent levels. The site content is described both at the conceptual level; using the ER …,ACM SIGMOD Record,2000,10
Do we really need a new query language for XML?,Giansalvatore Mecca; Paolo Merialdo; Paolo Atzeni,XML [XML98] has been proposed as a successor of HTML conceived to ease the task ofexchanging; manipulating and reusing data on the Internet (and in other frameworks). Themain advantage of XML with respect to its predecessor is a clear separation between theways in which data; structure (DTD) and layout (stylesheet) are encoded. At the same time;XML preserves and enhances the hypertextual linking mechanism [XLink98; XPointer98]that has been so successful for HTML. It is therefore presumable that we will very soonwitness the emergence of XML repositories on the Web; ie; large collections of Internet-available linked XML documents.The announced availability of these large repositoriesimposes to develop suitable techniques for querying and transforming XML data. Recentproposals of query languages for XML--such as; for example; XML-QL [Deu+ 98] and XQL …,QL,1998,10
IsaLog¬: A deductive language with negation for complex-object databases with hierarchies,Paolo Atzeni; Luca Cabibbo; Giansalvatore Mecca,Abstract The IsaLog¬ model and language are presented. The model has complex objectswith classes; relations; and isa hierarchies. The language is strongly typed and declarative.The main issue is the definition of the semantics of the IsaLog language. The novel featuresare mostly due to the interaction of hierarchies with negation in the body of rules. Twosemantics are presented and shown to be equivalent: a stratified semantics based on anoriginal notion of stratification; needed in order to correctly deal with hierarchies; and areduction to logic programming with function symbols. The solutions are based on a newtechnique (explicit Skolem functors) that provides a powerful tool for manipulating objectidentifiers.,International Conference on Deductive and Object-Oriented Databases,1993,10
Messing up with BART: error generation for evaluating data-cleaning algorithms,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti; Donatello Santoro,Abstract We study the problem of introducing errors into clean databases for the purpose ofbenchmarking data-cleaning algorithms. Our goal is to provide users with the highestpossible level of control over the error-generation process; and at the same time developsolutions that scale to large databases. We show in the paper that the error-generationproblem is surprisingly challenging; and in fact; NP-complete. To provide a scalablesolution; we develop a correct and efficient greedy algorithm that sacrifices completeness;but succeeds under very reasonable assumptions. To scale to millions of tuples; thealgorithm relies on several non-trivial optimizations; including a new symmetry property ofdata quality constraints. The trade-off between control and scalability is the main technicalcontribution of the paper.,Proceedings of the VLDB Endowment,2015,9
The World Wide Web and Databases: International Workshop WebDB'98: Valencia; Spain; March 27-28; 1998: Selected Papers,Alberto O Mendelzon; Paolo Atzeni; Giansalvatore Mecca,*,*,1999,9
Interactive and Deterministic Data Cleaning,Jian He; Enzo Veltri; Donatello Santoro; Guoliang Li; Giansalvatore Mecca; Paolo Papotti; Nan Tang,Abstract We present Falcon; an interactive; deterministic; and declarative data cleaningsystem; which uses SQL update queries as the language to repair data. Falcon does not relyon the existence of a set of pre-defined data quality rules. On the contrary; it encouragesusers to explore the data; identify possible problems; and make updates to fix them.Bootstrapped by one user update; Falcon guesses a set of possible sql update queries thatcan be used to repair the data. The main technical challenge addressed in this paperconsists in finding a set of sql update queries that is minimal in size and at the same timefixes the largest number of errors in the data. We formalize this problem as a search in alattice-shaped space. To guarantee that the chosen updates are semantically correct; Falconnavigates the lattice by interacting with users to gradually validate the set of sql update …,Proceedings of the 2016 International Conference on Management of Data SIGMOD 2016,2016,8
Semantic-based mappings,Giansalvatore Mecca; Guillem Rull; Donatello Santoro; Ernest Teniente,Abstract Data translation consists of the task of moving data from a source database to atarget database. This task is usually performed by developing mappings; ie; executabletransformations from the source to the target schema. However; it is often the case that aricher description of the target database semantics is available under the form of aconceptual schema. We investigate how the mapping process changes when such a richconceptualization of the target database is available. As a major contribution; we develop atranslation algorithm that automatically rewrites a mapping from the source databaseschema to the target conceptual schema into an equivalent mapping from the sourceschema to the underlying target database schema. Experiments show that our approachscales nicely to complex conceptual schemas and large databases.,International Conference on Conceptual Modeling,2013,8
Efficient queries over web views,Giansalvatore Mecca; Alberto O.  Mendelzon; Paolo Merialdo,Large Web sites are becoming repositories of structured information that can benefit frombeing viewed and queried as relational databases. However; querying these viewsefficiently requires new techniques. Data usually resides at a remote site and is organizedas a set of related HTML documents; with network access being a primary cost factor inquery evaluation. This cost can be reduced by exploiting the redundancy often found in sitedesign. We use a simple data model; a subset of the Araneus data model; to describe thestructure of a Web site. We augment the model with link and inclusion constraints thatcapture the redundancies in the site. We map relational views of a site to a navigationalalgebra and show how to use the constraints to rewrite algebraic expressions; reducing thenumber of network accesses. We show that similar techniques can be used to maintain …,IEEE Transactions on Knowledge and Data Engineering,2002,8
Efficient queries over web views,Giansalvatore Mecca; Alberto O.  Mendelzon; Paolo Merialdo,Large Web sites are becoming repositories of structured information that can benefit frombeing viewed and queried as relational databases. However; querying these viewsefficiently requires new techniques. Data usually resides at a remote site and is organizedas a set of related HTML documents; with network access being a primary cost factor inquery evaluation. This cost can be reduced by exploiting the redundancy often found in sitedesign. We use a simple data model; a subset of the Araneus data model; to describe thestructure of a Web site. We augment the model with link and inclusion constraints thatcapture the redundancies in the site. We map relational views of a site to a navigationalalgebra and show how to use the constraints to rewrite algebraic expressions; reducing thenumber of network accesses. We show that similar techniques can be used to maintain …,IEEE Transactions on Knowledge and Data Engineering,2002,8
Benchmarking the chase,Michael Benedikt; George Konstantinidis; Giansalvatore Mecca; Boris Motik; Paolo Papotti; Donatello Santoro; Efthymia Tsamoura,Abstract The chase is a family of algorithms used in a number of data management tasks;such as data exchange; answering queries under dependencies; query reformulation withconstraints; and data cleaning. It is well established as a theoretical tool for understandingthese tasks; and in addition a number of prototype systems have been developed. Whileindividual chase-based systems and particular optimizations of the chase have beenexperimentally evaluated in the past; we provide the first comprehensive and publiclyavailable benchmark---test infrastructure and a set of test scenarios---for evaluating chaseimplementations across a wide range of assumptions about the dependencies and the data.We used our benchmark to compare chase-based systems on data exchange and queryanswering tasks with one another; as well as with systems that can solve similar tasks …,Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,2017,6
The RoadRunner Project: Towards automatic extraction of web data,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,ROADRUNNER is a research project that aims at developing solutions for automaticallyextracting data from large HTML data sources. The target of our research are data-intensiveWeb sites; ie; HTML-based sites that publish large amounts of data in a fairly complexstructure. In our view; we aim at ideally seeing the data extraction process of a data-intensive Web site as a black-box taking as input the URL of an entry point to the site (eg thehome page); and returning as output data extracted from HTML pages in the site in astructured database-like format. This paper describes the top-level software architecture ofthe ROADRUNNER System; which has been specifically designed to automatize the dataextraction process. Several components of the system have already been implemented; andpreliminary experiments show the feasibility of our ideas. Data-intensive Web sites …,Proceedings of the International,2002,6
Query languages for sequence databases: Termination and complexity,Giansalvatore Mecca; Anthony J.  Bonner,This paper develops a query language for sequence databases; such as genome databasesand text databases. Unlike relational data; queries over sequential data can easily produceinfinite answer sets since the universe of sequences is infinite; even for a finite alphabet. Thechallenge is to develop query languages that are both highly expressive and finite. Thispaper develops such a language as a subset of a logic for string databases calledSequence Datalog. The main idea is to use safe recursion to control and limit unsaferecursion. The main results are the definition of a finite form of recursion; called domain-bounded recursion; and a characterization of its complexity and expressive power. Althoughfinite; the resulting class of programs is highly expressive since its data complexity iscomplete for the elementary functions.,IEEE Transactions on Knowledge and Data Engineering,2001,6
From Datalog to Sequence Datalog: Languages and Techniques for Querying Sequence Databases,G Mecca,*,*,1996,6
Model Finiteness and Funetionality in a Declarative Language with Oid Invention.,Luca Cabibbo; Giansalvatore Mecca,Abstract Two important properties of IsaLog programs are studied: model niteness andfunctionality. Finiteness refers to the property of a program of having a nite model over everyinput instance. Functionality requires a model to contain no contradictory information aboutobject values. These two properties are shown to be undecidable. This is a consequence ofthe ability of IsaLog programs to simulate computations of arbitrary Turing machines;provided their input is coded as a suitable instance. Weakly recursive programs; a restrictedclass of IsaLog programs; is then investigated. It is shown that every weakly recursiveprogram admits a nite model over every input instance. Moreover; models for this class ofprograms can be computed in polynomial time with respect to the number of objects in theinput instance.,SEBD,1994,6
On federated single sign-on in e-government interoperability frameworks,Giansalvatore Mecca; Michele Santomauro; Donatello Santoro; Enzo Veltri,We consider the problem of handling digital identities within service-oriented architecture(SOA) architectures. We explore federated; single sign-on (SSO) solutions based on identitymanagers and service providers. After an overview of the different standards and protocols;we introduce a middleware-based architecture to simplify the integration of legacy systemswithin such platforms. Our solution is based on a middleware module that decouples thelegacy system from the identity-management modules. We consider both standard point-to-point service architectures; and complex government interoperability frameworks; and reportexperiments to show that our solution provides clear advantages both in terms ofeffectiveness and performance.,International Journal of Electronic Governance,2016,4
Soluzioni Infrastrutturali Open Source per il Sistema Pubblico di Cooperazione Applicativa.,Giansalvatore Mecca; Alessandro Pappalardo; Salvatore Raunich,Abstract Il Sistema Pubblico di Cooperazione Applicativa (SPCoop) è stato standardizzatodal CNIPA nel 2005 e finalmente promette di raggiungere il livello del concretodispiegamento. Questo articolo fornisce una panoramica sullo stato di avanzamentodell'iniziativa. Per cominciare introduce l'architettura di SPCoop e il suo raffinamentoprodotto nell'ambito del recente progetto ICAR. Inoltre; confronta due soluzioni open sourceattualmente disponibili per la realizzazione dell'architettura.,SEBD,2008,4
The Spicy Project: A New Approach to Data Matching.,Angela Bonifati; Giansalvatore Mecca; Alessandro Pappalardo; Salvatore Raunich,*,SEBD,2006,4
A logical model for metadata in Web bases,P Atzeni; G Mecca; P Merialdo; G Sindoni,Abstract In systems that provide integrated management of both structured; database style;data and semistructured; Web-style; data; metadata may allow for the explicit managementof information about the structure and data content of pages. Unfortunately; the HTMLstandard doesn't offer any explicit framework for document metadata management. TheeXtensible Markup Language vice versa allows for the explicit description of the structure ofa document; but it lacks of database perspective. The aim of this paper is then to describe anapproach to the generation of database derived Web meta information that is based on alogical model for Web pages and to show how this approach can be effectively used for bothHTML and XML based systems.,ERCIM Workshop on metadata for Web databases,1998,4
RoadRunner: Towards Automatic Data Extraction from Large Web Sites; 2001,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,*,27th International Conference on Very Large Databases (VLDB 2001),*,4
Benchmarking Data Curation Systems.,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti; Donatello Santoro,Abstract Data curation includes the many tasks needed to ensure data maintains its valueover time. Given the maturity of many data curation tasks; including data transformation anddata cleaning; it is surprising that rigorous empirical evaluations of research ideas are soscarce. In this work; we argue that thorough evaluation of data curation systems imposesseveral major obstacles that need to be overcome. First; we consider the outputs generatedby a data curation system (for example; an integrated or cleaned database or a set ofconstraints produced by a schema discovery system). To compare the results of differentsystems; measures of output quality should be agreed upon by the community and; sincesuch measures can be quite complex; publicly available implementations of these measuresshould be developed; shared; and optimized. Second; we consider the inputs to the data …,IEEE Data Eng. Bull.,2016,3
Exploratory Computing: What is there for the Database Researcher?,Marcello Buoncristiano; Giansalvatore Mecca; Elisa Quintarelli; Manuel Roveri; Donatello Santoro; Letizia Tanca,Abstract. The need for effective tools helping users to make sense of very big datasets hasreceived a lot of attention lately. In this paper we propose a paradigm for databaseexploration which is in turn inspired by the exploratory computing vision [2]. We maydescribe exploratory computing as the step-by-step “conversation” of a user and a systemthat “help each other” to refine the data exploration process; ultimately gathering newknowledge that concretely fulfills the user needs. In this broad and innovative context; thispaper proposes a model to concretely perform this kind of exploration over a database. Themodel is general enough to encompass most data models and query languages that havebeen proposed for data management in the last few years. At the same time; it is preciseenough to provide a first formalization of the problem and reason about the research …,SEBD,2015,2
Mapping and Cleaning: the Llunatic Way,Floris Geerts; Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,Abstract We address the problem of bringing together two crucial activities in dataintegration and data quality; ie; transforming data using schema mappings; and fixingconflicts and inconsistencies using data repairing. This problem is made complex by severalfactors. First; schema mappings and data repairing have traditionally been considered asseparate activities; and research has progressed in a largely independent way in the twofields. Second; the elegant formalizations and the algorithms that have been proposed forboth tasks have had mixed fortune in scaling to large databases. In the paper; we introducea very general notion of a mapping and cleaning scenario that incorporates a wide variety offeatures; like; for example; user interventions. We develop a new semantics for thesescenarios that represents a conservative extension of previous semantics for schema …,*,2014,2
IQ-METER-an evaluation tool for data-transformation systems,Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,We call a data-transformation system any system that maps; translates and exchanges dataacross different representations. Nowadays; data architects are faced with a large variety oftransformation tasks; and there is huge number of different approaches and systems thatwere conceived to solve them. As a consequence; it is very important to be able to evaluatesuch alternative solutions; in order to pick up the right ones for the problem at hand. To dothis; we introduce IQ-Meter; the first comprehensive tool for the evaluation of data-transformation systems. IQ-Meter can be used to benchmark; test; and even learn the bestusage of data-transformation tools. It builds on a number of novel algorithms to measure thequality of outputs and the human effort required by a given system; and ultimately measures“how much intelligence” the system brings to the solution of a data-translation task.,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,2
The RoadRunner Web Data Extraction System.,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,Extracting data from HTML text files and making them available to computer applications isbecoming of utmost importance for developing several emerging e-services. This paperpresents RoadRunner; a research project that aims at developing solutions for automaticallyextracting data from large HTML data sources. We concentrate on data-intensive Web sites;that is; sites that deliver large amounts of data through a complex graph of linked HTMLpages. The paper describes the top-level software architecture of the RoadRunner System;which has been specifically designed to automatize the data extraction process. The paperis organized as follows. First; Section 2 illustrates an overview of the project and gives anintuition of its key ideas. Then; Section 3 describes the overall architecture of theRoadRunner system. Section 4 concludes the paper discussing related works.,SEBD,2001,2
Database cooperation: classification and middleware tools,Paolo Atzeni; Luca Cabibbo; Giansalvatore Mecca,Abstract New criteria for the classification for database cooperation; based on the nature ofthe component databases; are proposed. In fact; the traditional criteria-heterogeneity;distribution; and autonomy-are often constraints for the design process; rather than designparameters. In this case; other features of the component databases should be addressed. Amore useful classification can be based on 3 new criteria: 1. degree of transparency; 2.complexity of operations; and 3. level of liveliness of data. This leads to distinguishing 3main categories of systems: 1. multidatabases; 2. data warehouses; and 3. local informationsystems with external data.,Journal of Database Management,2000,2
Un database per gli schemi acquedottistici di adduzione: codifica e ipotesi di classificazione dei livelli di servizio,Pierluigi Claps; C Sileo; G Mecca,Summary A drinking water system is conceptually redesigned here using its basic elementsto evaluate its main operative characteristics. To this end; a relational database is built andorganised so as to preserve information on the characteristics of connectivity of the systemelements. This procedure presents interesting aspects with regard to the application ofpreliminary analysis even on large water systems; owing to the generality of theschematisation and to the wide availability of the database management system that wasused. The screening on service levels made with this procedure can provide usefulinformation in view of the application of more refined models for reliability evaluation. Anassessment of the operation service levels of the Basento water system in Basilicata (Italy) ispresented as a case study.,L’Acqua,2000,2
Design and Maintenance of Data-Intensive Web Sites (1997),Paolo Atzeni; Giansalvatore Mecca; Paolo Merialdo,*,Lecture Notes in Computer Science,*,2
BARt in Action: Error Generation and Empirical Evaluations of Data-Cleaning Systems,Donatello Santoro; Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti,Abstract Repairing erroneous or conflicting data that violate a set of constraints is animportant problem in data management. Many automatic or semi-automatic data-repairingalgorithms have been proposed in the last few years; each with its own strengths andweaknesses. Bart is an open-source error-generation system conceived to support thoroughexperimental evaluations of these data-repairing systems. The demo is centered aroundthree main lessons. To start; we discuss how generating errors in data is a complex problem;with several facets. We introduce the important notions of detectability and repairability of anerror; that stand at the core of Bart. Then; we show how; by changing the features of errors; itis possible to influence quite significantly the performance of the tools. Finally; we concretelyput to work five data-repairing algorithms on dirty data of various kinds generated using …,Proceedings of the 2016 International Conference on Management of Data,2016,1
Spatial data processing with MapReduce,Tilani Gunawardena; Annamaria Vicari; Giansalvatore Mecca,The current development of high performance parallel supercomputing infrastructures arepushing the boundaries of applications of science and are bringing new paradigms intoengineering practices and simulations. Earthquake engineering is also one of the majorfields; which benefits from above by looking for solutions in grid computing and cloudcomputing techniques. Generally; earthquake simulations involve analysis of petabytes ofdata. Analyzing these large amounts of data in parallel in thousands of nodes in computerclusters results in gaining high performances. Open source cloud solutions such as HadoopMapReduce; which is highly scalable and capable of processing large amount of datarapidly in parallel on large clusters provide better solution compared to RDBDM. Both GPUsand MapReduce are designed to support vast data parallelism. For performance …,Industrial and Information Systems (ICIIS); 2015 IEEE 10th International Conference on,2015,1
Error Generation for Evaluating Data Cleaning Algorithms,P Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti; Donatello Santoro,ABSTRACT We address the problem of generating errors within clean databases for thepurpose of benchmarking data-cleaning and data-repairing algorithms. Our goal is toprovide users with the highest possible level of control over the error generation processand at the same time develop a solution that scales to large databases. This is challenging;because the error generation problem is NP-complete. The main technical contribution ofthis paper is to develop an efficient PTIME algorithm which sacrifices completeness in anintelligent fashion that allows us to succeed under reasonable assumptions. However;scaling to databases with millions of tuples requires additional non-trivial optimizationsincluding exploiting a symmetry property of data quality constraints.,*,2015,1
Middleware-Oriented Government Interoperability Frameworks: A Comparison.,Giansalvatore Mecca; Michele Santomauro; Donatello Santoro; Enzo Veltri,Abstract: We discuss deployment solutions for e-Government Interoperability Frameworks(GIFs). We concentrate on middleware-oriented GIFs; ie; those in which middlewaremodules act as intermediaries among information systems that need to exchange data andservices. A prominent example is the Italian SPCoop interoperability framework. We reviewthe SPCoop architecture; and two popular open-source implementations of its core modules;called OpenSPCoop and freESBee. We argue that the comparison of these two solutions isrelevant since they obey to radically different philosophies; both in terms of the relationshipto the underlying J2EE container; and of their internal module organization. Then; wediscuss one of the main problems in large-scale deployment of SPCoop-like GIFs; namelythe need to quickly deploy a large number of middleware instances over a relatively small …,J. UCS,2014,1
A Short History of Schema Mapping Systems.,Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,There are many applications that need to exchange; correlate; and integrate heterogenousdata sources. These information integration tasks have long been identified as importantproblems and unifying theoretical frameworks have been advocated by databaseresearchers [5]. To solve these problems; a fundamental requirement is that of manipulatingmappings among data sources. The application developer is typically given two schemas–one called the source schema; the other called the target schema–that can be based ondifferent models; technologies; and rules. Mappings; also called schema mappings; areexpressions that specify how an instance of the source repository should be translated intoan instance of the target repository. In order to be useful in practical applications; theyshould have an executable implementation–for example; by means of SQL queries or …,SEBD,2012,1
Schema Mapping and Data Exchange Tools: Time for the Golden Age,Giansalvatore Mecca; Paolo Mecca,Abstract In the last 10 years; schema mapping management has become an importantresearch area in data transformation; exchange; and integration systems. The reasons for itssuccess can be found in the declarative nature of its building block (thus enabling cleansemantics and easy to use design tools) paired with the efficiency and modularity in thedeployment step. In this paper we sketch a line of evolution in schema-mappings and dataexchange systems; through what we identify as three main ages. We start presenting thefoundations of schema mapping tools and the first tools aimed at translating data from asource to a target schema in the first; heroic age. We then discuss the silver age; whenschema mapping tools have grown their way into complex systems and have beentranslated into both commercial and open-source tools. Finally; we show how recent …,it-Information Technology,2012,1
Ontological matchmaking in recommender systems,Angela Bonifati; Giansalvatore Mecca; Domenica Sileo; Gianvito Summa,Abstract: The electronic marketplace offers great potential for the recommendation ofsupplies. In the so called recommender systems; it is crucial to apply matchmakingstrategies that faithfully satisfy the predicates specified in the demand; and take into accountas much as possible the user preferences. We focus on real-life ontology-drivenmatchmaking scenarios and identify a number of challenges; being inspired by suchscenarios. A key challenge is that of presenting the results to the users in an understandableand clear-cut fashion in order to facilitate the analysis of the results. Indeed; such scenariosevoke the opportunity to rank and group the results according to specific criteria. A furtherchallenge consists of presenting the results to the user in an asynchronous fashion; iethe'push'mode; along with the'pull'mode; in which the user explicitly issues a query; and …,arXiv preprint arXiv:1010.2148,2010,1
Back to Gold's Age: Bridging the Gap Between Traditional Grammar Inference and Web Information Extraction.,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,Since Gold's Theorem (1967); grammar inference for regular languages has been athoroughly studied topic; with an elegant theoretical background and well establishedtechniques. One of the main contributions of these works is the study of properties of thoselanguages for which the inference process can be performed in a completely automatic way;and of the relative algorithms. Recently; information extraction from Web sites has imposeditself as a relevant research field for the so called “Semantic Web”. Since extraction isperformed by wrappers; which are essentially grammar parsers for the HTML code of Webpages; grammar inference could in principle play a fundamental role in this field. However;despite 30 years of research; due to some limitations of the traditional framework; essentiallynone of the recent approaches to Web information extraction reuse theories and …,SEBD,2002,1
IsaLog¬: a deductive language with negation for complex-object databases with hierarchies,Paolo Atzeni; Luca Cabibbo; Giansalvatore Mecca,Abstract IsaLog (¬) is a research activity aimed at developing a framework that integratesdeductive and object-oriented features. The data model has complex objects with classes;relations; and isa hierarchies; and the language is rule based. The main issue is thedefinition of the semantics of the language. For the (positive) IsaLog framework threedifferent semantics are given and proven to be equivalent: a model-theoretic semantics; afixpoint semantics; and a semantics based on a reduction to ordinary logic programmingwith functionasymbols. Then the semantics of the IsaLog¬ language is proposed. It presentsnovel features mostly due to the interaction of hierarchies with negation in the body of rules.Two semantics are presented for IsaLog¬ programs: a stratified semantics based on anoriginal notion of stratification; which takes into account hierarchies; and a reduction to …,Data & knowledge engineering,1997,1
Querying Genome Databases.,Giansalvatore Mecca,Abstract Genome Databases contain genetic information about human beings and otherspecies of living organisms. Since 1986; year in which the Human Genome Project started;they have grown exponentially in size every year. Due to the unusual structure of the storedinformation; traditional DBMS and query languages have provided only little support so far.In this paper we discuss some of the issues related to genomic information management.We mainly focus on query languages and present a logic; called Sequence Datalog;designed to be an effective tool for querying genome data.,SEBD,1995,1
Travel agency: a LOGIDATA+ application,Luca Cabibbo; Giansalvatore Mecca,Abstract Expressive power and flexibility of the LOGIDATA+ model and language areexplored. We analyze a program for travel agencies: the problem we consider is concernedwith the construction of travels on the basis of the clientś specifications. The program queriesthe database and generates a report with all the available choices in terms of transfers andaccomodations. The scheme and the clauses use a wide range of language functionalities;as complex objects; object identifiers; classes; relations; functions; is-a relationships andrecursion.,*,1993,1
Schema Mappings: From Data Translation to Data Cleaning,Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,Abstract Schema mapping management is an important research area in datatransformation; integration; and cleaning systems. The reasons for its success can be foundin the declarative nature of its building block (thus enabling clean semantics and easy to usedesign tools) paired with the efficiency and modularity in the deployment step. In this chapterwe cover the evolution of schema-mappings through what we identify as three main ages.We start presenting the foundations of schema mapping tools and the first tools aimed attranslating data from a source to a target schema in the first; heroic age. We then discuss thesilver age; when schema mapping tools have grown their way into complex systems andhave been translated into both commercial and open-source tools. Finally; we show howrecent results in schema-mapping are stimulating a third; golden age; with novel research …,*,2018,*
INDIANA the Database Explorer,Antonio Giuzio; Giansalvatore Mecca; Elisa Quintarelli; Manuel Roveri; Donatello Santoro; Letizia Tanca,Abstract We propose INDIANA; a system conceived to support a novel paradigm of databaseexploration. INDIANA guides users that are interested in gaining insights about a databaseby involving them in a “conversation”. During the conversation; the system proposes somefeatures of the data that are “interesting” from the statistical viewpoint. The user selects someof these as starting points; and INDIANA reacts by suggesting new features related to theones selected by the user. A key requirement in this context is the ability to exploretransactional databases; without the need to conduct complex pre-processing. To this end;we develop a number of novel algorithms to support interactive exploration of the database.We report an in-depth experimental evaluation to show that the proposed systemguarantees a very good trade-off between accuracy and scalability; and a user study that …,*,2017,*
Interactive Data Repairing: the FALCON Dive,Enzo Veltri; Donatello Santoro; Giansalvatore Mecca; Paolo Papotti; Jian He; Gouliang Li; Nan Tang,Abstract. In this paper we discuss Falcon; an interactive; deterministic; and declarative datacleaning system. Unlike traditional rule-based system; Falcon does not rely on the existenceof a set of pre-defined data quality rules; but it encourages users to explore the data; identifypossible problems; and make updates to fix them. The main technical challenge consists infinding a set of rules; expressed as sql update queries; that are semantically correct and thatfixes the largest number of errors in the data. Falcon navigates the lattice by interacting withusers to gradually checking the correctness of a set of rules. We have conducted extensiveexperiments using both real-world and synthetic datasets to show that Falcon can effectivelycommunicate with users in data repairing.,25th Italian Symposium on Advanced Database Systems; SEBD 2017,2017,*
Sequence Protein Identification by Randomized Sequence Database and Transcriptome Mass Spectrometry (SPIDER-TMS): From Manual to Automatic Application...,Raffaella Pascale; Gerarda Grossi; Gabriele Cruciani; Giansalvatore Mecca; Donatello Santoro; Renzo Sarli Calace; Patrizia Falabella; Giuliana Bianco,Sequence protein identification by a randomized sequence database and transcriptomemass spectrometry software package has been developed at the University of Basilicata inPotenza (Italy) and designed to facilitate the determination of the amino acid sequence of apeptide as well as an unequivocal identification of proteins in a high-throughput manner withenormous advantages of time; economical resource and expertise. The software package isa valid tool for the automation of a de novo sequencing approach; overcoming the mainlimits and a versatile platform useful in the proteomic field for an unequivocal identification ofproteins; starting from tandem mass spectrometry data. The strength of this software is that itis a user-friendly and non-statistical approach; so protein identification can be consideredunambiguous.,European Journal of Mass Spectrometry,2016,*
Database exploration: Problems and opportunities,Giansalvatore Mecca,The classical ways to access data; ie through a search engine or by querying a database;have today become insufficient when confronted with the new needs of data exploration andinterpretation: at a user request; a search engine retrieves a very large collection ofdocuments; while the query engine of a DBMS grants access to all the data that satisfycertain criteria. Once the documents or the data items have been received; the user can onlystart reading them and try to make sense of their contents; possibly relying on a rankingfunction that sorts the results in order of “relevance”. After all; this mechanism only workswell when the intention of the search or query is to locate a specific information; such as theaddress of a restaurant or the salaries of all the employees with a certain position. Instead;ranked retrieval and querying are no longer adequate when the need of the user is being …,Data Engineering Workshops (ICDEW); 2016 IEEE 32nd International Conference on,2016,*
GROM: a general rewriter of semantic mappings,Giansalvatore Mecca; Guillem Rull Fort; Donatello Santoro; Ernest Teniente López,Abstract We present GROM; a tool conceived to handle high-level schema mappingsbetween semantic descriptions of a source and a target database. GROM rewrites mappingsbetween the virtual; view-based semantic schemas; in terms of mappings between the twophysical databases; and then executes them. The system serves the purpose of teachingtwo main lessons. First; designing mappings among higher-level descriptions is oftensimpler than working with the original schemas. Second; as soon as the view-definitionlanguage becomes more expressive; to handle; for example; negation; the mappingproblem becomes extremely challenging from the technical viewpoint; so that one needs tofind a proper trade-off between expressiveness and scalability.,Advances in Database Technology-EDBT 2016: 19th International Conference on Extending Database Technology: Bordeaux; France; March 15-16: proceedings,2016,*
Rewriting Ontology-Based Mappings,Ernest Teniente Giansalvatore Mecca; Guillem Rull; Donatello Santoro,*,*,2015,*
An Overview of the Llunatic System.,Floris Geerts; Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,Data transformation and data cleaning are two very important research and applicationproblems. Data transformation; or data exchange [7]; relies on declarative schema mappingsto translate and integrate data coming from one or more source schemas into a differenttarget schema. Data cleaning; or data repairing [8]; uses declarative dataquality rules inorder to detect and remove errors and inconsistencies from the data. It is widely recognisedthat whenever mappings among different sources are in place; there is a strong need toclean and repair data. Despite this need; database research has so far investigated schemamappings and data repairing essentially in isolation. We present the LLUNATIC [11; 12]mapping and cleaning system; the first comprehensive proposal to handle schemamappings and data repairing in a uniform way. LLUNATIC is based on the intuition that …,SEBD,2014,*
On the Quality and Effectiveness of Data Transformation Systems,Giansalvatore Mecca; Paolo Papotti; Salvatore Raunich; Donatello Santoro,The problem of translating data among heterogeneous representations is a long standingissue in the IT industry and in database research. The first data translation systems dateback to the seventies. In these years; many different proposals have emerged to alleviate theburden of manually expressing complex transformations among different repositories; sothat today we have a very broad class of systems; ranging from schemamatching to schema-mappings and data-exchange; from data-integration to ETL; from object-relational mappingto data-fusion; and data-cleaning. These systems differ under many perspectives. There arevery procedural and very expressive systems; like those used in ETL. There are moredeclarative; but somehow less expressive schema-mapping systems. Some commercialsystems are essentially graphical user interfaces for defining XSLT queries. Others; like …,Proceedings of the 21st Italian Symposium on Database Systems (Sistemi Evoluti per Basi di Dati),2013,*
Sulla Classificazione delle Sedi di Pubblicazione nella Valutazione della Produzione Scientifica.,Giansalvatore Mecca; Marcello Buoncristiano; Donatello Santoro,Il problema della valutazione della produzione scientifica è di grande attualità nelleUniversità italiane. E'di questi giorni l'avvio delle procedure di valutazione nazionale dellaqualità della ricerca relativa al quadriennio 2004-2010; e la pubblicazione del DM n. 19 del27 gennaio 2012 (Ministero dell'Università e della Ricerca; 2012) che introduce il sistemanazionale di valutazione del sistema universitario e obbliga le università a dotarsi di unproprio sistema di autovalutazione basato su “parametri oggettivi; volti a misurare in ognimomento l'efficienza e l'efficacia della didattica e della ricerca messa in atto dai singoliatenei ea stimolare la competitività e la qualità degli stessi”. E'quindi ragionevole attendersiche nell'arco dei prossimi mesi tutte le università italiane dovranno dotarsi di modelli eprocedure per la misurazione della “performance” dei docenti (professori e ricercatori). In …,SEBD,2012,*
Noodles: a clustering engine for the web,Giansalvatore Mecca; Salvatore Raunich; Alessandro Pappalardo; Donatello Santoro,Abstract The paper describes the Noodles system; a clustering engine for Web and desktopsearches. By employing a new algorithm for document clustering; based on Latent SemanticIndexing; Noodles provides good classification power to simplify browsing of search resultsby casual users. In the paper; we provide some background about the problem of clusteringsearch results; give an overview of the novel techniques implemented in the system; andpresent its architecture and main features.,International Conference on Web Engineering,2007,*
Clustering Web and Desktop Searches.,Giansalvatore Mecca; Salvatore Raunich; Alessandro Pappalardo,*,SEBD,2007,*
Schema Matching and Query Discovery: The Spicy Way,A Bonifati; G Mecca; A Pappalardo; S Raunich; G Summa,*,*,2007,*
Core Database Technology Program Committee,Anastassia Ailamaki; Gustavo Alonso; Walid Aref; Lars Arge; Brian Babcock; Mikael Berndtsson; Elisa Bertino; Claudio Bettini; Michael Boehlen; Anthony Bonner; Philippe Bonnet; Alex Buchmann; Tiziana Catarci; Surajit Chaudhuri; Peter Dadam; Amol Deshpande; Asuman Dogac; Christos Faloutsos; Elena Ferrari; Johann-Christoph Freytag; Dieter Gawlick; Johannes Gehrke; Torsten Grust; Ralf Hartmut Güting; Jayant Haritsa; Chris Jermaine; Christoph Koch; George Kollios; Mong Li Lee; Wolfgang Lindner; David Lomet; Hongjun Lu; Samuel Madden; Giansalvatore Mecca; Alberto Mendelzon; Rosa Meo; Tova Milo; Michele Missikoff; C Mohan; Mario Nascimento; Shojiro Nishio; Ed Omiecinski; Norman Paton; Torben Bach Pedersen; Calton Pu; Philippe Pucheral; Raghu Ramakrishnan; Thomas Rölleke; Ken Ross; Gunther Saake; Albrecht Schmidt; Marc Scholl; Bernhard Seeger,Committee Chair: Martin Kersten; CWI; The Netherlands … Serge Abiteboul; INRIA; France AnastassiaAilamaki; Carnegie Mellon University; USA Gustavo Alonso; ETH Zurich; Switzerland WalidAref; Purdue University; USA Lars Arge; Aarhus University; Denmark Brian Babcock; StanfordUniversity; USA Mikael Berndtsson; University of Skövde; Sweden Elisa Bertino; PurdueUniversity; USA Claudio Bettini; University of Milan; Italy Michael Boehlen; Free University ofBolzano/Bozen; Italy Peter Boncz; CWI; The Netherlands Anthony Bonner; University ofToronto; Canada Philippe Bonnet; University of Copenhagen; Denmark Alex Buchmann; Universityof Darmstadt; Germany Tiziana Catarci; University of Rome 'La Sapienza'; Italy SurajitChaudhuri; Microsoft; USA Vassilis Christophides; FORTH; Greece Peter Dadam; Universityof Ulm; Germany Amol Deshpande; University of California; Berkeley; USA Asuman …,VLDB 2005: 31st International Conference on Very Large Data Bases: Proceedings of the 31st International Conference on Very Large Data Bases; Trondheim; Norway; August 30-September 2; 2005,2005,*
Esperienze e Sperimentazioni nell’Insegnamento Universitario dell’Informatica,Giansalvatore Mecca,Nell'Università italiana che cambia è importante sottoporre a valutazione critica anche ilmodo tradizionale di fare didattica. In questo lavoro vengono discusse alcune esperienze dicarattere metodologico per migliorare la qualità dell'insegnamento. L'obiettivo è quello difornire un contributo iniziale ad una più ampia discussione sui metodi e gli strumenti per ladidattica universitaria.,*,2003,*
Experiences in XML data management.,Giansalvatore Mecca; Paolo Merialdo; Paolo Atzeni; Valter Crescenzi,A large body of research has been recently motivated by the attempt to extend databasemanipulation techniques to data on the Web (see [16] for a survey). Most of these researchefforts–which range from the definition of Web query languages and the relatedoptimizations; to systems for Web site development and management; and to integrationtechniques–started before XML was introduced; and therefore have strived for a long time tohandle the highly heterogeneous nature of HTML pages. In the meanwhile; Web datasources have evolved from small; home-made collections of HTML pages into complexplatforms for distributed data access and application development; and XML promises toimpose itself as a more appropriate format for this new breed of Web sites. XML brings dataon the Web closer to databases; since; differently from HTML; it is based on a clean …,SEBD,2000,*
The World Wide Web and databases (Italian),M Missikoff; KG Jeffery; G Mecca; M Fioroni; G Meazza,*,Rivista di Informatica,1999,*
WWW e basi di dati,M Missikoff; KG Jeffery; G Mecca; M Fioroni; G Meazza,*,RIVISTA DI INFORMATICA-MILAN-,1999,*
Article No. SS981607,Eric Bach; Ian Barland; Paul Beame; Anthony Bonner; Samuel R Buss; Jin-Yi Cai; Herve Caussinus; Vinay K Chaudhri; Anne Condon; Mariano P Consens; Stephen Cook; P Diaconis; Danny Dolev; Guozhu Dong; Jeff Edmonds; Uriel Feige; Harold N Gabow; Elton Glaser; Vassos Hadzilacos; Torben Hagerup; Thomas A Henzinger; Russell Impagliazzo; Sanjay Jain; Jyrki Katajainen; Lydia E Kavraki; Idit Keidar; Joe Kilian; Jon Kleinberg; Phokion G Kolaitis; Peter W Kopke; Jean-Claude Latombe; Philip M Long; Oded Maler; Pierre McKenzie; Giansalvatore Mecca; Tova Milo; Peter Bro Miltersen; Rajeev Motwani; Stefan Nilsson; Noam Nisan; Naomi Nishimura; Philippe Picouet; Toniann Pitassi; Anuj Puri; Prabhakar Ragde; Prabhakar Raghavan; Rajeev Raman; Steven Rudich; Shmuel Safra; L Saloff-Coste; Leonard J Schulman; Alan Selman; Aravind Srinivasan; Jianwen Su; Celena Tanguay; Eva Tardos; Madhukar N Thakur; Denis Therien; Dieter van Melkebeek; Pravin Varaiya; Victor Vianu; Heribert Vollmer; Avi Wigderson,*,Journal of Computer and System Sciences,1998,*
The Araneus Project: Extending Database Techniques to the World Wide Web.,Giansalvatore Mecca; Paolo Merialdo; Alessandro Masci; Giuseppe Sindoni,Abstract. The Araneus project aims at developing tools for datamanagement on the WorldWide Web. We have implemented a system; called a Web-base Management system; formanaging Web data. The system is designed to support several classes of applications:(i)highlevel access to data in the Web;(ii) design; implementation and maintenance of Websites;(iii) cooperative applications on the Web. We discuss the lessons learned from ourexperiences with the system; ranging from database-style query interfaces to popular Websites; to the design and implementation of several sites; among which an integrated Webmuseum; which correlates data coming from several virtual museums on the Web.,SEBD,1998,*
IsaLog:: for Complex-Object Databases with Hierarchies,Paolo Atzeni; Luca Cabibbo; Giansalvatore Mecca,Abstract IsaLog: is a research activity aimed at developing a framework that integratesdeductive and object-oriented features. The data model has complex objects with classes;relations; and isa hierarchies; and the language is rule based. The main issue is the denition of the semantics of the language. For the positive IsaLog framework three di erentsemantics are given and proven to be equivalent: a model-theoretic semantics; a xpointsemantics; and a semantics based on a reduction to ordinary logic programming withfunction symbols. Then the semantics of the IsaLog,*,1996,*
Dipartimento di Discipline Scientifiche Universita di Roma Tre Via della Vasca Navale 84 00146 Roma,Paolo Atzeni; Luca Cabibbo; Giansalvatore Mecca,*,*,1996,*
Sequence Datalog: un prototipo per l'interrogazione di sequenze.,Salvatore Labonia; Giansalvatore Mecca,Abstract. In questo articolo viene descritto il prototipo del linguaggio Sequence Datalog.Sequence Dataloge un linguaggio di interrogazione per basi di dati contenenti sequenze;ovvero stringhe di caratteri su un alfabeto finito; il modello di dati a cui si riferiscee unasemplice estensione del modello relazionale; in cui le ennuple all'interno delle relazionipossono contenere; oltre che valori atomici come interi o caratteri; anche sequenze dicaratteri; di lunghezza non fissata a priori. La sintassi e la semantica del linguaggio sonoispirate al Datalog; esteso per consentire la manipolazione delle sequenze. Il prototipoestato implementato utilizzando il motore inferenziale di Coral ed arricchendolo conopportune primitive per la gestione delle sequenze.,SEBD,1996,*
Cooperazione di Basi di Dati Autonome: Criteri di Classificazione e Strumenti di Middleware?,Paolo Atzeniι; Luca Cabibboι; Giansalvatore Mecca,Sommario Vengono discusse alcune problematiche relative alla realizzazione di sistemiinformativi cooperanti; con particolare riferimento al ruolo delle basi di dati. Viene notato inprimo luogo come l'eterogeneit a; l'autonomia e la distribuzione; pur essendo presenti inmisura maggiore o minore; costituiscono; soprattutto nel breve termine; vincoli sui sistemicooperanti. Di conseguenza; vengono individuati nuovi criteri di classificazione: a il livello ditrasparenza; b la complessit a delle operazioni distribuite; c il livello di attualit a dei dati. Ci oporta ad individuare tre categorie principali di sistemi: i sistemi multidatabase; ii collezioni didati replicati; iii sistemi informativi locali con dati esterni. Per ciascuna di queste categorievengono individuate soluzioni architetturali basate su strumenti di middleware disponibili sulmercato.,*,1996,*
A declarative language for complex-object databases,P Atzeni; L Cabibbo; G Mecca,*,ATTI DEL CONGRESSO ANNUALE-ASSOCIAZIONE ITALIANA PER L INFORMATICA ED IL CALCOLO AUTOMATICO,1993,*
First International Workshop on Distributed XML Processing (DXP 2009),Alfredo Cuzzocrea; David Taniar; Denilson Barbosa; Sourav Bhowmick; Stéphane Bressan; Andrea Calì; Tiziana Catarci; Jerome Darmont; Ernesto Damiani; Sergio Flesca; Irini Fundulaki; Giorgio Ghelli; Mohand-Said Hacid; Anastasios Kementsietsidis; Laks VS Lakshmanan; Xuemin Lin; Chengfei Liu; Sanjay Madria; Giansalvatore Mecca; Apostolos Papadopoulos; Zbigniew Ras; Henryk Rybinski; Keun Ho Ryu; Bernhard Thalheim; Jeffrey Yu Xu; Demetris Zeinalipour,First International Workshop on Distributed XML Processing (DXP 2009) … AlfredoCuzzocrea; ICAR-CNR and University of Calabria; Italy David Taniar; Monash University; Australia… Alfredo Cuzzocrea; ICAR-CNR and University of Calabria; Italy David Taniar; MonashUniversity; Australia … Denilson Barbosa; University of Alberta; Canada Sourav Bhowmick;Nanyang University of Technology; Singapore Stéphane Bressan; National University ofSingapore; Singapore Andrea Calì; Oxford University; United Kingdom Tiziana Catarci; Universityof Rome “La Sapienza”; Italy Jerome Darmont; University of Lyon 2; France ErnestoDamiani; University of Milan; Italy Sergio Flesca; University of Calabria; Italy Irini Fundulaki;ICS-FORTH; Greece Giorgio Ghelli; University of Pisa; Italy Mohand-Said Hacid; University ofLyon 1; France Anastasios Kementsietsidis; IBM TJ Watson Research Center; USA Laks …,*,*,*
JM Vara; V. Andrikopoulos; MP Papazoglou; E. Marcos,C Zheng; W Shen; H Ghenniwa; G Mecca; M Santomauro; D Santoro; E Veltri; H Mezni; W Chainbi; K Ghedira; A Joha; M Janssen,JM Vara; V. Andrikopoulos; MP Papazoglou; E. Marcos: Towards Model-Driven EngineeringSupport for Service Evolution In the field of Service-Oriented Architecture (SOA) evolution is akey issue given the non-trivial nature of updating widely distributed and heterogeneoussystems. With this in mind; in this work we used some of the technologies developed in the...… C. Zheng; W. Shen; H. Ghenniwa: An Adaptive Intent Resolving Scheme for Service Discoveryand Integration Service discovery and integration is an important research area with efforts investedto explore the potential advantages of collaborative computing in general and service-orientedcomputing in particular. However; current technologies still … G. Mecca; M. Santomauro;D. Santoro; E. Veltri: Middleware-Oriented Government Interoperability Frameworks: A ComparisonWe discuss deployment solutions for e-Government Interoperability Frameworks (GIFs) …,*,*,*
Universit a degli Studi di Roma Tre,Giansalvatore Mecca; P Atzeni; Paolo Merialdo; A Masci; G Sindoni,ABSTRACT Large web sites are becoming repositories of structured information that canbene t from being viewed and queried as relational databases. However; querying theseviews e ciently requires new techniques. Data usually resides at a remote site and isorganized as a set of related HTML documents; with network access being a primary costfactor in query evaluation. This cost can be reduced by exploiting the redundancy oftenfound in site design. We use a simple data model; a subset of the Araneus data model; todescribe the structure of a web site. We augment the model with link and inclusionconstraints that capture the redundancies in the site. We map relational views of a site to anavigational algebra and show how to use the constraints to rewrite algebraic expressions;reducing the number of network accesses. We show that similar techniques can be used …,*,*,*
Dipartimento di Informatica e Sistemistica Universita di Roma\La Sapienza" Via Salaria 113| 00198 Roma; Italy,P Atzeni; L Cabibbo; G Mecca,*,*,*,*
Information Extraction by Convergent Boundary Classification/1 Aidan Finn and Nicholas Kushmerick IE Evaluation: Criticisms and Recommendations/7 A. Lavelli; M...,Yongzheng Zhang; Evangelos Milios; Nur Zincir-Heywood; Stephen Soderland; Oren Etzioni; Tal Shaked; Daniel S Weld; Un Yong Nahm; Raymond J Mooney; Kristina Lerman; Cenk Gazen; Steven Minton; Craig Knoblock; Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo; Farah Benamara; Doug Downey; Rakesh Gupta; Mykel J Kochenderfer; Cheng Niu; Wei Li; Rohini K Srihari; Ana-Maria Popescu; Alexander Yates; Rohit Joshi; Xiaoli Li; Sreeram Ramachandaran; Tze Yun Leong,IE Evaluation: Criticisms and Recommendations / 7 A. Lavelli; ME Califf; F. Ciravegna; D.Freitag; C. Giuliano; N. Kushmerick; and L. Romano … A Comparison of Keyword- andKeyterm-based Methods for Automatic Web Site Summarization / 15 Yongzheng Zhang; EvangelosMilios; and Nur Zincir-Heywood … The Use ofWeb-based Statistics to Validate Information Extraction/ 21 Stephen Soderland; Oren Etzioni; Tal Shaked; and Daniel S. Weld … Using Soft-MatchingMined Rules to Improve Information Extraction / 27 Un Yong Nahm and Raymond J. Mooney… Populating the SemanticWeb / 33 Kristina Lerman; Cenk Gazen; Steven Minton; and CraigKnoblock … Handling Irregularities in ROADRUNNER / 39 Valter Crescenzi; GiansalvatoreMecca; and Paolo Merialdo … A Model for Graded Levels of Generalizations in Intensional QueryAnswering / 45 Farah Benamara … Learning Text Patterns for Web Information …,*,*,*
Dipartimento di Informatica e Sistemistica Universita di Roma\La Sapienza" Via Salaria 113| I-00198 Roma; Italy,Luca Cabibbo; Giansalvatore Mecca,*,*,*,*
Universit a degli Studi di Roma Tre,Paolo Atzeni; Luca Cabibbo; Giansalvatore Mecca,ABSTRACT IsaLog (:) is a research activity aimed at developing a framework that integratesdeductive and object-oriented features. The data model has complex objects with classes;relations; and isa hierarchies; and the language is rule based. The main issue is the denition of the semantics of the language. For the (positive) IsaLog framework three di erentsemantics are given and proven to be equivalent: a model-theoretic semantics; a xpointsemantics; and a semantics based on a reduction to ordinary logic programming withfunction symbols. Then the semantics of the IsaLog: language is proposed. It presents novelfeatures mostly due to the interaction of hierarchies with negation in the body of rules. Twosemantics are presented for IsaLog: programs: a strati ed semantics based on an originalnotion of strati cation; which takes into account hierarchies; and a reduction to logic …,*,*,*
DIFA {Universit a della Basilicata 2 DIA {Universit a Roma Tre 3 DCI {Rutherford Appleton Lab.,P Atzeni; G Mecca; P Merialdo; G Sindoni,*,*,*,*
Universit egli Stu ii Rom 5re Dipartimento di Informatica e Automazione Via della Vasca Navale; 84-00146 Roma; Italy.,Data-Intensive Web Sites; Paolo Atzeni; Giansalvatore Mecca; Paolo Merialdo,Abs TRA CTM any W eb sites include sig nifi cant and su b stantial pieces of information; inaw ay th at is often di cult to sh are; correlate and maintain. In many cases the mana g ementof a W eb site can g reatly b ene fi t from the adoption of met h ods and tec h niq ues b orro wed from the data b ase fi eld. T h is paper introduces a met h odolo gy for desig nin g andmaintainin g lar ge W eb sites b ased on the assumption th at data to be pu b lis h ed in thesite are mana g ed usin ga DB MS. W e see the process of desig nin gthe site as the result oftwo intert w ined activities: the database design and the hypertext design. E ac h of th ese isfurt h er divided in a conceptual design ph ase and a logical design ph ase; b ased onspecifi c data models. A ne w lo g ical data model; called ad m; is used to descri b et hestructure of a W e bh yperte x t. It is pa g e-oriented; in the sense th at the main construct is …,*,*,*
Database Programming Languages (DBPL-5),Giansalvatore Mecca; Anthony J Bonner,Abstract This paper develops a query language for sequence databases; such as genomedatabases and text databases. Unlike relational data; queries over sequential data caneasily produce infinite answer sets; since the universe of sequences is infinite; even for afinite alphabet. The challenge is to develop query languages that are both highly expressiveand finite. This paper develops such a language. It is a subset of a recently developed logiccalled Sequence Datalog [19]. Sequence Datalog distinguishes syntactically betweensubsequence extraction and sequence construction. Extraction creates sequences ofbounded length; and leads to safe recursion; while construction can create sequences ofarbitrary length; and leads to unsafe recursion. In this paper; we develop syntacticrestrictions for Sequence Datalog that allow sequence construction but preserve …,*,*,*
A. Spasić; D. Janković,A Joha; M Janssen; G Mecca; M Santomauro; D Santoro; E Veltri; H Cai; B Xu; F Bu; S Charfi; H Ezzedine; C Kolski,A. Joha; M. Janssen: Design Choices Underlying the Software as a Service (SaaS) BusinessModel from the User Perspective: Exploring the Fourth Wave of Outsourcing Software as a Service(SaaS) can be viewed as the fourth wave of outsourcing. SaaS is a relatively new type of servicedelivery model in which a service provider delivers its services over the web to many users ona pay per use or period … G. Mecca; M. Santomauro; D. Santoro; E. Veltri: Middleware-OrientedGovernment Interoperability Frameworks: A Comparison We discuss deployment solutions fore-Government Interoperability Frameworks (GIFs). We concentrate on middleware-orientedGIFs; ie; those in which middleware modules act as intermediaries among information systemsthat need to exchange … H. Cai; B. Xu; F. Bu: A Conceptual Ontology-based ResourceMeta-Model towards Business-driven Information System Implementation Enterprises …,*,*,*
and Databases,Paolo Atzeni Alberto Mendelzon; Giansalvatore Mecca,Page 1. Paolo Atzeni Alberto Mendelzon Giansalvatore Mecca (Eds.) The World Wide Web andDatabases International Workshop WebDB'98 Valencia; Spain; March 27-28; 1998 SelectedPapers Springer Page 2. Table of Contents Internet Programming: Tools and Applications A UnifiedAlgorithm for Cache Replacement and Consistency in Web Proxy Servers 1 J. Shim; P.Scheuermann; R. Vingralek Transactional Services for the Internet 14 D. Billard On the Unificationof Persistent Programming and the World Wide Web .. 34 R. Connor; K. Sibson; P. ManghiIntegration and Access to Web Data Interactive Query and Search in Semistructured Databases52 R. Goldman; J. Widom Bringing Database Functionality to the WWW 63 D. Konopnicki; 0.Shmueli Fixpoint Calculus for Querying Semistructured Data 78 N. Bidoit; M. Ykhlef HypertextViews on Databases Incremental Maintenance of Hypertext Views 98 …,*,*,*
Letter from the Special Issue Editors,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renee J Miller; Paolo Papotti; Donatello Santoro,The prevalence of large volumes and varieties of accessible data is profoundly changing theway business; government and individuals approach decision making. Organizational bigdata investment strategies regarding what data to collect; clean; integrate; and analyze aretypically driven by some notion of perceived value. However; the value of the data isinescapably tied to the underlying quality of the data. Although for big data; value and qualitymay be correlated; they are conceptually different. For example; a complete and accurate listof the books read on April 1; 2016 by the special editors of this issue may not have muchvalue to anyone else. Whereas even partially complete and somewhat noisy GPS data frompublic transport vehicles may have a high perceived value for transport engineers and urbanplanners. In spite of significant advances in storage and compute capabilities; the time to …,*,*,*
Il Progetto freESBee: Soluzioni Infrastrutturali Open Source per il Sistema Pubblico di Cooperazione Applicativa,GIANSALVATORE MECCA; ALESSANDRO PAPPALARDO; SALVATORE RAUNICH; IL GRUPPO,Il Sistema Pubblico di Cooperazione Applicativa (SPCoop) è stato standardizzato dal CNIPAnel 2005 e finalmente promette di raggiungere il livello del concreto dispiegamento. Questoarticolo fornisce una panoramica sullo stato di avanzamento dell'iniziativa. Per cominciareintroduce l'architettura di SPCoop e il suo raffinamento prodotto nell'ambito del recenteprogetto ICAR. Inoltre; introduce il progetto freESBee; finalizzato alla realizzazione di unasoluzione infrastrutturale di carattere open-source per il Sistema Pubblico di CooperazioneApplicativa; e lo confronta con altre soluzioni open source attualmente disponibili per larealizzazione dell'architettura.,*,*,*
N. Piedra; J. Chicaiza; J. López; E. Tovar,GN Vo; R Lai; I van Zyl; R de la Harpe; G Mecca; M Santomauro; D Santoro; E Veltri; S Sanchez-Gordon; S Lujan-Mora,N. Piedra; J. Chicaiza; J. López; E. Tovar: Seeking Open Educational Resources to ComposeMassive Open Online Courses in Engineering Education An Approach based on Linked OpenData The OER movement has tended to define "openness" in terms of access to use and reuseeducational materials; and to address the geographical and financial barriers amongstudents; teachers and self-learners with open access to high quality … GN Vo; R. Lai: A SecureMulti-Layer e-Document Method for Improving e-Government Processes In recent years; therehas been a tremendous growth in e-Government services due to advances in Information CommunicationTechnology and the number of citizens engaging in e-Government transactions. In governmentadministration; it is very … I. van Zyl; R. de la Harpe: AT-HOME 2.0 - An Educational Frameworkfor Home-based Healthcare This paper intends to describe some of the primary social …,*,*,*
freESBee-SP: An e-Government Federated Single Sign-On Framework,Giansalvatore Mecca; Michele Santomauro; Donatello Santoro; Enzo Veltri,Abstract We consider the problem of handling digital identities within SOA architectures. Weexplore federated; single sign-on solutions based on Identity Managers and ServiceProviders. After an overview of the different standards and protocols; we introduce amiddleware-based architecture to simplify the integration of legacy systems within suchplatforms. Our solution is based on a middleware module that decouples the legacy systemfrom the identity-management modules. We consider both standard point-to-point servicearchitectures; and complex Government Interoperability Frameworks; and reportexperiments to show that our solution provides clear advantages both in terms ofeffectiveness and performance.,*,*,*
Rewriting Ontology-Based Mappings,Giansalvatore Mecca; Guillem Rull; Donatello Santoro; Ernest Teniente,Abstract Data translation consists of the task of moving data from a source database to atarget database. This task is usually performed by developing mappings; ie executabletransformations from the source to the target schema. However; a richer description of thetarget database semantics may be available in the form of an ontology. This is typicallydefined as a set of views over the base tables that provides a unified conceptual view of theunderlying data. We investigate how the mapping process changes when such a richconceptualization of the target database is available. We develop a translation algorithm thatautomatically rewrites a mapping from the source schema to the target ontology into anequivalent mapping from the source to the target databases. Then; we show how to handlethis problem when an ontology is available also for the source. Differently from previous …,*,*,*
The Spicy Project: A New Approach to Schema Matching,Angela Bonifati; Giansalvatore Mecca; Alessandro Pappalardo; Salvatore Raunich,One of the most intriguing challenges of the Internet era is the possibility to integratedisparate; distributed and heterogeneous data sources. A key step in any of theseintegration problems is that of matching source descriptions; ie; solving the so calledschema matching problem [13]. Informally speaking; given two repositories; R and S; findinga match between them means to derive a number of mappings between elements of R andsome corresponding elements of S. To give an example of fairly high generality; suppose Ris a relational database with a customer table with attributes (name; email;shippingAddress); suppose S is an XML repository; with a person element; whose childrenare (fidlName; email; city; zipCode; state); then a match between R and 5 might contain anumber of mapping elements; as follows. First; a number of elementlevel mappings …,*,*,*
diogene: A Methodology for the Certification of Education Systems,GIANSALVATORE MECCA; GIUSEPPE PENTASUGLIA; IRINA COVIELLO; ROSSANA PACIELLO,ABSTRACT This paper introduces the diogene approach to the certification of a learningsystem's products. The methodology developed in the paper allows an education institutionto produce a detailed certification for each student. The certification describes the subjectsand the depth of knowledge that a student has shown to possess during a learning cycle.,*,*,*
Valutazione della Ricerca: Manuale di Sopravvivenza per l'Informatico,GIANSALVATORE MECCA,Questo documento discute metodologie di valutazione della produzione scientifica diricercatori che appartengono al settore informatico. Il suo obiettivo è fornire un contributometodologico alla discussione sulla valutazione della ricerca; mettendo in luce alcuneparticolarità nelle pratiche adottate dagli informatici per la pubblicazione dei loro risultatiscientifici. Queste particolarità; peraltro ampiamente documentate nella letteraturainternazionale; rendono i criteri di valutazione della ricerca tipicamente adottati in altri settoridelle scienze fortemente inadeguati se applicati alla valutazione della ricerca informatica.Oltre a questo contributo relativo al metodo; il documento si spinge fino a discutere alcunialgoritmi specifici per la valutazione della produzione scientifica dei singoli e delle strutture;allo scopo di illustrare più concretamente come sia possibile procedere alla valutazione …,*,*,*
