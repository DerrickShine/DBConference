Schema mapping as query discovery,Renée J Miller; Laura M Haas; Mauricio A Hernández,Abstract To enable modern data intensive applications including data warehousing; globalinformation systems and electronic commerce; we must solve the schema mapping problemin which a source (legacy) database is mapped into a different; but fixed; target schema.Schema mapping involves the discovery of a query or set of queries that transform thesource data into the new structure. We introduce an interactive mapping creation paradigmbased on value correspondences that show how a value of a target attribute can be createdfrom a set of values of source attributes. We describe the use of the value correspondenceframework in Clio; a prototype tool for semi-automated schema mapping; and present analgorithm for query derivation from an evolving set of value correspondences.,VLDB,2000,698
Optimizing queries across diverse data sources,Laura Haas; Donald Kossmann; Edward Wimmers; Jun Yang,Businesses today need to interrelate data stored in diverse systems with differingcapabilities; ideally via a single high-level query interface. W e present the design of a queryoptimizer for Garlic [C+ 95]; a middleware system designed to integrate data from a broadrange of data sources with very different query capabilities. Garlic's optimizer extends therule-based approach of [Loh88] to work in a heterogeneous environment; by dening genericrules for the middleware and using wrapper-provided rules to encapsulate the capabilities ofeach data source. This approach offers great advantages in terms of plan quality;extensibility to new sources; incremental implementation of rules for new sources; and theability to express the capabilities of a diverse set of sources. We describe the design andimplementation of this optimizer; and illustrate its actions through an example.,*,1997,622
Towards heterogeneous multimedia information systems: The Garlic approach,Michael J Carey; Laura M Haas; Peter M Schwarz; Manish Arya; William F Cody; Ronald Fagin; Myron Flickner; Allen W Luniewski; Wayne Niblack; Dragutin Petkovic; Joachim Thomas; John H Williams; Edward L Wimmers,Provides an overview of the Garlic project; a new project at the IBM Almaden ResearchCenter. The goal of this project is to develop a system and associated tools for themanagement of large quantities of heterogeneous multimedia information. Garlic permitstraditional and multimedia data to he stored in a variety of existing data repositories;including databases; files; text managers; image managers; video servers; and so on; thedata is seen through a unified schema expressed in an object-oriented data model and canbe queried and manipulated using an object-oriented dialect of SQL; perhaps through anadvanced query/browser tool that we are also developing. The Garlic architecture isdesigned to be extensible to new kinds of data repositories; and access efficiency isaddressed via a" middleware" query processor that uses database query optimization …,Research Issues in Data Engineering; 1995: Distributed Object Management; Proceedings. RIDE-DOM'95. Fifth International Workshop on,1995,544
Distributed deadlock detection,K Mani Chandy; Jayadev Misra; Laura M Haas,Distributed deadlock models are presented for resource and communication deadlocks.Simple distributed algorithms for detection of these deadlocks are given. We show that alltrue deadlocks are detected and that no false deadlocks are reported. In our algorithms; noprocess maintains global information; all messages have an identical short length. Thealgorithms can be applied in distributed database and other message communicationsystems.,ACM Transactions on Computer Systems (TOCS),1983,540
Apparatus; system; and method for database provisioning,*,An apparatus; system; and method are disclosed for provisioning database resource withina grid database system. The apparatus comprises an analysis module and a provisionmodule. The analysis module analyzes a data query stream from an application to adatabase instance and determines that the data query stream exhibits a predeterminedperformance attribute. The provision module provisions a database resource in response toa determination that the data query stream exhibits the predetermined performance attribute.The provisioned database resource may be a database instance; a database server; or acache. The provisioning of the new database resource advantageously is substantiallytransparent to a client on the database system.,*,2010,466
Starburst mid-flight: as the dust clears (database project),Laura M.  Haas; Walter Chang; Guy M.  Lohman; John McPherson; Paul F.  Wilms; George Lapis; Bruce Lindsay; Hamid Pirahesh; Michael J.  Carey; Eugene Shekita,The purpose of the Starburst project is to improve the design of relational databasemanagement systems and enhance their performance; while building an extensible systemto better support nontraditional applications and to serve as a testbed for futureimprovements in database technology. The design and implementation of the Starburstsystem to date are considered. Some key design decisions and how they affect the goal ofimproved structure and performance are examined. How well the goal of extensibility hasbeen met is examined: what aspects of the system are extensible; how extensions can bedone; and how easy it is to add extensions. Some actual extensions to the system; includingthe experiences of the first real customizers; are discussed.,IEEE Transactions on knowledge and data engineering,1990,426
The Clio project: managing heterogeneity,Renée J Miller; Mauricio A Hernández; Laura M Haas; Ling-Ling Yan; CT Howard Ho; Ronald Fagin; Lucian Popa,Abstract Clio is a system for managing and facilitating the complex tasks of heterogeneousdata transformation and integration. In Clio; we have collected together a powerful set ofdata management techniques that have proven invaluable in tackling these difficultproblems. In this paper; we present the underlying themes of our approach and present abrief case study.,SIgMOD Record,2001,419
Extensible query processing in Starburst,Laura M Haas; Johann Christoph Freytag; Guy M Lohman; Hamid Pirahesh,Abstract Today's DBMSs are unable to support the increasing demands of the variousapplications that would like to use a DBMS. Each kind of application poses newrequirements for the DBMS. The Starburst project at IBM's Almaden Research Center aimsto extend relational DBMS technology to bridge this gap between applications and theDBMS. While providing a full function relational system to enable sharing acrossapplications; Starburst will also allow (sophisticated) programmers to add many kinds ofextensions to the base system's capabilities; including language extensions (eg; newdatatypes and operations); data management extensions (eg; new access and storagemethods) and internal processing extensions (eg; new join methods and new querytransformations). To support these features; the database query language processor must …,ACM SIGMOD Record,1989,380
DiscoveryLink: A system for integrated access to life sciences data sources,Laura M.  Haas; Peter M.  Schwarz; Prasad Kodali; Elon Kotlar; Julia E.  Rice; William C.  Swope,Vast amounts of life sciences data reside today in specialized data sources; with specializedquery processing capabilities. Data from one source often must be combined with data fromother sources to give users the information they desire. There are database middlewaresystems that extract data from multiple sources in response to a single query. IBM'sDiscoveryLink is one such system; targeted to applications from the life sciences industry.DiscoveryLink provides users with a virtual database to which they can pose arbitrarilycomplex queries; even though the actual data needed to answer the query may originatefrom several different sources; and none of those sources; by itself; is capable of answeringthe query. We describe the DiscoveryLink offering; focusing on two key elements; thewrapper architecture and the query optimizer; and illustrate how it can be used to …,IBM systems Journal,2001,353
Clio grows up: from research prototype to industrial tool,Laura M Haas; Mauricio A Hernández; Howard Ho; Lucian Popa; Mary Roth,Abstract Clio; the IBM Research system for expressing declarative schema mappings; hasprogressed in the past few years from a research prototype into a technology that is behindsome of IBM's mapping technology. Clio provides a declarative way of specifying schemamappings between either XML or relational schemas. Mappings are compiled into anabstract query graph representation that captures the transformation semantics of themappings. The query graph can then be serialized into different query languages;depending on the kind of schemas and systems involved in the mapping. Clio currentlyproduces XQuery; XSLT; SQL; and SQL/XML queries. In this paper; we revisit thearchitecture and algorithms behind Clio. We then discuss some implementation issues;optimizations needed for scalability; and general lessons learned in the road towards …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,317
Information integration in the enterprise,Philip A Bernstein; Laura M Haas,Software vendors offer numerous tools to reduce the effort; and hence the cost; of integrationand to improve the quality. Moreover; because information integration is a complex andmultifaceted task; many of these tools are highly specialized. The resulting profusion of toolscan be confusing. In this article; we try to clear up any confusion by:,Communications of the ACM,2008,304
Data-driven understanding and refinement of schema mappings,Ling Ling Yan; Renée J Miller; Laura M Haas; Ronald Fagin,Abstract At the heart of many data-intensive applications is the problem of quickly andaccurately transforming data into a new form. Database researchers have long advocatedthe use of declarative queries for this process. Yet tools for creating; managing andunderstanding the complex queries necessary for data transformation are still too primitive topermit widespread adoption of this approach. We present a new framework that uses dataexamples as the basis for understanding and refining declarative schema mappings. Weidentify a small set of intuitive operators for manipulating examples. These operators permita user to follow and refine an example by walking through a data source. We show that ouroperators are powerful enough both to identify a large class of schema mappings and todistinguish effectively between alternative schema mappings. These operators permit a …,ACM SIGMOD Record,2001,266
The Lowell database research self-assessment,Serge Abiteboul; Rakesh Agrawal; Phil Bernstein; Mike Carey; Stefano Ceri; Bruce Croft; David DeWitt; Mike Franklin; Hector Garcia Molina; Dieter Gawlick; Jim Gray; Laura Haas; Alon Halevy; Joe Hellerstein; Yannis Ioannidis; Martin Kersten; Michael Pazzani; Mike Lesk; David Maier; Jeff Naughton; Hans Schek; Timos Sellis; Avi Silberschatz; Mike Stonebraker; Rick Snodgrass; Jeff Ullman; Gerhard Weikum; Jennifer Widom; Stan Zdonik,A group of senior database researchers gathers every few years to assess the state of databaseresearch and to point out problem areas that deserve additional focus. This article summarizesthe discussion and conclusions of the sixth such meeting in Lowell; MA; in May 2003. It followsa number of earlier reports with similar goals; including [1; 2; 5-7] … Continuing thistradition; 25 senior database researchers representing a broad cross section of the field in termsof research interests; affiliations; and geography gathered in Lowell for two days of intensivediscussion on where the database field is and where it should be going … Several importantobservations came out of this meeting. Information management continues to be a critical componentof most complex software systems. We recommend that database researchers increase theirfocus on the integration of text; data; code; and streams; fusion of information from …,Communications of the ACM,2005,203
A snapshot differential refresh algorithm,Bruce Lindsay; Laura Haas; C Mohan; Hamid Pirahesh; Paul Wilms,Abstract This article presents an algorithm to refresh the contents of database snapshots. Adatabase snapshot is a read-only table whose contents are extracted from other tables in thedatabase. The snapshot contents can be periodically refreshed to reflect the current state ofthe database. Snapshots are useful in many applications as a cost effective substitute forreplicated data in a distributed database system. When the snapshot contents are a simplerestriction and projection of a single base table; differential refresh techniques can reducethe message and update costs of the snapshot refresh operation. The algorithm presentedannotates the base table to detect the changes which must be applied to the snapshot tableduring snapshot refresh. The cost of maintaining the base table annotations is minimal andthe amount of data transmitted during snapshot refresh is close to optimal in most …,ACM SIGMOD Record,1986,195
Data integration through database federation,Laura M Haas; Eileen Tien Lin; Mary A Roth,In a large modern enterprise; it is almost inevitable that different portions of the organizationwill use different systems to produce; store and search their critical data. Yet; it is only bycombining the information from these various systems that the enterprise can realize the fullvalue of the data they contain. Information integration is a ubiquitous need for today'sbusinesses. Database federation is one approach to information integration in which arelational database management system serves as middleware providing transparentaccess to a number of heterogeneous data sources. The goal of this paper is to demonstratethat database federation is a fundamental tool for information integration. While otherapproaches to information integration may be useful for particular integration problems;database federation has some unique characteristics that make it suitable for a broad …,IBM Systems Journal,2002,192
Clio: Schema mapping creation and data exchange,Ronald Fagin; Laura M Haas; Mauricio Hernández; Renée J Miller; Lucian Popa; Yannis Velegrakis,Abstract The Clio project provides tools that vastly simplify information integration.Information integration requires data conversions to bring data in different representationsinto a common form. Key contributions of Clio are the definition of non-procedural schemamappings to describe the relationship between data in heterogeneous schemas; a newparadigm in which we view the mapping creation process as one of query discovery; andalgorithms for automatically generating queries for data transformation from the mappings.Clio provides algorithms to address the needs of two major information integration problems;namely; data integration and data exchange. In this chapter; we present our algorithms forboth schema mapping creation via query discovery; and for query generation for dataexchange. These algorithms can be used in pure relational; pure XML; nested relational …,*,2009,186
Clio: A semi-automatic tool for schema mapping,Mauricio A Hernández; Renée J Miller; Laura M Haas,We consider the integration requirements of modern data intensive applications includingdata warehousing; global information systems and electronic commerce. At the heart ofthese requirements lies the schema mapping problem in which a source (legacy) databasemust be mapped into a different; but xed; target schema. The goal of schema mapping is thediscovery of a query or set of queries to map source databases into the new structure. Wedemonstrate Clio; a new semi-automated tool for creating schema mappings. Clio employs amapping-by-example paradigm that relies on the use of value correspondences describinghow a value of a target attribute can be created from a set of values of source attributes. Atypical session with Clio starts with the user loading a source and a target schema into thesystem. These schemas are read from either an underlying Object-Relational database or …,ACM SIGMOD Record,2001,181
R*: An overview of the architecture,R Williams; Dean Daniels; L Haas; George Lapis; Bruce G Lindsay; Pui Ng; Ron Obermarck; Patricia G Selinger; Adrian Walker; Paul F Wilms; Robert A Yost,Abstract R* is an experimental distributed database system being developed at IBMResearch to study the issues and problems of distributed data; management. R* consists ofa confederation of voluntarily co-operating sites; each supporting the relational model ofdata and communicating via IBM's CICS. A key feature of the architecture is to maintain theautonomy of each site. To achieve maximum site autonomy SQL statements are compiled;data objects are named and catalogued; and deadlock detection and recovery are allhandled in a distributed manner. The R* architecture; including transaction management;commit processing; deadlock detection; system recovery; object naming; catalogmanagement; and authorization checking is described. Some examples of the additions andchanges to the SQL language needed to support distributed function are given.,*,1981,181
Beauty and the beast: The theory and practice of information integration,Laura Haas,Abstract Information integration is becoming a critical problem for businesses andindividuals alike. Data volumes are sky-rocketing; and new sources and types of informationare proliferating. This paper briefly reviews some of the key research accomplishments ininformation integration (theory and systems); then describes the current state-of-the-art incommercial practice; and the challenges (still) faced by CIOs and application developers.One critical challenge is choosing the right combination of tools and technologies to do theintegration. Although each has been studied separately; we lack a unified (and certainly; aunifying) understanding of these various approaches to integration. Experience with avariety of integration projects suggests that we need a broader framework; perhaps even atheory; which explicitly takes into account requirements on the result of the integration …,International Conference on Database Theory,2007,177
Capabilities-based query rewriting in mediator systems,Yannis Papakonstantinou; Ashish Gupta; Laura Haas,Abstract Users today are struggling to integrate a broad range of information sourcesproviding different levels of query capabilities. Currently; data sources with different andlimited capabilities are accessed either by writing rich functional wrappers for the moreprimitive sources; or by dealing with all sources at a “lowest common denominator”. Thispaper explores a third approach; in which a mediator ensures that sources receive queriesthey can handle; while still taking advantage of all of the query power of the source. Wepropose an architecture that enables this; and identify a key component of that architecture;the Capabilities-Based Rewriter (CBR). The CBR takes as input a description of thecapabilities of a data source; and a query targeted for that data source. From these; the CBRdetermines component queries to be sent to the sources; commensurate with their …,Distributed and Parallel Databases,1998,177
Query processing in R,Guy M Lohman; C Mohan; Laura M Haas; Dean Daniels; Bruce G Lindsay; Patricia G Selinger; Paul F Wilms,Abstract This chapter describes how statements in the SQL language are processed by theR* distributed relational database management system. R* is an experimental adaptation ofSystem R to the distributed environment. The R* prototype is currently operational onmultiple machines running the MVS operating system; and is undergoing evaluation. The R*system is a confederation of autonomous; locally-administered databases that may begeographically dispersed; yet which appear to the user as a single database. Namingconventions permit R* to access tables at remote sites without resorting to a centralized orreplicated catalog; and without the user having to specify either the current location of or thecommunication commands required to access that table. SQL data definition statementsaffecting remote sites are interpreted through a distributed recursive call mechanism …,*,1985,168
PESTO: An integrated query/browser for object databases,Michael Carey; Laura Haas; Vivekananda Maganty; John H Williams,Abstract This paper describes the design and implementation of PESTO (Portable Explorerof STruct-ured Objects); an integrated user interface that supports browsing and querying ofobject databases. Like other object browsers; PESTO allows users to navigate in a hypertext-like fashion; following the relationships that exist among objects. In addition; PESTO allowsusers to formulate object queries through a unique; integrated query paradigm that presentsquerying as a natural extension of browsing; we call this paradigm\query-in-place." PESTO'squery facilities are quite powerful; including support for basic query operations such assimple selections; value-based joins; universal quanti cation; negation; and complexpredicates; as well as object-oriented queries including path predicates (implicit joins);queries over nested sets; ltered sets; and method invocation. PESTO can be ported with …,VLDB,1996,165
The Claremont report on database research,Rakesh Agrawal; Anastasia Ailamaki; Philip A Bernstein; Eric A Brewer; Michael J Carey; Surajit Chaudhuri; AnHai Doan; Daniela Florescu; Michael J Franklin; Hector Garcia-Molina; Johannes Gehrke; Le Gruenwald; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; Hank F Korth; Donald Kossmann; Samuel Madden; Roger Magoulas; Beng Chin Ooi; Tim O'Reilly; Raghu Ramakrishnan; Sunita Sarawagi; Michael Stonebraker; Alexander S Szalay; Gerhard Weikum,Abstract In late May; 2008; a group of database researchers; architects; users and punditsmet at the Claremont Resort in Berkeley; California to discuss the state of the research fieldand its impacts on practice. This was the seventh meeting of this sort in twenty years; andwas distinguished by a broad consensus that we are at a turning point in the history of thefield; due both to an explosion of data and usage scenarios; and to major shifts in computinghardware and platforms. Given these forces; we are at a time of opportunity for researchimpact; with an unusually large potential for influential results across computing; thesciences and society. This report details that discussion; and highlights the group'sconsensus view of new focus areas; including new database engine architectures;declarative programming languages; the interplay of structured and unstructured data …,ACM Sigmod Record,2008,163
The garlic project,Mary Tork Roth; Manish Arya; Laura M Haas; Michael J Carey; William Cody; Ronald Fagin; Peter M Schwarz; John Thomas; Edward L Wimmers,The goal of the Garlic [1] project is to build a multimedia information system capable of integratingdata that resides in different database systems as well as in a variety of non-data- base dataservers. This integration must be enabled while maintaining the independence of the dataservers; and without creating copies of their data. “Multimedia” should be inter- preted broadlyto mean not only images; video; and audio; but also text and application specific data types(eg; CAD draw- ings; medical objects; ...). Since much of this data is naturally modeled byobjects; Garlic provides an object-oriented schema to applications; interprets object queries;creates exe- cution plans for sending pieces of queries to the appropriate data servers; and assemblesquery results for delivery back to the applications. A significant focus of the project is supportfor “intelligent” data servers; ie; servers that provide media- specific indexing and query …,Sigmod Record,1996,162
Computation and communication in R*: A distributed database manager,Bruce G Lindsay; Laura M Haas; C Mohan; Paul F Wilms; Robert A Yost,This article presents and discusses the computation and communication model used by R*;a prototype distributed database management system. An R* computation consists of a treeof processes connected by virtual circuit communication paths. The process managementand communication protocols used by R* enable the system to provide reliable; distributedtransactions while maintaining adequate levels of performance. Of particular interest is theuse of processes in R* to retain user context from one transaction to another; in order toimprove the system performance and recovery characteristics.,ACM Transactions on Computer Systems (TOCS),1984,155
Method and system for organizing an annotation structure and for querying data and annotations,*,A method and apparatus for capturing annotations about database material in a way thatallows queries with conditions or predicates on both the database material and theannotations. Database material may be text; computer programs; graphics; audio;spreadsheets; or any other material which may be stored and indexed. Database materialmay be in one or multiple sources; and annotations may be stored together with the originalmaterial or in a separate store. Annotations can be used to capture information such asadditional facts about the database material; the opinions and judgments of experts aboutthe database material; and/or links to other related material. Annotations may be captured ina structured form to enhance queryability and semantic interpretation.,*,2003,152
Cost models do matter: Providing cost information for diverse data sources in a federated system,Mary Tork Roth; Laura M Haas; Fatma Ozcan,Abstract An important issue for federated systems of diverse data sources is how to optimizecross-source queries; without building knowledge of individual sources into the optimizer.Garlic is a federated system with an emphasis on extensibility and diverse sources. Toachieve these goals; data sources are attached to Garlic by means of a wrapper. Wrappersparticipate in query planning; telling Garlic what parts of a query a data source can do andhow much it will cost. This paper describes a framework through which wrappers provide thenecessary cost and cardinality information for optimization; and the facilities Garlic providesto make this task easier. Our framework makes it easy for wrappers to provide costinformation; requires few changes to a conventional bottomup optimizer and is easilyextensible to a broad range of sources. We believe that our framework for costing is the …,*,1999,141
Garlic: a new flavor of federated query processing for DB2,Vanja Josifovski; Peter Schwarz; Laura Haas; Eileen Lin,Abstract In a large modern enterprise; information is almost inevitably distributed amongseveral database management systems. Despite considerable attention from the researchcommunity; relatively few commercial systems have attempted to address this issue. Thispaper describes new technology that enables clients of IBM's DB2 Universal Database toaccess the data and specialized computational capabilities of a wide range of non-relationaldata sources. This technology; based on the Garlic prototype developed at the AlmadenResearch Center; complements and extends DB2's existing ability to federate relational datasources. The paper focuses on three topics. Firstly; we show how the DB2 catalogs are usedas an extensible repository for the metadata needed to access remotely-stored information.Secondly; we describe how the Garlic approach to query planning; in which source …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,116
Sing the truth about ad hoc join costs,Laura M Haas; Michael J Carey; Miron Livny; Amit Shukla,Abstract In this paper; we re-examine the results of prior work on methods for computing adhoc joins. We develop a detailed cost model for predicting join algorithm performance; andwe use the model to develop cost formulas for the major ad hoc join methods found in therelational database literature. We show that various pieces of “common wisdom” about joinalgorithm performance fail to hold up when analyzed carefully; and we use our detailed costmodel to derive op timal buffer allocation schemes for each of the join methods examinedhere. We show that optimizing their buffer allocations can lead to large performanceimprovements; eg; as much as a 400% improvement in some cases. We also validate ourcost model's predictions by measuring an actual implementation of each join algorithmconsidered. The results of this work should be directly useful to implementors of relational …,The VLDB Journal—The International Journal on Very Large Data Bases,1997,111
Challenges and Opportunities with big data 2011-1,Divyakant Agrawal; Philip Bernstein; Elisa Bertino; Susan Davidson; Umeshwas Dayal; Michael Franklin; Johannes Gehrke; Laura Haas; Alon Halevy; Jiawei Han; HV Jagadish; Alexandros Labrinidis; Sam Madden; Yannis Papakonstantinou; Jignesh Patel; Raghu Ramakrishnan; Kenneth Ross; Cyrus Shahabi; Dan Suciu; Shiv Vaithyanathan; Jennifer Widom,Abstract The promise of data-driven decision-making is now being recognized broadly; andthere is growing enthusiasm for the notion of``Big Data.''While the promise of Big Data is real--for example; it is estimated that Google alone contributed 54 billion dollars to the USeconomy in 2009--there is currently a wide gap between its potential and its realization.,*,2011,107
SECRET: a model for analysis of the execution semantics of stream processing systems,Irina Botan; Roozbeh Derakhshan; Nihal Dindar; Laura Haas; Renée J Miller; Nesime Tatbul,Abstract There are many academic and commercial stream processing engines (SPEs)today; each of them with its own execution semantics. This variation may lead to seeminglyinexplicable differences in query results. In this paper; we present SECRET; a model of thebehavior of SPEs. SECRET is a descriptive model that allows users to analyze the behaviorof systems and understand the results of window-based queries for a broad range ofheterogeneous SPEs. The model is the result of extensive analysis and experimentationwith several commercial and academic engines. In the paper; we describe the types ofheterogeneity found in existing engines; and show with experiments on real systems that ourmodel can explain the key differences in windowing behavior.,Proceedings of the VLDB Endowment,2010,104
Method for schema mapping and data transformation,*,A computer program product is provided that uses data examples as a basis forunderstanding and refining declarative schema mappings. The system of the presentinvention identifies a set of intuitive operators for manipulating examples includingestablishing value correspondences; data linking; data trimming; data walking; and datachasing. These operators allow a user to follow and refine an example by walking through adata source. In addition; these operators can identify a large class of schema mappings anddistinguish effectively between alternative schema mappings. With these operators; a user isable to quickly and intuitively build and refine complex data transformation queries that mapone data source into another while continuously verifying that the mapping is accurate andappropriate.,*,2006,104
Querying multimedia data from multiple repositories by content: the Garlic project,William F Cody; Laura M Haas; Wayne Niblack; Manish Arya; Michael J Carey; Ronald Fagin; Myron Flickner; Denis Lee; Dragutin Petkovic; Peter M Schwarz; Joachim Thomas; M Tork Roth; John H Williams; Edward L Wimmers,Abstract We describe Garlic; an object-oriented multimedia middleware query system. Garlicenables existing data management components; such as a relational database or a full textsearch engine; to be integrated into an extensible information management system thatpresents a common interface and user access tools. We focus in this paper on how QBIC; animage retrieval system that provides content-based image queries; can be integrated intoGarlic. This results in a system in which a single query can combine visual and nonvisualdata using type-specific search techniques; enabling a new breed of multimediaapplications,*,1995,102
The Claremont report on database research,Rakesh Agrawal; Anastasia Ailamaki; Philip A Bernstein; Eric A Brewer; Michael J Carey; Surajit Chaudhuri; Anhai Doan; Daniela Florescu; Michael J Franklin; Hector Garcia-Molina; Johannes Gehrke; Le Gruenwald; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; Hank F Korth; Donald Kossmann; Samuel Madden; Roger Magoulas; Beng Chin Ooi; Tim O'Reilly; Raghu Ramakrishnan; Sunita Sarawagi; Michael Stonebraker; Alexander S Szalay; Gerhard Weikum,Here; we explore the conclusions of this self-assessment. It is by definition somewhatinward-focused but may be of interest to the broader computing community as both a windowinto upcoming directions in database research and a description of some of the community issuesand initiatives that surfaced. We describe the group's consensus view of new focus areas forresearch; including database engine architectures; declarative programming languages; interplayof structured data and free text; cloud data services; and mobile and virtual worlds. We also reporton discussions of the database community's growth and processes that may be of interest toother research areas facing similar challenges … Over the past 20 years; small groups of databaseresearchers have periodically gathered to assess the state of the field and propose directionsfor future research. 1;3;4;5;6;7 Reports of the meetings served to foster debate within the …,Communications of the ACM,2009,100
Method for refreshing multicolumn tables in a relational data base using minimal information,*,A method for refreshing a relational data base snapshot manifest in the form of remote read-only copies of selected portions of a base table. The method takes advantage of the fact thateach tuple of the base table has a unique identifier TID associated therewith from the timethe record is inserted until it is deleted. The TID references the physical location of the tuplewithin its relation. Two system-maintained fields are added to the base table. These arePREVTID and update ID. One system-maintained column is required in the snapshot table;ie BASE TID. Lastly; a column in the catalog of the snapshot is also maintained; ieSNAPHIGH. When the refresh of the snapshot table is required; a single scan of the baserelation; in ascending TID sequence; is performed on the base table. The scan produces aseries of messages which contain the incremental changes required to update the …,*,1986,85
An Introduction to Distributed Query Compilation in R.,Thomas J. Watson IBM Research Center. Research Division; D Daniels; P Selinger; L Haas; B Lindsay; C Mohan; A Walker; P Wilms,*,*,1982,82
Tapes hold data; too: challenges of tuples on tertiary store,Michael J Carey; Laura M Haas; Miron Livny,Enormous quantities of data are being accumulated by both the commercial and scientificcommunities. In the hope that analyzing past activities will help them improve their busi-ness(eg; profile sales by customer groups and products for the last 24 months); many commercialenterprises keep a record of every customer transaction they have ever performed; typicallyin some DBMS. Yet; the total volume of,ACM SIGMOD Record,1993,71
Exploiting extensible dbms in integrated geographic information systems,Laura M Haas; William F Cody,Abstract A major focus of recent database research has been extensible databasemanagement systems (EDBMS). These systems' goal is to accommodate the increasingamount of digitized data that is different in structure; in size and in processing needs fromtraditional transaction data. Geographic Information Systems (GIS); for example; havedeveloped effective and specialized data representation and analysis techniques for spatialdata that must be supported by any underlying DBMS. In this paper we show how EDBMStechnology can support GIS applications through powerful data modeling and datamanagement functions that are analogous to; and integrated with; those employed formanaging traditional data. After first examining the needs of GIS and the capabilities ofEDBMS; we indicate through a series of related examples how a particular EDBMS …,Symposium on Spatial Databases,1991,68
Incorporating data types in an extensible database architecture,Paul F Wilms; Peter M Schwarz; H-J Schek; Laura M Haas,Abstract The limited set of data types and the difficulty of incorporating new ones in currentDBMSs prevent many applications from efficiently using a DBMS to store data. We discussthe merits of introducing externally defined types (EDTs) to bridge the gap between the datastructures of the application programming language and the data model of the database. Weshow that the DBMS needs to know the set of functions to be called on an EDT to permiteasy use of EDTs both in programming languages and by the DBMS; and we propose waysto make the definition and implementation of new types in the DBMS easier on both theDBMS and the type implementor. Functions are central to our approach; as they may beinvoked from within internal DBMS components by suitable extensions. We examine how tocreate access paths to support externally denned predicate functions by materialization or …,*,1988,64
Extensible database management systems,Michael Carey; Laura Haas,Work in the area of extensible database management systems addresses two problems withtoday's (primarily relational) DBMSs. First; current systems are lacking when it comes tomeeting the needs of emerging applications such as CAD/CAM; multi-media office systems;image management; statistical data management; and text manipulation. Such applicationsrequire support for new data types; functions; complex objects; storage techniques; andaccess methods for greater modeling power and performance. Extensible DBMS efforts aimto support the addition of such new features. Second; current systems tend to be monolithicin nature; making it difficult to incorporate recent advances in DBMS technology in a timelymanner. Extensible DBMS efforts seek to ease the problem of tracking technologydevelopments; simplifying the incorporation of new algorithms and new kinds of storage …,ACM SIGMOD Record,1990,60
Attribute classification using feature analysis,Felix Naumann; Ching-Tien Ho; Xuqing Tian; Laura M Haas; Nimrod Megiddo,Abstract Database integration and migration are important; but labor-intensive tasks. Totransform data from one representation to another; an expert user must identify and expresscorrespondences between different attributes of different schemata. There are potentiallymany attributes in a source schema that might correspond to a particular target attribute. Ouraim is to ease the burden of the user by classifying source attributes so that they can beautomatically and intelligently matched to target attributes. For categorical data; we presenta novel variation of existing Naяve Bayes classiication techniques based on domain-independent feature selection. For numerical data; we use a quantile-based classiicationmethod; discovering characteristic distributions of the data. We show through extensiveexperiments that automatic classiication of attributes is both feasible and useful for …,icde,2002,57
System for organizing an annotation structure and for querying data and annotations,*,A method and apparatus for capturing annotations about database material in a way thatallows queries with conditions or predicates on both the database material and theannotations. Database material may be text; computer programs; graphics; audio;spreadsheets; or any other material which may be stored and indexed. Database materialmay be in one or multiple sources; and annotations may be stored together with the originalmaterial or in a separate store. Annotations can be used to capture information such asadditional facts about the database material; the opinions and judgments of experts aboutthe database material; and/or links to other related material. Annotations may be captured ina structured form to enhance queryability and semantic interpretation.,*,2006,52
Clio: Schema Mapping Creation and Data Exchange; Conceptual Modeling: Foundations and Applications: Essays in Honor of John Mylopoulos,Ronald Fagin; Laura M Haas; Mauricio Hernández; Renée J Miller; Lucian Popa; Yannis Velegrakis,*,*,2009,51
Data structures for efficient broker implementation,Anthony Tomasic; Luis Gravano; Calvin Lue; Peter Schwarz; Laura Haas,Abstract With the profusion of text databases on the Internet; it is becoming increasingly hardto find the most useful databases for a given query. To attack this problem; several existingand proposed systems employ brokers to direct user queries; using a local database ofsummary information about the available databases. This summary information musteffectively distinguish relevant databases and must be compact while allowing efficientaccess. We offer evidence that one broker; GlOSS; can be effective at locating databases ofinterest even in a system of hundreds of databased and can examine the performance ofaccessing the GlOSS summeries for two promising storage methods: the grid file andpartitioned hashing. We show that both methods can be tuned to provide good performancefor a particular workload (within a broad range of workloads); and we discuss the …,ACM Transactions on Information Systems (TOIS),1997,51
The beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,Abstract Every few years a group of database researchers meets to discuss the state ofdatabase research; its impact on practice; and important new directions. This reportsummarizes the discussion and conclusions of the eighth such meeting; held October 14-15;2013 in Irvine; California. It observes that Big Data has now become a defining challenge ofour time; and that the database research community is uniquely positioned to address it; withenormous opportunities to make transformative impact. To do so; the report recommendssignificantly more attention to five research areas: scalable big/fast data infrastructures;coping with diversity in the data management landscape; end-to-end processing andunderstanding of data; cloud services; and managing the diverse roles of people in the datalife cycle.,ACM SIGMOD Record,2014,47
The Beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,A group of database researchers meets periodically to discuss the state of the field and its keydirections going forward. Past meetings were held in 1989; 6 1990; 11 1995; 12 1996; 101998; 7 2003; 1 and 2008. 2 Continuing this tradition; 28 database researchers and two invitedspeakers met in October 2013 at the Beckman Center on the University of California-Irvine campusfor two days of discussions. The meeting attendees represented a broad cross-section ofinterests; affiliations; seniority; and geography. Attendance was capped at 30 so the meetingwould be as interactive as possible. This article summarizes the conclusions from thatmeeting; an extended report and participant presentations are available at http://beckman.cs.wisc.edu … The meeting participants quickly converged on big data as a defining challengeof our time. Big data arose due to the confluence of three major trends. First; it has …,Communications of the ACM,2016,45
Using Fagin's algorithm for merging ranked results in multimedia middleware,Edward L Wimmers; Laura M Haas; Mary Tork Roth; Christoph Braendli,A distributed multimedia information system allows users to access data of differentmodalities; from different data sources; ranked by various combinations of criteria. Fagin(1996) gives an algorithm for efficiently merging multiple ordered streams of ranked results;to form a new stream ordered by a combination of those ranks. In this paper we describe theimplementation of Fagin's algorithm in an actual multimedia middleware system; including anovel; incremental version of the algorithm that supports dynamic exploration of data. Weshow that the algorithm would perform well as part of a single multimedia server and caneven be effective in the distributed environment (for a limited set of queries); but that theassumptions it makes about random access limit its applicability dramatically. Ourexperience provides a better understanding of an important algorithm; and exposes an …,Cooperative Information Systems; 1999. CoopIS'99. Proceedings. 1999 IFCIS International Conference on,1999,45
System and method for loading a cache with query results,*,A system and method for avoiding accessing a remote database twice; once during queryexecution and again to retrieve objects identified by the query; when an application requiresobjects on which to operate. During query plan generation in response to a request fordatabase objects from an application; a query optimizer inserts cache operators into thecandidate plans; and then a cost-benefit analysis is undertaken to identify the best plan. Thebest plan is then used to execute the query; with the cache operators causing objectsidentified during query execution to be cached locally to the requesting application as thequery is being executed; thereby avoiding requiring the application to access the databaseafter query execution.,*,2005,42
Data access and management services on grid,Vijayshankar Raman; Inderpal Narang; Chris Crone; Laura Haas; Susan Malaika; Tina Mukai; Dan Wolfson; Chaitan Baru,Abstract An increasing number of applications manage data at very large scale; of both sizeand distribution. In this paper; we discuss data management and access services for suchapplications; in the context of a grid. The complexity of data management on a grid arisesfrom the scale; dynamism; autonomy; and distribution of data sources. The main argument ofthis paper is that these complexities should be made transparent to grid applications;through a layer of virtualization services. We start by discussing the various dimensions oftransparent data access; and illustrate the benefits they provide in the context of a specificapplication. We then present a layer of grid data virtualization services that provide suchtransparency and enable ease of information access on a grid. These services supportfederated access to distributed data; dynamic discovery of data sources based on content …,GGF; DAIS group,2002,42
Integrating life sciences data-with a little Garlic,Laura M Haas; Prasad Kodali; Julia E Rice; Peter M Schwarz; William C Swope,Vast amounts of life sciences data today reside in specialized data sources; with specializedquery processing capabilities. Data from one source must often be combined with data fromother sources to give users the information they desire. Database middleware systems suchas Garlic allow users to combine data from multiple sources in a single query. Garlicprovides the user with a virtual database to which they can pose arbitrarily complex queries;though the actual data needed to answer the query may be stored in several differentsources; and those sources may not even possess all the functionality needed to answersuch a query themselves. The Garlic technology; as incorporated in IBM's DB2 product;forms the basis of the DiscoveryLink service offering for the life sciences industry. Wedescribe the DiscoveryLink offering; focusing on two key contributions of Garlic; the …,Bio-Informatics and Biomedical Engineering; 2000. Proceedings. IEEE International Symposium on,2000,37
Panel: Is generic metadata management feasible?,Philip A Bernstein; Laura M Haas; Matthias Jarke; Erhard Rahm; Gio Wiederhold,The database field has worked on metadata-related problems for 30 years. Examplesinclude data translation and migration; schema evolution; database design;schema/ontology integration; XML wrapper generation; data scrubbing and transformationfor data warehouses; message mapping for e-business; and schema-driven web site design.Tools that address these problems are strikingly similar in their design. Arguably; we aremaking very little progress; since we keep reapplying the same old 1970's techniques ofdata translation [9] and views to one new problem after another; without getting muchleverage from each succeeding step. Despite all the research on the above tools; we haveso far been unable to offer generalpurpose database technology that factors out the similaraspects of these tools into generic database infrastructure.,VLDB,2000,36
Loading a cache with query results,Laura M Haas; Donald Kossmann; Ioana Ursu,Abstract Data intensive applications today usually run in either a clientserver or amiddleware environment. In either case; they must efficiently handle both database queries;-which process large numbers of data objects; and application logic; which involves fine-grained object accesses (eg; method calls). We propose a wholistic approach to speedingup such applications: we load the cache of a system with relevant objects as a by-product ofquery processing. This can potentially improve the performance of the application; byeliminating the need to fault in objects. However; it can also increase the cost of queries byforcing them to handle more data; thus potentially reducing the performance of theapplication. In this paper; we examine both heuristic and cost-based strategies for decidingwhat to cache; and when to do so. We show how these strategies can be integrated into …,VLDB,1999,35
R*: A Research Project on Distributed Relational DBMS.,Laura M.  Haas; Patricia G.  Selinger; Elisa Bertino; Dean Daniels; Bruce G.  Lindsay; Guy M.  Lohman; Yoshifumi Masunaga; C Mohan; Pui Ng; Paul F.  Wilms; Robert A.  Yost,*,IEEE Database Eng. Bull.,1982,35
Views and security in distributed database management systems,Elisa Bertino; Laura M Haas,Abstract Views are used in database systems to present data to different applications in aform reflecting their individual needs. The view mechanism contributes to data protection;independence; and isolation. In this paper we first discuss some issues concerningDistributed Database security and then the design of distributed views providing securityfeatures. The following issues concerning views are discussed: represention; change;authorization; and usage.,International Conference on Extending Database Technology,1988,34
A distributed deadlock detection algorithm for a resource-based system,Thomas J. Watson IBM Research Center. Research Division; LM Haas; C Mohan,*,*,1983,34
Services for Data Access and Data Processing on Grids,Vijayshankar Raman; Inderpal Narang; Chris Crone; Laura Haas; Susan Malaika; Tina Mukai; Dan Wolfson; Chaitan Baru,Abstract An increasing number of grid applications manage data at very large scale; of bothsize and distribution. In this paper we discuss data access and data processing services forsuch applications; in the context of a grid. The complexity of data management on a gridarises from the scale; dynamism; autonomy; and distribution of data sources. The mainargument of this paper is that these complexities should be made transparent to gridapplications; through a layer of virtualization services. We start by discussing the variousdimensions of transparent data access and processing; and illustrate their benefits in thecontext of a specific application. We then present a layer of grid data virtualization servicesthat provide such transparency and enable ease of data access and processing. Theseservices support federated access to distributed data; dynamic discovery of data sources …,Global Grid Formum Document GFD-I,2003,33
Modeling the execution semantics of stream processing engines with SECRET,Nihal Dindar; Nesime Tatbul; Renée J Miller; Laura M Haas; Irina Botan,Abstract There are many academic and commercial stream processing engines (SPEs)today; each of them with its own execution semantics. This variation may lead to seeminglyinexplicable differences in query results. In this paper; we present SECRET; a model of thebehavior of SPEs. SECRET is a descriptive model that allows users to analyze the behaviorof systems and understand the results of window-based queries (with time-and tuple-basedwindows) for a broad range of heterogeneous SPEs. The model is the result of extensiveanalysis and experimentation with several commercial and academic engines. In the paper;we describe the types of heterogeneity found in existing engines and show with experimentson real systems that our model can explain the key differences in windowing behavior.,The VLDB Journal,2013,30
System for annotating a data object by creating an interface based on a selected annotation structure,*,A method and apparatus for capturing annotations about database material in a way thatallows queries with conditions or predicates on both the database material and theannotations. Database material may be text; computer programs; graphics; audio;spreadsheets; or any other material which may be stored and indexed. Database materialmay be in one or multiple sources; and annotations may be stored together with the originalmaterial or in a separate store. Annotations can be used to capture information such asadditional facts about the database material; the opinions and judgments of experts aboutthe database material; and/or links to other related material. Annotations may be captured ina structured form to enhance queryability and semantic interpretation.,*,2012,30
Design and implementation of the MaxStream federated stream processing architecture,Irina Botan; Younggoo Cho; Roozbeh Derakhshan; Nihal Dindar; Laura M Haas; Kihong Kim; Chulwon Lee; Girish Mundada; Ming-Chien Shan; Nesime Tatbul; Ying Yan; Beomjin Yun; Jin Zhang,Despite the availability of several commercial data stream processing engines (SPEs); itremains hard to develop and maintain streaming applications. A major difficulty is the lack ofstandards; and the wide (and changing) variety of application requirements. Consequently;existing SPEs vary widely in data and query models; APIs; functionality; and optimizationcapabilities. This has led to some organizations using multiple SPEs; based on theirapplication needs. Furthermore; management of stored data and streaming data are stillmostly separate concerns; although applications increasingly require integrated access toboth. In the MaxStream project; our goal is to design and build a federated streamprocessing architecture that seamlessly integrates multiple autonomous and heterogeneousSPEs with traditional databases; and hence facilitates the incorporation of new …,Technical report/ETH; Department of Computer Science,2009,29
Federated stream processing support for real-time business intelligence applications,Irina Botan; Younggoo Cho; Roozbeh Derakhshan; Nihal Dindar; Laura Haas; Kihong Kim; Nesime Tatbul,Abstract In this paper; we describe the MaxStream federated stream processing architectureto support real-time business intelligence applications. MaxStream builds on and extendsthe SAP MaxDB relational database system in order to provide a federator over multipleunderlying stream processing engines and databases. We show preliminary results onusefulness and performance of the MaxStream architecture on the SAP Sales andDistribution Benchmark.,International Workshop on Business Intelligence for the Real-Time Enterprise,2009,28
An extensible processor for an extended relational query language,Thomas J. Watson IBM Research Center. Research Division; LM Haas; WF Cody; JC Freytag; G Lapis; BG Lindsay; GM Lohman; K Ono; H Pirahesh,*,*,1988,28
TRAMP: Understanding the behavior of schema mappings through provenance,Boris Glavic; Gustavo Alonso; Renée J Miller; Laura M Haas,Abstract Though partially automated; developing schema mappings remains a complex andpotentially error-prone task. In this paper; we present TRAMP (TRAnsformation MappingProvenance); an extensive suite of tools supporting the debugging and tracing of schemamappings and transformation queries. TRAMP combines and extends data provenance withtwo novel notions; transformation provenance and mapping provenance; to explain therelationship between transformed data and those transformations and mappings thatproduced that data. In addition we provide query support for transformations; data; and allforms of provenance. We formally define transformation and mapping provenance; presentan efficient implementation of both forms of provenance; and evaluate the resulting systemthrough extensive experiments.,Proceedings of the VLDB Endowment,2010,27
Schema and data: A holistic approach to mapping; resolution and fusion in information integration,Laura M Haas; Martin Hentschel; Donald Kossmann; Renée J Miller,Abstract To integrate information; data in different formats; from dif-ferent; potentiallyoverlapping sources; must be related and transformed to meet the users' needs. Ten yearsago; Clio introduced nonprocedural schema mappings to describe the relationship betweendata in heteroge-neous schemas. This enabled powerful tools for mapping discovery andintegration code generation; greatly simplifying the integration process. However; furtherprogress is needed. We see an opportunity to raise the level of abstraction further; toencompass both data-and schema-centric integration tasks and to isolate applications fromthe details of how the integration is accomplished. Holistic information integration supports it-eration across the various integration tasks; leveraging information about both schema anddata to improve the integrated result. Integration inde-pendence allows applications to be …,International Conference on Conceptual Modeling,2009,26
An optimizer for heterogeneous systems with non-standard data and search capabilities,Laura Haas; Donald Kossmann; Edward Wimmers; Jun Yang,Much of the world's nonstandard data resides in specialized data sources. This data mustoften be queried together with data from other sources to give users the information theydesire. Queries that can span several specialized sources with diverse search capabilitiespose new challenges for query optimization. This paper describes the design and illustratesthe use of a general purpose optimizer that can be taught about the capabilities of a newdata source. Our optimizer produces plans of high quality even in the presence ofnonstandard data; strange methods; and unusual query processing capabilities; and it easyto add new sources of standard or nonstandard data,*,1996,25
View management in distributed data base systems,Elisa Bertino; Laura M Haas; Bruce G Lindsay,The structure of data to be stored by a Data Base Management System (DBMS) is usuallydecided by a database administrator. Individual users and applications are generallyinterested in only a subset of the data stored in the database. Often; they wish to see thissubset structured in a way which reflects their particular needs. Since it is not generallypossible to structure a database so as to please aU of its users; some mechanism is neededwhereby each user can view the data according to his (her) own requirements. Therepresentation of the data structure as seen by a user is often referred to as an externalschema; the view mechanism is a means by which a DBMS can support various externalschemas.,*,1983,22
Challenges and Opportunities with Big Data,Elisa Bertino; Philip Bernstein; Divyakant Agrawal; Susan Davidson; Umeshwas Dayal; Michael Franklin; Johannes Gehrke; Laura Haas; Alon Halevy; Jiawei Han; HV Jadadish; Alexandros Labrinidis; Sam Madden; Yannis Papokonstantinou; Jignesh Patel; Raghu Ramakrishnan; Kenneth Ross; Cyrus Shahabi; Dan Suciu; Shiv Vaithyanathan; Jennifer Widom,Abstract The promise of data-driven decision-making is now being recognized broadly; andthere is growing enthusiasm for the notion of" Big Data". While the promise of Bid Data isreal-for example; it is estimated that Google alone contributed 54 billion dollars to the USeconomy in 2009-there is currently a wide gap between its potential and its realization.,*,2011,21
Towards an information infrastructure for the grid,Serge Bourbonnais; Vitthal M Gogate; Laura M Haas; Randy W Horman; Susan Malaika; Inderpal Narang; Vijayshankar Raman,In this paper we present our vision of an information infrastructure for grid computing; whichis based on a service-oriented architecture. The infrastructure supports a virtualized view ofthe computing and data resources; is autonomic (driven by policies) in order to meetapplication goals for quality of service; and is compatible with the standards beingdeveloped in the technical community. We describe how we are implementing this vision inIBM today and how we expect the implementation to evolve in the future.,IBM systems journal,2004,20
Data access interoperability in the IBM database family,Michael J.  Carey; Laura M.  Haas; Jim Kleewein; Berthold Reinwald,Business enterprises; and society in general; are becoming increasingly dependent oncomputer systems. As a result; we are now awash in a sea of data—data of all shapes andsizes—making heterogeneous data management a tremendously relevant challenge today.Moreover; the problem of data heterogeneity is itself varied; with different applicationsposing a variety of requirements. Some applications need to access and/or manage data inseveral; possibly many; different database systems—some with different data models. Otherapplications need to access and/or manage external data; eg; data stored in file systems orother specialized data repositories; together with data sets residing in one or moredatabases. Still other applications need to compose business objects from a combination oflegacy data and legacy transactions (eg; travel reservation systems) provided by multiple …,IEEE Data Eng. Bull.,1998,20
Schema management,Periklis Andritsos; Ronald Fagin; Ariel Fuxman; Laura M Haas; Mauricio A Hernández; Howard Ho; Anastasios Kementsietsidis; Phokion G Kolaitis; Renée J Miller; Felix Naumann; Lucian Popa; Yannis Velegrakis,Abstract Clio is a management system for heterogeneous data that couples a traditionaldatabase management engine with additional tools for managing schemas (models of data)and mappings between schemas. In this article; we provide a brief overview of Clio andplace our solutions in the context of the rich research literature on data integration andtransformation. Clio is the result of an on-going collaboration between the University ofToronto and IBM Almaden Research Center in which we are addressing both foundationaland systems issues related to heterogeneous data; schema; and integration management.,*,2002,19
Extensibility in the Starburst experimental database system,Bruce Lindsay; Laura Haas,Abstract The Starburst experimental database system is a research tool for experimentingwith advanced database implementation techniques. As such; it is important to be able toextend the system in a variety of ways to facilitate the evaluation of the design andimplementation of various advanced database management functions. In addition; manyapplications that use (or wish to use) database systems to store their persistent state canbenefit from specialized constructs or facilities; either for better performance; or to modelmore closely the application domain. The conflicting goals of application-specific facilitiesand integration of information across application boundaries are (perhaps) best addressedby a database system supporting domain-specific extensions in the context of a commondata model. A common data model; mentation of large portions of the system. Many …,*,1990,19
‘The Melampus Project: Toward An Omniscient Computing System,Luis-Felipe Cabrera; Laura Haas; Joel Richardson; Peter Schwarz; Jim Stamos,Abstract: Software technology has not evolved to accommodate the rapidly increasingvolume and diversity of on-line data. Finding data is often very difficult; and once found; datafrom different sources is often difficult to combine because of a lack of common formats andsemantics. The advent of distributed systems has highlighted the problem by multiplying theamount of data that is hard to find and hard to use. A fundamental problem contributing tothese shortcomings is the lack of a comprehensive model in which to describe andmanipulate the range of entities in a system. This paper introduces Melampus; a systemdesigned to address this problem. The overall goal of Melampus is to define a powerful datamodel; to build a prototype system that implements the model; and to build sampleapplications that highlight both strengths and weaknesses of the model. We hypothesize …,RJ-7515; IBM; Almaden,1990,16
A demonstration of the MaxStream federated stream processing system,Irina Botan; Younggoo Cho; Roozbeh Derakhshan; Nihal Dindar; Ankush Gupta; Laura Haas; Kihong Kim; Chulwon Lee; Girish Mundada; Ming-Chien Shan; Nesime Tatbul; Ying Yan; Beomjin Yun; Jin Zhang,MaxStream is a federated stream processing system that seamlessly integrates multipleautonomous and heterogeneous Stream Processing Engines (SPEs) and databases. In thispaper; we propose to demonstrate the key features of MaxStream using two applicationscenarios; namely the Sales Map & Spikes business monitoring scenario and the LinearRoad Benchmark; each with a different set of requirements. More specifically; we will showhow the MaxStream Federator can translate and forward the application queries to twodifferent commercial SPEs (Coral8 and StreamBase); as well as how it does so undervarious persistency requirements.,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,15
Computation & communication in R: a distributed database manager,Bruce G Lindsay; Laura M Haas; Paul F Wilms; Robert A Yost,Abstract R* is an experimental prototype distributed database management system. Thecomputation needed to perform a sequence of multisite user transactions in R* is structuredas a tree of processes communicating over virtual circuit communication links. Distributedcomputation can be supported by providing a server process per site which performsrequests on behalf of remote users. Alternatively; a new process could be created to serviceeach incoming request. Instead of using a shared server process or using the process perrequest approach; R* creates a process associated with the computation of the user on thefirst request to the remote site. This process is incorporated into the tree of processes servinga single user and is retained for the duration of the user computation. This approach allowsR* to factor some of the request execution overhead into the process creation phase; and …,ACM SIGOPS Operating Systems Review,1983,13
DiscoveryLink,Laura M Haas; Barbara A Eckman; Prasad Kodali; Eileen T Lin; Julia E Rice; Peter M Schwarz,*,*,2004,12
New challenges in information integration,Laura M Haas; Aya Soffer,Abstract Information integration is the cornerstone of modern business informatics. It is apervasive problem; rarely is a new application built without an initial phase of gathering andintegrating information. Information integration comes in a wide variety of forms. Historically;two major approaches were recognized: data federation and data warehousing. Today; weneed new approaches; as information integration becomes more dynamic; while coping withgrowing volumes of increasingly dirty and diverse data. At the same time; informationintegration must be coupled more tightly with the applications and the analytics that willleverage the integrated results; to make the integration process more tractable and theresults more consumable.,International Conference on Data Warehousing and Knowledge Discovery,2009,11
Just-in-time data integration in action,Martin Hentschel; Laura Haas; Renée J Miller,Abstract Today's data integration systems must be flexible enough to support the typicaliterative and incremental process of integration; and may need to scale to hundreds of datasources. In this work we present a novel data integration system that offers great flexibilityand scalability. Our approach to data integration is unique in that it executes mapping rulesat query runtime using annotations. On top; we have built the People People Peopleapplication. It allows users to search for people; display information about people; andbrowse through a network of related people; where the data is integrated from local andremote data sources. The demo presents all features of our underlying data integrationengine through a set of motivating scenarios.,Proceedings of the VLDB Endowment,2010,10
System and method for schema method,*,A system and method for generating a source schema-to-target schema mapping includesgrouping user-defined value correspondences into potential sets such that; for eachpotential set; at most one value correspondence per target attribute exists. The valuecorrespondences represent functions and filters for deriving values of target attributes fromvalues of source attributes. Candidate sets are selected from the potential sets and thengrouped into ranked covers. Using the best cover; an SQL query representing a sourceschema-to-target schema mapping is generated.,*,2006,8
The IBM research accelerated discovery lab,Laura Haas; Melissa Cefkin; Cheryl Kieliszewski; Wil Plouffe; Mary Roth,Data analytics is becoming central to modern society. In the business world; financialinstitutions rely on data analysis to detect and prevent fraud; retailers combine transactiondata with social media and emails to build a better understanding of their clients; industrialgiants use sensor data to improve their carbon footprint. Meanwhile; bioinformatics;astronomy; and particle physics are just a few of the sciences that are being transformed bythe availability of large data sets and new techniques for analyzing data. Cities;governments and social agencies are leveraging data analytics for important causes suchas improving public health and planning for (and reacting to) natural disasters. But the pathfrom raw data to insight; or better yet; predictive or prescriptive capabilities; is still long;errorprone; and expensive. First; data must be acquired–not only the pertinent domain …,ACM SIGMOD Record,2014,7
A first step towards integration independence,Laura M Haas; Renée J Miller; Donald Kossmann; Martin Hentschel,Two major forms of information integration; federation and materialization; continue todominate the market; embedded in separate products; each with their strengths andweaknesses. Application developers must make difficult choices among techniques andproducts; choices that are hard to change later. We propose a new design principle;Integration Independence; for integration engines. Integration independence frees theapplication designer from deciding how to integrate data. We then describe a new; adaptiveinformation integration engine that provides the ability to index base data or to materializetransformed data; giving us a flexible platform for experimentation.,Data Engineering Workshops (ICDEW); 2010 IEEE 26th International Conference on,2010,7
An Architecture for Transparent Access to Diverse Data Sources,Mary Tork Roth; Peter Schwarz; Laura Haas,Most large organizations have collected a considerable amount of data; and have investedheavily in systems and applications to manage and access that data. Powerful applicationscan be created by combining information stored in these historically separate data sources.There are several approaches to provide an integrated view of heterogeneous data. Oneapproach is to access data through a component database management system (CDBMS);a middleware system that provides an integrated view of heterogeneous legacy data withoutchanging how or where the data are stored. CDBMS relies on a particular kind ofcomponent; namely; wrappers; to encapsulate the underlying data and mediate between thedata source and the middleware. This chapter describes Garlic wrapper componentarchitecture and summarizes an experience of building wrappers for 10 data sources with …,*,2001,7
The power behind the throne: Information integration in the age of data-driven discovery,Laura M Haas,Abstract Integrating data has always been a challenge. The information managementcommunity has made great progress in tackling this challenge; both on the theory and thepractice. But in the last ten years; the world has changed dramatically. New platforms;devices and applications have made huge volumes of heterogeneous data available atspeeds never contemplated before; while the quality of the available data has if anythingdegraded. Unstructured and semi-structured formats and no-sql data stores undercut the oldreliable tools of schema; forcing applications to deal with data at the instance level. Deepexpertise in the data and domain; in the tools and systems for integration and analysis; inmathematics; computer science; and business are needed to discover insights from data; butrarely are all of these skills found in a single individual or even team. Meanwhile; the …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,5
Time for our field to grow up,Anastassia Ailamaki; Laura Haas; HV Jagadish; David Maier; Tamer Özsu; Marianne Winslett,Abstract Compared to centuries of physics and millennia of mathematics; the 50-year-historyof computer science and information management research makes us the toddlers of thescientific community. Yet during our brief existence; we've revolutionized the world and; notcontent with that; gone on to build and study virtual worlds. We have justly taken pride in ouraccomplishments; and developed our own unique way of conducting research; unlike otherscientific and engineering fields. But cracks have appeared in this edifice we have built. Theconference system that served us so well for our first 50 years is falling apart. Our ever-increasing population competes ever more energetically for a finite set of resources. Otherscientific and engineering disciplines still think that our field equates to programming; andlook down on us. While we may also look down on them; it is undeniably true that high …,Proceedings of the VLDB Endowment,2010,5
One platform for mining structured and unstructured data: dream or reality?,Dina Bitton; Franz Faerber; Laura Haas; Jayavel Shanmugasundaram,Although enterprises commonly utilize sophisticated data integration technology andbusiness intelligence tools for analysis of structured data; analysis of unstructured data is aseparate process and is often limited to capabilities supported by a search engine. Usershave separate and vastly different interfaces for structured and unstructured data: BusinessIntelligence for structured data and Search for unstructured.,Proceedings of the 32nd international conference on Very large data bases,2006,5
TWO APPROACHES TO DEADLOCK IN DISTRIBUTED SYSTEMS.,Laura Myers Haas,A distributed system consists of a finite collection of processes which communicate with oneanother exclusively through messages. One critical problem in distributed systems is the possibilityof deadlock. Deadlock occurs when a set of processes is unable to proceed because each processneeds to communicate with another member of the set; and no two processes in the set are waitingto communicate with each other. Two possible approaches to deadlock are detection andprevention. This dissertation makes contributions in both areas … In the area of detection; adistributed deadlock detection algorithm is proposed. The algorithm is proven correct: it is provenboth to detect all deadlocks which might occur and not to falsely detect deadlocks which do notexist. The algorithm applies the Dijkstra-Scholten algorithm for detecting termination of diffusingcomputations over static graphs to the dynamic structure of a wait-for graph.,*,1982,5
Debugging data exchange with vagabond,Boris Glavic; Jiang Du; Renée J Miller; Gustavo Alonso; Laura M Haas,In this paper; we present Vagabond; a system that uses a novel holistic approach to helpusers to understand and debug data exchange scenarios. Developing such a scenario is acomplex and labor-intensive process where errors are often only revealed in the targetinstance produced as the result of this process. This makes it very hard to debug suchscenarios; especially for non-power users. Vagabond aides a user in debugging byautomatically generating possible explanations for target instance errors identified by theuser. Schema mappings are declarative constraints that model the relationship between asource and a target schema. Data exchange systems; such as Clio [6]; ORCHESTRA [5];and many others; use schema mappings to produce an instance of the target schema basedon an instance of the source schema. Creating a mapping between two schemata is a …,Proceedings of the VLDB Endowment,2011,4
Scalable Data Integration by Mapping Data to Queries,Martin Hentschel; Donald Kossmann; Daniela Florescu; Laura Haas; Tim Kraska; Renée J Miller,The goal of a data integration system is to allow users to query diverse information sourcesthrough a schema that is familiar to them. However; there may be many different users whomay have dif-ferent preferred schemas; and the data may be stored in data sources whichuse still other schemas. To integrate data; mapping rules must be defined to map entities ofthe data sources to entities of the users' schemas. In large information systems with manydata sources which serve sophisticated applications; there can be many such mapping rulesand they can be complex. The purpose of this paper is to study the per-formance ofalternative query processing techniques for data integration systems with many complexmapping rules. A new approach; mapping data to queries (MDQ); is presented. Throughextensive performance experiments; it is shown that this approach performs well for …,Technical report/[ETH; Department of Computer Science,2009,4
SIGMOD'98: Proceedings of ACM SIGMOD International Conference on Management of Data: June 1-4; 1998; Seattle; Washington; USA,Laura Haas; Ashutosh Tiwary,*,*,1998,4
Explaining the execution semantics of sliding window queries over data streams: a work in progress report,Irina Botan; Roozbeh Derakhshan; Nihal Dindar; Laura Haas; Renee Miller; Nesime Tatbul,*,ETH Zurich; Tech. Rep.,2009,3
Building an information infrastructure for enterprise applications,Laura Haas,Abstract In a modern enterprise; it is inevitable that different portions of the organization willuse different systems to produce; store and search their critical data. Competition; evolvingtechnology; mergers; acquisitions; and geographic distribution all contribute to this diversity.Only by combining these various systems can the enterprise realize the full value of the datathey contain. Yet building new applications across these various information sources can beamazingly painful; forcing developers to discover what data is where; figure out what itmeans; and learn myriad different interfaces.,International Conference on Trends in Enterprise Application Architecture,2005,3
R*: a distributed data sharing system,Robert A Yost; Laura M Haas,*,Distributed systems; Vol. II: distributed data base systems,1986,3
Big data,HOW HOMEOWNERS SELECT WHO; WILL REMODEL THEIR KITCHEN; HOW HOMEOWNERS SEL,1] When eLcient use of space is a priority; MTI Baths' ultracompact Wall-Mounted VanitySinks; made from the company's Engineered Solid Stone; save space by integraঞng sink;vanity; and built-in storage. Two sizes are available; with space on the vanity top for faucetmounঞng. Bowl placement may be speciCed right or le [; or to allow more surface area forwall-mounted faucets. mঞbaths. com For more informaঞon; circle No. 810 on reader servicecard 2] Inspired by Greek and Roman architectural elements and the work of 17th centuryBriঞsh architect Inigo Jones; Kallista's Inigo Collecঞon features sculptural hubs and Cnialsfor tradiঞonal styling rich in detail. The collecঞon includes faucets; vaniঞes; lighঞng; andaccessories in four Cnishes. kallista. com No. 811,*,2014,2
Cutting the Knot: Explaining the Execution Semantics of Sliding Window Queries over Data Streams,Irina Botan; Roozbeh Derakhshan; Nihal Dindar; Laura Haas; Renee Miller; Nesime Tatbul,Abstract—Despite the availability of several data stream processing engines (SPEs) today; itremains hard to develop and maintain streaming applications. Existing SPEs vary widely intheir data and query models and capabilities. A lack of standards; and the wide (andchanging) variety of application requirements; restrict portability. Users find it difficult to knowwhich system to use; and even to understand the behavior of the system they choose. Ourgoal in this paper is to propose a formalism that can be used to explain a major subset ofthese different behaviors. We first provide an in-depth analysis of the heterogeneity problemacross three well-known and commonly used stream processing systems (two commercialand one academic); focusing on the execution semantics of sliding window queries. We thenpropose a simple yet powerful formal model that captures and explains the core …,ETH Zurich; Computer Science; Tech. Rep.; June,2009,2
Caching and Recovery of Catalog Informatin in Distributed Query Compilation,LM Haas; G Lindsay,*,IBM Technical Disclosure Bulletin,1985,2
A Distributed Deadlock Detection Algorithm and Its Correctness Proof.,KM Chandy; J Misra; L Haas,Abstract: This paper presents a very simple distributed algorithm for deadlock detection in anetwork of processes. The algorithm is proven correct; ie; we show that all true deadlocksare detected and no false reporting of deadlock occurs. In the algorithm no processmaintains global information. All messages have identical length and are short; consisting ofa single node name and a sequence number. Work is based on the work of Dijkstra andScholten on termination detection of diffusing computations. Descriptors:* ALGORITHMS;*COMPUTERS;* MESSAGE PROCESSING;* DOWNTIME; COMPUTER PROGRAMS; DATABASES; MATHEMATICAL MODELS; DATA PROCESSING; SHUTDOWNS;COMPUTATIONS; DETECTION; VALIDATION; ACCURACY; COMMUNICATIONSNETWORKS; SEARCHING; THEOREMS,*,1982,2
Information for people,Laura M Haas; Steve B Cousins,Ordinary people have access to unprecedented volumes of information today. Researchersin the fields of information management (IM) and human-computer interaction (HCI) arereacting to this challenge from their own unique perspectives. Having access to a billionrecords is cool; but having access to a billion people is awesome. In this paper; we look atrecent research from both communities; and speculate on how interactions between thecommunities could enhance the user experience of information.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,1
The Impact of Site Autonomy on R*: A Distributed Relational DBMS,PG Selinger; E Bertino; D Daniels; LM Haas; B Lindsay; G Lohman; Y Masunaga; C Mohan; P Ng; P Wilms; R Yost,This chapter discusses some of the issues raised in the implementation of a distributedrelational database management system constrained by the design objective of siteautonomy. Ease-of-use; software maintenance; authorization; performance; catalogmanagement; transaction commit protocols; concurrency control and data communicationare all impacted by the notion of site autonomy. These issues are discussed in the context ofR*; an experimental distributed database management system (DDBMS) which emphasizessite autonomy.,Databases-Role and Structure: An Advanced Course,1984,1
Site autonomy issues in R∗: A distributed database management system,P Selinger; Dean Daniels; L Haas; B Lindsay; Pui Ng; P Wilms; R Yost,Abstract A distributed database management system (DDBMS) must simplify the user's taskof defining applications which manipulate shared data stored at multiple computing sites. Tothis end; the DDBMS must support transparent access to remote data. That is; any operationallowed on local data should also be possible on remote data. At the same time; becausedifferent computing sites are controlled by different individuals or organizations; the DDBMSmust preserve each site's autonomy over its own data. This paper discusses some of theissues raised in the implementation of a DDBMS by the requirements of site autonomy. Theissues will be discussed from the perspective of the R∗ research project at IBM's San JoseResearch Lab.,Information sciences,1983,1
Leveraging Data and People to Accelerate Data Science,Laura Haas,Doing data science-extracting insight by analyzing data-is not easy. Data science is used toanswer interesting questions that typically involve multiple diverse data sources; manydifferent types of analysis; and often; large and messy data volumes. To answer one of thesequestions; several types of expertise may be needed to understand the context and domainbeing served; to import and transform individual data sets; to implement effective machinelearning and/or statistical methods; to design and program applications and interfaces toextract and share data and insights; and to manage the data and systems used for analysisand storage. In the IBM Research Accelerated Discovery Lab; we are studying how datascientists work; and using what we learn to help them gain insights faster. In this talk; we willlook at what we have learned to date; through user studies and experience with tens of …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,*
Data Science Education: We're Missing the Boat; Again,Bill Howe; Michael Franklin; Laura Haas; Tim Kraska; Jeffrey Ullman,In the first wave of data science education programs; data engineering topics (systems;scalable algorithms; data management; integration) tended to be de-emphasized in favor ofmachine learning and statistical modeling. The anecdotal evidence suggests this was amistake: data scientists report spending most of their time grappling with data far upstream ofmodeling activities. A second wave of data science education is emerging; one withincreased emphasis on practical issues in ethics; legal compliance; scientific reproducibility;data quality; and algorithmic bias. The data engineering community has a second chance toinfluence these programs beyond just providing a set of tools. In this panel; we'll discuss therole of data engineering in data science education programs; and how best to capitalize onemerging opportunities in this space.,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,*
Welcome message from the organizers,Laura Haas; Vipin Kumar; Howard Ho; Beng Chin Ooi; Mohammed J Zaki; Morris Hui-I Hsiao; Jian Li; Sudarsan Rachuri; Shipeng Yu; Feng Luo; Kemafor Ogan; Saumyadipta Pyne; Xiaohua Hu,Powered by Pure; Scopus & Elsevier Fingerprint Engine™ © 2017 Elsevier BV.,Unknown Journal,2015,*
INCORPORATING DATA TYPES,PF Wilms; PM Schwarz; HJ Schek; LM Haas,Abstract The limited set of data types and the difficulty of incorporating new ones in currentDBMSs prevent many applications from efficiently using a DBMS to store data. We discussthe merits of introducing externally defined types (EDTs) to bridge the gap between the datastructures of the application programming language and the data model of the database. Weshow that the DBMS needs to know the set of functions to be called on an EDT to permiteasy use of EDTs both in programming languages and by the DBMS; and we propose waysto make the definition and implementation of new types in the DBMS easier on both theDBMS and the type implementor. Functions are central to our approach; as they may beinvoked from within internal,Proceedings of the Third International Conference on Data and Knowledge Bases: Improving Usability and Responsiveness,2014,*
Special Section: Context-Oriented Information Integration Foreword,Manish Bhide; Laura Haas; Zack Ives; Mukesh Mohania,*,*,2010,*
Special issue: best papers of VLDB 2005,Laura M Haas; Christian S Jensen; Martin L Kersten,It has become a tradition to dedicate an issue of The VLDB Journal to selected papers fromthe International Conference on Very Large Databases. This issue is dedicated to the 31stVLDB conference; which was held in Trondheim; Norway from August 30 to September 2;2005. Following the vision of the VLDB Endowment to further broaden the scope ofdatabase research; the conference encompassed four program committees: core databasetechnology; infrastructure for information systems; industrial applications and experiences;and demonstrations. An estimated 2;000+ authors from virtually all over the world submitted563 papers. The core database technology committee received 322 submissions; out ofwhich 53 were accepted; the infrastructure for information systems committee received 195submissions; out of which 32 were accepted; and the industrial application and …,The VLDB Journal,2007,*
Information management technology in Asia,Laura Haas,*,International Conference on Management of Data: Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,*
The Lowell Database Research Self Assessment,Stan Zdonik; Jennifer Widom; Gerhard Weikum; Jeff Ullman; Rick Snodgrass; Mike Stonebraker; Avi Silberschatz; Timos Sellis; Hans Schek; Jeff Naughton; David Maier; Serge Abiteboul; Rakesh Agrawal; Phil Bernstein; Mike Carey; Stefano Ceri; Bruce Croft; David DeWitt; Mike Franklin; Hector Garcia Molina; Dieter Gawlick; Jim Gray; Laura Haas; Alon Halevy; Joe Hellerstein; Yannis Ioannidis; Martin Kersten; Michael Pazzani; Mike Lesk,Abstract A group of senior database researchers gathers every few years to assess the stateof database research and to point out problem areas that deserve additional focus. Thisreport summarizes the discussion and conclusions of the sixth ad-hoc meeting held May 4-6;2003 in Lowell; Mass. It observes that information management continues to be a criticalcomponent of most complex software systems. It recommends that database researchersincrease focus on: integration of text; data; code; and streams; fusion of information fromheterogeneous data sources; reasoning about uncertain data; unsupervised data mining forinteresting correlations; information privacy; and self-adaptation and repair.,*,2003,*
What will they think of next?,Laura Haas,What will they think of next? in: Intelligent Enterprise; 1 February 2003; vol.6; no. 3; p.S42(6); ISSN: 1524-3621. by: Laura Haas A young man is admitted to the hospital suffering froma relatively rare flu. But it's the third case the examining physician has seen this week; and hewants more information. Puzzled; the doctor takes a blood sample and orders otherprocedures. Then he sits down at his computer and asks (from a simple GUI) for a series of analyseson the samples; with the results compared to those of patients nation- wide with similarsymptoms. He also requests that the diagnosis; treatment; and outcome records be retrievedfor patients whose profiles match the young man's. He then requests occurrence informationfrom the Centers for Disease Control and Prevention (CDC) on each diagnosis returned. Withinminutes he finds the most effective treatment for other patients suffering from this …,Intelligent Enterprise,2003,*
The Stanford IBM Management of Multiple Information Systems (TSIMMIS)-IBM Almaden Research Center,Allen W Luniewski; Laura Haas,Abstract: This report is a compilation of a summary of the research work and publicationsproduced as a result of information systems management projects. This project attacked twoproblems of relevance to the Intelligent Integration of Information (I3) program: theclassification of semi-structured data to allow ingestion of that data into a data managementsystems; and the creation of" wrapper" technology to allow integration of multiple; disparate;information sources into a central query engine. Descriptors:* COMPUTER PROGRAMS;*INFORMATION SYSTEMS;* INFORMATION RETRIEVAL; DATA BASES; OPTIMIZATION;CLASSIFICATION. Subject Categories: INFORMATION SCIENCE COMPUTERPROGRAMMING AND SOFTWARE Distribution Statement: APPROVED FOR PUBLICRELEASE DEFENSE TECHNICAL INFORMATION CENTER 8725 John J. Kingman Road …,*,2000,*
Users today are struggling to integrate a broad range of information sources providing different levels of query capabilities. Currently; data sources with different and...,Yannis Papakonstantinou; Ashish Gupta; Laura Haas,In this paper we present recovery techniques for distributed main-memory databases;specifically for client-server and shared-disk architectures. We present a recovery schemefor client-server architectures which is based on shipping log records to the server; and tworecovery schemes for shared-disk architectures—one based on page shipping; and theother based on broadcasting of the log of updates...,Distributed and Parallel Databases,1998,*
Reminescences on Influential Papers-R Snodgrass; ed,L Haas; A Mendelzohn; M Ozsoyoglu; J Paredaens; K Ramamntham; N Roussopoulos; J Widom; P Yu,*,SIGMOD RECORD,1998,*
Data Engineering,Michael A Olson; Wei Michael Hong; Michael Ubell; Michael Stonebraker; Chad Carson; Virginia E Ogle; Laura M Haas; Donald Kossman; Edward L Wimmers; Jun Yang,Abstract As network connectivity has continued its explosive growth and as storage deviceshave become smaller; faster; and less expensive; the number of online digitized images hasincreased rapidly. Successful queries on large; heterogeneous image collections cannotrely on the use of text matching alone. In this paper we describe how we use image analysisin conjunction with an object relational database to provide both textual and content-basedqueries on a very large collection of digital images. We discuss the e ects of featurecomputation; retrieval speed; and development issues on our feature storage strategy.,*,1996,*
Database Research at the IBM Almaden Research Center,Laura M Haas; Patricia G Selinger,Research on database at the IBM Almaden Research Center covers a broad range of topics.We feel the main challenges in the database area are to enable increased performance bothfor transaction processing and for complex; ad hoc queries; and to raise the level ofabstraction provided by database systems; enabling databases to serve a broader range ofapplications. We are addressing the first with work in advanced database algorithms and onhigh performance architectures (including parallelism and large semiconductor memories).For the second we are exploring extensions to relational capabilities; investigating new;object-oriented approaches; and experimenting with a variety of technologies for someclasses of applications. Our work also ranges from pure exploratory to applied research. Weare collaborating with IBM product development laboratories in a joint effort; the Database …,ACM SIGMOD Record,1991,*
ACM TODS Publication Policy,Don S.  Batory; Philip A.  Bernstein; Umeshwar Dayal; Laura M.  Haas; Theo Haerder; Won Kim; David Maier; Gerard Salton; Gio Wiederhold,*,SIGMOD Record,1989,*
INCORPORA TING DA TA TYPES,PP Wilms; PM Schwarz; HJ Schek; LM Haas,Abstract The limited set of data types and the difficulty of incorporating new ones in currentDBMSs prevent many applications from efficiently using a DBMS to store data. We discussthe merits of introducing externally defined types (EDTs) to bridge the gap between the datastructures of the application programming language and the data model of the database. Weshow that the DBMS needs to know the set of functions to be called on an EDT to permiteasy use of EDTs both in programming languages and by the DBMS; and we propose waysto make the definition and implementation of new types in the DBMS easier on both theDBMS and the type Lm pi cm en tor. Functions are central to our approach; as they may beinvoked from within internal DBMS components by suitable extensions. We examine how tocreate access paths to support externally defined predicate functions by materialization or …,Proceedings of the Third International Conference on Data and Knowledge Bases--Improving Usability and Responsiveness: June 28-30; 1988; Jerusalem; Israel,1988,*
PUC/RJ,Marco A Casanova; Centro Cientifico de Brasilia; Jim Clifford; Dean Daniels; Umeshwar Dayal; Sakti P Ghosh; Laura M Haas; Matthias Jarke; Yahiko Kambayashi,*,Query processing in database systems,1985,*
Shorthand Views in Distributed Database Management Systems,E Bertinof; LM Haas; BG Lindsay,*,Computing 85: a broad perspective of current developments: proceedings of the Eighth International Computing Symposium; Florence; Italy; 27-29; March; 1985,1985,*
n Computer ystems,Anita K Jones; Michael D Schroeder; Andrew D Birrell; Roger M Needham; Bruce G Lindsay; Laura M Haas; C Mohan,Grapevine is a distributed; replicated system that provides message delivery; naming;authentication; resource location; and access control services in an internet of computers.The system; described in a previous paper [1]; was designed and implemented severalyears ago. We now have had operational experience with the system under substantial load.In this paper we report on what we have learned from using Grapevine.,ACM Trans; on Computer Systems,1984,*
DISTRIBUTED DATA BASES HJ Schneider (editor) North-Holland Publishing Company C Gl. 1982,Dean Daniels; Patricia Selinger; Laura Haas; Bruce Lindsay; C Mohan; Adrian Walker; Paul Wilms,*,Tutorial; recent advances in distributed data base management,1984,*
Two approaches to deadlock in distributed systems[Ph. D. Thesis],LM HAAS,*,*,1981,*
A Deadlock Absence Proof Technique Applied to a Multiple Copy Consistency Protocol,Laura M Haas; K Mani Chandy; Jayadev Misra,Abstract This paper proposes a general method for proving absence of deadlock indistributed networks of communicating processes; where messages are the means ofcommunication. This method is simple and; in particular; easy to use. We demonstrate theseproperties by applying the method to one solution to an important problem in distributeddatabase systems: Ellis* algorithm for maintaining consistency in multiple copies ofdatabases.,*,1980,*
An Optimizer for Heterogeneous Systems with NonStandard Data and Search Capabilities Laura M. Haas* Donald Kossmann† Edward L. Wimmersd Jun Yangj IBM...,Laura M Haas,Abstract Much of the world's nonstandard data resides in specialized data sources. This datamust often be queried together with data from other sources to give users the informationthey desire. Queries that can span several specialized sources with diverse searchcapabilities pose new challenges for query optimization. This paper describes the designand illustrates the use of a general purpose optimizer that can be taught about thecapabilities of a new data source. Our optimizer produces plans of high quality even in thepresence of nonstandard data; strange methods; and unusual query processing capabilities;and makes it easy to add new sources of standard or nonstandard data.,*,*,*
Views and Security in Distributed Database Management Systems,E Be&no; LM Haas,*,*,*,*
A DEADLOCK ABSENCE PROOF TECHNIQUE,Laura M Haas; J Misra,Abstract This paper proposes a general method for proving absence of deadlock indistributed networks of communicating processes; where messages are the means ofcomunication. This method is simple and; in particular; easy to use. We demonstrate theseproperties by applying the method to one solution to an important problem in distributeddatabase systems: Ellis' algorithm for maintaining consistency in multiple copies ofdatabases.,*,*,*
Information Integration Isn’t Simple,Laura Haas,There are many information integration problems. Information integration is more than “dataintegration”[1]; more than warehousing [3]; more than “data exchange”[2]. These well-studied problems all address aspects of the overall information integration problem; as doesresearch on discovery; ontologies; data cleansing; entity resolution and even; to someextent; search. In both the research and the commercial worlds; solutions to each of theseproblems come in many flavors. Thus; a key challenge to anyone attempting to integrateinformation is; first; to identify the technologies and tools needed; and then; to compose theintegration engines and tooling into a usable system that accomplishes their goals. In thispaper; we examine briefly the multiplicity of solutions that might be brought to bear toaddress a typical integration task. We then suggest specific areas of research to address …,*,*,*
Data Engineering,Serge Abiteboul; Sophie Cluet; Tova Milo; Pini Mogilevsky; Jerome Siméon; Sagit Zohar; Philip A Bernstein; Thomas Bergstraesser; Marco Carrer; Ashok Joshi; Paul Lin; Alok Srivastava; Laura Haas; Renee Miller; Bartholomew Niswonger; Mary Tork Roth; Peter Schwarz; Edward Wimmers,Membership in the TC on Data Engineering (http: www. is open to all current members of theIEEE Computer Society who are interested in database systems. The web page for the DataEngineering Bulletin is http://www. research. microsoft. com/research/db/debull. The webpage for the TC on Data Engineering is http://www. ccs. neu. edu/groups/IEEE/tcde/index.html.,*,*,*
VLDB Endowment Board of Trustees,Gerhard Weikum; Laura M Haas; Paolo Atzeni; Michael J Franklin; Amr El Abbadi; Gustavo Alonso; Peter MG Apers; Elisa Bertino; Peter Buneman; Johann Christoph Freytag; HV Jagadish; Christian S Jensen; Donald Kossmann; David Lomet; Renée J Miller; Shojiro Nishio; Beng Chin Ooi; Meral Ozsoyoglu; Krithi Ramamritham; Raghu Ramakrishnan; Stanley B Zdonik,The VLDB Endowment is a non-profit foundation whose objective is to promote scientific andeducational activities in the area of large-scale data; information; and knowledgemanagement. The Endowment serves as the steering committee for the VLDB conferenceseries. The Endowment also sponsors various scholarly activities. It has established aprogram that supports summer schools; tutorials; and other training activities of this kind; incountries that could otherwise not afford the expenses for such events. The Endowment isalso the main sponsor of the biennial Conference on Innovative Data Systems Research(CIDR); and it runs the VLDB Journal; one of the most successful journals in the databasearea. On various activities; the Endowment closely cooperates with ACM SIGMOD. TheVLDB Endowment has a board of 21 elected trustees; who are the legal guardians of the …,*,*,*
Keynote Speeches,Yannis Ioannidis; Ricardo Baeza-Yates; Laura M Haas,Abstract: Provides an abstract for each of the keynote presentations and a brief professionalbiography of each presenter. The complete presentations were not made available for publicationas part of the conference proceedings … A not-for-profit organization; IEEE is the world's largesttechnical professional organization dedicated to advancing technology for the benefit ofhumanity. © Copyright 2017 IEEE - All rights reserved. Use of this web site signifies your agreementto the terms and conditions.,*,*,*
Bulletin of the Technical Committee on,Michael A Olson; Wei Michael Hong; Michael Ubell; Michael Stonebraker; Chad Carson; Virginia E Ogle; Laura M Haas; Donald Kossman; Edward L Wimmers; Jun Yang,*,Urbana,*,*
Program Vice-Chairs,Divy Agarwal; Gustavo Alonso; Soumen Chakrabarti; Anindya Datta; Laura Haas; Don Kossmann; David Lomet; Gultekin Ozsoyoglu; Tamer Ozsu; Jerome Simeon; KY Whang; Tansel Abdullah Uz; Sihem Amer-Yahia; Arun K Mazumdar; BR Badrinath; Sujata Banerjee; David Bell; Elisa Bertino; Subhash Bhalla; Bharat Bhargava; Alex Biliris; Phil Bohannon; Philippe Bonnet; Sang K Cha; Sharma Chakravarthy; Kevin Chang; Surajit Chaudhuri; Sudarshan S Chawathe; Ming-Syan Chen; Arbee Chen; Chung-Min Chen; Ahmed Elmagarmid; Hakan Ferhatosmanoglu; Daniela Florescu; Peter Frankhauser,*,*,*,*
Workshop Officers,Laura Haas; Zachary Ives; Mukesh Mohania; Manish Bhide; Divy Agrawal; Phil Bernstein; Kevin Chang; Yi Chen; Alin Deutsch; AnHai Doan; Alon Halevy; Mizuho Iwaihara; Masaru Kitsuregawa; Craig Knoblock; Sergey Melnik; Ullas Nambiar; Felix Naumann; Evaggelia Pitoura; Prasan Roy; Michael Schrefl; Kohichi Takeda; Wang-Chiew Tan; Millist Vincent; Ji-Rong Wen,*,*,*,*
Univ. of Toronto,Renee J Miller; Mauricio A Hernandez; Laura M Haas; Lingling Yan; CT Howard Ho; Ronald Fagin; Lucian Popa,*,*,*,*
Data Engineering,Ronald Fagin; Ariel Fuxman; Laura M Haas; Mauricio A Hernández; Howard Ho; Anastasios Kementsietsidis; Renée J Miller; Felix Nauman; Lucian Popa; Yannis Velegrakis; Charlotte Vilarem; Ling-Ling Yan; Andrea Calı; Diego Calvanese; Giuseppe De Giacomo; Maurizio Lenzerini,Bulletin of the Technical Committee on Data Engineering September 2002 Vol. 25 No. 3 IEEEComputer Society Letters Letter from the Editor-in-Chief...................................................... DavidLomet 1 Letter from the Special Issue Editor.................................................. Renée J. Miller 2 SpecialIssue on Integration Management Data Integration: Where Does the Time Go?...... LenSeligman; Arnon Rosenthal; Paul Lehner; Angela Smith 3 Integration Through a Practitioner'sEye.... Srinivasa Narayanan; Subbu N. Subramanian and the Tavant Team 11 Data … EditorialBoard Editor-in-Chief David B. Lomet Microsoft Research One Microsoft Way; Bldg. 9 RedmondWA 98052-6399 lomet@ microsoft. com Associate Editors Umeshwar Dayal Hewlett-PackardLaboratories 1501 Page Mill Road; MS 1142 Palo Alto; CA 94304 Johannes Gehrke Departmentof Computer Science Cornell University Ithaca; NY 14853 Christian S. Jensen …,Urbana,*,*
Bulletin of the Technical Committee on,Serge Abiteboul; Sophie Cluet; Tova Milo; Pini Mogilevsky; Jerome Siméon; Sagit Zohar; Philip A Bernstein; Thomas Bergstraesser; Marco Carrer; Ashok Joshi; Paul Lin; Alok Srivastava; Laura Haas; Renee Miller; Bartholomew Niswonger; Mary Tork Roth; Peter Schwarz; Edward Wimmers,*,*,*,*
Bulletin of the Technical Committee on,Michael J Carey; Laura M Haas; James Kleewein; Berthold Reinwald; Steve Olson; Richard Pledereder; Phil Shaw; David Yach; Michael Stonebraker; Paul Brown; Martin Herbach; Mike Higgs; Bruce Cottman,*,*,*,*
Data Engineering,Michael J Carey; Laura M Haas; James Kleewein; Berthold Reinwald; Steve Olson; Richard Pledereder; Phil Shaw; David Yach; Michael Stonebraker; Paul Brown; Martin Herbach; Mike Higgs; Bruce Cottman,Membership in the TC on Data Engineering (http: www. is open to all current members of theIEEE Computer Society who are interested in database systems. The web page for the DataEngineering Bulletin is http://www. research. microsoft. com/research/db/debull. The webpage for the TC on Data Engineering is http://www. ccs. neu. edu/groups/IEEE/tcde/index.html.,*,*,*
Welcome Message from the General Chairs,Gillian Dobbie; Laura Haas,*,*,*,*
Data Structures for E cient Broker Implementation,Anthony Tomasicy; Luis Gravanoz; Calvin Luex; Peter Schwarz; Laura Haas,Abstract With the profusion of text databases on the Internet; it is becoming increasingly hardto nd the most useful databases for a given query. To attack this problem; several existingand proposed systems employ brokers to direct user queries; using a local database ofsummary information about the available databases. This summary information must eectively distinguish relevant databases; and must be compact while allowing e cient access.We o er evidence that one broker; GlOSS; can be e ective at locating databases of interesteven in a system of hundreds of databases; and examine the performance of accessing theGlOSS summaries for two promising storage methods: the grid le and partitioned hashing.We show that both methods can be tuned to provide good performance for a particularworkload (within a broad range of workloads); and discuss the tradeo s between the two …,*,*,*
