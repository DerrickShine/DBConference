Webtables: exploring the power of tables on the web,Michael J Cafarella; Alon Halevy; Daisy Zhe Wang; Eugene Wu; Yang Zhang,Abstract The World-Wide Web consists of a huge number of unstructured documents; but italso contains structured data in the form of HTML tables. We extracted 14.1 billion HTMLtables from Google's general-purpose web crawl; and used statistical classificationtechniques to find the estimated 154M that contain high-quality relational data. Becauseeach relational table has its own" schema" of labeled and typed columns; each such tablecan be considered a small structured database. The resulting corpus of databases is largerthan any other corpus we are aware of; by at least five orders of magnitude.,Proceedings of the VLDB Endowment,2008,522
The MADlib analytics library: or MAD skills; the SQL,Joseph M Hellerstein; Christoper Ré; Florian Schoppmann; Daisy Zhe Wang; Eugene Fratkin; Aleksander Gorajek; Kee Siong Ng; Caleb Welton; Xixuan Feng; Kun Li; Arun Kumar,Abstract MADlib is a free; open-source library of in-database analytic methods. It provides anevolving suite of SQL-based algorithms for machine learning; data mining and statistics thatrun at scale within a database engine; with no need for data import/export to other tools. Thegoal is for MADlib to eventually serve a role for scalable database systems that is similar tothe CRAN library for R: a community repository of statistical methods; this time written withscale and parallelism in mind. In this paper we introduce the MADlib project; including thebackground that led to its beginnings; and the motivation for its open-source nature. Weprovide an overview of the library's architecture and design patterns; and provide adescription of various statistical methods in that context. We include performance andspeedup results of a core design pattern from one of those methods over the Greenplum …,Proceedings of the VLDB Endowment,2012,235
BayesStore: managing large; uncertain data repositories with probabilistic graphical models,Daisy Zhe Wang; Eirinaios Michelakis; Minos Garofalakis; Joseph M Hellerstein,Abstract Several real-world applications need to effectively manage and reason about largeamounts of data that are inherently uncertain. For instance; pervasive computingapplications must constantly reason about volumes of noisy sensory readings for a variety ofreasons; including motion prediction and human behavior modeling. Such probabilistic dataanalyses require sophisticated machine-learning tools that can effectively model thecomplex spatio/temporal correlation patterns present in uncertain sensory data.Unfortunately; to date; most existing approaches to probabilistic database systems haverelied on somewhat simplistic models of uncertainty that can be easily mapped onto existingrelational architectures: Probabilistic information is typically associated with individual datatuples; with only limited or no support for effectively capturing and reasoning about …,Proceedings of the VLDB Endowment,2008,159
Uncovering the Relational Web.,Michael J Cafarella; Alon Y Halevy; Yang Zhang; Daisy Zhe Wang; Eugene Wu,ABSTRACT The World-Wide Web consists of a huge number of unstructured hypertextdocuments; but it also contains structured data in the form of HTML tables. Many of thesetables contain both relational-style data and a small “schema” of labeled and typed columns;making each such table a small structured database. The WebTables project is an effort toextract and make use of the huge number of these structured tables on the Web. A cleancollection of relational-style tables could be useful for improving web search; schemadesign; and many other applications. This paper describes the first stage of the WebTablesproject. First; we give an in-depth study of the Web's HTML table corpus. For example; weextracted 14.1 billion HTML tables from a several-billion-page portion of Google'sgeneralpurpose web crawl; and estimate that 154 million of these tables contain high …,WebDB,2008,133
Probabilistic Data Management for Pervasive Computing: The Data Furnace Project.,Minos N Garofalakis; Kurt P Brown; Michael J Franklin; Joseph M Hellerstein; Daisy Zhe Wang; Eirinaios Michelakis; Liviu Tancau; Eugene Wu; Shawn R Jeffery; Ryan Aipperspach,Abstract The wide deployment of wireless sensor and RFID (Radio Frequency IDentification)devices is one of the key enablers for next-generation pervasive computing applications;including large-scale environmental monitoring and control; context-aware computing; and“smart digital homes”. Sensory readings are inherently unreliable and typically exhibit strongtemporal and spatial correlations (within and across different sensing devices); effectivereasoning over such unreliable streams introduces a host of new data managementchallenges. The Data Furnace project at Intel Research and UC-Berkeley aims to build aprobabilistic data management infrastructure for pervasive computing environments thathandles the uncertain nature of such data as a first-class citizen through a principledframework grounded in probabilistic models and inference techniques.,IEEE Data Eng. Bull.,2006,50
Functional Dependency Generation and Applications in Pay-As-You-Go Data Integration Systems.,Daisy Zhe Wang; Xin Luna Dong; Anish Das Sarma; Michael J Franklin; Alon Y Halevy,ABSTRACT Recently; the opportunity of extracting structured data from the Web has beenidentified by a number of research projects. One such example is that millions of relational-style HTML tables can be extracted from the Web. Traditional data integration approachesdo not scale over such corpora with hundreds of small tables in one domain. To solve thisproblem; previous work has proposed pay-as-you-go data integration systems to provide;with little up-front cost; base services over loosely-integrated information. One keycomponent of such systems; which has received little attention to date; is the need for aframework to gauge and improve the quality of the integration. We propose a frameworkbased on functional dependencies (FDs). Unlike in traditional database design; where FDsare specified as statements of truth about all possible instances of the database; in web …,WebDB,2009,44
Knowledge expansion over probabilistic knowledge bases,Yang Chen; Daisy Zhe Wang,Abstract Information extraction and human collaboration techniques are widely applied inthe construction of web-scale knowledge bases. However; these knowledge bases are oftenincomplete or uncertain. In this paper; we present ProbKB; a probabilistic knowledge basedesigned to infer missing facts in a scalable; probabilistic; and principled manner using arelational DBMS. The novel contributions we make to achieve scalability and high qualityare: 1) We present a formal definition and a novel relational model for probabilisticknowledge bases. This model allows an efficient SQL-based inference algorithm forknowledge expansion that applies inference rules in batches; 2) We implement ProbKB onmassive parallel processing databases to achieve further scalability; and 3) We combineseveral quality control methods that identify erroneous rules; facts; and ambiguous …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,39
Hybrid in-database inference for declarative information extraction,Daisy Zhe Wang; Michael J Franklin; Minos Garofalakis; Joseph M Hellerstein; Michael L Wick,Abstract In the database community; work on information extraction (IE) has centered on twothemes: how to effectively manage IE tasks; and how to manage the uncertainties that arisein the IE process in a scalable manner. Recent work has proposed a probabilistic database(PDB) based declarative IE system that supports a leading statistical IE model; and anassociated inference algorithm to answer top-k-style queries over the probabilistic IEoutcome. Still; the broader problem of effectively supporting general probabilistic inferenceinside a PDB-based declarative IE system remains open. In this paper; we explore the in-database implementations of a wide variety of inference algorithms suited to IE; includingtwo Markov chain Monte Carlo algorithms; the Viterbi and the sum-product algorithms. Wedescribe the rules for choosing appropriate inference algorithms based on the model; the …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,36
Querying probabilistic information extraction,Daisy Zhe Wang; Michael J Franklin; Minos Garofalakis; Joseph M Hellerstein,Abstract Recently; there has been increasing interest in extending relational queryprocessing to include data obtained from unstructured sources. A common approach is touse stand-alone Information Extraction (IE) techniques to identify and label entities withinblocks of text; the resulting entities are then imported into a standard database andprocessed using relational queries. This two-part approach; however; suffers from two maindrawbacks. First; IE is inherently probabilistic; but traditional query processing does notproperly handle probabilistic data; resulting in reduced answer quality. Second;performance inefficiencies arise due to the separation of IE from query processing. In thispaper; we address these two problems by building on an in-database implementation of aleading IE model---Conditional Random Fields using the Viterbi inference algorithm. We …,Proceedings of the VLDB Endowment,2010,32
Ontological Pathfinding: Mining First-Order Knowledge from Large Knowledge Bases,Yang Chen; Sean Goldberg; Daisy Zhe Wang; Soumitra Siddharth Johri,*,ACM SIGMOD,2016,24
Probabilistic declarative information extraction,Daisy Zhe Wang; Eirinaios Michelakis; Michael J Franklin; Minos Garofalakis; Joseph M Hellerstein,Unstructured text represents a large fraction of the world's data. It often contains snippets ofstructured information (eg; people's names and zip codes). Information Extraction (IE)techniques identify such structured information in text. In recent years; database researchhas pursued IE on two fronts: declarative languages and systems for managing IE tasks; andprobabilistic databases for querying the output of IE. In this paper; we make the first step tomerge these two directions; without loss of statistical robustness; by implementing a state-of-the-art statistical IE model-Conditional Random Fields (CRF)-in the setting of a ProbabilisticDatabase that treats statistical models as first-class data objects. We show that the Viterbialgorithm for CRF inference can be specified declaratively in recursive SQL. We also showthe performance benefits relative to a standalone open-source Viterbi implementation …,2010 IEEE 26th International Conference on Data Engineering (ICDE 2010),2010,23
Automatic knowledge base construction using probabilistic extraction; deductive reasoning; and human feedback,Daisy Zhe Wang; Yang Chen; Sean Goldberg; Christan Grant; Kun Li,Abstract We envision an automatic knowledge base construction system consisting of threeinter-related components. MADden is a knowledge extraction system applying statistical textanalysis methods over database systems (DBMS) and massive parallel processing (MPP)frameworks; ProbKB performs probabilistic reasoning over the extracted knowledge toderive additional facts not existing in the original text corpus; CAMeL leverages humanintelligence to reduce the uncertainty resulting from both the information extraction andprobabilistic reasoning processes.,Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,2012,17
Selectivity estimation for extraction operators over text data,Daisy Zhe Wang; Long Wei; Yunyao Li; Frederick Reiss; Shivakumar Vaithyanathan,Recently; there has been increasing interest in extending relational query processing toefficiently support extraction operators; such as dictionaries and regular expressions; overtext data. Many text processing queries are sophisticated in that they involve multipleextraction and join operators; resulting in many possible query plans. However; there hasbeen little research on building the selectivity or cost estimation for these extractionoperators; which is crucial for an optimizer to pick a good query plan. In this paper; we definethe problem of selectivity estimation for dictionaries and regular expressions; and propose todevelop document synopses over a text corpus; from which the selectivity can be estimated.We first adapt the language models in the Natural Language Processing literature to formthe top-k n-gram synopsis as the baseline document synopsis. Then we develop two …,2011 IEEE 27th International Conference on Data Engineering,2011,11
Efficient In-Database Analytics with Graphical Models.,Daisy Zhe Wang; Yang Chen; Christan Earl Grant; Kun Li,*,IEEE Data Eng. Bull.,2014,10
UDA-GIST: an in-database framework to unify data-parallel and state-parallel analytics,Kun Li; Daisy Zhe Wang; Alin Dobra; Christopher Dudley,Abstract Enterprise applications need sophisticated in-database analytics in addition totraditional online analytical processing from a database. To meet customers' pressingdemands; database vendors have been pushing advanced analytical techniques intodatabases. Most major DBMSes offer User-Defined Aggregate (UDA); a data-drivenoperator; to implement many of the analytical techniques in parallel. However; UDAs can notbe used to implement statistical algorithms such as Markov chain Monte Carlo (MCMC);where most of the work is performed by iterative transitions over a large state that can not benaively partitioned due to data dependency. Typically; this type of statistical algorithmrequires pre-processing to setup the large state in the first place and demands post-processing after the statistical inference. This paper presents General Iterative State …,Proceedings of the VLDB Endowment,2015,9
CASTLE: Crowd-assisted system for text labeling and extraction,Sean Louis Goldberg; Daisy Zhe Wang; Tim Kraska,Abstract The amount of text data has been growing exponentially and with it the demand forimproved information extraction (IE) efforts to analyze and query such data. While automaticIE systems have proven useful in controlled experiments; in practice the gap betweenmachine learning extraction and human extraction is still quite large. In this paper; wepropose a system that uses crowdsourcing techniques to help close this gap. One of thefundamental issues inherent in using a large-scale human workforce is deciding the optimalquestions to pose to the crowd. We demonstrate novel solutions using mutual informationand token clustering techniques in the domain of bibliographic citation extraction. Ourexperiments show promising results in using crowd assistance as a cost-effective way toclose up the” last mile” between extraction systems and a human annotator.,First AAAI Conference on Human Computation and Crowdsourcing,2013,8
Web-scale knowledge inference using markov logic networks,DZW Yang Chen; Daisy Zhe Wang,Abstract In this paper; we present our on-going work on ProbKB; a PROBabilistic KnowledgeBase constructed from web-scale extracted entities; facts; and rules represented as aMarkov logic network (MLN). We aim at web-scale MLN inference by designing a novelrelational model to represent MLNs and algorithms that apply rules in batches. Errors arehandled in a principled and elegant manner to avoid error propagation and unnecessaryresource consumption. MLNs infer from the input a factor graph that encodes a probabilitydistribution over extracted and inferred facts. We run parallel Gibbs sampling algorithms onGraphlab to query this distribution. Initial experiment results show promising scalability ofour approach.,ICML workshop on Structured Learning: Inferring Graphs from Structured and Unstructured Inputs,2013,7
Madden: query-driven statistical text analytics,Christan Earl Grant; Joir-dan Gumbs; Kun Li; Daisy Zhe Wang; George Chitouras,Abstract In many domains; structured data and unstructured text are both important naturalresources to fuel data analysis. Statistical text analysis needs to be performed over text datato extract structured information for further query processing. Typically; developers will needto connect multiple tools to build off-line batch processes to perform text analytic tasks.MADden is an integrated system developed for relational database systems such asPostgreSQL and Greenplum for real-time ad hoc query processing over structured andunstructured data. MADden implements four important text analytic functions that we havecontributed to the MADlib open source library for textual analytics. In this demonstration; wewill show the capability of the MADden text analytic library using computational journalismas the driving application. We show real-time declarative query processing over multiple …,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,7
Gptext: Greenplum parallel statistical text analysis framework,Kun Li; Christan Grant; Daisy Zhe Wang; Sunny Khatri; George Chitouras,Abstract Many companies keep large amounts of text data inside of relational databases.Several challenges exist in using state-of-the-art systems to perform analysis on suchdatasets. First; expensive big data transfer cost must be paid up front to move data betweendatabases and analytics systems. Second; many popular text analytics packages do notscale up to production sized datasets. In this paper; we introduce GPText; Greenplumparallel statistical text analysis framework that addresses the above problems by supportingstatistical inference and learning algorithms natively in a massively parallel processingdatabase system. GPText seamlessly integrates the Solr search engine and appliesstatistical algorithms such as k-means and LDA using MADLib; an open source library forscalable in-database analytics which can be installed on Post-greSQL and Greenplum. In …,Proceedings of the Second Workshop on Data Analytics in the Cloud,2013,6
Probabilistic complex event triggering,Daisy Zhe Wang; Eirinaios Michelakis; Liviu Tancau,Abstract. Recently; wireless sensor devices have been widely deployed in variousapplication settings (including environmental research; control systems; etc.). Because of theinherent unreliability of sensor readings; any kind of reasoning in sensor environmentsneeds to carefully account for noise. The key goal of pcet is to build an infrastructure that canautomatically infer and reason about the probabilities of triggered events; using a principledprobabilistic model for the underlying sensor data. Through such probabilistic reasoning;pcet can incorporate uncertainly factors and make finer–grain decisions on eventoccurrences. This is achieved through the use of a Bayesian Network to directly model andexploit correlations across different sensors and the definition of a complex–event language;which allows users/applications to create hierarchies of higher-level events. As …,*,2009,6
Knowledge extraction and outcome prediction using medical notes,Ryan Cobb; Sahil Puri; Daisy Zhe Wang; Tezcan Baslanti; Azra Bihorac,Abstract The increasing use of electronic health records (EHR) has allowed for anunprecedented ability to perform analysis on patient data. By training a number of statisticalmachine learning classifiers over the unstructured text found in admission notes andoperating procedures; prediction of a surgical procedure's outcome can be performed. Weextend an initial bag-of-words model to a bag-of-concepts model; which uses cTakes andUMLS to extract medical terms and concepts from medical notes. We also extend cTakes toimprove the knowledge extraction. Lastly; we propose a knowledge exchange component;which allows physicians to provide feedback on outcome results to further tune theunderlying classifier.,ICML workshop on Role of Machine Learning in Transforming Healthcare; Atlanta; Georgia; USA,2013,5
Declarative information extraction in a probabilistic database system,D Wang; Eirinaios Michelakis; M Garofalakis; MJ Franklin; Joseph M Hellerstein,ABSTRACT Full-text documents represent a large fraction of the world's data. Although notstructured per se; they often contain snippets of structured information within them: eg;names; addresses; and document titles. Information Extraction (IE) techniques identify suchstructured information in text. In recent years; database research has pursued IE on twofronts: declarative languages and systems for managing IE tasks; and IE as an uncertaindata source for Probabilistic Databases. It is natural to consider merging these twodirections; but efforts to do so have had to compromise on the statistical robustness of IEalgorithms in order to fit with early Probabilistic Database models. In this paper; we bridgethe gap between these ideas by implementing a state-of-the-art statistical IE approach–Conditional Random Fields (CRFs)–in the setting of Probabilistic Databases that treat …,Proc. of the 26th ICDE Conf,2010,5
ScaLeKB: scalable learning and inference over large knowledge bases,Yang Chen; Daisy Zhe Wang; Sean Goldberg,Abstract Recent years have seen a drastic rise in the construction of web knowledge bases(eg; Freebase; YAGO; DBPedia). These knowledge bases store structured information aboutreal-world people; places; organizations; etc. However; due to the limitations of humanknowledge; web corpora; and information extraction algorithms; the knowledge bases arestill far from complete. To infer the missing knowledge; we propose the OntologicalPathfinding (OP) algorithm to mine first-order inference rules from these web knowledgebases. The OP algorithm scales up via a series of optimization techniques; including a newparallel-rule-mining algorithm; a pruning strategy to eliminate unsound and inefficient rulesbefore applying them; and a novel partitioning algorithm to break the learning task intosmaller independent sub-tasks. Combining these techniques; we develop a first rule …,The VLDB Journal,2016,4
ArchimedesOne: query processing over probabilistic knowledge bases,Xiaofeng Zhou; Yang Chen; Daisy Zhe Wang,Abstract Knowledge bases are becoming increasingly important in structuring andrepresenting information from the web. Meanwhile; web-scale information poses significantscalability and quality challenges to knowledge base systems. To address these challenges;we develop a probabilistic knowledge base system; A rchimedes O ne; by scaling up theknowledge expansion and statistical inference algorithms. We design a web interface forusers to query and update large knowledge bases. In this paper; we demonstrate the Archimedes O ne system to showcase its efficient query and inference engines. Thedemonstration serves two purposes: 1) to provide an interface for users to interact with Archimedes O ne through load; search; and update queries; and 2) to validate ourapproaches of knowledge expansion by applying inference rules in batches using …,Proceedings of the VLDB Endowment,2016,4
A machine learning based topic exploration and categorization on surveys,Clint P George; Doris Z Wang; Joseph N Wilson; Liana M Epstein; Philip Garland; Annabell Suh,This paper describes an automatic topic extraction; categorization; and relevance rankingmodel for multi-lingual surveys and questions that exploits machine learning algorithmssuch as topic modeling and fuzzy clustering. Automatically generated question and surveycategories are used to build question banks and category-specific survey templates. First;we describe different pre-processing steps we considered for removing noise in themultilingual survey text. Second; we explain our strategy to automatically extract surveycategories from surveys based on topic models. Third; we describe different methods tocluster questions under survey categories and group them based on relevance. Last; wedescribe our experimental results on a large group of unique; real-world survey datasetsfrom the German; Spanish; French; and Portuguese languages and our refining methods …,Machine Learning and Applications (ICMLA); 2012 11th International Conference on,2012,4
In-database batch and query-time inference over probabilistic graphical models using UDA–GIST,Kun Li; Xiaofeng Zhou; Daisy Zhe Wang; Christan Grant; Alin Dobra; Christopher Dudley,Abstract To meet customers' pressing demands; enterprise database vendors have beenpushing advanced analytical techniques into databases. Most major DBMSes use user-defined aggregates (UDAs); a data-driven operator; to implement analytical techniques inparallel. However; UDAs alone are not sufficient to implement statistical algorithms wheremost of the work is performed by iterative transitions over a large state that cannot be naivelypartitioned due to data dependency. Typically; this type of statistical algorithm requires pre-processing to set up the large state in the first place and demands post-processing after thestatistical inference. This paper presents general iterative state transition (GIST); a newdatabase operator for parallel iterative state transitions over large states. GIST receives astate constructed by a UDA and then performs rounds of transitions on the state until it …,The VLDB Journal,2016,3
University of Florida DSR lab system for KBP slot filler validation 2015,Miguel Rodriguez; Sean Goldberg; Daisy Zhe Wang,Abstract In this paper we present a Slot filler Validation (SFV) system that uses a semi-supervised ensemble learning approach to aggregate the results from multiple slot fillersfrom the Cold Start track. We apply Bipartite Graph-based Consensus Maximization (BGCM)to combine the output of supervised stacked ensemble methods with the output of slot fillingruns that can't be trained. By using BGCM we are also able to leverage a small set ofassessed fillers to increase the performance of the system. The ensemble resultsoutperformed the best cold start run; the best filtered runs; and other ensemble systems.,Proceedings of the Eighth Text Analysis Conference (TAC2015),2015,3
SMART Electronic Legal Discovery Via Topic Modeling.,Clint Pazhayidam George; Sahil Puri; Daisy Zhe Wang; Joseph N Wilson; William F Hamilton,Abstract Electronic discovery is an interesting sub problem of information retrieval in whichone identifies documents that are potentially relevant to issues and facts of a legal case froman electronically stored document collection (a corpus). In this paper; we considerrepresenting documents in a topic space using the well-known topic models such as latentDirichlet allocation and latent semantic indexing; and solving the information retrievalproblem via finding document similarities in the topic space rather doing it in the corpusvocabulary space. We also develop an iterative SMART ranking and categorizationframework including human-in-the-loop to label a set of seed (training) documents andusing them to build a semi-supervised binary document classification model based onSupport Vector Machines. To improve this model; we propose a method for choosing …,FLAIRS Conference,2014,3
Granularity conscious modeling for probabilistic databases,Eirinaios Michelakis; Daisy Zhe Wang; Minos Garofalakis; Joseph M Hellerstein,The convergence of embedded sensor systems and stream query processing suggests animportant role for database techniques; in managing data that only partially and of-teninaccurately capture the state of the world. Reasoning about uncertainty as a first classcitizen; inside a database system; becomes an increasingly important operation forprocessing non deterministic data. An essential step for such an approach lies in the choiceof the appropriate un-certainty model; that captures the probabilistic information in the data;both accurately and at the right semantic de-tail level. This paper introduces HierarchicalFirst-Order Graphical Models (HFGMs); an intuitive and economical representation of thedata correlations stored in a Proba-bilistic Data Management system; in a hierarchicalsetting. HFGM semantics allow for an efficient summarization of the probabilistic model …,Seventh IEEE International Conference on Data Mining Workshops (ICDMW 2007),2007,3
Scalable Image Retrieval with Multimodal Fusion,Yang Peng; Xiaofeng Zhou; Daisy Zhe Wang; Chunsheng Victor Fang,Abstract As the number of images grows rapidly on the Internet; the scalability of imageretrieval systems becomes a significant issue. In this paper; we propose two distributedclustering algorithms to scale up the bag-of-visual-words model on millions of images andbillions of visual features by leveraging distributed systems. We also introduce a multimodalfusion model to utilize textual data to improve the quality of image retrieval. Our experimentson multimodal datasets demonstrated our fusion approach can achieve high retrieval qualitycompared to image-only retrieval and text-only retrieval.,*,2015,2
SigmaKB: multiple probabilistic knowledge base fusion,Miguel Rodríguez; Sean Goldberg; Daisy Zhe Wang,Abstract The interest in integrating web-scale knowledge bases (KBs) has intensified in thelast several years. Research has focused on knowledge base completion between two KBswith complementary information; lacking any notion of uncertainty or method of handlingconflicting information. We present S igma KB; a knowledge base system that utilizesConsensus Maximization Fusion and user feedback to integrate and improve the queryresults of a total of 71 KBs. This paper presents the architecture and demonstration details.,Proceedings of the VLDB Endowment,2016,1
Multimodal Ensemble Fusion for Disambiguation and Retrieval,Yang Peng; Xiaofeng Zhou; Daisy Zhe Wang; Ishan Patwa; Dihong Gong; Chunsheng Victor Fang,In this paper; we first identify the correlative and complementary relations among multiplemodalities. Then we propose a multimodal ensemble fusion model to capture thecomplementary relation and correlative relation between two modalities (images and text)and explain why this ensemble fusion model works. Experimental results on the UIUC-ISDdataset and the Google-MM dataset show our ensemble fusion model outperformsapproaches using only single modality for disambiguation and retrieval. Word sensedisambiguation and information retrieval are the use cases we studied to demonstrate theeffectiveness of our ensemble fusion model.,IEEE MultiMedia,2016,1
Impact of atmospheric correction and image filtering on hyperspectral classification of tree species using support vector machine,Morteza Shahriari Nia; Daisy Zhe Wang; Stephanie Ann Bohlman; Paul Gader; Sarah J Graves; Milenko Petrovic,Hyperspectral images can be used to identify savannah tree species at the landscape scale;which is a key step in measuring biomass and carbon; and tracking changes in speciesdistributions; including invasive species; in these ecosystems. Before automated speciesmapping can be performed; image processing and atmospheric correction is oftenperformed; which can potentially affect the performance of classification algorithms. Wedetermine how three processing and correction techniques (atmospheric correction;Gaussian filters; and shade/green vegetation filters) affect the prediction accuracy ofclassification of tree species at pixel level from airborne visible/infrared imagingspectrometer imagery of longleaf pine savanna in Central Florida; United States. Speciesclassification using fast line-of-sight atmospheric analysis of spectral hypercubes …,Journal of Applied Remote Sensing,2015,1
Consensus Maximization Fusion of Probabilistic Information Extractors,Miguel Rodrıguez; Sean Goldberg; Daisy Zhe Wang,*,*,*,1
Probabilistic Ensemble Fusion for Multimodal Word Sense Disambiguation,Yang Peng; Daisy Zhe Wang; Ishan Patwa; Dihong Gong; Chunsheng Victor Fang,With the advent of abundant multimedia data on the Internet; there have been researchefforts on multimodal machine learning to utilize data from different modalities. Currentapproaches mostly focus on developing models to fuse low-level features from multiplemodalities and learn unified representation from different modalities. But most related workfailed to justify why we should use multimodal data and multimodal fusion; and few of themleveraged the complementary relation among different modalities. In this paper; we firstidentify the correlative and complementary relations among multiple modalities. Then wepropose a probabilistic ensemble fusion model to capture the complementary relationbetween two modalities (images and text). Experimental results on the UIUC-ISD datasetshow our ensemble approach outperforms approaches using only single modality. Word …,2015 IEEE International Symposium on Multimedia (ISM),2015,*
Query-Driven Sampling for Collective Entity Resolution,Christan Grant; Daisy Zhe Wang; Michael L Wick,Entity Resolution is the process of determining records (mentions) in a database thatcorrespond to the same real-world entity. Traditional pairwise ER methods can lead toinconsistencies and low accuracy due to localized decisions. Leading ER systems solve thisproblem by collectively resolving all records using a probabilistic graphical model andMarkov chain Monte Carlo (MCMC) inference. However; for large datasets; this is anextremely expensive process. One key observation is that such exhaustive ER processincurs a huge up-front cost; which is wasteful in practice because most users are interestedin only a small subset of entities. In this paper; we advocate pay-as-you-go entity resolutionby developing a number of query-driven collective ER techniques. We introduce two classesof SQL queries that involve ER operators-selection-driven ER and join-driven ER. We …,arXiv preprint arXiv:1508.03116,2015,*
A Challenge for Long-Term Knowledge Base Maintenance,Christan Earl Grant; Daisy Zhe Wang,Knowledge bases (KBs) are repositories of interconnected facts with an inference engine.Companies are increasingly populating KBs with facts from disparate sources to create acentral repository of information to provide users with a richer and more integrated userexperience [Herman and Delurey 2013]. Additionally; inference over the constructed KB canproduce new facts not specifically mentioned in the KB. Google is now employing KBs tosurface additional information for user search [Dong et al. 2014a]. Manually constructedKBs; such as YAGO [Hoffart et al. 2013] and DBpedia [Auer et al. 2007]; are increasinglybeing used as the gold standard and ground truth of newer KBs [Dong et al. 2014b].However; the growing number of KBs inside an organization require a sufficiently high levelof quality and must be meticulously maintained. Both YAGO and DBPedia were …,Journal of Data and Information Quality (JDIQ),2015,*
Streaming Fact Extraction for Wikipedia Entities at Web-Scale,Morteza Shahriari Nia; Christan Grant; Yang Peng; Daisy Zhe Wang; Milenko Petrovic,Abstract Wikipedia. org is the largest online resource for free information and is maintainedby a small number of volunteer editors. The site contains 4.3 million english articles; thesepages can easily be neglected; becoming out of date. Any news-worthy event may requirean update of several pages. To address this issue of stale articles we create a system thatreads in a stream diverse web documents and recommends facts to be added to specifiedWikipedia pages. We developed a three-stage streaming system that creates models ofWikipedia pages; filters out irrelevant documents and extracts facts that are relevant toWikipedia pages. The systems is evaluated over a 500M page web corpus and 139Wikipedia pages. Our results show a promising framework for fast fact extraction fromarbitrary web pages for Wikipedia. Wikipedia. org (WP) is the largest and most popular …,The Twenty-Seventh International Flairs Conference,2014,*
A Probabilistic Knowledge Base System.,Daisy Zhe Wang,Page 1. A PROBABILISTIC KNOWLEDGE BASE SYSTEM Daisy Zhe Wang CISE; Universityof Florida Data Science Lab; Database Research Group 01/07/2013 Page 2. Knowledge Bases •A knowledge base is a collection of entity; facts; relationships that conforms with a certain datamodel. • A knowledge base helps machine understand humans; languages; and the world. •Examples 1: Google Knowledge Graph Page 3. Knowledge Bases • A knowledge base is acollection of entity; facts; relationships that conforms with a certain data model. • A knowledgebase helps machine understand humans; languages; and the world. • Examples 2: NELL (NeverEnding Language Learner) Page 4. Knowledge Bases • A knowledge base is a collection ofentity; facts; relationships that conforms with a certain data model. • A knowledge base helpsmachine understand humans; languages; and the world …,CIDR,2013,*
Extracting and Querying Probabilistic Information in BayesStore,Zhe Wang,Abstract During the past few years; the number of applications that need to process large-scale data has grown remarkably. The data driving these applications are often uncertain; asis the analysis; which often involves probabilistic models and statistical inference. Examplesinclude sensor-based monitoring; information extraction; and online advertising. Suchapplications require probabilistic data analysis (PDA); which is a family of queries over data;uncertainties; and probabilistic models that involve relational operators from databaseliterature; as well as inference operators from statistical machine learning (SML) literature.Prior to our work; probabilistic database research advocated an approach in whichuncertainty is modeled by attaching probabilities to data items. However; such systems donot and cannot take advantage of the wealth of SML research; because they are unable …,*,2011,*
Optimizing Sampling-based Entity Resolution over Streaming Documents,Christan Earl Grant; Daisy Zhe Wang,Abstract Increasingly; organizations have employed methods to understand unstructuredtext across the web. Entity resolution is used to identify mentions in large; streaming textcorpora. Sampling-based entity resolution using Markov Chain Monte Carlo (MCMC)techniques guarantees convergence to a stationary distribution and can jump out of a localoptimum. When performing entity resolution over streams of incoming data; the growingquantity of data amplifies two central issues. First; because the sampling process is random;many iterations are wasted attempting to resolve unambiguous entities. Second; thequadratic runtime for scoring entities becomes prohibitive for largest entities. Frequentstreaming updates from the web exacerbate these difficulties. In this paper; we discuss thecreation of a proposal optimizer; in the spirit of database optimizers. This optimizer …,*,*,*
University of Florida Knowledge Base Acceleration Notebook,Morteza Shahriari Nia; Christan Grant; Yang Peng; Daisy Zhe Wang; Milenko Petrovic,Abstract In this paper we will present the system design and algorithm adopted by theGatorDSR team; University of Florida to efficiently process TREC KBA 2013—SSF track.Here we will describe the system as well as the details the algorithms used to extract slotvalues for the given slot name. Scalability; efficiency; precision and recall are the majorgoals of this work; given the overly limited time limitation and available computationalresources.,*,*,*
Bonsai: Interactive Supervision for Machine Learning,David Purdy; Daisy Zhe Wang,Abstract We introduce Bonsai; a visual system developed for statistical machine learningresearchers to explore and interact with the model building process and to comparebetween different models over the same data set. The system is especially valuable forclassification problems arising from large and high dimensional data sets; where manualinspection or construction of classification models can be prohibitively time-consuming. Inaddition; the system encourages a machine learning guided tour through the data;improving the user's understanding of the data and participation in the modeling process. Incontrast to much previous work; the emphasis is on considering the joint space of the dataand multiple machine learning models; rather than providing either an interface for manualclassification or for post-construction analysis of a single model.,*,*,*
