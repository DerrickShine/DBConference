A class of parallel tiled linear algebra algorithms for multicore architectures,Alfredo Buttari; Julien Langou; Jakub Kurzak; Jack Dongarra,Abstract As multicore systems continue to gain ground in the high performance computingworld; linear algebra algorithms have to be reformulated or new algorithms have to bedeveloped in order to take advantage of the architectural features on these new processors.Fine grain parallelism becomes a major requirement and introduces the necessity of loosesynchronization in the parallel execution of an operation. This paper presents algorithms forthe Cholesky; LU and QR factorization where the operations can be represented as asequence of small tasks that operate on square blocks of data. These tasks can bedynamically scheduled for execution based on the dependencies among them and on theavailability of computational resources. This may result in out of order execution of taskswhich will completely hide the presence of intrinsically sequential tasks in the …,Parallel Computing,2009,502
Numerical linear algebra on emerging architectures: The PLASMA and MAGMA projects,Emmanuel Agullo; Jim Demmel; Jack Dongarra; Bilel Hadri; Jakub Kurzak; Julien Langou; Hatem Ltaief; Piotr Luszczek; Stanimire Tomov,Abstract The emergence and continuing use of multi-core architectures and graphicsprocessing units require changes in the existing software and sometimes even a redesign ofthe established algorithms in order to take advantage of now prevailing parallelism. ParallelLinear Algebra for Scalable Multi-core Architectures (PLASMA) and Matrix Algebra on GPUand Multics Architectures (MAGMA) are two projects that aims to achieve high performanceand portability across a wide range of multi-core architectures and hybrid systemsrespectively. We present in this document a comparative study of PLASMA's performanceagainst established linear algebra packages and some preliminary results of MAGMA onhybrid multi-core and GPU systems.,Journal of Physics: Conference Series,2009,392
Communication-optimal parallel and sequential QR and LU factorizations,James Demmel; Laura Grigori; Mark Hoemmen; Julien Langou,We present parallel and sequential dense QR factorization algorithms that are both optimal(up to polylogarithmic factors) in the amount of communication they perform and just asstable as Householder QR. We prove optimality by deriving new lower bounds for thenumber of multiplications done by “non-Strassen-like” QR; and using these in knowncommunication lower bounds that are proportional to the number of multiplications. We notonly show that our QR algorithms attain these lower bounds (up to polylogarithmic factors);but that existing LAPACK and ScaLAPACK algorithms perform asymptotically morecommunication. We derive analogous communication lower bounds for LU factorization andpoint out recent LU algorithms in the literature that attain at least some of these lowerbounds. The sequential and parallel QR algorithms for tall and skinny matrices lead to …,SIAM Journal on Scientific Computing,2012,263
Parallel tiled QR factorization for multicore architectures,Alfredo Buttari; Julien Langou; Jakub Kurzak; Jack Dongarra,Abstract As multicore systems continue to gain ground in the high-performance computingworld; linear algebra algorithms have to be reformulated or new algorithms have to bedeveloped in order to take advantage of the architectural features on these new processors.Fine-grain parallelism becomes a major requirement and introduces the necessity of loosesynchronization in the parallel execution of an operation. This paper presents an algorithmfor the QR factorization where the operations can be represented as a sequence of smalltasks that operate on square blocks of data (referred to as 'tiles'). These tasks can bedynamically scheduled for execution based on the dependencies among them and on theavailability of computational resources. This may result in an out-of-order execution of thetasks that will completely hide the presence of intrinsically sequential tasks in the …,Concurrency and Computation: Practice and Experience,2008,208
Algorithm-based fault tolerance applied to high performance computing,George Bosilca; Rémi Delmas; Jack Dongarra; Julien Langou,Abstract We present a new approach to fault tolerance for High Performance Computingsystem. Our approach is based on a careful adaptation of the Algorithm-Based FaultTolerance technique [K. Huang; J. Abraham; Algorithm-based fault tolerance for matrixoperations; IEEE Transactions on Computers (Spec. Issue Reliable & Fault-Tolerant Comp.)33 (1984) 518–528] to the need of parallel distributed computation. We obtain a stronglyscalable mechanism for fault tolerance. We can also detect and correct errors (bit-flip) on thefly of a computation. To assess the viability of our approach; we have developed a fault-tolerant matrix–matrix multiplication subroutine and we propose some models to predict itsrunning time. Our parallel fault-tolerant matrix–matrix multiplication scores 1.4 TFLOPS on484 processors (cluster jacquard. nersc. gov) and returns a correct result while one …,Journal of Parallel and Distributed Computing,2009,161
Tiled QR factorization algorithms,Henricus Bouwmeester; Mathias Jacquelin; Julien Langou; Yves Robert,Abstract This work revisits existing algorithms for the QR factorization of rectangular matricescomposed of p× q tiles; where p≥ q. Within this framework; we study the critical paths andperformance of algorithms such as Sameh-Kuck; Fibonacci; Greedy; and those found withinPLASMA. Although neither Fibonacci nor Greedy is optimal; both are shown to beasymptotically optimal for all matrices of size p= q 2 f (q); where f is any function such thatlim+∞ f= 0. This novel and important complexity result applies to all matrices where p and qare proportional; p= λq; with λ≥ 1; thereby encompassing many important situations inpractice (least squares). We provide an extensive set of experiments that show thesuperiority of the new algorithms for tall matrices.,Proceedings of 2011 International Conference for High Performance Computing; Networking; Storage and Analysis,2011,149
The impact of multicore on math software,Alfredo Buttari; Jack Dongarra; Jakub Kurzak; Julien Langou; Piotr Luszczek; Stanimire Tomov,Abstract Power consumption and heat dissipation issues are pushing the microprocessorsindustry towards multicore design patterns. Given the cubic dependence between corefrequency and power consumption; multicore technologies leverage the idea that doublingthe number of cores and halving the cores frequency gives roughly the same performancereducing the power consumption by a factor of four. With the number of cores on multicorechips expected to reach tens in a few years; efficient implementations of numerical librariesusing shared memory programming models is of high interest. The current message passingparadigm used in ScaLAPACK and elsewhere introduces unnecessary memory overheadand memory copy operations; which degrade performance; along with the making it harderto schedule operations that could be done in parallel. Limiting the use of shared memory …,International Workshop on Applied Parallel Computing,2006,143
Exploiting the performance of 32 bit floating point arithmetic in obtaining 64 bit accuracy (revisiting iterative refinement for linear systems),Julie Langou; Piotr Luszczek; Jakub Kurzak; Alfredo Buttari; Jack Dongarra,Recent versions of microprocessors exhibit performance characteristics for 32 bit floatingpoint arithmetic (single precision) that is substantially higher than 64 bit floating pointarithmetic (double precision). Examples include the Intel's Pentium IV and M processors;AMD's Opteron architectures and the IBM's Cell Broad Engine processor. When working insingle precision; floating point operations can be performed up to two times faster on thePentium and up to ten times faster on the Cell over double precision. The performanceenhancements in these architectures are derived by accessing extensions to the basicarchitecture; such as SSE2 in the case of the Pentium and the vector functions on the IBMCell. The motivation for this paper is to exploit single precision operations wheneverpossible and resort to double precision at critical stages while attempting to provide the …,SC 2006 Conference; Proceedings of the ACM/IEEE,2006,142
Accelerating scientific computations with mixed precision algorithms,Marc Baboulin; Alfredo Buttari; Jack Dongarra; Jakub Kurzak; Julie Langou; Julien Langou; Piotr Luszczek; Stanimire Tomov,Abstract On modern architectures; the performance of 32-bit operations is often at least twiceas fast as the performance of 64-bit operations. By using a combination of 32-bit and 64-bitfloating point arithmetic; the performance of many dense and sparse linear algebraalgorithms can be significantly enhanced while maintaining the 64-bit accuracy of theresulting solution. The approach presented here can apply not only to conventionalprocessors but also to other technologies such as Field Programmable Gate Arrays (FPGA);Graphical Processing Units (GPU); and the STI Cell BE processor. Results on modernprocessor architectures and the STI Cell BE are presented. Program summary Program title:ITER-REF Catalogue identifier: AECO_v1_0 Program summary URL: http://cpc. cs. qub. ac.uk/summaries/AECO_v1_0. html Program obtainable from: CPC Program Library …,Computer Physics Communications,2009,120
Flexible development of dense linear algebra algorithms on massively parallel architectures with DPLASMA,George Bosilca; Aurelien Bouteiller; Anthony Danalis; Mathieu Faverge; Azzam Haidar; Thomas Herault; Jakub Kurzak; Julien Langou; Pierre Lemarinier; Hatem Ltaief; Piotr Luszczek; Asim YarKhan; Jack Dongarra,We present a method for developing dense linear algebra algorithms that seamlessly scalesto thousands of cores. It can be done with our project called DPLASMA (DistributedPLASMA) that uses a novel generic distributed Direct Acyclic Graph Engine (DAGuE). Theengine has been designed for high performance computing and thus it enables scaling oftile algorithms; originating in PLASMA; on large distributed memory systems. The underlyingDAGuE framework has many appealing features when considering distributed-memoryplatforms with heterogeneous multicore nodes: DAG representation that is independent ofthe problem-size; automatic extraction of the communication from the dependencies;overlapping of communication and computation; task prioritization; and architecture-awarescheduling and management of tasks. The originality of this engine lies in its capacity to …,Parallel and Distributed Processing Workshops and Phd Forum (IPDPSW); 2011 IEEE International Symposium on,2011,114
Algorithm 842: A set of GMRES routines for real and complex arithmetics on high performance computers,Valérie Frayssé; Luc Giraud; Serge Gratton; Julien Langou,Abstract In this article we describe our implementations of the GMRES algorithm for both realand complex; single and double precision arithmetics suitable for serial; shared memory anddistributed memory computers. For the sake of portability; simplicity; flexibility and efficiencythe GMRES solvers have been implemented in Fortran 77 using the reverse communicationmechanism for the matrix-vector product; the preconditioning and the dot productcomputations. For distributed memory computation; several orthogonalization procedureshave been implemented to reduce the cost of the dot product calculation; which is a well-known bottleneck of efficiency for the Krylov methods. Either implicit or explicit calculation ofthe residual at restart are possible depending on the actual cost of the matrix-vector product.Finally the implemented stopping criterion is based on a normwise backward error.,ACM Transactions on Mathematical Software (TOMS),2005,112
Fault tolerant high performance computing by a coding approach,Zizhong Chen; Graham E Fagg; Edgar Gabriel; Julien Langou; Thara Angskun; George Bosilca; Jack Dongarra,Abstract As the number of processors in today's high performance computers continues togrow; the mean-time-to-failure of these computers are becoming significantly shorter thanthe execution time of many current high performance computing applications. Althoughtoday's architectures are usually robust enough to survive node failures without sufferingcomplete system failure; most today's high performance computing applications can notsurvive node failures and; therefore; whenever a node fails; have to abort themselves andrestart from the beginning or a stable-storage-based checkpoint. This paper explores theuse of the floating-point arithmetic coding approach to build fault survivable highperformance computing applications so that they can adapt to node failures without abortingthemselves. Despite the use of erasure codes over Galois field has been theoretically …,Proceedings of the tenth ACM SIGPLAN symposium on Principles and practice of parallel programming,2005,110
The loss of orthogonality in the Gram-Schmidt orthogonalization process,Luc Giraud; Julien Langou; Miroslav Rozloznik,Abstract In this paper; we study numerical behavior of several computational variants of theGram-Schmidt orthogonalization process. We focus on the orthogonality of computedvectors which may be significantly lost in the classical or modified Gram-Schmidt algorithm;while the Gram-Schmidt algorithm with reorthogonalization has been shown to computevectors which are orthogonal to machine precision level. The implications for practicalimplementation and its impact on the efficiency in the parallel computer environment areconsidered.,Computers & Mathematics with Applications,2005,106
Mixed precision iterative refinement techniques for the solution of dense linear systems,Alfredo Buttari; Jack Dongarra; Julie Langou; Julien Langou; Piotr Luszczek; Jakub Kurzak,By using a combination of 32-bit and 64-bit floating point arithmetic; the performance ofmany dense and sparse linear algebra algorithms can be significantly enhanced whilemaintaining the 64-bit accuracy of the resulting solution. The approach presented here canapply not only to conventional processors but also to exotic technologies such as FieldProgrammable Gate Arrays (FPGA); Graphical Processing Units (GPU); and the Cell BEprocessor. Results on modern processor architectures and the Cell BE are presented.,The International Journal of High Performance Computing Applications,2007,100
Rounding error analysis of the classical Gram-Schmidt orthogonalization process,Luc Giraud; Julien Langou; Miroslav Rozložník; Jasper van den Eshof,Abstract This paper provides two results on the numerical behavior of the classical Gram-Schmidt algorithm. The first result states that; provided the normal equations associated withthe initial vectors are numerically nonsingular; the loss of orthogonality of the vectorscomputed by the classical Gram-Schmidt algorithm depends quadratically on the conditionnumber of the initial vectors. The second result states that; provided the initial set of vectorshas numerical full rank; two iterations of the classical Gram-Schmidt algorithm are enoughfor ensuring the orthogonality of the computed vectors to be close to the unit roundoff level.,Numerische Mathematik,2005,91
Handbook of parallel computing: models; algorithms and applications,Sanguthevar Rajasekaran; John Reif,The ability of parallel computing to process large data sets and handle time-consumingoperations has resulted in unprecedented advances in biological and scientific computing;modeling; and simulations. Exploring these recent developments; the Handbook of ParallelComputing: Models; Algorithms; and Applications provides comprehensive coverage on allaspects of this field. The first section of the book describes parallel models. It covers evolvingcomputational systems; the decomposable bulk synchronous model; parallel random accessmachine-on-chip architecture; the parallel disks model; mobile agents; fault-tolerantcomputing; hierarchical performance modeling; the partitioned optical passive star network;and the reconfigurable mesh model. The subsequent section on parallel algorithmsexamines networks of workstations; grid and packet scheduling; the derandomization …,*,2007,78
LU factorization for accelerator-based systems,Emmanuel Agullo; Cédric Augonnet; Jack Dongarra; Mathieu Faverge; Julien Langou; Hatem Ltaief; Stanimire Tomov,Multicore architectures enhanced with multiple GPUs are likely to become mainstream HighPerformance Computing (HPC) platforms in a near future. In this paper; we present thedesign and implementation of an LU factorization using tile algorithm that can fully exploitthe potential of such platforms in spite of their complexity. We use a methodology derivedfrom previous work on Cholesky and QR factorizations. Our contributions essentially consistof providing new CPU/GPU hybrid LU kernels; studying the impact on performance of thelooking variants as well as the storage layout in presence of pivoting; tuning the kernels fortwo different machines composed of multiple recent NVIDIA Tesla S1070 (four GPUs total)and Fermi-based S2050 GPUs (three GPUs total); respectively. The hybrid tile LUasymptotically achieves 1 Tflop/s in single precision on both hardwares. The performance …,Computer Systems and Applications (AICCSA); 2011 9th IEEE/ACS International Conference on,2011,73
A set of GMRES routines for real and complex arithmetics,Valérie Frayssé; Luc Giraud; Serge Gratton; J Langou,Abstract In this report we describe the implementations of the GMRES algorithm for both realand complex; single and double precision arithmetics suitable for serial; shared memory anddistributed memory computers. For the sake of simplicity; flexibility and efficiency theGMRES solvers have been implemented using the reverse communication mechanism forthe matrixvector product; the preconditioning and the dot product computations. Fordistributed memory computation several orthogonalization procedures have beenimplemented to reduce the cost of the dot product calculation; that is a well-knownbottleneck of efficiency for the Krylov methods. Finally the implemented stopping criterion isbased on a normwise backward error. After a short presentation of the GMRES methods andof the solution of the least-squares problems in real and complex arithmetic; we give a …,Tech. Rep. TR/PA/97/49; CERFACS; France,1997,57
Plasma users guide,Emmanuel Agullo; Jack Dongarra; Bilel Hadri; Jakub Kurzak; Julie Langou; Julien Langou; Hatem Ltaief; Piotr Luszczek; Asim YarKhan,PLASMA version 1.0 was released in November 2008 as a prototype software providingproof-of-concept implementation of a linear equations solver based on LU factorization; SPDlinear equations solver based on Cholesky factorization and least squares problem solverbased on QR and LQ factorizations; with support for real arithmetic in double precision only.The publication of this users's guide coincides with the release of version 2.0 of the PLASMAsoftware; which extends the functionality and robustness of the software by introducing thefollowing features:,*,2009,48
Self-adapting numerical software (SANS) effort,Jack Dongarra; George Bosilca; Zizhong Chen; Victor Eijkhout; Graham E Fagg; Erika Fuentes; Julien Langou; Piotr Luszczek; Jelena Pjesivac-Grbovic; Keith Seymour; Haihang You; Sathish S Vadhiyar,The challenge for the development of next-generation software is the successfulmanagement of the complex computational environment while delivering to the scientist thefull power of flexible compositions of the available algorithmic alternatives. Self-adaptingnumerical software (SANS) systems are intended to meet this significant challenge. Theprocess of arriving at an efficient numerical solution of problems in computational scienceinvolves numerous decisions by a numerical expert. Attempts to automate such decisionsdistinguish three levels: algorithmic decision; management of the parallel environment; andprocessor-specific tuning of kernels. Additionally; at any of these levels we can decide torearrange the user's data. In this paper we look at a number of efforts at the University ofTennessee to investigate these areas.,IBM Journal of Research and Development,2006,48
Hierarchical QR factorization algorithms for multi-core clusters,Jack Dongarra; Mathieu Faverge; Thomas Herault; Mathias Jacquelin; Julien Langou; Yves Robert,Abstract This paper describes a new QR factorization algorithm which is especially designedfor massively parallel platforms combining parallel distributed nodes; where a node is amulti-core processor. These platforms represent the present and the foreseeable future ofhigh-performance computing. Our new QR factorization algorithm falls in the category of thetile algorithms which naturally enables good data locality for the sequential kernels executedby the cores (high sequential performance); low number of messages in a paralleldistributed setting (small latency term); and fine granularity (high parallelism). Each tilealgorithm is uniquely characterized by its sequence of reduction trees. In the context of acluster of nodes; in order to minimize the number of inter-processor communications(aka;“communication-avoiding”); it is natural to consider hierarchical trees composed of …,Parallel Computing,2013,44
Performance optimization and modeling of blocked sparse kernels,Alfredo Buttari; Victor Eijkhout; Julien Langou; Salvatore Filippone,We present a method for automatically selecting optimal implementations of sparse matrix-vector operations. Our software “AcCELS”(Accelerated Compress-storage Elements forLinear Solvers) involves a setup phase that probes machine characteristics; and a run-timephase where stored characteristics are combined with a measure of the actual sparse matrixto find the optimal kernel implementation. We present a performance model that is shown tobe accurate over a large range of matrices.,The International Journal of High Performance Computing Applications,2007,42
Recovery patterns for iterative methods in a parallel unstable environment,Julien Langou; Zizhong Chen; George Bosilca; Jack Dongarra,Several recovery techniques for parallel iterative methods are presented. First; theimplementation of checkpoints in parallel iterative methods is described and analyzed. Thena simple checkpoint-free fault-tolerant scheme for parallel iterative methods; the lossyapproach; is presented. When one processor fails and all its data is lost; the system isrecovered by computing a new approximate solution using the data of the nonfailedprocessors. The iterative method is then restarted with this new vector. The main advantageof the lossy approach over standard checkpoint algorithms is that it does not increase thecomputational cost of the iterative solver when no failure occurs. Experiments are presentedthat compare the different techniques. The fault-tolerant FT-MPI library is used. Both iterativelinear solvers and eigensolvers are considered.,SIAM Journal on Scientific Computing,2007,40
A note on the error analysis of classical Gram–Schmidt,Alicja Smoktunowicz; Jesse L Barlow; Julien Langou,Abstract An error analysis result is given for classical Gram–Schmidt factorization of a fullrank matrix A into A= QR where Q is left orthogonal (has orthonormal columns) and R isupper triangular. The work presented here shows that the computed R satisfies RTR= ATA+E where E is an appropriately small backward error; but only if the diagonals of R arecomputed in a manner similar to Cholesky factorization of the normal equations matrix. Atthe end of the article; implications for classical Gram–Schmidt with reorthogonalization arenoted. A similar result is stated in Giraud et al.(Numer Math 101 (1): 87–100; 2005).However; for that result to hold; the diagonals of R must be computed in the mannerrecommended in this work.,Numerische Mathematik,2006,38
Iterative methods for solving linear systems with multiple right-hand sides,Julien Langou,*,*,2003,38
When modified Gram–Schmidt generates a well‐conditioned set of vectors,Luc Giraud; Julien Langou,In this paper; we show why the modified Gram–Schmidt algorithm generates a well‐conditioned set of vectors. This result holds under the assumption that the initial matrix is not'too ill‐conditioned'in a way that is quantified. As a consequence we show that if twoiterations of the algorithm are performed; the resulting algorithm produces a matrix whosecolumns are orthogonal up to machine precision. Finally; we illustrate through a numericalexperiment the sharpness of our result.,IMA Journal of Numerical Analysis,2002,37
Prospectus for the next LAPACK and ScaLAPACK libraries,James W Demmel; Jack Dongarra; Beresford Parlett; William Kahan; Ming Gu; David Bindel; Yozo Hida; Xiaoye Li; Osni Marques; E Jason Riedy; Christof Voemel; Julien Langou; Piotr Luszczek; Jakub Kurzak; Alfredo Buttari; Julie Langou; Stanimire Tomov,Abstract New releases of the widely used LAPACK and ScaLAPACK numerical linearalgebra libraries are planned. Based on an on-going user survey (www. netlib. org/lapack-dev) and research by many people; we are proposing the following improvements: Fasteralgorithms; including better numerical methods; memory hierarchy optimizations;parallelism; and automatic performance tuning to accommodate new architectures; Moreaccurate algorithms; including better numerical methods; and use of extra precision;Expanded functionality; including updating and downdating; new eigenproblems; etc. andputting more of LAPACK into ScaLAPACK; Improved ease of use; eg; via friendlier interfacesin multiple languages. To accomplish these goals we are also relying on better softwareengineering techniques and contributions from collaborators at many institutions.,International Workshop on Applied Parallel Computing,2006,31
Communication-avoiding parallel and sequential QR factorizations,James Demmel; Laura Grigori; Mark Hoemmen; Julien Langou,Abstract We present parallel and sequential dense QR factorization algorithms that areoptimized to avoid communication. Some of these are novel; and some extend earlier work.Communication includes both messages between processors (in the parallel case); anddata movement between slow and fast memory (in either the sequential or parallel cases).Our first algorithm; Tall Skinny QR (TSQR); factors m× n matrices in a one-dimensional (1-D)block cyclic row layout; storing the Q factor (if desired) implictly as a tree of blocks ofHouseholder reflectors. TSQR is optimized for matrices with many more rows than columns(hence the name). In the parallel case; TSQR requires no more than the minimum number ofmessages Θ (log P) between P processors. In the sequential case; TSQR transfers 2mn+ o(mn) words between slow and fast memory; which is the theoretical lower bound; and …,CoRR abs/0806.2159,2008,30
Evolving Computational Systems.,Selim G Akl,Page 1. Page 2. Evolving Computational Systems Selim G. Akl School of Computing Queen'sUniversity Kingston; Ontario; Canada Page 3. WHAT IS THE MOST IMPORTANT IDEA INCOMPUTER SCIENCE? SG Akl; Evolving Computational Systems; Queen's University; February2007 1 Page 4. SIMULATION SG Akl; Evolving Computational Systems; Queen's University;February 2007 2 Page 5. ANY ALGORITHM + ANY PROGRAMMING LANGUAGE + ANYCOMPUTER SG Akl; Evolving Computational Systems; Queen's University; February 20073 Page 6. Simulation is the main reason behind the success of the computer as the most inuential invention of the 20th century. SG Akl; Evolving Computational Systems; Queen'sUniversity; February 2007 4 Page 7. SG Akl; Evolving Computational Systems; Queen'sUniversity; February 2007 5 Page 8. SIMULATION + UNIVERSALITY …,*,2007,28
Flexible variants of block restarted GMRES methods with application to geophysics,Henri Calandra; Serge Gratton; Julien Langou; Xavier Pinel; Xavier Vasseur,In a wide number of applications in computational science and engineering the solution oflarge linear systems of equations with several right-hand sides given at once is required.Direct methods based on Gaussian elimination are known to be especially appealing in thatsetting. Nevertheless; when the dimension of the problem is very large; preconditioned blockKrylov space solvers are often considered as the method of choice. The purpose of thispaper is thus to present iterative methods based on block restarted GMRES that allowvariable preconditioning for the solution of linear systems with multiple right-hand sides. Theuse of flexible methods is especially of interest when approximate possibly iterative solversare considered in the preconditioning phase. First we introduce a new variant of blockflexible restarted GMRES that includes a strategy for detecting when a linear combination …,SIAM Journal on Scientific Computing,2012,27
Convergence in backward error of relaxed GMRES,Luc Giraud; Serge Gratton; Julien Langou,This work is the follow-up of the experimental study presented in [A. Bouras and V. Frayssé;SIAM J. Matrix Anal. Appl.; 26 (2005); pp. 660–678]. It is based on and extends sometheoretical results in [V. Simoncini and DB Szyld; SIAM J. Sci. Comput.; 25 (2003); pp. 454–477; J. van den Eshof and GLG Sleijpen; SIAM J. Matrix Anal. Appl.; 26 (2004); pp. 125–153]. In a backward error framework we study the convergence of GMRES when the matrix-vector products are performed inaccurately. This inaccuracy is modeled by a perturbation ofthe original matrix. We prove the convergence of GMRES when the perturbation size isproportional to the inverse of the computed residual norm; this implies that the accuracy canbe relaxed as the method proceeds which gives rise to the terminology “relaxed GMRES.”As for the exact GMRES we show under proper assumptions that only happy breakdowns …,SIAM Journal on Scientific Computing,2007,27
Using spectral low rank preconditioners for large electromagnetic calculations,Iain S Duff; Luc Giraud; Julien Langou; Emeric Martin,Abstract For solving large dense complex linear systems that arise in electromagneticcalculations; we perform experiments using a general purpose spectral low rank updatepreconditioner in the context of the GMRES method preconditioned by an approximateinverse preconditioner. The goal of the spectral preconditioner is to improve theconvergence properties by shifting by one the smallest eigenvalues of the originalpreconditioned system. Numerical experiments on parallel distributed memory computersare presented to illustrate the efficiency of this technique on large and challenging real-lifeindustrial problems. Copyright© 2004 John Wiley & Sons; Ltd.,International Journal for Numerical Methods in Engineering,2005,27
Multithreading in the PLASMA Library,Jakub Kurzak; Piotr Luszczek; Asim YarKhan; Mathieu Faverge; Julien Langou; Henricus Bouwmeester; Jack Dongarra; JJ Dongarra; M Faverge; T Herault,Chapter 5 Multithreading in the PLASMA Library Jakub Kurzak University of Tennessee; KnoxvillePiotr Luszczek University of Tennessee; Knoxville Asim YarKhan University of Tennessee; KnoxvilleMathieu Faverge University of Tennessee; Knoxville Julien Langou University of Colorado; DenverHenricus Bouwmeester University of Colorado; Denver Jack Dongarra University ofTennessee; Knoxville Oak Ridge National Laboratory University of Manchester 5.1Introduction............................................................... 5.1. 1 PLASMA Design Principles....................................... 5.1. 2 PLASMA Software Stack … 120 Multicore Computing:Algorithms; Architectures; and Applications 5.1 Introduction Parallel Linear Algebra Softwarefor Multicore Architectures (PLASMA) is a numerical software library for solving problems in denselinear algebra on systems of multicore processors and multisocket systems of multicore …,Multicore Computing: Algorithms; Architectures; and Applications,2013,26
Towards an efficient tile matrix inversion of symmetric positive definite matrices on multicore architectures,Emmanuel Agullo; Henricus Bouwmeester; Jack Dongarra; Jakub Kurzak; Julien Langou; Lee Rosenberg,Abstract The algorithms in the current sequential numerical linear algebra libraries (egLAPACK) do not parallelize well on multicore architectures. A new family of algorithms; thetile algorithms; has recently been introduced. Previous research has shown that it is possibleto write efficient and scalable tile algorithms for performing a Cholesky factorization; a(pseudo) LU factorization; a QR factorization; and computing the inverse of a symmetricpositive definite matrix. In this extended abstract; we revisit the computation of the inverse ofa symmetric positive definite matrix. We observe that; using a dynamic task scheduler; it isrelatively painless to translate existing LAPACK code to obtain a ready-to-be-executed tilealgorithm. However we demonstrate that; for some variants; non trivial compiler techniques(array renaming; loop reversal and pipelining) need then to be applied to further increase …,International Conference on High Performance Computing for Computational Science,2010,26
Building fault survivable MPI programs with FT-MPI using diskless checkpointing,Zizhong Chen; Graham E Fagg; Edgar Gabriel; Julien Langou; Thara Angskun; George Bosilca; Jack Dongarra,ABSTRACT As the number of processors in today's high performance computers continuesto grow; the mean-time-to-failure of these computers are becoming significantly shorter thanthe execution time of many current high performance computing applications. Althoughtoday's architectures are usually robust enough to survive node failures without sufferingcomplete system failure; most today's high performance computing applications can notsurvive node failures and; therefore; whenever there is a node failure; have to abortthemselves and restart from the beginning or a stable-storage-based checkpoint. In thispaper; we present how to build fault survivable high performance computing applicationswith FT-MPI; a fault tolerant version of MPI we developed; using diskless checkpointing sothat these applications can survive node failures without aborting themselves. We …,Proceedings for ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming,2005,25
Parallel tools for solving incremental dense least squares problems: application to space geodesy,Arc Baboulin; Luc Giraud; Serge Gratton; Julien Langou,We present a parallel distributed solver that enables us to solve incremental dense leastsquares arising in some parameter estimation problems. This solver is based onScaLAPACK [8] and PBLAS [9] kernel routines. In the incremental process; the observationsare collected periodically and the solver updates the solution with new observations using aQR factorization algorithm. It uses a recently defined distributed packed format [3] thathandles symmetric or triangular matrices in ScaLAPACK-based implementations. Weprovide performance analysis on IBM pSeries 690. We also present an example ofapplication in the area of space geodesy for gravity field computations with someexperimental results.,Journal of Algorithms & Computational Technology,2009,23
Computing the conditioning of the components of a linear least‐squares solution,Marc Baboulin; Jack Dongarra; Serge Gratton; Julien Langou,Abstract In this paper; we address the accuracy of the results for the overdetermined full ranklinear least-squares problem. We recall theoretical results obtained in (SIAM J. Matrix Anal.Appl. 2007; 29 (2): 413–433) on conditioning of the least-squares solution and thecomponents of the solution when the matrix perturbations are measured in Frobenius orspectral norms. Then we define computable estimates for these condition numbers and weinterpret them in terms of statistical quantities when the regression matrix and the right-handside are perturbed. In particular; we show that in the classical linear statistical model; theratio of the variance of one component of the solution by the variance of the right-hand sideis exactly the condition number of this solution component when only perturbations on theright-hand side are considered. We explain how to compute the variance–covariance …,Numerical Linear Algebra with Applications,2009,22
On the round-off error analysis of the Gram-Schmidt algorithm with reorthogonalization,Luc Giraud; Julien Langou; Miroslav Rozloznık,Abstract. In this paper we analyse the numerical behavior of the Gram-Schmidtorthogonalization process with reorthogonalization. Assuming numerical nonsingularity ofthe matrix we prove that two steps of iterative Gram-Schmidt process are enough forpreserving the orthogonality of computed vectors close to the machine precision level. Wegive a rounding error analysis of classical reorthogonalization and modified Gram-Schmidtalgorithm with (exactly one) reorthogonalization and relate our results to the approach usedin the Kahan-Parlett “twice is enough” algorithm as well as to results shown by Abdelmalek;Daniel et al; Hoffmann and others.,*,2002,22
Any admissible cycle‐convergence behavior is possible for restarted GMRES at its initial cycles,Eugene Vecharynski; Julien Langou,Abstract We show that any admissible cycle-convergence behavior is possible for restartedGMRES at a number of initial cycles; moreover the spectrum of the coefficient matrix alonedoes not determine this cycle-convergence. The latter can be viewed as an extension of theresult of Greenbaum; Pták and Strakoš (SIAM Journal on Matrix Analysis and Applications1996; 17 (3): 465–469) to the case of restarted GMRES. Copyright© 2010 John Wiley &Sons; Ltd.,Numerical Linear Algebra with Applications,2011,17
A Robust Criterion for the Modified Gram--Schmidt Algorithm with Selective Reorthogonalization,Luc Giraud; Julien Langou,A new criterion for selective reorthogonalization in the modified Gram--Schmidt algorithm isproposed. We study its behavior in the presence of rounding errors. We give somecounterexample matrices which prove that the standard criteria might fail. Throughnumerical experiments; we illustrate that our new criterion seems to be suitable also for theclassical Gram--Schmidt algorithm with selective reorthogonalization.,SIAM Journal on Scientific Computing,2003,17
Rectangular full packed format for cholesky's algorithm: factorization; solution; and inversion,Fred G Gustavson; Jerzy Waśniewski; Jack J Dongarra; Julien Langou,Abstract We describe a new data format for storing triangular; symmetric; and Hermitianmatrices called Rectangular Full Packed Format (RFPF). The standard two-dimensionalarrays of Fortran and C (also known as full format) that are used to represent triangular andsymmetric matrices waste nearly half of the storage space but provide high performance viathe use of Level 3 BLAS. Standard packed format arrays fully utilize storage (array space)but provide low performance as there is no Level 3 packed BLAS. We combine the goodfeatures of packed and full storage using RFPF to obtain high performance via using Level 3BLAS as RFPF is a standard full-format representation. Also; RFPF requires exactly thesame minimal storage as packed the format. Each LAPACK full and/or packed triangular;symmetric; and Hermitian routine becomes a single new RFPF routine based on eight …,ACM Transactions on Mathematical Software (TOMS),2010,15
On the parallel solution of large industrial wave propagation problems,Luc Giraud; Julien Langou; G Sylvand,The use of Fast Multipole Methods (FMM) combined with embedded Krylov solverspreconditioned by a sparse approximate inverse is investigated for the solution of largelinear systems arising in industrial acoustic and electromagnetic simulations. We use aboundary element integral equation method to solve the Helmholtz and the Maxwellequations in the frequency domain. The resulting linear systems are solved by iterativesolvers using FMM to accelerate the matrix-vector products. The simulation code isdeveloped in a distributed memory environment using message passing and it has out-of-core capabilities to handle very large calculations. When the calculation involves oneincident wave; one linear system has to be solved. In this situation; embedded solvers canbe combined with an approximate inverse preconditioner to design extremely robust …,Journal of Computational Acoustics,2006,15
Exploiting Mixed Precision Floating Point Hardware in Scientific Computations.,Alfredo Buttari; Jack Dongarra; Jakub Kurzak; Julie Langou; Julien Langou; Piotr Luszczek; Stanimire Tomov,Abstract. By using a combination of 32-bit and 64-bit floating point arithmetic; theperformance of many dense and sparse linear algebra algorithms can be significantlyenhanced while maintaining the 64-bit accuracy of the resulting solution. The approachpresented here can apply not only to conventional processors but also to exotictechnologies such as Field Programmable Gate Arrays (FPGA); Graphical Processing Units(GPU); and the Cell BE processor. Results on modern processor architectures and the CellBE are presented.,High Performance Computing Workshop,2006,15
Solving large linear systems with multiple right-hand sides,Julien Langou,The starting point of this thesis is a problem posed by the electromagnetism group at EADS-CCR: How to solve several linear systems with the same coefficient matrix but various right-hand sides? For the targeted application; the matrices are complex; dense and huge (oforder of a few millions). Because such matrices cannot be computed nor stored in numericalsimulations involved in a design process; the use of an iterative scheme with anapproximate matrix-vector product is the only alternative. The matrix-vector product isperformed using the Fast Multipole Method. In this context; the goal of this thesis is to adaptKrylov solvers so that they handle efficiently multiple right-hand sides. Some preliminaryworks dealing with one right hand side show that GMRES was an efficient and robust solverfor that application. Consequently; we mainly focus; in this thesis; on variants of GMRES …,*,2003,14
QCG-OMPI: MPI applications on grids,Emmanuel Agullo; Camille Coti; Thomas Herault; Julien Langou; Sylvain Peyronnet; Ala Rezmerita; Franck Cappello; Jack Dongarra,Abstract Computational grids present promising computational and storage capacities. Theycan be made by punctual aggregation of smaller resources (ie; clusters) to obtain a large-scale supercomputer. Running general applications is challenging for several reasons. Thefirst one is inter-process communication: processes running on different clusters must beable to communicate with one another in spite of security equipments such as firewalls andNATs. Another problem raised by grids for communication-intensive parallel application iscaused by the heterogeneity of the available networks that interconnect processes with oneanother. In this paper we present how QCG-OMPI can execute efficient parallel applicationson computational grids. We first present an MPI programming; communication and executionmiddleware called QCG-OMPI. We then present how applications can make use of the …,Future Generation Computer Systems,2011,13
Communication-avoiding parallel and sequential QR and LU factorizations,James Demmel; Laura Grigori; Mark Hoemmen; Julien Langou,Abstract We present parallel and sequential dense QR factorization algorithms that areoptimized to avoid communication. Some of these are novel; and some extend earlier work.Communication includes both messages between processors (in the parallel case); anddata movement between slow and fast memory (in either the sequential or parallel cases).Our first algorithm; Tall Skinny QR (TSQR); factors m× n matrices in a one-dimensional (1-D)block cyclic row layout; storing the Q factor (if desired) implicitly as a tree of blocks ofHouseholder reflectors. TSQR is optimized for matrices with many more rows than columns(hence the name). In the parallel case; TSQR requires no more than the minimum number ofmessages Θ (log P) between P processors. In the sequential case; TSQR transfers 2mn+ o(mn) words between slow and fast memory; which is the theoretical lower bound; and …,SIAM Journal of Scientific Computing,2008,13
Stability analysis of QR factorization in an oblique inner product,Bradley R Lowery; Julien Langou,Abstract: In this paper we consider the stability of the QR factorization in an oblique innerproduct. The oblique inner product is defined by a symmetric positive definite matrix A. Weanalyze two algorithm that are based a factorization of A and converting the problem to theEuclidean case. The two algorithms we consider use the Cholesky decomposition and theeigenvalue decomposition. We also analyze algorithms that are based on computing theCholesky factor of the normal equa-tion. We present numerical experiments to show theerror bounds are tight. Finally we present performance results for these algorithms as well asGram-Schmidt methods on parallel architecture. The performance experiments demonstratethe benefit of the communication avoiding algorithms.,arXiv preprint arXiv:1401.5171,2014,12
Conjugate-gradient eigenvalue solvers in computing electronic properties of nanostructure architectures,Stanimire Tomo; Julien Langou; Jack Dongarra; Andrew Canning; Lin-Wang Wang,In this paper we report on our efforts to test and expand the current state-of-the-art ineigenvalue solvers applied to the field of nanotechnology. We singled out the non-linearConjugate Gradients (CG) methods as the backbone of our efforts for their previous successin predicting the electronic properties of large nanostructures and made a library of threedifferent solvers (two recent and one new) that we integrated into the Parallel Energy SCAN(PESCAN) code to perform a comparison. The methods and their implementation are tunedto the specifics of the physics problem. The main requirements are to be able to find (1) afew; approximately 4-10; of the (2) interior eigenstates; including (3) repeated eigenvalues;for (4) large Hermitian matrices.,International Journal of Computational Science and Engineering,2006,12
Recovery patterns for iterative methods in a parallel unstable environment,George Bosilca; Zizhong Chen; Jack Dongarra; Julien Langou,Abstract A simple checkpoint-free fault-tolerant scheme for parallel iterative methods isgiven. Assuming that when one processor fails; all its data is lost and the system isrecovered with a new processor; this scheme computes a new approximate solution from thedata of the non-failed system. The iterative method is then restarted from this new vector.The main advantage of this technique over standard checkpoint is that there is no extracomputation added in the iterative solver. In particular; if no failure occurs; the fault-tolerantapplication is the same as the original application. The main drawback is that theconvergence after failure of the method is no longer the same as the original method. In thispaper; we present this recovery technique as well as some implementations of checkpointsin iterative methods. Finally; experiments are presented to compare the two techniques …,*,2004,12
Predicting the electronic properties of 3D; million-atom semiconductor nanostructure architectures,A Zunger; A Franceschetti; G Bester; WB Jones; Kwiseon Kim; PA Graf; LW Wang; A Canning; O Marques; C Voemel; J Dongarra; J Langou; S Tomov,Abstract The past~ 10 years have witnessed revolutionary breakthroughs both in synthesisof quantum dots (leading to nearly monodispersed; defect-free nanostructures) and incharacterization of such systems; revealing ultra narrow spectroscopic lines of< 1 meVwidth; exposing new intriguing effects; such as multiple exciton generation; fine-structuresplitting; quantum entanglement; multiexciton recombination and more. These discoverieshave led to new technological applications including quantum computing and ultra-highefficiency solar cells. Our work in this project is based on two realizations/observations: First;that the dots exhibiting clean and rich spectroscopic and transport characteristics are ratherbig. Indeed; the phenomenology indicated above is exhibited only by the well-passivateddefect-free quantum dots containing at least a few thousand atoms (colloidal) and even a …,Journal of Physics: Conference Series,2006,11
The cycle-convergence of restarted GMRES for normal matrices is sublinear,Eugene Vecharynski; Julien Langou,Page 1. Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. SIAM J.SCI. COMPUT. c 2010 Society for Industrial and Applied Mathematics Vol. 32; No. 1; pp.186–196 THE CYCLE-CONVERGENCE OF RESTARTED GMRES FOR NORMAL MATRICESIS SUBLINEAR ∗ E. VECHARYNSKI† AND J. LANGOU† Abstract. We prove that thecycle-convergence of the restarted GMRES applied to a system of linear equations with a normalcoefficient matrix is sublinear. Key words. restarted generalized minimal residual method;convergence; normal; diagonaliz- able; sublinear; superlinear AMS subject classification. 65F10DOI. 10.1137/080727403 1. Introduction. The generalized minimal residual method (GMRES)was orig- inally introduced by Saad and Schultz [13] in 1986; and has become a popular methodfor solving non-Hermitian systems of linear equations; (1.1) Ax = b; A ∈ Cn×n ; b ∈ Cn …,SIAM Journal on Scientific Computing,2010,10
The problem with the linpack benchmark 1.0 matrix generator,Jack J Dongarra; Julien Langou,The International Journal of High Performance Computing Applications; Volume 23; No. 1; Spring2009; pp. 5–13 DOI: 10.1177/1094342008098683 © 2009 SAGE Publications Los Angeles;London; New Delhi and Singapore Figures 1; 4–15 appear in color online: http://hpc.sagepub.com … THE PROBLEM WITH THE LINPACK BENCHMARK 1.0 MATRIX GENERATOR … AbstractWe characterize the matrix sizes for which the Linpack Benchmark 1.0 matrix generator constructsa matrix with identical columns … Key words: Linpack benchmark; pseudo-random numbergenerator; random matrix; HPL; TOP500 … 1 Introduction Since 1993; twice a year; a list of thesites operating the 500 most powerful computer systems has been released by the TOP500 project(http://www.top500.org/). A sin- gle number is used to rank computer systems based on the resultsobtained on the High Performance Linpack Bench- mark (HPL Benchmark). The HPL …,The International Journal of High Performance Computing Applications,2009,10
All Reduce Algorithms: Application to Householder QR Factorization,Julien Langou,QR factorizations of tall and skinny matrices with their data partitioned vertically acrossseveral processors arise in a wide range of applications. For example; in iterativeeigensolvers (see the PRIMME; BLANCZOS; BLOPEX; ANASAZI software) or in iterativemethods with multiple right-hand sides (see the BELOS software) or in s-step GMRESmethod for a single right-hand side. Various methods exist to perform the QR factorization ofsuch matrices: Gram-Schmidt; Householder; or CholeskyQR. In this talk; we present theAllreduce QR factorization. The idea of this method is to avoid the lattency bottleneckinherently present in the Gram-Schmidt and Householder QR factorization. When applied forthe Householder QR factorization; this method is provably stable and performs; in ourexperiments; from four to eight times faster than ScaLAPACK routines on tall and skinny …,Proceedings of the 2007 International Conference on Preconditioning Techniques for Large Sparse Matrix Problems in Scientific and Industrial Applicatipns,2007,10
A critical path approach to analyzing parallelism of algorithmic variants. Application to Cholesky inversion,Henricus Bouwmeester; Julien Langou,Abstract: Algorithms come with multiple variants which are obtained by changing themathematical approach from which the algorithm is derived. These variants offer a widespectrum of performance when implemented on a multicore platform and we seek tounderstand these differences in performances from a theoretical point of view. To that aim;we derive and present the critical path lengths of each algorithmic variant for our applicationproblem which enables us to determine a lower bound on the time to solution. This metricprovides an intuitive grasp of the performance of a variant and we present numericalexperiments to validate the tightness of our lower bounds on practical applications. Our casestudy is the Cholesky inversion and its use in computing the inverse of a symmetric positivedefinite matrix. Subjects: Distributed; Parallel; and Cluster Computing (cs. DC) Cite as …,arXiv preprint arXiv:1010.2000,2010,9
Implementing communication-optimal parallel and sequential qr factorizations,James Demmel; Laura Grigori; Mark Hoemmen; Julien Langou,Abstract: We present parallel and sequential dense QR factorization algorithms for tall andskinny matrices and general rectangular matrices that both minimize communication; andare as stable as Householder QR. The sequential and parallel algorithms for tall and skinnymatrices lead to significant speedups in practice over some of the existing algorithms;including LAPACK and ScaLAPACK; for example up to 6.7 x over ScaLAPACK. The parallelalgorithm for general rectangular matrices is estimated to show significant speedups overScaLAPACK; up to 22x over ScaLAPACK. Subjects: Numerical Analysis (math. NA) Cite as:arXiv: 0809.2407 [math. NA](or arXiv: 0809.2407 v1 [math. NA] for this version) Submissionhistory From: Julien Langou [view email][v1] Sun; 14 Sep 2008 17: 35: 16 GMT (1493kb; D),arXiv preprint arXiv:0809.2407,2008,8
A note on relaxed and flexible GMRES,L Giraud; S Gratton; J Langou,Abstract We consider the solution of a linear system of equations using the GMRES iterativemethod. In [3]; a strategy to relax the accuracy of the matrix-vector product is proposed forgeneral systems and illustrated on a large set of numerical experiments. This work is basedon some heuristic considerations and proposes a strategy that often enables a convergenceof the GMRES iterates xk within a relative normwise backward error less than a targetaccuracy. Finally a significant step toward a theoretical explanation of the observedbehaviour of the relaxed GMRES is proposed in [16; 17]. In these works; importantjustifications are brought to the fact that a relaxation of the matrix-vector product proportionalto the inverse of the norm of the residual may enable the convergence of the relaxedGMRES. In this paper we extend these works; we establish a computable relaxation …,*,2004,8
A Rank-k Update Procedure for Reorthogonalizing the Orthogonal Factor from Modified Gram--Schmidt,Luc Giraud; Serge Gratton; Julien Langou,The modified Gram--Schmidt algorithm is a well-known and widely used procedure toorthogonalize the column vectors of a given matrix. When applied to ill-conditioned matricesin floating point arithmetic; the orthogonality among the computed vectors may be lost. In thiswork; we propose an a posteriori reorthogonalization technique based on a rank-k update ofthe computed vectors. The level of orthogonality of the set of vectors built gets better when kincreases and finally reaches the machine precision level for a large enough k. The rank ofthe update can be tuned in advance to monitor the orthogonality quality. We illustrate theefficiency of this approach in the framework of the seed-GMRES technique for the solution ofan unsymmetric linear system with multiple right-hand sides. In particular; we reportexperiments on numerical simulations in electromagnetic applications where a rank-one …,SIAM journal on matrix analysis and applications,2004,8
Efficient parallel iterative solvers for the solution of large dense linear systems arising from the boundary element method in electromagnetism,Guillaume Alléon; Bruno Carpentieri; Iain S Duff; Luc Giraud; E Martin; G Sylvand,Abstract The boundary element method has become a popular tool for the solution ofMaxwell's equations in electromagnetism. It discretizes only the surface of the radiatingobject and gives rise to linear systems that are smaller in size compared to those arisingfrom finite element or finite difference discretizations. However; these systems areprohibitevely demanding in terms of memory for direct methods and challenging to solve byiterative methods. In this paper we address the iterative solution via preconditioned Krylovmethods of electromagnetic scattering problems expressed in an integral formulation; withmain focus on the design of the preconditioner. We consider an approximate inverse methodbased on the Frobeniusnorm minimization with a pattern prescribed in advance. Thepreconditioner is constructed from a sparse approximation of the dense coefficient matrix …,Proceedings of the International Conference on Supercomputing in Nuclear Application (SNA); Paris,2003,8
Robust selective Gram-Schmidt reorthogonalization,Luc Giraud; Julien Langou,Abstract. A new criterion for selective reorthogonalization in the Gram-Schmidt procedure isgiven. We establish its comportment in presence of rounding errors when the criterion isused with modified Gram-Schmidt algorithm and show counter-example matrices whichprove that standard criteria are not always valid. Experimentally; our criterion is fine also forthe classical Gram-Schmidt algorithm with reorthogonalization. AMS Subject Classification:65F25; 65G50; 15A23.,*,2002,8
Matrices over runtime systems at exascale,Emmanuel Agullo; George Bosilca; Berenger Bramas; Cedric Castagnede; Olivier Coulaud; Eric Darve; Jack Dongarra; Mathieu Faverge; Nathalie Furmento; Luc Giraud; Xavier Lacoste; Julien Langou; Hatem Ltaief; Matthias Messner; Raymond Namyst; Pierre Ramet; Toru Takahashi; Samuel Thibault; Stanimire Tomov; Ichitaro Yamazaki,The goal of Matrices Over Runtime Systems at Exascale (MORSE) project is to design denseand sparse linear algebra methods that achieve the fastest possible time to an accuratesolution on large-scale multicore systems with GPU accelerators; using all the processingpower that future high end systems can make available. In this poster; we propose aframework for describing linear algebra algorithms at a high level of abstraction anddelegating the actual execution to a runtime system in order to design software whoseperformance is portable accross architectures. We illustrate our methodology on threeclasses of problems: dense linear algebra; sparse direct methods and fast multipolemethods. The resulting codes have been incorporated into Magma; Pastix and ScalFMMsolvers; respectively.,High Performance Computing; Networking; Storage and Analysis (SCC); 2012 SC Companion:,2012,7
Computing the R of the QR factorization of tall and skinny matrices using MPI_Reduce,Julien Langou,Abstract: A QR factorization of a tall and skinny matrix with n columns can be represented asa reduction. The operation used along the reduction tree has in input two n-by-n uppertriangular matrices and in output an n-by-n upper triangular matrix which is defined as the Rfactor of the two input matrices stacked the one on top of the other. This operation is binary;associative; and commutative. We can therefore leverage the MPI library capabilities byusing user-defined MPI operations and MPI_Reduce to perform this reduction. The resultingcode is compact and portable. In this context; the user relies on the MPI library to select areduction tree appropriate for the underlying architecture. Subjects: Numerical Analysis(math. NA) Cite as: arXiv: 1002.4250 [math. NA](or arXiv: 1002.4250 v1 [math. NA] for thisversion) Submission history From: Julien Langou [view email][v1] Tue; 23 Feb 2010 06 …,arXiv preprint arXiv:1002.4250,2010,7
LAPACK working note 191: A class of parallel tiled linear algebra algorithms for multicore architectures,A Buttari; J Langou; J Kurzak; JJ Dongarra,*,*,2007,7
Applied parallel computing. State of the art in scientific computing: 8th International Workshop; PARA 2006; Umeå; Sweden; June 2006; Revised Selected Papers,Bo Kågström; Erik Elmroth; Jack Dongarra; Jerzy Wasniewski,The Eighth International Workshop on Applied Parallel Computing (PARA 2006) was held inUmeå; Sweden; June 18–21; 2006. The workshop was organized by the High PerformanceComputing Center North (HPC2N) and the Department of Computing Science at UmeåUniversity. The general theme for PARA 2006 was “State of the Art in Scientific and ParallelComputing.” Topics covered at PARA 2006 included basic algorithms and software forscientific; parallel and grid computing; tools and environments for developing high-performance computing applications; as well as a broad spectrum of applications fromscience and engineering. The workshop included 7 plenary keynote presentations; 15invited minisymposia organized in 30 sessions; and 16 sessions of contributed talks. Theminisymposia and the contributed talks were held in five to six parallel sessions. The …,*,2007,7
Hash functions for datatype signatures in MPI,Julien Langou; George Bosilca; Graham Fagg; Jack Dongarra,Abstract Detecting misuse of datatypes in an application code is a desirable feature for anMPI library. To support this goal we investigate the class of hash functions based onchecksums to encode the type signatures of MPI datatype. The quality of these hashfunctions is assessed in terms of hashing; timing and comparing to other functions publishedfor this particular problem (Gropp; 7th European PVM/MPI Users' Group Meeting; 2000) orfor other applications (CRCs). In particular hash functions based on Galois Field enablesgood hashing; computation of the signature of unidatatype in O (1) and computation of theconcatenation of two datatypes in O (1) additionally.,European Parallel Virtual Machine/Message Passing Interface Users’ Group Meeting,2005,7
LAPACK Working Note 190: Parallel Tiled QR Factorization for Multicore Architectures,A Buttari; J Langou; J Kurzak; JJ Dongarra,*,*,2007,6
A backward/forward recovery approach for the preconditioned conjugate gradient method,Massimiliano Fasi; Julien Langou; Yves Robert; Bora Uçar,Abstract Several recent papers have introduced a periodic verification mechanism to detectsilent errors in iterative solvers. Chen (2013; pp. 167–176) has shown how to combine sucha verification mechanism (a stability test checking the orthogonality of two vectors andrecomputing the residual) with checkpointing: the idea is to verify every d iterations; and tocheckpoint every c× d iterations. When a silent error is detected by the verificationmechanism; one can rollback to and re-execute from the last checkpoint. In this paper; wealso propose to combine checkpointing and verification; but we use algorithm-based faulttolerance (ABFT) rather than stability tests. ABFT can be used for error detection; but also forerror detection and correction; allowing a forward recovery (and no rollback nor re-execution) when a single error is detected. We introduce an abstract performance model …,Journal of Computational Science,2016,5
PLASMA Users’ Guide. Parallel Linear Algebra Software for Multicore Architectures,Emmanuel Agullo; Jack Dongarra; Bilel Hadri; Jakub Kurzak; Julie Langou; Julien Langou; Hatem Ltaief; Piotr Luszczek; Asim YarKhan,*,Rapport technique; Innovative Computing Laboratory; University of Tennessee,2011,5
A distributed packed storage for large dense parallel in‐core calculations,Marc Baboulin; Luc Giraud; Serge Gratton; Julien Langou,Abstract In this paper we propose a distributed packed storage format that exploits thesymmetry or the triangular structure of a dense matrix. This format stores only half of thematrix while maintaining most of the efficiency compared with a full storage for a wide rangeof operations. This work has been motivated by the fact that; in contrast to sequential linearalgebra libraries (eg LAPACK); there is no routine or format that handles packed matrices inthe currently available parallel distributed libraries. The proposed algorithms exclusively usethe existing ScaLAPACK computational kernels; which proves the generality of theapproach; provides easy portability of the code and provides efficient re-use of existingsoftware. The performance results obtained for the Cholesky factorization show that ourpacked format performs as good as or better than the ScaLAPACK full storage algorithm …,Concurrency and Computation: Practice and Experience,2007,5
Résolution de systèmes linéaires de grande taille avec plusieurs seconds membres,Julien Langou,Résumé Le point de départ de cette thèse est un problème posé par le groupeélectromagnétisme de EADS-CCR: comment résoudre plusieurs systèmes linéaires avec lamême matrice mais différents seconds membres? Pour l'application voulue; les matricessont complexes; denses et de grande taille (de l'ordre de quelques millions). Comme detelles matrices ne peuvent être ni calculées; ni stockées dans un processus industriel;l'utilisation d'un produit matrice-vecteur approché est la seule alternative. En l'occurrence; leproduit matrice-vecteur est effectué en utilisant la méthode multipôle rapide. Dans cecontexte; le but de cette thèse est d'adapter les méthodes itératives de type Krylov de tellessorte qu'elles traitent efficacement les nombreux seconds membres. Nous nous concentronsparticulièrement sur l'algorithme GMRES et ses variantes. Les schémas d' …,*,2003,5
PLASMA users’ guide; 2009,E Agullo; J Dongarra; B Hadri; J Kurzak; J Langou; J Langou; H Ltaief; P Luszczek; A YarKhan,*,*,*,5
Reordering strategy for blocking optimization in sparse linear solvers,Grégoire Pichon; Mathieu Faverge; Pierre Ramet; Jean Roman,Solving sparse linear systems is a problem that arises in many scientific applications; andsparse direct solvers are a time-consuming and key kernel for those applications and formore advanced solvers such as hybrid direct-iterative solvers. For this reason; optimizingtheir performance on modern architectures is critical. The preprocessing steps of sparsedirect solvers---ordering and block-symbolic factorization---are two major steps that lead to areduced amount of computation and memory and to a better task granularity to reach a goodlevel of performance when using blas kernels. With the advent of GPUs; the granularity ofthe block computation has become more important than ever. In this paper; we present areordering strategy that increases this block granularity. This strategy relies on block-symbolic factorization to refine the ordering produced by tools such as Metis or Scotch …,SIAM Journal on Matrix Analysis and Applications,2017,4
Designing LU-QR hybrid solvers for performance and stability,Mathieu Faverge; Julien Herrmann; Julien Langou; Bradley R Lowery; Yves Robert; Jack Dongarra,This paper introduces hybrid LU-QR algorithms for solving dense linear systems of the formAx= b. Throughout a matrix factorization; these algorithms dynamically alternate LU withlocal pivoting and QR elimination steps; based upon some robustness criterion. LUelimination steps can be very efficiently parallelized; and are twice as cheap in terms ofoperations; as QR steps. However; LU steps are not necessarily stable; while QR steps arealways stable. The hybrid algorithms execute a QR step when a robustness criterion detectssome risk for instability; and they execute an LU step otherwise. Ideally; the choice betweenLU and QR steps must have a small computational overhead and must provide a satisfactorylevel of stability with as few QR steps as possible. In this paper; we introduce severalrobustness criteria and we establish upper bounds on the growth factor of the norm of the …,Parallel and Distributed Processing Symposium; 2014 IEEE 28th International,2014,4
PLASMA version 2.0 user guide,E Agullo; J Dongarra; B Hadri; J Kurzak; J Langou; J Langou; H Ltaief; P Luszczek; A YarKhan,*,*,2009,4
Comparison of nonlinear conjugate-gradient methods for computing the electronic properties of nanostructure architectures,Stanimire Tomov; Julien Langou; Andrew Canning; Lin-Wang Wang; Jack Dongarra,Abstract In this article we report on our efforts to test and expand the current state-of-the-artin eigenvalue solvers applied to the field of nanotechnology. We singled out the nonlinearconjugate gradients (CG) methods as the backbone of our efforts for their previous successin predicting the electronic properties of large nanostructures and made a library of threedifferent solvers (two recent and one new) that we integrated into the parallel PESCAN(Parallel Energy SCAN) code [3] to perform a comparison.,International Conference on Computational Science,2005,4
NanoPSE: Nanoscience Problem Solving Environment for atomistic electronic structure of semiconductor nanostructures,Wesley B Jones; Gabriel Bester; Andrew Canning; Alberto Franceschetti; Peter A Graf; Kwiseon Kim; Julien Langou; Lin-Wang Wang; Jack Dongarra; Alex Zunger,Abstract Researchers at the National Renewable Energy Laboratory and their collaboratorshave developed over the past~ 10 years a set of algorithms for an atomistic description ofthe electronic structure of nanostructures; based on plane-wave pseudopotentials andconfigurationinteraction. The present contribution describes the first step in assemblingthese various codes into a single; portable; integrated set of software packages. Thispackage is part of an ongoing research project in the development stage. Components ofNanoPSE include codes for atomistic nanostructure generation and passivation; valenceforce field model for atomic relaxation; code for potential field generation; empiricalpseudopotential method solver; strained linear combination of bulk bands method solver;configuration interaction solver for excited states; selection of linear algebra methods; and …,Journal of Physics: Conference Series,2005,4
Level-3 Cholesky factorization routines improve performance of many Cholesky algorithms,Fred G Gustavson; Jerzy Waśniewski; Jack J Dongarra; José R Herrero; Julien Langou,Abstract Four routines called DPOTF3i; i= a; b; c; d; are presented. DPOTF3i are a novel typeof level-3 BLAS for use by BPF (Blocked Packed Format) Cholesky factorization andLAPACK routine DPOTRF. Performance of routines DPOTF3i are still increasing when theperformance of Level-2 routine DPOTF2 of LAPACK starts decreasing. This is our mainresult and it implies; due to the use of larger block size nb; that DGEMM; DSYRK; andDTRSM performance also increases! The four DPOTF3i routines use simple registerblocking. Different platforms have different numbers of registers. Thus; our four routines havedifferent register blocking sizes. BPF is introduced. LAPACK routines for POTRF and PPTRFusing BPF instead of full and packed format are shown to be trivial modifications of LAPACKPOTRF source codes. We call these codes BPTRF. There are two variants of BPF: lower …,ACM Transactions on Mathematical Software (TOMS),2013,3
Level-3 Cholesky Factorization Routines as Part of Manu Cholesky Algorithms,Fred G Gustavson; Jerzy Wasniewski; Jack J Dongarra; José R Herrero; Julien Langou,Some Linear Algebra Libraries use Level-2 routines during the factorization part of anyLevel-3 block factorization algorithm. We discuss four Level-3 routines called DPOTF3i; i= a;b; c; d; a new type of BLAS; for the factorization part of a block Cholesky factorizationalgorithm for use by LAPACK routine DPOTRF or for BPF (Blocked Packed Format)Cholesky factorization. The four routines DPOTF3i are Fortran routines. Our main result isthat performance of routines DPOTF3i is still increasing when the performance of Level-2routine DPOTF2 of LAPACK starts to decrease. This means that the performance of DGEMM;DSYRK; and DTRSM will increase due to their use of larger block sizes and also by makingless passes over the matrix elements. We present corroborating performance results forDPOTF3i versus DPOTF2 on a variety of common platforms. The four DPOTF3i routines …,*,2011,3
2011 IEEE International Parallel & Distributed Processing Symposium Flexible Development of Dense Linear Algebra Algorithms on Massively Parallel Architectures...,George Bosilca; Aurelien Bouteiller; Anthony Danalis; Mathieu Faverge; Azzam Haidar; Thomas Herault; Jakub Kurzak; Julien Langou; Pierre Lemarinier; Hatem Ltaief; Piotr Luszczek; Asim Yarkhan; Jack Dongarra,Abstract—We present a method for developing dense linear algebra algorithms thatseamlessly scales to thousands of cores. It can be done with our project called DPLASMA(Distributed PLASMA) that uses a novel generic distributed Direct Acyclic Graph Engine(DAGuE). The engine has been designed for high performance computing and thus itenables scaling of tile algorithms; originating in PLASMA; on large distributed memorysystems. The underlying DAGuE framework has many appealing features when consideringdistributed-memory platforms with heterogeneous multicore nodes: DAG representation thatis independent of the problem-size; automatic extraction of the communication from thedependencies; overlapping of communication and computation; task prioritization; andarchitecture-aware scheduling and management of tasks. The originality of this engine …,*,2010,3
A reorthogonalization procedure for modified Gram–Schmidt algorithm based on a rank-k update,Luc Giraud; Serge Gratton; Julien Langou,Abstract The modified Gram–Schmidt algorithm is a well–known and widely used procedureto orthogonalize the column vectors of a given matrix. When applied to ill–conditionedmatrices in floating point arithmetic; the orthogonality among the computed vectors may belost. In this work; we propose an a posteriori reorthogonalization technique based on a rank–k update of the computed vectors. The level of orthogonality of the set of vectors built getsbetter when k increases and finally reaches the machine precision level for a large enoughk. The rank of the update can be tuned in advance to monitor the orthogonality quality. Weillustrate the efficiency of this approach in the framework of the Seed–GMRES technique forthe solution of an unsymmetric linear system with multiple right–hand sides. In particular; wereport experiments on numerical simulations in electromagnetic applications where a …,Soumisa SIAM Journal of Matrix Analysis and Applications,2003,3
Mixing LU and QR factorization algorithms to design high-performance dense linear algebra solvers,Mathieu Faverge; Julien Herrmann; Julien Langou; Bradley Lowery; Yves Robert; Jack Dongarra,Abstract This paper introduces hybrid LU–QR algorithms for solving dense linear systems ofthe form Ax= b A x= b. Throughout a matrix factorization; these algorithms dynamicallyalternate LU with local pivoting and QR elimination steps based upon some robustnesscriterion. LU elimination steps can be very efficiently parallelized; and are twice as cheap interms of floating-point operations; as QR steps. However; LU steps are not necessarilystable; while QR steps are always stable. The hybrid algorithms execute a QR step when arobustness criterion detects some risk for instability; and they execute an LU step otherwise.The choice between LU and QR steps must have a small computational overhead and mustprovide a satisfactory level of stability with as few QR steps as possible. In this paper; weintroduce several robustness criteria and we establish upper bounds on the growth factor …,Journal of Parallel and Distributed Computing,2015,2
On matrix balancing and eigenvector computation,Rodney James; Julien Langou; Bradley R Lowery,Abstract: Balancing a matrix is a preprocessing step while solving the nonsymmetriceigenvalue problem. Balancing a matrix reduces the norm of the matrix and hopefully thiswill improve the accuracy of the computation. Experiments have shown that balancing canimprove the accuracy of the computed eigenval-ues. However; there exists examples wherebalancing increases the eigenvalue condition number (potential loss in accuracy);deteriorates eigenvector accuracy; and deteriorates the backward error of the eigenvaluedecomposition. In this paper we propose a change to the stopping criteria of the LAPACKbalancing al-gorithm; GEBAL. The new stopping criteria is better at determining when amatrix is nearly balanced. Our experiments show that the new algorithm is able to maintaingood backward error; while improving the eigenvalue accuracy when possible. We …,arXiv preprint arXiv:1401.5766,2014,2
Communication-optimal parallel and sequential QR and,JW Demmel; Laura Grigori; Mark Frederick Hoemmen; Julien Langou,Abstract We present parallel and sequential dense QR factorization algorithms that are bothoptimal (up to polylogarithmic factors) in the amount of communication they perform; and justas stable as Householder QR. Our ﬁrst algorithm; Tall Skinny QR (TSQR); factors m>< nmatrices in a one-dimensional (1-D) block cyclic row layout; and is optimized for m>> 11.Our second algorithm; CAQR (Communication-Avoiding QR); factors general rectangularmatrices distributed in a two-dimensional block cyclic layout. It invokes TSQR for each blockcolumn factorization. The new algorithms are superior in both theory and practice. VVe haveextended known lower bounds on communication for sequential and parallel matrixmultiplication to provide latency lower bounds; and show these bounds apply to the LU andQR decompositions. We not only show that our QR algorithms attain these lower bounds …,*,2008,2
Handbook of Linear Algebra,L Han; M Neumann,*,Discrete Mathematics and its Applications; Chapman & Hall/CRC; Boca Raton; FL,2007,2
Modeling the LU factorization for SMP clusters,Jack Dongarra; Emmanuel Jeannot; Julien Langou,In this paper we target the problem of modeling the LU factorization in the context of a 2-Dblock-cyclic distribution. Modeling the LU factorization is an important challenge as it can helpto understand the scalability of the algorithm and helps the user to compute both the best blocksize and the optimal processor grid-size. Modeling the LU factorization has been addressedin several prior works [1; 2; 3]. In [1]; a very crude model is proposed. It is simply used for showingthe scalability of ScaLapack. It does not use the processor grid shape whereas it is well knownthat it has a great impact on the performance. In [2]; the authors propose a very fine model withmultiple parameters for the environment (more than 30!). They use this model to compute theoptimal grid size. However; they do not show how to compute these parameters. In [3]; the authorspropose a simple model with only three parameters for the environment (the bandwidth …,Proceeedings of Parallel Matrix Algorithms and Applications (PMAA’06); IRISA; Rennes; France,2006,2
Another proof for modified Gram-Schmidt with reorthogonalization,Luc Giraud; Julien Langou,Abstract. In this note; we consider the modified Gram-Schmidt algorithm withreorthogonalization applied on a numerical nonsingular matrix; we explain why the resultingset of vectors is orthogonal up to the machine precision level. To establish this result; weshow that a certain L-criterion is necessarily verified after the second reorthogonalizationstep; then we prove that this L-criterion implies the desired level of orthogonality. If the L-criterion is verified after the first orthogonalization step; then there is no need toreorthogonalize. From this simple observation; we deduce that the L-criterion is aninteresting selective reorthogonalization criterion for modified Gram-Schmidt algorithm. AMSSubject Classification: 65F25; 65G50; 15A23. 1.,CERFACS Working Note,2002,2
Disaster survival guide in petascale computing: an algorithmic approach,Jack J Dongarra; Zizhong Chen; George Bosilca; Julien Langou,CiteSeerX - Document Details (Isaac Councill; Lee Giles; Pradeep Teregowda):,*,2001,2
Parallel Tiled QR Factorization for Multicore Architectures LAPACK Working Note# 190,Alfredo Buttaril; Julien Langoug; Jakub Kurzakl; Jack Dongarra12,Abstract. As niulticore systems continue to gain ground in the High Performance ComputingWorld; linear algebra algorithms have to be reformulated or new algorithms have to bedeveloped in order to take advantage of the architectural features on these new processors.Fine grain parallelism becomes a major requirement and introduces the necessity of loosesynchronization in the parallel execution of an operation. This paper presents an algorithmfor the QR factorization where the operations can be represented as a sequence of smalltasks that operate on square blocks of data. These tasks can be dynamically scheduled forexecution based on the dependencies among them and on the availability of coinputationalresources. This may result in an out of order execution of the tasks which will completelyhide the presence of intrinsically sequential tasks in the factorization. Performance …,*,*,2
Fast Parallel Randomized QR with Column Pivoting Algorithms for Reliable Low-Rank Matrix Approximations,Jianwei Xiao; Ming Gu; Julien Langou,*,2017 IEEE 24th International Conference on High Performance Computing (HiPC),2017,1
Bidiagonalization with parallel tiled algorithms,Mathieu Faverge; Julien Langou; Yves Robert; Jack Dongarra,Abstract: We consider algorithms for going from a" full" matrix to a condensed" bandbidiagonal" form using orthogonal transformations. We use the framework of" algorithms bytiles". Within this framework; we study:(i) the tiled bidiagonalization algorithm BiDiag; whichis a tiled version of the standard scalar bidiagonalization algorithm; and (ii) the R-bidiagonalization algorithm R-BiDiag; which is a tiled version of the algorithm which consistsin first performing the QR factorization of the initial matrix; then performing the band-bidiagonalization of the R-factor. For both bidiagonalization algorithms BiDiag and R-BiDiag; we use four main types of reduction trees; namely FlatTS; FlatTT; Greedy; and anewly introduced auto-adaptive tree; Auto. We provide a study of critical path lengths forthese tiled algorithms; which shows that (i) R-BiDiag has a shorter critical path length than …,arXiv preprint arXiv:1611.06892,2016,1
A Greedy Algorithm for Optimally Pipelining a Reduction,Bradley R Lowery; Julien Langou,Abstract: Collective communications are ubiquitous in parallel applications. We present twonew algorithms for performing a reduction. The operation associated with our reductionneeds to be associative and commutative. The two algorithms are developed under twodifferent communication models (unidirectional and bidirectional). Both algorithms use agreedy scheduling scheme. For a unidirectional; fully connected network; we prove that ourgreedy algorithm is optimal when some realistic assumptions are respected. Previousalgorithms fit the same assumptions and are only appropriate for some given configurations.Our algorithm is optimal for all configurations. We note that there are some configurationwhere our greedy algorithm significantly outperform any existing algorithms. This resultrepresents a contribution to the state-of-the art. For a bidirectional; fully connected …,arXiv preprint arXiv:1310.4645,2013,1
Any decreasing cycle-convergence curve is possible for restarted GMRES,Eugene Vecharynski; Julien Langou,Abstract: Given a matrix order $ n $; a restart parameter $ m $($ m< n $); a decreasingpositive sequence $ f (0)> f (1)>...> f (q)\geq 0$; where $ q< n/m $; it is shown that there exitsan $ n $-by-$ n $ matrix $ A $ and a vector $ r_0 $ with $\| r_0\|= f (0) $ such that $\| r_k\|= f(k) $; $ k= 1;...; q $; where $ r_k $ is the residual at cycle $ k $ of restarted GMRES withrestart parameter $ m $ applied to the linear system $ Ax= b $; with initial residual $ r_0= b-Ax_0 $. Moreover; the matrix $ A $ can be chosen to have any desired eigenvalues. We canalso construct arbitrary cases of stagnation; namely; when $ f (0)> f (1)>...> f (i)= f (i+ 1)\geq0$ for any $ i< q $. The restart parameter can be fixed or variable. Subjects: NumericalAnalysis (math. NA) MSC classes: 65F10 Report number: UC Denver Center forComputational Mathematics Technical Report# 279 Cite as: arXiv: 0907.3573 [math. NA] …,arXiv preprint arXiv:0907.3573,2009,1
Gram-Schmidt orthogonalization: 100 years and more,SJ Leon; Walter Gander; J Langou; Å Björck,Page 1. Gram-Schmidt Orthogonalization: 100 Years and More Steven Leon; Walter Gander;Åke Björck; Lucien Langou September 12; 2008 Steven Leon; Walter Gander; Åke Björck; LucienLangou Gram-Schmidt Orthogonalization: 100 Years and More Page 2. Outline of Talk ► EarlyHistory (1795–1907) ► Middle History 1. The work of Åke Björck Least squares; Stability; Lossof orthogonality 2. The work of Heinz Rutishauser Selective reorthogonalization andSuperorthogonalization ► Modern Research 1. Recent results on the Gram-Schmidt algorithms2. Column Pivoting and Rank Revealing factorizations 3. Applications to Krylov SubspaceMethods Steven Leon; Walter Gander; Åke Björck; Lucien Langou Gram-SchmidtOrthogonalization: 100 Years and More Page 3. Earlier History ► Least Squares - Gauss andLegendre ► Laplace 1812; Analytic Theory of Probabilities (1814; 1820) …,Manuscript to appear,2008,1
New eigensolvers for large-scale nanoscience simulations,A Canning; O Marques; C Voemel; Lin-Wang Wang; J Dongarra; J Langou; S Tomov,Abstract We present results for applications to nanosystems of state-of-the-art iterativeeigensolvers based on conjugate gradients and variants of Davidson in the context of semi-empirical plane wave electronic structure calculations. We are concerned with thecomputation of electronic and optical properties of nanosystems using the Energy SCANmethod to compute interior eigenstates around the band gap that determine their properties.Numerically; this interior Hermitian eigenvalue problem poses several challenges; withrespect to both accuracy and efficiency. All the iterative eigensolvers are seeking theminimal eigenvalues of the folded operator with reference shift in the band-gap. The testedmethods include standard conjugate-gradient (CG)-based Rayleigh quotient minimization;Locally optimal block-preconditioned CG (LOBPCG) and two variants of the (Jacobi-) …,Journal of Physics: Conference Series,2008,1
Prospectus for a dense linear algebra software library,James Demmel; Jack Dongarra; Beresford Parlett; William Kahan; Ming Gu; David Bindel; Yozo Hida; Xiaoye Li; Osni Marques; E Jason Riedy; Christof Voemel; Julien Langem; Piotr Luszczek; Jakub Kurzak; Alfredo Buttari; Julie Langou; Stanimire Tomov,Dense linear algebra (DLA) forms the core of many scientific computing applications.Consequently; there is continuous interest and demand for the development of increasinglybetter algorithms in the field. Here'better'has a broad meaning; and includes improvedreliability accuracy robustness; case of use; and most importantly new or improvedalgorithms that would more efficiently use the available computational resources to speedup the computation. The rapidly evolving high end computing systems and the closedependence of DLA algorithms on the computational environment is what makes the fieldparticularly dynamic. A typical example of the importance and impact of this dependence isthe development of LAPACK [4](and later ScaLAPACK [20]) as a successor to the wellknown and formerly widely used LINPACK [41] and EISPACK [41] libraries. Both …,*,2006,1
Rounding error analysis of the classical Gram-Schmidt process and its applications,Luc Giraud; Julien Langou; Miro Rozloznık; Jasper van den Eshof,In this contribution we focus on the Gram-Schmidt (GS) orthogonalization process whichalso produces a QR factorization of A. Two basic computational variants of the Gram-Schmidt process exist: the classical Gram-Schmidt (CGS) algorithm and the modified Gram-Schmidt (MGS) algorithm (see eg [2; 12]). From a numerical point of view; both thesetechniques may produce a set of vectors which is far from orthogonal and sometimes theorthogonality can be lost completely [1; 11]. Generally accepted view is that the CGSalgorithm is unreliable while the MGS algorithm has much better numerical properties [11;13]. Björck [1] has shown that for a numerically nonsingular matrix A the loss of orthogonalityin MGS occurs in a predictable way and it can be bounded by a term proportional to thecondition number κ (A) and to the roundoff unit u. Therefore; the loss of orthogonality of …,*,2005,1
A reorthogonalization procedure for MGS applied to a low rank deficient matrix,Julien Langou; Luc Giraud; Serge Gratton,Abstract Keywords: Gram-Schmidt; orthogonality; parallelization. We consider the ModifiedGram-Schmidt orthogonalization applied to a matrix A∈ R m× n. This corresponds to a QRfactorization: A= QR. We study this algorithm in finite precision computation when the matrixA has a numerical rank deficiency k. This subject has already been dealt with success byBjörck and Paige in 1992 [1]. They give useful bounds in term of norms. We extend theirresults to provide bounds on singular values. In order to make it more clear; we present ourresults in term of numerical rank. In particular; we show that if; Rank (A)= n− k; then∃ E sothat  0≤ Rank (E)≤ k ˆQ= Q− E; ˆ QT ˆ Q= I; A= ˆ QR. This result says that in finite precisioncomputation; Q looses orthogonality in just a few directions that are given by E; we can alsocontrol the magnitude in each of these directions by the associated singular values of A …,*,2002,1
LAPACK Working Note 93 Installation Guide for ScaLAPACK1,LS Blackford; A Cleary; J Choi; JJ Dongarra; J Langou; A Petitet; RC Whaley; J Demmel; I Dhillon; O Marques; K Stanley; D Walker,Abstract This working note describes how to install and test version 1.8 of ScaLAPACK. Themost significant change in this release of ScaLAPACK is the externalisation of the LAPACKroutines. Now ScaLAPACK requires to have the LAPACK library installed besides BLACS;BLAS and MPI or PVM. This will allow the user to use the latest LAPACK algorithms;modifications without the need of reinstalling the ScaLAPACK library. Two new routines toallow read and write from files have been added. Also a complete ScaLAPACK example hasbeen added in the main directory. The design of the testing/timing programs for theScaLAPACK codes is also discussed.,*,1997,1
Bidiagonalization and R-Bidiagonalization: Parallel Tiled Algorithms; Critical Paths and Distributed-Memory Implementation,Mathieu Faverge; Julien Langou; Yves Robert; Jack Dongarra,We study tiled algorithms for going from a" full" matrix to a condensed" band bidiagonal"form using orthog-onal transformations:(i) the tiled bidiagonalization algorithm BIDIAG;which is a tiled version of the standard scalar bidiago-nalization algorithm; and (ii) the R-bidiagonalization algorithm R-BIDIAG; which is a tiled version of the algorithm whichconsists in first performing the QR factorization of the initial matrix; then performing the band-bidiagonalization of the R-factor. For both BIDIAG and R-BIDIAG; we use four main types ofreduction trees; namely FLATTS; FLATTT; GREEDY; and a newly introduced auto-adaptivetree; AUTO. We provide a study of critical path lengths for these tiled algorithms; whichshows that (i) R-BIDIAG has a shorter critical path length than BIDIAG for tall and skinnymatrices; and (ii) GREEDY based schemes are much better than earlier proposed …,Parallel and Distributed Processing Symposium (IPDPS); 2017 IEEE International,2017,*
A Backward/Forward Recovery Approach for the Preconditioned Conjugate Gradient Algorithm,Massimiliano Fasi; Julien Langou; Yves Robert; Bora Uçar,Several recent papers have introduced a periodic verification mechanism to detect silenterrors in iterative solvers. Chen [PPoPP'13; pp. 167--176] has shown how to combine such averification mechanism (a stability test checking the orthogonality of two vectors andrecomputing the residual) with checkpointing: the idea is to verify every $ d $ iterations; andto checkpoint every $ c\times d $ iterations. When a silent error is detected by the verificationmechanism; one can rollback to and re-execute from the last checkpoint. In this paper; wealso propose to combine checkpointing and verification; but we use algorithm-based faulttolerance (ABFT) rather than stability tests. ABFT can be used for error detection; but also forerror detection and correction; allowing a forward recovery (and no rollback nor re-execution) when a single error is detected. We introduce an abstract performance model …,*,2015,*
A Makespan Lower Bound for the Scheduling of the Tiled Cholesky Factorization based on ALAP scheduling,Willy Quach; Julien Langou,Abstract: Due to the advent of multicore architectures and massive parallelism; the tiledCholesky factorization algorithm has recently received plenty of attention and is oftenreferenced by practitioners as a case study. It is also implemented in mainstream denselinear algebra libraries. However; we note that theoretical study of the parallelism of thisalgorithm is currently lacking. In this paper; we present new theoretical results about the tiledCholesky factorization in the context of a parallel homogeneous model withoutcommunication costs. We use standard flop-based weights for the tasks. For a $ t $-by-$ t $matrix; we know that the critical path of the tiled Cholesky algorithm is $9 t-10$ and that theweight of all tasks is $ t^ 3$. In this context; we prove that no schedule with less than $0.185t^ 2$ processing units can finish in a time less than the critical path. In perspective; a …,arXiv preprint arXiv:1510.05107,2015,*
Mixing LU and QR factorization algorithms to design high-performance dense linear algebra solvers Mathieu Faverge; Julien Herrmann; Julien Langou; Bradley Low...,Yves Robert; Jack Dongarra,Abstract This paper introduces hybrid LU-QR algorithms for solving dense linear systems ofthe form Ax= b. Throughout a matrix factorization; these algorithms dynamically alternate LUwith local pivoting and QR elimination steps based upon some robustness criterion. LUelimination steps can be very efficiently parallelized; and are twice as cheap in terms offloating-point operations; as QR steps. However; LU steps are not necessarily stable; whileQR steps are always stable. The hybrid algorithms execute a QR step when a robustnesscriterion detects some risk for instability; and they execute an LU step otherwise. The choicebetween LU and QR steps must have a small computational overhead and must provide asatisfactory level of stability with as few QR steps as possible. In this paper; we introduceseveral robustness criteria and we establish upper bounds on the growth factor of the …,*,2015,*
Topic 10: Parallel Numerical Algorithms,Julien Langou; Matthias Bolten; Laura Grigori; Marian Vajteršic,Abstract The solution of large-scale problems in Computational Science and Engineeringrelies on the availability of accurate; robust and efficient numerical algorithms and softwarethat are able to exploit the power offered by modern computer architectures. Such algorithmsand software provide building blocks for prototyping and developing novel applications; andfor improving existing ones; by relieving the developers from details concerning numericalmethods as well as their implementation in new computing environments.,European Conference on Parallel Processing,2013,*
PLASMA Users’ Guide,LAPACK Working Note XXX; Jakub Kurzak; Julien Langou; Hatem Ltaief; Piotr Luszczek; Asim YarKhan; Mathieu Faverge; Azzam Haidar; Joshua Hoffman; Alfredo Buttari; Bilel Hadri,PLASMA version 1.0 was released in November 2008 as a prototype software providingproof-of-concept implementation of a linear equations solver based on LU factorization; SPDlinear equations solver based on Cholesky factorization and least squares problem solverbased on QR and LQ factorizations; with support for real arithmetic in double precision only.The publication of this Users' Guide coincides with the September 2010 release of version2.3 of PLASMA; with the following set of features:,*,2010,*
Translation and modern interpretation of Laplace's Th\'eorie Analytique des Probabilit\'es; pages 505-512; 516-520,Julien Langou,Abstract: The text of Laplace;\textit {Sur l'application du calcul des probabilit\'es\a laphilosophie naturelle;}(Th\'eorie Analytique des Probabilit\'es. Troisi\eme\'Edition. PremierSuppl\'ement); 1820; is quoted in the context of the Gram-Schmidt algorithm. We provide anEnglish translation of Laplace's manuscript (originally in French) and interpret the algorithmsof Laplace in a contemporary context. The two algorithms given by Laplace computes themean and the variance of two components of the solution of a linear statistical model. Thefirst algorithm can be interpreted as {\em reverse square-root-free modified Gram-Schmidt byrow} algorithm on the regression matrix. The second algorithm can be interpreted as the{\em reverse square-root-free Cholesky} algorithm. Subjects: Numerical Analysis (math. NA);Statistics Theory (math. ST) Report number: UC Denver CCM Technical Report# 280 Cite …,arXiv preprint arXiv:0907.4695,2009,*
As multicore systems continue to gain ground in the high performance computing world; linear algebra algorithms have to be reformulated or new algorithms have to...,Alfredo Buttari; Julien Langou; Jakub Kurzak; Jack Dongarra,Asynchronous executions of a distributed algorithm differ from each other due to thenondeterminism in the order in which the messages exchanged are handled. In manysituations of interest; the asynchronous executions induced by restricting nondeterminismare more efficient; in an application-specific sense; than the others. In this work; we definepartially ordered executions of a distributed algorithm...,Parallel Computing,2009,*
Rectangular Full Packed Format for Cholesky's Algorithm: Factorization; Solution and Inversion are already in LAPACK,Fred G Gustavson; Jerzy Wasniewski; Jack J Dongarra; Julie Langou,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2389484853 …,*,2009,*
International Journal of High Performance,Jack Dongarra; Julien Langou,Abstract Over the last 20 years; the open-source community has provided more and moresoftware on which the world's highperformance computing systems depend for performanceand productivity. The community has invested millions of dollars and years of effort to buildkey components. However; although the investments in these separate software elementshave been tremendously valuable; a great deal of productivity has also been lost because ofthe lack of planning; coordination; and key integration of technologies necessary to makethem work together smoothly and efficiently; both within individual petascale systems andbetween different systems. It seems clear that this completely uncoordinated developmentmodel will not provide the software needed to support the unprecedented parallelismrequired for peta/exascale computation on millions of cores; or the flexibility required to …,International Journal of High Performance Computing Applications,2009,*
C Interface to LAPACK (Proposal),David Bindel; Michael Chuvelev; James Demmel; Greg Henry; Julie Langou; Julien Langou; Vladimir Koldakov; Shane Story,*,*,2008,*
Advanced MPI programming,Julien Langou; George Bosilca,Abstract MPI provides a large range of features allowing various approaches for parallelcomputing. This tutorial will present interesting features from the MPI-1 standard. Thesefeatures extend the user knowledge about MPI way beyond the few basic standardfunctions; giving them the opportunity to implement better; simpler and potentially fasterparallel algorithms. This tutorial will cover several features from medium level to advanced ofthe MPI-1 standard to enable users to exploit fully MPI.,European Parallel Virtual Machine/Message Passing Interface Users’ Group Meeting,2007,*
International Journal of High Performance,Alfredo Buttari; Victor Eijkhout; Julien Langou; Salvatore Filippone,Abstract We present a method for automatically selecting optimal implementations of sparsematrix-vector operations. Our software “AcCELS”(Accelerated Compress-storage Elementsfor Linear Solvers) involves a setup phase that probes machine characteristics; and a run-time phase where stored characteristics are combined with a measure of the actual sparsematrix to find the optimal kernel implementation. We present a performance model that isshown to be accurate over a large range of matrices.,International Journal of High Performance Computing Applications,2007,*
Parallel tools for solving incremental dense least squares problems. Application to space geodesy. LAPACK Working Note 179,Marc Baboulin; Luc Giraud; Serge Gratton; Julien Langou,Abstract We present a parallel distributed solver that enables us to solve incremental denseleast squares arising in some parameter estimation problems. This solver is based onScaLA-PACK [8] and PBLAS [9] kernel routines. In the incremental process; the observationsare collected periodically and the solver updates the solution with new observations using aQR factorization algorithm. It uses a recently defined distributed packed format [3] thathandles symmetric or triangular matrices in ScaLAPACK-based implementations. Weprovide performance analysis on IBM pSeries 690. We also present an example ofapplication in the area of space geodesy for gravity field computations with someexperimental results.,*,2006,*
Recent advances in dense linear algebra: minisymposium abstract,Daniel Kressner; Julien Langou,Abstract These last past years have seen a tremendous amount of new results incomputational dense linear algebra. New algorithms have been developed to increase thespeed of convergence of eigensolvers; to improve the final accuracy of solvers; to improvethe parallel efficiency of applications; and to harness even better the capability of ourcomputing platforms. Of particular interest for this minisymposium are new algorithms thatoutperform the algorithms used in Sca/LAPACK's current routines or match expected newfunctionality of the library.,International Workshop on Applied Parallel Computing,2006,*
Performance evaluation of eigensolvers in nanostructurecomputations,Jack Dongarra; Julien Langou; Stanimire Tomov; Andrew Canning; Osni Marques; Christof Vomel; Lin-Wang Wang,Alternatively; a conjugate cradient (GC) based technique can be used to successivelyminimize the Rayleigh quotient function f (xi)=(x∗ i Hxi)/(x∗ i xi); where the gradient is givenby∇ f (xi)= Hxi− xi (x∗ i Hxi)/(x∗ i xi). The method can also be implemented in a blockedform; whereby the minimization is simultaneously applied to a set of orthogonal vectors Xi.Together with a suitable preconditioner; this approach (PCG) has been used to solve anumber of problems of practical interest. A potential improvement over the PCG algorithm isthe local optimal preconditioned conjugate gradient (LOPCG) method [4]; which extendsPCG by applying Rayleigh Ritz on span (wi; xi; xi− 1); where wi= P∇ f (xi) for an appropriatepreconditioner P. As before; the method can be implemented in a blocked form(LOBPCG)[5]. Currently; we use a folded spectrum mechanism to compute the interior …,*,2006,*
Towards bulk based preconditioning for quantum dot computations,Jack Dongarra; Julien Langou; Stanimire Tomov; Andrew Channing; Osni Marques; Christof Vomel; Lin-Wang Wang,Abstract—This article describes how to accelerate the con-vergence of PreconditionedConjugate Gradient (PCG) type eigensolvers for the computation of several states aroundthe band gap of colloidal quantum dots. Our new approach uses the Hamiltonian from thebulk materials constituent for the quantum dot to design an efficient preconditioner for thefolded spectrum PCG method. The technique described shows promising results whenapplied to CdSe quantum dot model problems. We show a decrease in the number ofiteration steps by at least a factor of 4 compared to the previously used diagonalpreconditioner.,*,2006,*
New eigensolvers and preconditioners for large scale nanoscience simulations,Andrew Canning; Osni Marques; Lin-Wang Wang; Christof Voemel; Stanimire Tomov; Julien Langou,Abstract First-principles materials science calculations typically involve a self-consistentsolution of the Kohn-Sham equations. These types of methods typically scale with the cubeof the system size and can only be used to study systems of up to a thousand atoms. Tostudy larger systems we use semi-empirical potentials or approximated ab initio potentialssuch as those constructed using the charge patching method. Using these types ofpotentials does not require a selfconsistent solution of our effective single particle equationsand we can solve directly for the few states of interest around the gap. This leads to amethod that is effectively O (N) if we consider the number of states we require to be fixed asthe system size increases. The solution of our single particle equations now becomes aninterior eigenvalue problem for a few states around a given energy rather than the self …,APS Meeting Abstracts,2006,*
Self-adapting numerical software (SANS) effort,J Langou; P Luszczek; J Pjesivac-Grbovic; K Seymour; H You; SS Vadhiyar,The challenge for the development of next-generation software is the successfulmanagement of the complex computational environment while delivering to the scientist thefull power of flexible compositions of the available algorithmic alternatives. Selfadaptingnumerical software (SANS) systems are intended to meet this significant challenge. Theprocess of arriving at an efficient numerical solution of problems in computational scienceinvolves numerous decisions by a numerical expert. Attempts to automate such decisionsdistinguish three levels: algorithmic decision; management of the parallel environment; andprocessor-specific tuning of kernels. Additionally; at any of these levels we can decide torearrange the user's data. In this paper we look at a number of efforts at the University ofTennessee to investigate these areas.,*,2006,*
MR2262760 (2008e: 65141) 65F25,Alicja Smoktunowicz; Jesse L Barlow; Julien Langou,An error analysis associated with classical Gram-Schmidt factorization is presented. Let A bea full rank matrix and let A= QR be the classical Gram-Schmidt factorization for A; where Q isleft orthogonal (has orthogonal columns) and R is upper triangular. The authors show thatthe computed R satisfies RT R= AT A+ E where E is an appropriately small backward error;but only if the diagonals of R are computed in a manner similar to Cholesky factorization ofthe normal equation matrix. If these diagonal elements are computed as in the standardversion of classical Gram-Schmidt; no such bounds may be guaranteed. When developingalgorithms for classical Gram-Schmidt similar to those in [JL Barlow; A. Smoktunowicz andH. Erbay; BIT 45 (2005); no. 2; 259–285; MR2176194 (2006h: 65061); JW Daniel et al.;Math. Comp. 30 (1976); no. 136; 772–795; MR0431641 (55# 4638)] with …,Numer. Math,2006,*
Parallel Linear Algebra Software,Victor Eijkhout; Julien Langou; Jack Dongarra,In this chapter we discuss numerical software for linear algebra problems on parallelcomputers. We focus on some of the most common numerical operations: linear systemsolving and eigenvalue computations. Numerical operations such as linear system solvingand eigenvalue calculations can be applied to two different kinds of matrix storage: denseand sparse. Dense systems are in general used when essentially every matrix element isnonzero; sparse systems are used whenever a sufficiently large number of matrix elementsis zero that a specialized storage scheme is warranted. For an introduction to sparsestorage; see [3]. Because the two classes are so different; usually different numericalsoftwares apply to them. We discuss ScaLAPACK and PLAPACK as the choices for denselinear system solving (see section 13.1). For solving sparse linear systems; there exist two …,*,2006,*
The Solution of the Interior Eigenvalue Problem for Large Scale Nanosystems,Andrew Canning; Julien Langou,Abstract First-principles materials science calculations typically involve a self-consistentsolution of the Kohn-Sham equations. These types of methods typically scale with the cubeof the system size and can only be used to study systems of up to a thousand atoms. Tostudy larger systems we use semi-empirical potentials or approximated ab initio potentialssuch as those constructed using the charge patching method. Using these types ofpotentials does not require a selfconsistent solution of our effective single particle equationsand we can solve directly for the few states of interest around the gap. The solution of oursingle particle equations now becomes an interior eigenvalue problem for a few statesaround a given energy rather than the self-consistent solution for the lowest n states where nis the number of bands. In this talk I will compare different methods (conjugate gradient …,APS Meeting Abstracts,2005,*
International Workshop on Computational Nano-Science and Technology-Comparison of Nonlinear Conjugate-Gradient Methods for Computing the Electronic Prop...,Stanimire Tomov; Julien Langou; Andrew Canning; Lin-Wang Wang; Jack Dongarra,*,Lecture Notes in Computer Science,2005,*
Practical implementation of an inexact GMRES method,L Giraud; S Gratton; J Langou,*,*,2004,*
Emeric MARTIN Using spectral low rank preconditioners for large electromagnetic calculations.,Iain DUFF; Luc GIRAUD; Julien LANGOU,We focus on the solution of a sequence of linear systems arising in electromagnetic radarcross section; and having the same coefficient matrix but different right-hand sides. Theproblem consists in solving M1AX= M1B; where M1 is a left preconditioner; A a large densecomplex symmetric matrix that arises from boundary element method; X the block ofunknowns vectors; and B the block of right-hand sides. Our study starts from the observationthat when the matrix M1A has some eigenvalues near zero; the convergence of the Krylovmethods is often slow. The following proposition from [1] shows that we can construct anupdate Mc from spectral information of M1A to correct M1 such as the new preconditionedsystem M2Au= M2b no longer has eigenvalues in a certain neighbourhood of zero. Assumethat M1A is diagonalizable:,Computing,2002,*
Efficient solutions of industrial electromagnetism problems on parallel computers,Emeric Martin; Iain Duff; Luc Giraud; Julien Langou,• Use of the BEM-EFIE formulation arising from the Maxwell equations.➢ large densecomplex linear systems.• Direct solution not feasible; matrix storage too expensive…➢ 1 MDof 14 Tbytes; about 2 years at 10 Gflops sustained.•… but matrix-vector product can beperformed in less than flops; thanks to the Fast Multipole Method (FMM).[1]• New applyingcost in flops and in storage:.• Iterative solvers with robust preconditioners become thealternative.,Numerical Algorithms,1997,*
A Class of Parallel Tiled Linear Algebra Algorithms for Multicore Architectures LAPACK Working Note# 191,Alfredo Butteri; Julien Langem; Jakub Kurzak; Jack Dongarra12; Tennessee Ridge,Abstract. As multicore systems continue to gain ground in the High Performance Computingworld; linear algebra algorithms have to be reformulated or new algorithms have to bedeveloped in order to take advantage of the architectural features on these new processors.Fine grain parallelism becomes a major requirement and introduces the necessity of loosesynchronization in the parallel execution of an operation. This paper presents an algorithmfor the Cholcsky; LU and QR factorization where the operations can be represented as asequence of small tasks that operate on square blocks of data. These tasks can bedynamically scheduled for execution based on the dependencies among them and on theavailability of computational resources. This may result in an out of order execution of thetasks which will completely hide the presence of intrinsically sequential tasks in the …,*,*,*
Rectangular Full Packed Format for CholeskyPs Algorithm: Factorization; Solution and Inversion are already in LAPACK Fred G. Gustavson,TJ lBM; Jack J Dongarra; Julien Langou,We describe a new data format for storing triangular; symmetric; and Hermitian matricescalled RFPF (Rectangular Full Packed Format). The standard two dimensional arrays ofFortran and C (also known as full format) that are used to represent triangular and symmetricmatrices waste nearly half of the storage space but provide high performance via the use ofLevel 3 BLAS. Standard packed format arrays fully utilize storage (array space) but providelow performance as there is no Level 3 packed BLAS. We combine the good features ofpacked and full storage using RFPF to obtain high performance via using Level 3 BLAS asRFPF is a standard full format representation. Also; RFPF requires exactly the same minimalstorage as packed format. Each LAPACK full and/or packed triangular; symmetric; andHermitian routine becomes a single new RFPF routine based on eight possible data …,*,*,*
Prospectus for a Dense Linear Algebra Software Library,Beresford Parlett William Kahan Ming Gu; Alfredo Buttari; Julie Langou; Stanimire Tomov,Dense linear algebra (DLA) forms the core of many scientiﬁc computing applications.Consequently; there is continuous interest and demand for the development of increasinglybetter algorithms in the ﬁeld. Here'better'has a broad meaning; and includes improvedreliability; accuracy; robustness; ease of use; and most importantly new or improvedalgorithms that would more efﬁciently use the available computational resources to speedup the computation. The rapidly evolving high end computing systems and the closedependence of DLA algorithms on the computational environment is what makes the ﬁeldparticularly dynamic. A typical example of the importance and impact of this dependence isthe development of LAPACK [4](and later ScaLAPACK [20]) as a successor to the wellknown and formerly widely used LINPACK [41] and EISPACK [41] libraries. Both …,*,*,*
Hierarchical QR Factorization Algorithms for Multi-Core Cluster Systems,Julien Langou,Abstract In this presentation; we present our continuing research on tiled algorithms for QRfactorization. Given an initial matrix; the tile QR factorization algorithm breaks down thematrix into tiles and then perform a QR factorization by performing sequential LAPACK-likekernels on the tiles [1; 2; 3]. Tile algorithms will naturally enables good data locality for thesequential kernels executed by the cores (high sequential performance); low number ofmessages in a parallel distributed setting (small latency term); and fine granularity (highparallelism). Each tile algorithm is uniquely characterized by its sequence of reduction trees.By changing the sequence of reduction trees; we can design algorithms with much lesscommunication than LAPACK algorithms in two-level memory systems; algorithms withmuch less communication in parallel distributed systems than ScaLAPACK algorithms …,Householder Symposium XIX June 8-13; Spa Belgium,*,*
Prospectus for a Dense Linear Algebra Software Library,Alfredo Buttari; Julie Langou; Stanimire Tomov,Dense linear algebra (DLA) forms the core of many scientiﬁc computing applications.Consequently; there is continuous interest and demand for the development of increasinglybetter algorithms in the ﬁeld. Here'better'has a broad meaning; and includes improvedreliability; accuracy; robustness; ease of use; and most importantly new or improvedalgorithms that would more efficiently use the available computational resources to speedup the computation. The rapidly evolving high end computing systems and the closedependence of DLA algorithms on the computational environment is what makes the ﬁeldparticularly dynamic. A typical example of the importance and impact of this dependence isthe development of LAPACK [4](and later ScaLAPACK [20]) as a successor to the wellknown and formerly widely used LINPACK [41] and EISPACK [41] libraries. Both …,*,*,*
LAPACK Working Note# 224 QR Factorization of Tall and Skinny Matrices in a Grid Computing Environment,Emmanuel Agullo; Camille Coti; Jack Dongarra; Thomas Herault; Julien Langou,Abstract Previous studies have reported that common dense linear algebra operations donot achieve speed up by using multiple geographical sites of a computational grid. Becausesuch operations are the building blocks of most scientific applications; conventionalsupercomputers are still strongly predominant in high-performance computing and the useof grids for speeding up large-scale scientific problems is limited to applications exhibitingparallelism at a higher level. We have identified two performance bottlenecks in thedistributed memory algorithms implemented in ScaLAPACK; a state-of-the-art dense linearalgebra library. First; because ScaLAPACK assumes a homogeneous communicationnetwork; the implementations of ScaLAPACK algorithms lack locality in their communicationpattern. Second; the number of messages sent in the ScaLAPACK algorithms is …,*,*,*
Résolutions performantes de problemes industriels de grande taille en électromagnétisme sur machines paralleles.,Emeric MARTIN; Luc GIRAUD; Julien LANGOU,La modélisation numérique de situations physiques aboutit souventa la résolution d'unesérie d'équations dont les inconnues représentent des grandeurs physiques que l'onsouhaite déterminer. On cherche donca résoudre un systeme de n équations linéairesregroupées sous la forme:,*,*,*
HOW (UN-) STABLE IS THE GRAM-SCHMIDT PROCESS?,M Rozloznık; L Giraud; J Langou,One of the orthogonalization techniques frequently used also in computational statistics isthe Gram-Schmidt process [5];[7]. It has two basic computational variants: the classical Gram-Schmidt (CGS) algorithm and the modified Gram-Schmidt (MGS) algorithm. From anumerical point of view; both of these techniques may produce a set of vectors which is farfrom orthogonal and sometimes the orthogonality can be completely lost. A generally agreedopinion is that the MGS algorithm has much better numerical properties than the CGSalgorithm. Björck [1] has proved that; for a matrix A with numerical full rank; the loss oforthogonality in MGS occurs in a predictable way and it can be bounded by a termproportional to the condition number κ (A) and to the unit roundoff u. Many textbooks (see eg[2];[3];[6]) give examples where the orthogonality of the vectors computed by CGS is lost …,*,*,*
A REORTHOGONALIZATION FOR MGS,Julien Langou; Luc Giraud; Serge Gratton,*,*,*,*
AllReduce algorithms: application to Householder QR factorization Co-authored by,J Langou,QR factorizations of tall and skinny matrices with their data partitioned vertically acrossseveral processors arise in a wide range of applications. Various methods exist to performthe QR factorization of such matrices: Gram-Schmidt; Householder; or CholeskyQR. Wepresent the Allreduce Householder QR factorization. This method is stable and performs; inour experiments; from four to eight times faster than ScaLAPACK routines on tall and skinnymatrices. The idea of Allreduce algorithms can be extended to 2D block-cyclic LU or QRfactorization. We will not review Gram-Schmidt algorithms and Householder algorithms butsay a few words on the CholeskyQR algorithm. To understand the CholeskyQR algorithm;we need to know that; for a nonsingular m–by–n matrix A; m≥ n; the Cholesky factor of thenormal equations of A is the R-factor of the QR-factorization of A (see eg [6; Exercise 23.1 …,*,*,*
Notes on the Convergence of the Restarted GMRES,Eugene Vecharynski; Julien Langou,Figure 1: GMRES–DR (15; 5); full GMRES and GMRES (15) are run on SAYLR4; a matrix oforder 3564 from Matrix Market (left); comparison of MINRES solvers (GMRES's) andGalerkin projection solvers (FOM's) on the bidiagonal matrix with entries 0.01; 0.1; 1; 2;...;997; 998 on the main diagonal and 1's on the super diagonal (right).,*,*,*
The use of bulk information to improve the scalability of parallel band gap computations for quantum dots,Andrew Canning; Julien Langou; Osni Marques; Stanimire Tomov; Christof Vömel; Lin-Wang Wang,Abstract. We consider the parallel computation of interior eigenstates of large Hermitianmatrices arising from plane-wave discretization of effective single-particle Schrödingerequations. We apply a folded-spectrum approach based on Preconditioned ConjugateGradient (PCG) to compute only a small number of eigenstates close to the band gap whoselocation determines electronic and optical properties of the system. We show how to improvethe scalability of the eigensolver from observing that the quantum dot band states can bewell approximated by states of bulk systems. We make use of these cheaply computablebulk eigenstates to improve the choice of the starting vector and the preconditioner for theeigensolver.,*,*,*
Interior state computation of nano structures,Andrew Canning; Jack Dongarra; Julien Langou; Osni Marques; Stanimire Tomov; Christof Voemel; Lin-Wang Wang,Abstract. We are concerned with the computation of electronic and optical properties ofquantum dots. Using the Energy SCAN (ESCAN) method with empirical pseudopotentials;we compute interior eigenstates around the band gap which determine their properties.Numerically; this interior Hermitian eigenvalue problem poses several challenges; both withrespect to accuracy and efficiency. Using these criteria; we evaluate several state-of-the artpreconditioned iterative eigensolvers on a range of CdSe quantum dots of various sizes. Allthe iterative eigensolvers are seeking for the minimal eigenvalues of the folded operator withreference shift in the band-gap. The tested methods include standard Conjugate-Gradient(CG)-based Rayleigh-Quotient minimization; Locally Optimal Block-Preconditioned CG(LOBPCG) and two variants of the Jacobi Davidson method: JDQMR and GD+ 1. Our …,*,*,*
