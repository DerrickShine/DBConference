Efficient structural joins on indexed XML documents,Shu-Yao Chien; Zografoula Vagena; Donghui Zhang; Vassilis J Tsotras; Carlo Zaniolo,This chapter proposes efficient structural join algorithms in the presence of tag indices.Queries on XML documents typically combine selections on element contents; and; via pathexpressions; the structural relationships between tagged elements. Structural joins are usedto find all pairs of elements satisfying the primitive structural relationships specified in thequery; namely; parent–child and ancestor descendant relationships. Efficient support forstructural joins is thus the key to efficient implementations of XML queries. The recentlyproposed node numbering schemes enable the capturing of the XML document structureusing traditional indices. The problem of managing and querying XML documents efficientlypose interesting challenges for database researchers. XML documents can have a rathercomplex internal structure; in fact; an XML document can be viewed as an ordered tree …,*,2002,453
Advanced database systems,Carlo Zaniolo,The database field has experienced a rapid and incessant growth since the development ofrelational databases. The progress in database systems and applications has produced adiverse landscape of specialized technology areas that have often become the exclusivedomain of research specialists. Examples include active databases; temporal databases;object-oriented databases; deductive databases; imprecise reasoning and queries; andmultimedia information systems. This book provides a systematic introduction to and an in-depth treatment of these advanced database areas. It supplies practitioners and researcherswith authoritative coverage of recent technological advances that are shaping the future ofcommercial database systems and intelligent information systems. Advanced DatabaseSystems was written by a team of six leading specialists who have made significant …,*,1997,408
Database relations with null values,Carlo Zaniolo,Abstract A new formal approach is proposed for modeling incomplete database informationby means of null values. The basis of our approach is an interpretation of nulls whichobviates the need for more than one type of null. The conceptual soundness of thisapproach is demonstrated by generalizing the formal framework of the relational data modelto include null values. In particular; the set-theoretical properties of relations with nulls arestudied and the definitions of set inclusion; set union; and set difference are generalized. Asimple and efficient strategy for evaluating queries in the presence of nulls is provided. Theoperators of relational algebra are then generalized accordingly. Finally; the deep-rootedlogical and computational problems of previous approaches are reviewed to emphasize thesuperior practicability of the solution.,Journal of Computer and System Sciences,1984,327
Optimization of Nonrecursive Queries.,Ravi Krishnamurthy; Haran Boral; Carlo Zaniolo,Abstract State-of-the-art optimization approaches for relational database systems; eg; thoseused in systems such as OBE; SQL/DS; and commercial INGRES. when used for queries innon-traditional database applications; suffer from two problems. First; the time complexity oftheir optimization algorithms; being combinatoric; is exponential in the number of relations tobe joined in the query. Their cost is therefore prohibitive in situations such as deductivedatabases and logic oriented languages for knowledge bases; where hundreds of joins maybe required. The second problem with the traditional approaches is that; albeit effective intheir specific domain; it is not clear whether they can be generalized to differentscenarios(eg parallel evaluation) since they lack a formal model to define the assumptionsand critical factors on which their valiclity depends. This paper proposes a solution to …,VLDB,1986,325
The database language GEM,Carlo Zaniolo,Abstract GEM (an acronym for General Entity Manipulator) is a general-purpose query andupdate language for the DSIS data model; which is a semantic data model of the Entity-Relationship type. GEM is designed as an easy-to-use extension of the relational languageQUEL; providing support for the notions of entities with surrogates; aggregation;generalization; null values; and set-valued attributes.,ACM Sigmod Record,1983,300
Stable models and non-determinism in logic programs with negation,Domenico Sacca; Carlo Zaniolo,Abstract Previous researchers have proposed generalizations of Horn clause logic tosupport negation and non-determinism as two separate extensions. In this paper; we showthat the stable model semantics for logic programs provides a unified basis for the treatmentof both concepts. First; we introduce the concepts of partial models; stable models; stronglyfounded models and deterministic models and other interesting classes of partial modelsand study their relationships. We show that the maximal deterministic model of a program isa subset of the intersection of all its stable models and that the well-founded model of aprogram is a subset of its maximal deterministic model. Then; we show that the use of stablemodels subsumes the use of the non-deterministic choice construct in LDL and provides analternative definition of the semantics of this construct. Finally; we provide a constructive …,Proceedings of the ninth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1990,281
The LDL system prototype,Danette Chimenti; Ruben Gamboa; Ravi Krishnamurthy; Shamim Naqvi; Shalom Tsur; Carlo Zaniolo,The logic data language (LDL) system provides a declarative logic-based language andintegrates relational database and logic programming technologies so as to supportadvanced data and knowledge-based applications. A comprehensive overview of thesystem and a description of LDL language and the compilation techniques employed totranslate LDL queries into target query execution plans on the stored data are presented.The architecture and runtime environment of the system and the optimization techniquesemployed in order to improve the performance and assure the safety of the compiled queriesare given. The experience gained so far with the system and application areas where theLDL approach appears to be particularly effective are discussed.,IEEE Transactions on Knowledge and Data Engineering,1990,251
Query languages and data models for database sequences and data streams,Yan-Nei Law; Haixun Wang; Carlo Zaniolo,Abstract We study the fundamental limitations of relational algebra (RA) and SQL insupporting sequence and stream queries; and present effective query language and datamodel enrichments to deal with them. We begin by observing the well-known limitations ofSQL in application domains which are important for data streams; such as sequence queriesand data mining. Then we present a formal proof that; for continuous queries on datastreams; SQL suffers from additional expressive power problems. We begin by focusing onthe notion of nonblocking (NB) queries that are the only continuous queries that can besupported on data streams. We characterize the notion of nonblocking queries by showingthat they are equivalent to monotonic queries. Therefore the notion of NB-completeness forRA can be formalized as its ability to express all monotonic queries expressible in RA …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,224
Analysis and design of relational schemata for database systems.,Carlo Antonio Zaniolo,*,*,1976,208
LDL: a logic-based data-language,Shalom Tsur; Carlo Zaniolo,Abstract In this paper we describe the considerations that led us to the design of LDL andnrovide an overview of the features of-this language. L $ L is designed to combine theflexibility of logic programming with the high performance of the relational databasetechnology. The design offers an improved mode of control over the existing logicprogramming languages together with an enriched repertoire of data objects and constructs;including: sets; updates and negation. These advantages are realized by means of acompilation technology.,VLDB,1986,205
Graceful database schema evolution: the prism workbench,Carlo A Curino; Hyun J Moon; Carlo Zaniolo,Abstract Supporting graceful schema evolution represents an unsolved problem fortraditional information systems that is further exacerbated in web information systems; suchas Wikipedia and public scientific databases: in these projects based on multipartycooperation the frequency of database schema changes has increased while tolerance fordowntimes has nearly disappeared. As of today; schema evolution remains an error-proneand time-consuming undertaking; because the DB Administrator (DBA) lacks the methodsand tools needed to manage and automate this endeavor by (i) predicting and evaluatingthe effects of the proposed schema changes;(ii) rewriting queries and applications tooperate on the new schema; and (iii) migrating the database. Our PRISM system takes a bigfirst step toward addressing this pressing need by providing:(i) a language of Schema …,Proceedings of the VLDB Endowment,2008,171
Efficient management of multiversion documents by object referencing,Shu-Yao Chien; Vassilis J Tsotras; Carlo Zaniolo,Abstract Traditional approaches to versioning documents are edit-based; and representsuccessive versions using edit scripts. This paper proposes a reference-based versioningscheme that preserves the rich logical structure of the evolving document via objectreferences. This approach produces better support for queries; and reconciles the storage-level and transport-level representations of multiversioned XML documents. In particular; wepresent efficient algorithms for supporting projection and selection queries; and for queryingthe document evolution history. Then; we show that our representation is also efficient at thetransport level; where XML documents are exchanged between remote parties. In fact; withthe reference-based scheme; an XML document's history can also be viewed and processedas yet another XML document. Finally; we demonstrate the effectiveness of the new …,VLDB,2001,157
Fast and light boosting for adaptive mining of data streams,Fang Chu; Carlo Zaniolo,Abstract Supporting continuous mining queries on data streams requires algorithms that (i)are fast;(ii) make light demands on memory resources; and (iii) are easily to adapt to conceptdrift. We propose a novel boosting ensemble method that achieves these objectives. Thetechnique is based on a dynamic sample-weight assignment scheme that achieves theaccuracy of traditional boosting without requiring multiple passes through the data. Thetechnique assures faster learning and competitive accuracy using simpler base models. Thescheme is then extended to handle concept drift via change detection. The change detectionapproach aims at significant data changes that could cause serious deterioration of theensemble performance; and replaces the obsolete ensemble with one built from scratch.Experimental results confirm the advantages of our adaptive boosting scheme over …,Pacific-Asia Conference on Knowledge Discovery and Data Mining,2004,141
Object-oriented programming in Prolog,Carlo Zaniolo,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Proc. of 1985 International Symp. on Logic Programming; IEEE,1985,140
Optimization of sequence queries in database systems,Reza Sadri; Carlo Zaniolo; Amir Zarkesh; Jafar Adibi,Abstract The need to search for complex and recurring patterns in database sequences isshared by many applications. In this paper; we discuss how to express and supportefficiently sophisticated sequential pattern queries in databases. Thus; we first introduceSQL-TS; an extension of SQL; to express these patterns; and then we study how to optimizesearch queries for this language. We take the optimal text search algorithm of Knuth; Morrisand Pratt; and generalize it to handle complex queries on sequences. Our algorithm exploitsthe inter-dependencies between the elements of a sequential pattern to minimize repeatedpasses over the same data. Experimental results on typical sequence queries; such asdouble bottom queries; confirm that substantial speedups are achieved by our newoptimization techniques.,Proceedings of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2001,135
Negation and aggregates in recursive rules: the LDL++ approach,Carlo Zaniolo; Natraj Arni; KayLiang Ong,Abstract The problem of allowing non-monotonic constructs; such as negation andaggregates; in recursive programs represents a difficult challenge faced by current researchin deductive databases. In this paper; we present a solution that combines generality withefficiency; as demonstrated by its implementation in the new LDL++ system. A novel andgeneral treatment of set aggregates; allowing for user-defined aggregates; is alsopresented.,International Conference on Deductive and Object-Oriented Databases,1993,134
Metaqueries for data mining,Wei-Min Shen; KayLiang Ong; Carlo Zaniolo,Abstract This chapter presents a framework that uses metaqueries to integrate inductivelearning methods with deductive database technologies in the context of knowledgediscovery from databases. Metaqueries are second-order predicates or templates; and areused for (1) Guiding deductive data collection;(2) Focusing attention for inductive learning;and (3) Assisting human analysts in the discovery loop. We describe in detail a system thatuses this idea to unify a Bayesian Data Cluster with the Logical Data Language (LDL++);and show the results of three case studies; namely; discovering regularities from aknowledge base; discovering patterns and errors from a large telecommunication database;and discovering patterns and errors from a large chemical database. The patternsdiscovered using metaqueries are implication rules with probabilities. These rules can …,*,1996,130
The representation and deductive retrieval of complex objects,Carlo Zaniolo,Abstract The Relational Data Model and Relational Calculus are extended with Unificationand non-recursive Horn Clauses from Logic. The benefits gained include better versatilityand a richer functionality for expressing complex Jack; deductive queries and rule-basedinfer-ences. Applications include semantic data models for Databases; frames forKnowledge-based systems; and Complex Objects for CAD. An Extended Relational Algebra(ERA) is introduced that has the same expres-sive power as the new Calculus. Thealgorithm given for translating from Calculus to ERA supplies a sound basis for thecompilation of these Horn clauses; and their implementation using query optimization andother techniques currently used in database systems.,Proceedings of the 11th international conference on Very Large Data Bases-Volume 11,1985,124
The generalized counting method for recursive logic queries,Domenico Sacca; Carlo Zaniolo,Abstract This paper treats the problem of implementing efficiently recursive Horn clausesqueries; including those with function symbols. In particular; the situation is studied wherethe initial bindings of the arguments in the recursive query goal can be used in the top-down(as in backward chaining) execution phase to improve the efficiency and; often; to guaranteethe termination of the forward chaining execution phase that implements the fixpointcomputation for the recursive query. A general method is given for solving these queries; themethod performs an analysis of the binding-passing behavior of the query; and thenreschedules the overall execution as two fixpoint computations derived as results of thisanalysis. The first such computation emulates the propagation of bindings in the top-downphase; the second generates the desired answer by proving the goals left unsolved …,Theoretical Computer Science,1986,122
A data stream language and system designed for power and extensibility,Yijian Bai; Hetal Thakkar; Haixun Wang; Chang Luo; Carlo Zaniolo,Abstract By providing an integrated and optimized support for user-defined aggregates(UDAs); data stream management systems (DSMS) can achieve superior power andgenerality while preserving compatibility with current SQL standards. This is demonstratedby the Stream Mill system that; through is Expressive Stream Language (ESL); efficientlysupports a wide range of applications-including very advanced ones such as data streammining; streaming XML processing; time-series queries; and RFID event processing. ESLsupports physical and logical windows (with optional slides and tumbles) on both built-inaggregates and UDAs; using a simple framework that applies uniformly to both aggregatefunctions written in an external procedural languages and those natively written in ESL. Theconstructs introduced in ESL extend the power and generality of DSMS; and are …,Proceedings of the 15th ACM international conference on Information and knowledge management,2006,121
Efficient schemes for managing multiversionXML documents,S-Y Chien; Vassilis J Tsotras; Carlo Zaniolo,Abstract Multiversion support for XML documents is needed in many critical applications;such as software configuration control; cooperative authoring; web information warehouses;and” e-permanence” of web documents. In this paper; we introduce efficient and robusttechniques for:(i) storing and retrieving;(ii) viewing and exchanging; and (iii) queryingmultiversion XML documents. We first discuss the limitations of traditional version controlmethods; such as RCS and SCCS; and then propose novel techniques that overcome theirlimitations. Initially; we focus on the problem of managing secondary storage efficiently; andintroduce an edit-based versioning scheme that enhances RCS with an effective clusteringpolicy based on the concept of page-usefulness. The new scheme drastically improvesversion retrieval at the expense of a small (linear) space overhead. However; the edit …,The VLDB Journal—The International Journal on Very Large Data Bases,2002,117
Storing and querying multiversion XML documents using durable node numbers,Shu-Yao Chien; Vassilis J Tsotras; Carlo Zaniolo; Donghui Zhang,Managing multiple versions of XML documents represents an important problem for manytraditional applications; such as software configuration control; as well as new ones; such aslink permanence of web documents. Research on managing multiversion XML documentsseeks to provide efficient and robust techniques for storing; retrieving and querying suchdocuments. In this paper we present a novel approach to version management that achievesthese objectives by a scheme based on Durable Node Numbers and timestamps for theelements of XML documents. We first present efficient storage and retrieval techniques formultiversion documents. Then; we explore the indexing and clustering strategies needed toassure efficient support for complex queries on content and on document evolution.,Web Information Systems Engineering; 2001. Proceedings of the Second International Conference on,2001,117
RFID data processing with a data stream query language,Yijian Bai; Fusheng Wang; Peiya Liu; Carlo Zaniolo; Shaorong Liu,RFID technology provides significant advantages over traditional object-trackingtechnologies and is increasingly adopted and deployed in real applications. RFIDapplications generate large volume of streaming data; which have to be automaticallyfiltered; processed; and transformed into semantic data; and integrated into businessapplications. Indeed; RFID data are highly temporal; and RFID observations form complextemporal event patterns which can be very different for various RFID applications. Thus; it isdesirable to have a general RFID data processing framework with a powerful language; forthe end users to express a variety of queries on RFID data streams; as well as detectingcomplex events patterns. While data stream management systems (DSMSs) are emergingfor optimized stream data processing; they usually lack the language construct support for …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,108
XML document versioning,Shu Yao Chien; Vassilis J Tsotras; Carlo Zaniolo,Abstract Managing multiple versions of XML documents represents an important problem;because of many applications ranging from traditional ones; such as software configurationcontrol; to new ones; such as link permanence of web documents. Research on managingmultiversion XML documents seeks to provide efficient and robust techniques for (i) storingand retrieving;(ii) exchanging; and (iii) querying such documents. In this paper; we first showthat traditional version control methods; such as RCS; and SCCS; fall short from satisfyingthese three requirements; and discuss alternative solutions. First; we enhance RCS with atemporal page clustering policy to achieve objective (i). Then; we discuss a reference-basedversioning scheme that achieves both objectives (i) and (ii) and is also effective atsupporting simple queries. The topic of supporting complex queries; including temporal …,ACM SIGMOD Record,2001,107
An implementation of GEM: supporting a semantic data model on a relational back-end.,Shalom Tsur; Carlo Zaniolo,Abstract This paper presents a simple approach for extending the relational system INGRESinto one supporting a semantic data model it describe a DBMS consisting of (i) a user-friendly front-end; supporting the GEM semantic data model and query language under theUNIX time-sharing system; and (ii) a dedicated back-end processor providing efficientsupport for database transactions; concurrency control and recovery GEM extends therelational model to support the notions of entities with surrogates; the relationships ofaggregation and generalization; null values and set-valued attributes; and provides simpleextensions of QUEL to handle these new constructs in this proposed implementation ofGEM; the relational database processor IDM 500 by Britton-Lee is used as the back-endmachine,ACM SIGMOD Record,1984,106
Early accurate results for advanced analytics on mapreduce,Nikolay Laptev; Kai Zeng; Carlo Zaniolo,Abstract Approximate results based on samples often provide the only way in whichadvanced analytical applications on very massive data sets can satisfy their time andresource constraints. Unfortunately; methods and tools for the computation of accurate earlyresults are currently not supported in MapReduce-oriented systems although these areintended for'big data'. Therefore; we proposed and implemented a non-parametric extensionof Hadoop which allows the incremental computation of early results for arbitrary work-flows;along with reliable on-line estimates of the degree of accuracy achieved so far in thecomputation. These estimates are based on a technique called bootstrapping that has beenwidely employed in statistics and can be applied to arbitrary functions and data distributions.In this paper; we describe our Early Accurate Result Library (EARL) for Hadoop that was …,Proceedings of the VLDB Endowment,2012,105
Verifying and mining frequent patterns from large windows over data streams,Barzan Mozafari; Hetal Thakkar; Carlo Zaniolo,Mining frequent itemsets from data streams has proved to be very difficult because ofcomputational complexity and the need for real-time response. In this paper; we introduce anovel verification algorithm which we then use to improve the performance of monitoring andmining tasks for association rules. Thus; we propose a frequent itemset mining method forsliding windows; which is faster than the state-of-the-art methods¿ in fact; its running timethat is nearly constant with respect to the window size entails the mining of much largerwindows than it was possible before. The performance of other frequent itemset miningmethods (including those on static data) can be improved likewise; by replacing theircounting methods (eg; those using hash trees) by our verification algorithm.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,104
Magic counting methods,Domenico Sacca; Carlo Zaniolo,Abstract The problem considered is that of implementing recursive queries; expressed in alogic-based language; by efficient fixpoint computations. In particular; the situation is studiedwhere the initial bindings in the recursive predicate can be used to restrict the search spaceand ensure safety of execution. Two key techniques previously proposed to solve thisproblem are (i) the highly efficient counting method; and (ii) the magic set method which issafe in a wider range of situations than (i). In this paper; we present a family of methods;called the magic counting methods; that combines the advantages of (i) and (ii). This ismade possible by the similarity of the strategies used by the counting method and the magicset method for propagating the bindings. This paper introduces these new methods;examines their computational complexity; and illustrates the trade-offs between the family …,ACM SIGMOD Record,1987,103
An adaptive nearest neighbor classification algorithm for data streams,Yan-Nei Law; Carlo Zaniolo,Abstract In this paper; we propose an incremental classification algorithm which uses a multi-resolution data representation to find adaptive nearest neighbors of a test point. Thealgorithm achieves excellent performance by using small classifier ensembles whereapproximation error bounds are guaranteed for each ensemble size. The very low updatecost of our incremental classifier makes it highly suitable for data stream applications. Testsperformed on both synthetic and real-life data indicate that our new classifier outperformsexisting algorithms for data streams in terms of accuracy and computational costs.,European Conference on Principles of Data Mining and Knowledge Discovery,2005,102
Expressing and optimizing sequence queries in database systems,Reza Sadri; Carlo Zaniolo; Amir Zarkesh; Jafar Adibi,Abstract The need to search for complex and recurring patterns in database sequences isshared by many applications. In this paper; we investigate the design and optimization of aquery language capable of expressing and supporting efficiently the search for complexsequential patterns in database systems. Thus; we first introduce SQL-TS; an extension ofSQL to express these patterns; and then we study how to optimize the queries for thislanguage. We take the optimal text search algorithm of Knuth; Morris and Pratt; andgeneralize it to handle complex queries on sequences. Our algorithm exploits theinterdependencies between the elements of a pattern to minimize repeated passes over thesame data. Experimental results on typical sequence queries; such as double bottomqueries; confirm that substantial speedups are achieved by our new optimization …,ACM Transactions on Database Systems (TODS),2004,102
Schema evolution in wikipedia: toward a web information system benchmark,Carlo A Curino; Letizia Tanca; Hyun J Moon; Carlo Zaniolo,Abstract Evolving the database that is at the core of an Information System represents adifficult maintenance problem that has only been studied in the framework of traditionalinformation systems. However; the problem is likely to be even more severe in webinformation systems; where open-source software is often developed through thecontributions and collaboration of many groups and individuals. Therefore; in this paper; wepresent an indepth analysis of the evolution history of the Wikipedia database and itsschema; Wikipedia is the best-known example of a large family of web information systemsbuilt using the open-source software MediaWiki. Our study is based on:(i) a set of SchemaModification Operators that provide a simple conceptual representation for complex schemachanges; and (ii) simple software tools to automate the analysis. This framework allowed …,In International Conference on Enterprise Information Systems (ICEIS,2008,98
A unified semantics for active and deductive databases,Carlo Zaniolo,Abstract These two rule-oriented paradigms of databases have been the focus of extensiveresearch and are now coming of age in the commercial DBMS world. However; the systemsdeveloped so far support well only one of the two paradigms—thus limiting the effectivenessof such systems in many applications that require complete integration of both kinds of rules.In this paper; we discuss the technical problems that make such an integration difficult; andtrace their roots to a lack of a unified underlying semantics. Then; we review recentadvances in the semantics of non-monotonic logic and show that they can be used to unifythe foundations of active databases and deductive databases. Finally; we outline the designa new rule language for databases that integrates a deductive system with a trigger-basedDBMS.,*,1994,97
On the implementation of a simple class of logic queries for databases,Domenico Saccà; Carlo Zaniolo,The paper is organized as follows. In Section 2; we define CSL queries and study thebinding propagation problem. In Section 3; we focus on l-bound CSL queries; where thebinding propagates to a single (but not always the same)'argument of the recursivepredicate. In Section 4; we study the problem of implementing these queries. We use aunifying framework to provide a simple description of the following four methods: thecounting method (informally described in p+]); the eager method (similar to that in [HN]); themagic set method (presented in [B+]); and a new method here introduced; called magiccounting; which combines the advantages of the first and the third. Extensions to and proofsof these results are given in [SZ].+~~~ work was done while thii author was visiting at MCC.Permission to copy without fee all or part of this material is granted provided that the …,Proceedings of the fifth ACM SIGACT-SIGMOD symposium on Principles of database systems,1985,96
Composite temporal events in active database rules: A logic-oriented approach,Iakovos Motakis; Carlo Zaniolo,Abstract Several database systems support active rules; and are currently being extendedwith languages for detecting complex patterns of temporal events. These languages haveused for their definition and implementation; an assortment of formalisms ranging from Finite-State Machines; to Petri Nets and Event Graphs. In this paper; we show that the semantics ofdeductive databases supply a more general and complete basis for the definition of suchlanguages. In fact; we develop a model; whereby an active rule with a composite event partcan be viewed as an equivalent set of Datalog 1 S rules. We use this approach to providethe complete formal specification of the EPL system developed at UCLA and wedemonstrate its generality by modeling concepts and constructs of other systems.,International Conference on Deductive and Object-Oriented Databases,1995,95
Relating Stable Models and AI Planning Domains.,VS Subrahmanian; Carlo Zaniolo,Abstract In this paper; we show that there is a simple connection between logicprogramming and planning. The main result of this paper is the following: given anyplanning domain consisting of an initial state; and a set of operation definitions; this domaincan be translated; in linear-time; to a logic program such that a given goal G is achievable inthe planning domain iff a related goal G* is true in some stable model of the logic programobtained by the translation. We show that this translation yields at least two interestingconsequences:(1) methods to update databases can be used to handle surprises whenexecuting plans (ie a surprise occurs when an initial plan is partly executed; but one of theresulting intermediate states differs; perhaps due to external reasons; from what ispredicted).(2) rigid actions; which are actions that must be executed when their pre …,ICLP,1995,93
Design of relational views over network schemas,Carlo Zaniolo,Abstract An algorithm is presented for designing relational views over network schemasto:(1) support general query and update capability;(2) preserve the information content of thedata base and (3) provide independence from its physical organization. The proposedsolution is applicable to many existing CODASYL databases without data or schemaconversion. The particular declarations of a CODASYL schema which supply sources oflogical data definition are first identified. Then the view design algorithm is derived on thebasis of a formal analysis of the semantic constraints established by these declarations. Anew form of data structure diagram is also introduced to visualize these constraints.,Proceedings of the 1979 ACM SIGMOD international conference on Management of data,1979,93
Prolog: a database query language for all seasons,Carlo Zaniolo,Google; Inc. (search) …,Proceedings from the first international workshop on Expert database systems,1986,92
Temporal aggregation in active database rules,Iakovos Motakis; Carlo Zaniolo,Abstract An important feature of many advanced active database prototypes is support forrules triggered by complex patterns of events. Their composite event languages providepowerful primitives for event-based temporal reasoning. In fact; with one importantexception; their expressive power matches and surpasses that of sophisticated languagesoffered by Time Series Management Systems (TSMS); which have been extensively used fortemporal data analysis and knowledge discovery. This exception pertains to temporalaggregation; for which; current active database systems offer only minimal support; if any. Inthis paper; we introduce the language TREPL; which addresses this problem. The TREPLprototype; under development at UCLA; offers primitives for temporal aggregation thatexceed the capabilities of state-of-the-art composite event languages; and are …,ACM SIGMOD Record,1997,90
-ATLAS: A Small but Complete SQL Extension for Data Mining and Data Streams,Haixun Wang; Carlo Zaniolo; Chang Richard Luo,This chapter implements ATLAS; a powerful database language and system that enablesusers to develop complete data-intensive applications in structured query language (SQL)—by writing new aggregates and table functions in SQL; rather than in procedural languagesas in current Object-Relational systems. As a result; ATLAS'SQL is Turing-complete; and isvery suitable for advanced data intensive applications; such as data mining and streamqueries. The ATLAS system is now available for download along with a suite of applicationsincluding various data mining functions that have been coded in ATLAS'SQL; and executewith a modest performance overhead with respect to the same applications written in C/C++.The demonstration reveals ATLAS'internal architecture by focusing on several of its keycomponents; such as the query rewrite module; the query-plan generation module; and …,*,2003,88
Optimal sampling from sliding windows,Vladimir Braverman; Rafail Ostrovsky; Carlo Zaniolo,Abstract A sliding windows model is an important case of the streaming model; where onlythe most “recent” elements remain active and the rest are discarded. The sliding windowsmodel is important for many applications (see; eg; Babcock; Babu; Datar; Motwani andWidom (PODS 02); and Datar; Gionis; Indyk and Motwani (SODA 02)). There are two equallyimportant types of the sliding windows model–windows with fixed size (eg; where itemsarrive one at a time; and only the most recent n items remain active for some fixed parametern); and timestamp-based windows (eg; where many items can arrive in “bursts” at a singlestep and where only items from the last t steps remain active; again for some fixed parametert). Random sampling is a fundamental tool for data streams; as numerous algorithmsoperate on the sampled data instead of on the entire stream. Effective sampling from …,Journal of Computer and System Sciences,2012,86
SQLST: A spatio-temporal data model and query language,Cindy Xinmin Chen; Carlo Zaniolo,Abstract In this paper; we propose a query language and data model for spatio-temporalinformation; including objects of time-changing geometry. Our objective is to minimize theextensions required in SQL; or other relational languages; to support spatio-temporalqueries. We build on the model proposed by Worboys where each state of a spatial object iscaptured as a snapshot of time; then; we use a directed-triangulation model to representspatial data; and a point-based model to represent time at the conceptual level. Spatio-temporal reasoning and queries can be fully expressed with no new constructs; but user-defined aggregates; such as AREA and INSIDE for spatial relationships; DURATION andCONTAIN for temporal ones; and MOVING_DISTANCE for spatio-temporal ones. We alsoconsider the implementation problem under the assumption that; for performance reasons …,International Conference on Conceptual Modeling,2000,85
Version management of XML documents,Chien Shu-Yao; Vassilis J Tsotras; Carlo Zaniolo,Abstract The problem of ensuring efficient storage and fast retrieval for multi-versionstructured documents is important because of the recent popularity of XML documents andsemistructured information on the web. Traditional document version control systems; egRCS; which model documents as a sequence of lines of text and use the shortest edit scriptto represent version differences; can be inefficient and they do not preserve the logicalstructure of the original document. Therefore; we propose a new approach where thestructure of the documents is preserved intact; and their sub-objects are timestampedhierarchically for efficient reconstruction of current and past versions. Our technique; calledthe Usefulness Based Copy Control (UBCC); is geared towards efficient versionreconstruction while using small storage overhead. Our analysis and experiments …,International Workshop on the World Wide Web and Databases,2000,78
On the design of relational database schemata,Carlo Zaniolo; Miachel A Meklanoff,Abstract The purpose of this paper is to present a new approach to the conceptual design ofrelational databases based on the complete relatability conditions (CRCs). It is shown thatcurrent database design methodology based upon the elimination of anomalies is notadequate. In contradistinction; the CRCs are shown to provide a powerful criticism fordecomposition. A decomposition algorithm is presented which (1) permits decomposition ofcomplex relations into simple; well-defined primitives;(2) preserves all the originalinformation; and (3) minimizes redundancy. The paper gives a complete derivation of theCRCs; beginning with a unified treatment of functional and multivalued dependencies; andintroduces the concept of elementary functional dependencies and multiple elementarymultivalued dependencies. Admissibility of covers and validation of results are also …,ACM Transactions on Database Systems (TODS),1981,78
Using SQL to build new aggregates and extenders for object-relational systems,Haixun Wang; Carlo Zaniolo,Note: OCR errors may be found in this Reference List extracted from the full text article. ACMhas opted to expose the complete List rather than only correct and linked references … {7} CharlesElkan. "Boosting and Naive Bayesian Learning". Technical report no cs97-557; Dept. of ComputerScience and Engineering; UCSD; September 1997 … {9} Jeijun Kong; Cindy Chen and CarloZaniolo: A Temporal Extension of SQL for Object Relational Databases; submitted forpublication; 2000 … {10} ISO/IEC JTC1/SC21 N10489; ISO//IEC 9075; "Committee Draft(CD); Database Language SQL"; July 1996 … {11} ISO DBL LHR-004 and ANSIX3H2-95-364; "(ISO/ANSI Working Draft) Database language SQL3"; Jim Melton (ed); dated1995.,Proceedings of the 26th International Conference on Very Large Data Bases,2000,77
Managing and querying transaction-time databases under schema evolution,Hyun J Moon; Carlo A Curino; Alin Deutsch; Chien-Yi Hou; Carlo Zaniolo,Abstract The old problem of managing the history of database information is now made moreurgent and complex by fast spreading web information systems; such as Wikipedia. OurPRIMA system addresses this difficult problem by introducing two key pieces of newtechnology. The first is a method for publishing the history of a relational database in XML;whereby the evolution of the schema and its underlying database are given a unifiedrepresentation. This temporally grouped representation makes it easy to formulatesophisticated historical queries on any given schema version using standard XQuery. Thesecond key piece of technology is that schema evolution is transparent to the user: shewrites queries against the current schema while retrieving the data from one or moreschema versions. The system then performs the labor-intensive and error-prone task of …,Proceedings of the VLDB Endowment,2008,76
Non-determinism in deductive databases,Fosca Giannotti; Dino Pedreschi; Domenico Sacca; Carlo Zaniolo,Abstract This paper examines the problem of adding non-deterministic constructs to adeclarative database language based on Horn Clause Logic. We revise a previouslyproposed approach; the choice construct introduced by Krishnamurthy and Naqvi; from theviewpoints of amenability to efficient implementation and expressive power. Thus; we definea construct called dynamic choice; which is consistent with the fixpoint-based semantics;cures the deficiencies of the former approach; and leads to efficient implementations in theframework of deductive databases. Also the new construct extends the expressive power ofDatalog programs considerably; as it allows to express negation under Closed WorldAssumption; as well as a class of relevant deterministic problems.,International Conference on Deductive and Object-Oriented Databases,1991,75
Atlas: A native extension of sql for data mining,Haixun Wang; Carlox Zaniolo,Abstract A lack of power and extensibility in their query languages has seriously limited thegenerality of DBMSs and hampered their ability to support data mining applications. Thus;there is a pressing need for more general mechanisms for extending DBMSs to supportefficiently database-centric data mining appliacations. To satisfy this need; we propose anew extensibility mechanism for SQL-compliant DBMSs; and demonstrate its power insupporting decision support applications. The key extension is the ability of defining newtable functions and aggregate functions in SQL—rather than in external procedurallanguages as Object-Relational (OR) DBMSs currently do. This simple extension turns SQLinto a powerful language for decision-support applications; including ROLAPs; time-seriesqueries; stream-oriented processing; and data mining functions. First; we discuss the use …,*,2003,74
Efficient complex query support for multiversion XML documents,Shu-Yao Chien; Vassilis J Tsotras; Carlo Zaniolo; Donghui Zhang,Abstract Managing multiple versions of XML documents represents a critical requirement formany applications. Also; there has been much recent interest in supporting complex querieson XML data (eg; regular path expressions; structural projections; DIFF queries). In thispaper; we examine the problem of supporting efficiently complex queries on multiversionedXML documents. Our approach relies on a scheme based on durable node numbers (DNNs)that preserve the order among the XML tree nodes and are invariant with respect to updates.Using the document's DNNs various complex queries are reduced to combinations of partialversion retrieval queries. We examine three indexing schemes to efficiently evaluate partialversion retrieval queries in this environment. A thorough performance analysis is thenpresented to reveal the advantages of each scheme.,International Conference on Extending Database Technology,2002,72
Implementation of recursive queries for a data language based on pure horn logic,Domenico Sacca; Carlo Zaniolo,Abstract This paper treats the problem of implementing efficiently queries expressed by Hornclauses containing recursive predicates; including those with function symbols. In particular;the situation is studied where the initial bindings of the arguments in the recursive querygoal can be used in the top-down (as in backward chaining) execution phase to improve theefficiency and; often; to guarantee the termination of the forward chaining execution phasethat implements the fixpoint computation for a recursive query.(To ensure efficient support fordatabase applications this fixpoint computation is actually carried out by relational algebraoperators.) A general approach is given for solving these queries; the approach performs ananalysis of the binding passing behavior of the query; and then reschedules the overallexecution as two fixpoint computations derived as results of this analysis. One such …,ICLP,1987,71
Safety and Compilation of Non-recursive Horn Clauses.,Carlo Zaniolo,*,Expert Database Conf.,1986,70
Minimum and maximum predicates in logic programming,Sumit Ganguly; Sergio Greco; Carlo Zaniolo,Abstract A novel approach is proposed for ezpresaing and computing eficienily a large cla88of problem8; including jinding the shortest path in a graph; that were previously consideredimpervious to an efiient treatment in the declarative framework of logic-baaed languageu.Our approach w based on the u8e of ruin and nmx predicate having a jht-order semanticadefined using mleu w “th negation in their bodien. We show that when certain monotonicitycondition8 hold then (1) there ezists a total well-founded model for these progmrnncontaining negation;(2) this model can be computed eflciently using a procedure calledgreedy fixpoint; and (3) the original program can be rewritten into a more eficient one bypuuhing rnin and max predicate8 into recursion. The greedy jizpoint evaluation of theprogram expressing the shorted path problem coincideu with Dijkdra's algon” thm.,Proceedings of the tenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1991,69
The deductive database system [Lscr][Dscr][Lscr]++,Faiz Arni; KayLiang Ong; Shalom Tsur; Haixun Wang; Carlo Zaniolo,Abstract This paper describes the [Lscr][Dscr][Lscr]++ system and the research advancesthat have enabled its design and development. We begin by discussing the newnonmonotonic and nondeterministic constructs that extend the functionality of the[Lscr][Dscr][Lscr]++ language; while preserving its model-theoretic and fixpoint semantics.Then; we describe the execution model and the open architecture designed to support thesenew constructs and to facilitate the integration with existing DBMSs and applications. Finally;we describe the lessons learned by using [Lscr][Dscr][Lscr]++ on various tested applications;such as middleware and datamining.,Theory and Practice of Logic Programming,2003,67
Active database rules with transaction-conscious stable-model semantics,Carlo Zaniolo,Abstract Semantics represents a major problem area for active databases in as much as (i)there is no formal framework for defining an implementation-independent semantics ofactive rules; and (ii) the various systems developed so far have ad-hoc operationalsemantics that are widely different from each other. This situation contributes to the difficultyof predicting the run-time behavior of sets of rules: thus; ensuring the termination of a givenset of rules is currently viewed as a major research issue. In this paper; we introduce adurable change semantics for active database rules; this semantics improves Starburst'sdeferred activation notion with concepts taken from Postgres and Heraclitus and thesemantic foundations of deductive databases. We provide a formal logic-based model forthis transaction-oriented semantics; show that it is amenable to efficient implementation …,International Conference on Deductive and Object-Oriented Databases,1995,63
A new normal form for the design of relational database schemata,Carlo Zaniolo,Abstract This paper addresses the problem of database schema design in the framework ofthe relational data model and functional dependencies. It suggests that both Third NormalForm (3NF) and Boyce-Codd Normal Form (BCNF) supply an inadequate basis for relationalschema design. The main problem with 3NF is that it is too forgiving and does not enforcethe separation principle as strictly as it should. On the other hand; BCNF is incompatible withthe principle of representation and prone to computational complexity. Thus a new normalform; which lies between these two and captures the salient qualities of both is proposed.The new normal form is stricter than 3NF; but it is still compatible with the representationprinciple. First a simpler definition of 3NF is derived; and the analogy of this new definition tothe definition of BCNF is noted. This analogy is used to derive the new normal form …,ACM Transactions on Database Systems (TODS),1982,62
Design and implementation of a logic based language for data intensive applications,Carlo Zaniolo,Abstract Ongoing research on the Logic Data Language (LDL) pursues a new applicationfocus and a new implementation technology for Logic Programming. In LDL; the functionalityof Prolog is enhanced with full database capabilities; such as; schema facilities forextensional information; transaction management and recovery. The implementationtechnology of LDL is; however; very different from that of Prolog; since it is based onmechanisms suitable for secondary storage; such as matching and least fixpointcomputations; rather than on SLD-resolution and unification. This paper describes the mainresearch challenges tackled and the solution approaches taken in designing and buildingtwo experimental LDL systems. Thus; the following topics are discussed:(a) the design ofnon-Horn constructs (such as negation; sets and updates) and the definition of a formal …,ICLP/SLP,1988,59
Automating the database schema evolution process,Carlo Curino; Hyun Jin Moon; Alin Deutsch; Carlo Zaniolo,Abstract Supporting database schema evolution represents a long-standing challenge ofpractical and theoretical importance for modern information systems. In this paper; wedescribe techniques and systems for automating the critical tasks of migrating the databaseand rewriting the legacy applications. In addition to labor saving; the benefits delivered bythese advances are many and include reliable prediction of outcome; minimization ofdowntime; system-produced documentation; and support for archiving; historical queries;and provenance. The PRISM/PRISM++ system delivers these benefits; by solving the difficultproblem of automating the migration of databases and the rewriting of queries and updates.In this paper; we present the PRISM/PRISM++ system and the novel technology that made itpossible. In particular; we focus on the difficult and previously unsolved problem of …,The VLDB Journal,2013,58
Temporal queries in XML document archives and web warehouses,Fusheng Wang; Carlo Zaniolo,By storing the successive versions of a document in an incremental fashion; XMLrepositories and data warehouses achieve:(i) the efficient preservation of critical information;and (ii) the ability of supporting historical queries on the evolution of documents and theircontents. In this paper; we present efficient techniques for managing multi-version documenthistories and supporting powerful temporal queries on such documents. Our approachconsists in:(i) concisely representing the successive versions of a document as an XMLdocument that implements a temporally grouped data model; and (ii) using XML querylanguages; such as XQuery; to express complex queries on the content of a particularversion; and on the temporal evolution of the document elements and their contents. Weshow that the data definition & manipulation framework of XML & XQuery can support …,Temporal Representation and Reasoning; 2003 and Fourth International Conference on Temporal Logic. Proceedings. 10th International Symposium on,2003,58
XBiT: an XML-based bitemporal data model,Fusheng Wang; Carlo Zaniolo,Abstract Past research work on modeling and managing temporal information has; so far;failed to elicit support in commercial database systems. The increasing popularity of XMLoffers a unique opportunity to change this situation; inasmuch as XML and XQuery supporttemporal information much better than relational tables and SQL. This is the importantconclusion claimed in this paper where we show that valid-time; transaction-time; andbitemporal databases can be naturally viewed in XML using temporally-grouped datamodels. Then; we show that complex historical queries; that would be very difficult toexpress in SQL on relational tables; can now be easily expressed in standard XQuery onsuch XML-based representations. We first discuss the management of transaction-time andvalid-time histories and then extend our approach to bitemporal histories. The approach …,International Conference on Conceptual Modeling,2004,56
An adaptive learning approach for noisy data streams,Fang Chu; Yizhou Wang; Carlo Zaniolo,Two critical challenges typically associated with mining data streams are concept drift anddata contamination. To address these challenges; we seek learning techniques and modelsthat are robust to noise and can adapt to changes in timely fashion. We approach the stream-mining problem using a statistical estimation framework; and propose a fast and robustdiscriminative model for learning noisy data streams. We build an ensemble of classifiers toachieve timely adaptation by weighting classifiers in a way that maximizes the likelihood ofthe data. We further employ robust statistical techniques to alleviate the problem of noisesensitivity. Experimental results on both synthetic and real-life data sets demonstrate theeffectiveness of this model learning approach.,Data Mining; 2004. ICDM'04. Fourth IEEE International Conference on,2004,56
Object identity and inheritance in deductive databases—an evolutionary approach,Carlo Zaniolo,This paper proposes simple extensions to logic-based languages for deductive databases tosupport the key notions of object identity and object inheritance of object-oriented systems.Thus the paper shows how; given a logic program P and its minimal model M (P); it ispossible to construct a program P′ and its minimal model M (P′) such that M (P′) isbasically obtained from M (P) by prefixing each fact of M (P) with a unique identifier. This isaccomplished through the use of LDL non-deterministic choice constructs (or alternativelythrough negation under the stable model semantics). This allow a user to view facts andpredicates of ordinary logic programs as objects and refer to them through their identifiers—in both the extensional database and the intensional one. Then; syntactic shorthandconventions are introduced to facilitate reference based reasoning and access to objects …,*,1990,56
High-performance complex event processing over XML streams,Barzan Mozafari; Kai Zeng; Carlo Zaniolo,Abstract Much research attention has been given to delivering high-performance systemsthat are capable of complex event processing (CEP) in a wide range of applications.However; many current CEP systems focus on processing efficiently data having a simplestructure; and are otherwise limited in their ability to support efficiently complex continuousqueries on structured or semi-structured information. However; XML streams represent avery popular form of data exchange; comprising large portions of social network and RSSfeeds; financial records; configuration files; and similar applications requiring advanced CEPqueries. In this paper; we present the XSeq language and system that support CEP on XMLstreams; via an extension of XPath that is both powerful and amenable to an efficientimplementation. Specifically; the XSeq language extends XPath with natural operators to …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,54
The analytical bootstrap: a new method for fast error estimation in approximate query processing,Kai Zeng; Shi Gao; Barzan Mozafari; Carlo Zaniolo,Abstract Sampling is one of the most commonly used techniques in Approximate QueryProcessing (AQP)-an area of research that is now made more critical by the need for timelyand cost-effective analytics over" Big Data". Assessing the quality (ie; estimating the error) ofapproximate answers is essential for meaningful AQP; and the two main approaches used inthe past to address this problem are based on either (i) analytic error quantification or (ii) thebootstrap method. The first approach is extremely efficient but lacks generality; whereas thesecond is quite general but suffers from its high computational overhead. In this paper; weintroduce a probabilistic relational model for the bootstrap process; along with rigoroussemantics and a unified error model; which bridges the gap between these two traditionalapproaches. Based on our probabilistic framework; we develop efficient algorithms to …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,51
Formal semantics for composite temporal events in active database rules,Iakovos Motakis; Carlo Zaniolo,Abstract A major thrust of current research in active databases focuses on allowing complexpatterns of temporal events to serve as preconditions for rule triggering. Currently; there isno common formalism for specifying the semantics of composite event languages. Differentsystems have used an assortment of different techniques; including Finite State Automata;Petri Nets and Event Graphs. In this paper; we propose a unifying approach; based on asyntax-directed translation of composite event expressions into Datalog 1S rules; whoseformal semantics defines the meaning of the original expressions. We demonstrate ourmethod by providing a formal specification of the Event Pattern Language (EPL) developedat UCLA. This method overcomes problems and limitations affecting previous approachesand is applicable to other languages such as ODE; SNOOP and SAMOS—thus; allowing …,Journal of Systems Integration,1997,50
Programming with non-determinism in deductive databases,Fosca Giannotti; Sergio Greco; Domenico Saccà; Carlo Zaniolo,Abstract While non-determinism has long been established as a key concept in logic pro-gramming; its importance in the context of deductive databases was recognized onlyrecently. This paper provides an overview of recent results on this topic with the aim ofproviding an introduction to the theory and practice of non-determinism in deductivedatabases. In particular we (i) recall the main results linking non-deterministic constructs indatabase languages to the theory of data complexity and the expressibility hierarchy ofquery languages;(ii) provide a reasoned introduction to effective programming with non-deterministic constructs;(iii) compare the usage of non-deterministic constructs in languagessuch as LDL++ to that of traditional logic programming languages;(iv) discuss the linkbetween the semantics of logic programs with non-deterministic constructs and the stable …,Annals of Mathematics and Artificial Intelligence,1997,48
Update rewriting and integrity constraint maintenance in a schema evolution support system: PRISM++,Carlo A Curino; Hyun Jin Moon; Alin Deutsch; Carlo Zaniolo,Abstract Supporting legacy applications when the database schema evolves represents along-standing challenge of practical and theoretical importance. Recent work has producedalgorithms and systems that automate the process of data migration and query adaptation;however; the problems of evolving integrity constraints and supporting legacy updates underschema and integrity constraints evolution are significantly more difficult and have thus farremained unsolved. In this paper; we address this issue by introducing a formal evolutionmodel for the database schema structure and its integrity constraints; and use it to deriveupdate mapping techniques akin to the rewriting techniques used for queries. Thus; we (i)propose a new set of Integrity Constraints Modification Operators (ICMOs);(ii) characterizethe impact on integrity constraints of structural schema changes;(iii) devise …,Proceedings of the VLDB Endowment,2010,46
Compilation of set terms in the logic data language (LDL),Oded Shmueli; Shalom Tsur; Carlo Zaniolo,Abstract We propose compilation methods for the efficient support of set-term matching inHorn-clause programs. Rather than using general-ourpose set-matching algorithms; we takethe approach of formulating at compile time specialized computation plans that; by takingadvantage of information available in the given rules; limit the number of alternativesexplored. Our strategy relies on rewriting techniques to transform the problem into an“ordinary” Horn-clause compilation problem; with minimal additional overhead. Theexecution cost of the rewritten rules is substantially lower than that of the original rules; andthe additional cost of compilation can thus be amortized over many executions.,The Journal of Logic Programming,1992,46
Optimization in a logic based language for knowledge and data intensive applications,Ravi Krishnamurthy; Carlo Zaniolo,Abstract This paper describes the optimization approach taken to ensure the safe andefficient execution of applications written in LDL; which is a declarative language based onHorn Clause Logic and intended for data intensive and knowledge based applications. Inorder to generalize the strategy successfully used in relational database systems we firstcharacterize the optimization problem in terms of its execution space; cost functions andsearch algorithm. Then we extend this framework to deal with rules; complex terms;recursion and various problems resulting from the richer expressive power of Logic. Amongthese is the termination problem (safety); whereby an unsafe execution is treated as anextreme case of poor execution.,International Conference on Extending Database Technology,1988,46
An overview of the LDL system,Danette Chimenti; Anthony B.  O'Hare; Ravi Krishnamurthy; Shalom Tsur; Carolyn West; Carlo Zaniolo,*,IEEE Data Eng. Bull.,1987,46
Temporal queries and version management in XML-based document archives,Fusheng Wang; Carlo Zaniolo,Abstract By storing the successive versions of a document in an incremental fashion; XMLrepositories and data warehouses achieve:(i) the efficient preservation of critical informationand (ii) the ability to support historical queries on the evolution of documents and theircontents. In this paper; we present efficient techniques for managing multi-version documenthistories and supporting powerful temporal queries on such documents. Our approachconsists of:(i) concisely representing the successive versions of a document as an XMLdocument that implements a temporally-grouped data model and (ii) using XML querylanguages; such as XQuery; to express complex queries on the content of a particularversion; and on the temporal evolution of the document elements and contents. We showthat the data definition and manipulation framework of XML and XQuery can effectively …,Data & Knowledge Engineering,2008,44
A native extension of SQL for mining data streams,Chang Luo; Hetal Thakkar; Haixun Wang; Carlo Zaniolo,ESL1 enables users to develop stream applications in an SQL-like high level language that providesthe ease-of-use of a declarative language; which is Turing complete in terms of expressive power[11]. In the database community; there is much interest in data stream mining applications[13; 16; 20] and in Data Stream Management Systems (DSMS) [21; 14; 19; 18]; but; while highlydesirable; a marriage between the two faces difficult research challenges. The difficulty of thetask is demonstrated by traditional databases where; due to query language limitations; miningtasks are typically car- ried out using a cache mining approach: ie; by first moving the data fromthe DBMSs into a (memory or file based) cache; and then writing procedural programs to minethe cache [17]. But a similar solution is not possible for DSMS; since programming lan- guagescannot handle well massive data streams unless we intro- duce primitives for managing …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,44
CMP: A fast decision tree classifier using multivariate predictions,Haixun Wang; Carlo Zaniolo,Most decision tree classifiers are designed to keep class histograms for single attributes;and to select a particular attribute for the next split using said histograms. We propose atechnique where; by keeping histograms on attribute pairs; we achieve: a significant speed-up over traditional classifiers based on single attribute splitting; and the ability of buildingclassifiers that use linear combinations of values from non-categorical attribute pairs as splitcriterion. Indeed; by keeping two-dimensional histograms; CMP can often predict the bestsuccessive split; in addition to computing the current one; therefore; CMP is normally able togrow more than one level of a decision tree for each data scan. CMP's performanceimprovements are also due to techniques whereby non-categorical attributes are discretizedwithout loss in classification accuracy; in fact; we introduce simple techniques; whereby …,Data Engineering; 2000. Proceedings. 16th International Conference on,2000,44
Publishing and querying the histories of archived relational databases in XML,Fusheng Wang; Carlo Zaniolo,There is much current interest in publishing and viewing databases as XML documents. Thegeneral benefits of this approach follow from the popularity of XML and the tool set availablefor visualizing and processing information encoded in this universal standard. In this paper;we explore the additional and unique benefits achieved by this approach on temporaldatabase applications. We show that XML views combined with XQuery can providesurprisingly effective solutions to the problem of supporting historical queries on past contentof database relations and their evolution. Indeed; using XML; the histories of databaserelations can be naturally represented by temporally grouped data models. Thus; we identifymappings from relations to XML that are most conducive to modeling and querying databasehistories; and show that temporal queries that would be very difficult to express in SQL …,Web Information Systems Engineering; 2003. WISE 2003. Proceedings of the Fourth International Conference on,2003,41
Semantics and expressive power of nondeterministic constructs in deductive databases,Fosca Giannotti; Dino Pedreschi; Carlo Zaniolo,Abstract Nondeterministic extensions are needed in logic-based languages; such as first-order relational languages and Datalog; to enhance their expressive power and support theefficient formulation of low-complexity problems and database queries. In this paper; westudy the semantics and expressive power of the various nondeterministic constructsproposed in the past; including various versions of the choice operator and the witnessoperator. The paper develops a model-theoretic semantics; a fixpoint semantics; and anoperational semantics for these constructs; and characterizes their power of expressingdeterministic and nondeterministic queries. The paper presents various soundness andcompleteness results and establishes an expressiveness hierarchy that correlates thevarious operators with each other and with other constructs such as negation and fixpoint.,Journal of Computer and System Sciences,2001,41
DATALOG queries with stratified negation and choice: from P to D P,Sergio Greco; Domenico Saccà; Carlo Zaniolo,Abstract This paper introduces a unified solution to the problem of extending stratifiedDATALOG to express DB-complexity classes ranging from P to DP. The solution is based on(i) stratified negation as the core of a simple; declarative semantics for negation;(ii) the useof a “choice” construct to capture non-determinism of stable models (iii) the ability to bind aquery execution to the complexity class that includes the problem at hand; and (iv) a generalalgorithm that ensures efficient execution for the different complexity classes. We thus obtaina class of DATALOG programs that preserves computational tractability; while achievingcompleteness for a wide range of complexity classes.,International Conference on Database Theory,1995,41
Logic-based user-defined aggregates for the next generation of database systems,Carlo Zaniolo; Haixun Wang,Summary In this paper; we provide logic-based foundations for the extended aggregateconstructs required by advanced database applications. In particular; we focus on datamining applications and show that they require user-defined aggregates extended with earlyreturns. Thus; we propose a simple formalization of extended user-defined aggregates usingthe nondeterministic construct of choice. We obtain programs that have a formal semanticsbased on the concept of total stable models; but are also amenable to efficientimplementation. Our formalization leads to a simple syntactic characterization of user-defined aggregates that are monotone with respect to set containment. Therefore; theseaggregates can be freely used in recursive programs; and the fixpoints for such programscan be computed efficiently using the standard techniques of deductive databases. We …,*,1999,40
A formal approach to the definition and the design of conceptual schemata for databased systems,Carlo Zaniolo; Michel A Melkaoff,Abstract A formal approach is proposed to the definition and the design of conceptualdatabase diagrams to be used as conceptual schemata in a system featuring a multilevelschema architecture; and as an aid for the design of other forms of schemata. We considerER (entity-relationship) diagrams; and we introduce a new representation called CAZ-graphs. A rigorous connection is established between these diagrams and some formalconstraints used to describe relationships in the framework of the relational data model.These include functional and multivalued dependencies of database relations. The basis forour schemata is a combined representation for two fundamental structures underlying everyrelation: the first defined by its minimal atomic decompositions; the second by its elementaryfunctional dependencies. The interaction between these two structures is explored; and …,ACM Transactions on Database Systems (TODS),1982,40
User defined aggregates in object-relational systems,Haixun Wang; Carlo Zaniolo,User-defined aggregates are essential in many advanced database applications;particularly in expressing data mining functions; but they find little support in current systemsincluding object-relational databases. Three serious limitations of current systems are (i) theinability to introduce new aggregates (eg; by coding them in procedural language asoriginally proposed in SQL3);(ii) the inability to return partial results during the computation(eg to support online aggregation); and (iii) the inability to use aggregates in recursivequeries (eg to express bill of materials and optimized graph searches). In this paper; wepresents a unified solution to these problems which realizes the SQL3 original proposal foruser-defined aggregates (U-DAs); and adds significant improvements in terms of expressivepower and ease of use: in fact our SQL-AG system also supports online aggregation …,Data Engineering; 2000. Proceedings. 16th International Conference on,2000,39
Universal temporal extensions for database languages,Cindy Xinmin Chen; Carlo Zaniolo,Temporal reasoning and temporal query languages present difficult research problems oftheoretical interest and practical importance. One problem is the chasm between point-based temporal reasoning and interval-based reasoning. Another problem is the lack ofrobustness and universality in many proposed solutions; whereby temporal extensionsdesigned for one language cannot be easily applied to other query languages; egextensions proposed for SQL cannot be applied to QBE or Datalog. In this paper; we providea simple solution to both problems by observing that all query languages support (i) single-value-based reasoning; and (ii) aggregate-based reasoning; and then showing that thesetwo modalities can be naturally extended to support; respectively; point-based and interval-based temporal queries. We follow TSQL2 insofar as practical requirements are …,Data Engineering; 1999. Proceedings.; 15th International Conference on,1999,39
The prism workwench: Database schema evolution without tears,Carlo A Curino; Hyun J Moon; MyungWon Ham; Carlo Zaniolo,Information Systems are subject to a perpetual evolution; which is particularly pressing inWeb Information Systems; due to their distributed and often collaborative nature. Suchcontinuous adaptation process; comes with a very high cost; because of the intrinsiccomplexity of the task and the serious rami¿ cations of such changes upon database-centricInformation System softwares. Therefore; there is a need to automate and simplify theschema evolution process and to ensure predictability and logical independence uponschema changes. Current relational technology makes it easy to change the databasecontent or to revise the underlaying storage and indexes but does little to support logicalschema evolution which nowadays remains poorly supported by commercial tools. ThePRISM system demonstrates a major new advance toward automating schema evolution …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,38
A sequential pattern query language for supporting instant data mining for e-services,Reza Sadri; Carlo Zaniolo; Amir M Zarkesh; Jafar Adibi,Abstract Many e-commerce applications; including online auctions; personalization; andtargeted advertising; require mining web-logs; transaction trails; and similar sequentialpatterns. Often; an" instant" response during an active trading session is required in criticalapplications. Therefore; e-services need efficient tools to perform fast; scalable searches forsequential patterns. Here; we describe SQL-TS; an extension of SQL that is highly optimizedfor searching patterns in sequences; and discuss its many uses in e-services.,VLDB,2001,38
Archis: an xml-based approach to transaction-time temporal database systems,Fusheng Wang; Carlo Zaniolo; Xin Zhou,Abstract Effective support for temporal applications by database systems represents animportant technical objective that is difficult to achieve since it requires an integrated solutionfor several problems; including (i) expressive temporal representations and data models;(ii)powerful languages for temporal queries and snapshot queries;(iii) indexing; clustering andquery optimization techniques for managing temporal information efficiently; and (iv)architectures that bring together the different pieces of enabling technology into a robustsystem. In this paper; we present the ArchIS system that achieves these objectives bysupporting a temporally grouped data model on top of RDBMS. ArchIS'architecture uses (a)XML to support temporally grouped (virtual) representations of the database history;(b)XQuery to express powerful temporal queries on such views;(c) temporal clustering and …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,37
Automating database schema evolution in information system upgrades,Carlo Curino; Hyun J Moon; Carlo Zaniolo,Abstract The complexity; cost; and down-time currently created by the database schemaevolution process is the source of incessant problems in the life of information systems and amajor stumbling block that prevent graceful upgrades. Furthermore; our studies shows thatthe serious problems encountered by traditional information systems are now furtherexacerbated in web information systems and cooperative scientific databases where thefrequency of schema changes has increased while tolerance for downtimes has nearlydisappeared. The PRISM project seeks to develop the methods and tools that turn this error-prone and time-consuming process into one that is controllable; predictable and avoidsdown-time. Toward this goal; we have assembled a large testbed of schema evolutionhistories; and developed a language of Schema Modification Operators (SMO) to express …,Proceedings of the 2nd International Workshop on Hot Topics in Software Upgrades,2009,36
A comparative study of version management schemes for XML documents,Shu-Yao Chien; Vassilis J Tsotras; Carlo Zaniolo,Abstract The problem of managing multiple versions for XML documents and semistructureddata is of significant interest in many DB applications and web-related services. Traditionaldocument version control schemes; such as RCS; suffer from the following two problems. Atthe logical level; they conceal the structure of the documents by modeling them assequences of text lines; and storing a document's evolution as a line-edit script. At thephysical level; they can incur in severe storage or processing costs because of their inabilityto trade-off storage with computation. To solve these problems; we propose versionmanagement strategies that preserve the structure of the original document; and apply andextend DB techniques to minimize storage and processing costs. Therefore; we propose andcompare three schemes for XML version management; namely; the Usefulness-Based …,Time Center Technical Report TR. 51,2000,36
Deductive databases: achievements and future directions,Jeffrey D Ullman; Carlo Zaniolo,Abstract In the recent years; Deductive Databases have been the focus of intense research;which has brought dramatic advances in theory; systems and applications. A salient featureof deductive databases is their capability of supporting a declarative; rule-based style ofexpressing queries and applications on databases. As such; they find applications indisparate areas; such as knowledge mining from databases; and computer-aided designand manufacturing systems. In this paper; we briefly review the key concepts behinddeductive databases and their newly developed enabling technology. Then; we describecurrent research on extending the functionality and usability of deductive databases and onproviding a synthesis of deductive databases with procedural and object-orientedapproaches.,ACM SIGMOD Record,1990,36
Deterministic and non-deterministic stable models,Domenico Sacca; CARO ZANIOLO,Stable models have been first introduced in the domain of total interpretations (T-stablemodels); where the existence of multiple T-stable models for the same program provides apowerful mechanism to express non-determinism. Stable models have been later extendedto the domain of partial interpretations (P-stable models). In this paper; we show that thepresence of multiple P-stable models need not be a direct manifestation of non-determinism;for it can be instead an expression of assorted degrees of undefinedness. To separate thetwo factors; non-determinism and undefinedness; this paper introduces the notion ofdeterministic stable models and strictly non-deterministic ones. Deterministic stable modelsform an interesting family; having a lattice structure where the well-founded model serves asthe bottom; the top of the lattice; the maximum deterministic stable model; resolves …,Journal of Logic and Computation,1997,35
Relational languages and data models for continuous queries on sequences and data streams,Carlo Zaniolo Yan-Nei Law; Haixun Wang,*,ACM Trans. Database Syst.,2011,34
Partial Models and Three-Valued Models in Logic Programs with Negation.,Domenico Sacca; Carlo Zaniolo,Abstract Much of the current work in non-monotonic logic pursues the generalization ofconcepts such as wen-founded models and stable models using three-valued logic. Thisapproach is also effective in dealing with incomplete and undefined information that isfrequently found in knowledge bases. However; it also suffers from drawbacks; including thefact that; in multi-valued logic; there is more than one meaningful way to assign a meaning torules in a program. In this paper; we present a reconstruction of theory of negation in logicrules which deals with incompleteness and undefinedness using the standard two-valuedlogic. Simple extensions of the notion of unfounded sets are used to define the concept ofpartial models and the notions of partial well-founded models and partial stable models. Weprove that the partial stable models so defined are equivalent to the three-valued stable …,LPNMR,1991,34
Optimal load shedding with aggregates and mining queries,Barzan Mozafari; Carlo Zaniolo,To cope with bursty arrivals of high-volume data; a DSMS has to shed load while minimizingthe degradation of Quality of Service (QoS). In this paper; we show that this problem can beformalized as a classical optimization task from operations research; in ways thataccommodate different requirements for multiple users; different query sensitivities to loadshedding; and different penalty functions. Standard non-linear programming algorithms areadequate for non-critical situations; but for severe overloads; we propose a more efficientalgorithm that runs in linear time; without compromising optimality. Our approach isapplicable to a large class of queries including traditional SQL aggregates; statisticalaggregates (eg; quantiles); and data mining functions; such as k-means; naive Bayesianclassifiers; decision trees; and frequent pattern discovery (where we can even specify a …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,33
Very fast estimation for result and accuracy of big data analytics: The EARL system,Nikolay Laptev; Kai Zeng; Carlo Zaniolo,Approximate results based on samples often provide the only way in which advancedanalytical applications on very massive data sets (akabig data') can satisfy their time andresource constraints. Unfortunately; methods and tools for the computation of accurate earlyresults are currently not supported in big data systems (eg; Hadoop). Therefore; we proposea nonparametric accuracy estimation method and system to speedup big data analytics. Ourframework is called EARL (Early Accurate Result Library) and it works by predicting thelearning curve and choosing the appropriate sample size for achieving the desired errorbound specified by the user. The error estimates are based on a technique calledbootstrapping that has been widely used and validated by statisticians; and can be appliedto arbitrary functions and data distributions. Therefore; this demo will elucidate (a) the …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,29
Using XML to build efficient transaction-time temporal database systems on relational databases,Fusheng Wang; Xin Zhou; Carlo Zaniolo,In this paper; we present the ArchIS system that achieves full-functionality transaction-timedatabases without requiring temporal extensions in XML or database standards.ArchIS'architecture uses (a) XML to support temporally grouped (virtual) representations ofthe database history;(b) XQuery to express powerful temporal queries on such views;(c)temporal clustering and indexing techniques for managing the actual historical data in arelational database; and (d) SQL/XML for executing the queries on the XML views asequivalent queries on the relational database. The performance studies presented in thepaper show that ArchIS is quite effective at storing and retrieving under complex queryconditions the transaction-time history of relational databases.,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,28
Copy-based versus edit-based version management schemes for structured documents,Shu-Yao Chien; Vassilis J Tsotras; Carlo Zaniolo,Managing multiple versions of XML documents and semistructured data represents aproblem of growing interest. Traditional version control methods; such as RCS; use editscripts representing changes in the document to support the incremental reconstruction ofdifferent versions. The edit-based approaches have been enhanced with a replicationscheme called UBCC (Chien et al.; 2000). UBCC is based on the notion of page usefulnessand ensures effective management for multi-version documents in terms of both retrievaland storage cost. These improvements notwithstanding; the edit-based representationsuffers from limited generality and flexibility-eg; it cannot represent changes such asrearranging the document or duplicating parts of its content. To solve these problems; thepaper proposes a copy-based UBCC versioning scheme; which also provides a simpler …,Research Issues in Data Engineering; 2001. Proceedings. Eleventh International Workshop on,2001,28
Logical foundations of continuous query languages for data streams,Carlo Zaniolo,Abstract Data Stream Management Systems (DSMS) have attracted much interest from thedatabase community; and extensions of relational database languages were proposed forexpressing continuous queries on data streams. However; while relational databases werebuilt on the solid bedrock of logic; the same cannot be said for DSMS. Thus; a logic-basedreconstruction of DSMS languages and their unique computational model is long overdue.Indeed; the banning of blocking queries and the fact that stream data are ordered by theirarrival timestamps represent major new aspects that have yet to be characterized by simpletheories. In this paper; we show that these new requirements can be modeled using thefamiliar deductive database concepts of closed-world assumption and explicit localstratification. Besides its obvious theoretical interest; this approach leads to the design of …,*,2012,27
SMM: A data stream management system for knowledge discovery,Hetal Thakkar; Nikolay Laptev; Hamid Mousavi; Barzan Mozafari; Vincenzo Russo; Carlo Zaniolo,The problem of supporting data mining applications proved to be difficult for databasemanagement systems and it is now proving to be very challenging for data streammanagement systems (DSMSs); where the limitations of SQL are made even more severeby the requirements of continuous queries. The major technical advances that achievedseparately on DSMSs and on data stream mining algorithms have failed to converge andproduce powerful data stream mining systems. Such systems; however; are essential sincethe traditional pull-based approach of cache mining is no longer applicable; and the push-based computing mode of data streams and their bursty traffic complicate applicationdevelopment. For instance; to write mining applications with quality of service (QoS) levelsapproaching those of DSMSs; a mining analyst would have to contend with many …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,27
Greedy by choice,Sergio Greco; Carlo Zaniolo; Sumit Ganguly,Abstract The greedy paradigm of algorithm design is a well known tool used for efficientlysolving many classical computational problems within the framework of procedurallanguages. However; it is very difficult to express these algorithms within the declarativeframework of logic-based languages. In this paper; we extend the framework of Datalog-likelanguages to provide simple and declarative formulations of such problems; withcomputational complexities comparable to those of procedural formulations. This isachieved through the use of constructs; such as least and choice; that have semanticsreducible to that of negative programs under stable model semantics. Therefore; we showthat the formulation of greedy algorithms using these constructs lead to a syntactic class ofprograms; called stage-stratified programs; that are easily recognized at compile time …,Proceedings of the eleventh ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1992,27
Composite temporal events in active databases: a formal semantics,Iakovos Motakis; Carlo Zaniolo,Abstract Active databases must support rules triggered by complex patterns of compositetemporal events. This paper proposes a general method for specifying the semantics ofcomposite event specification languages. The method is based on a syntax-directedtranslation of the composite event expressions into Datalog 1 S; whose formal semantics isthen used to define the meaning of the original event expressions. We show that the methodis applicable to languages such as ODE; Snoop and SAMOS that are based respectively onthe formalisms of Finite State Machines; Event Graphs and Petri Nets. The proposed methodovercomes various problems and limitations affecting such formalisms.,*,1995,26
On the unification of active databases and deductive databases,Carlo Zaniolo,Abstract These two rule-oriented paradigms of databases have been the focus of extensiveresearch and are now coming of age in the commercial DBMS world. However; the systemsdeveloped so far support well only one of the two paradigms—thus limiting the effectivenessof such systems in many applications that require complete integration of both kinds of rules.In this paper; we discuss the technical problems that make such an integration difficult; andtrace their roots to a lack of a unified underlying semantics. Then; we review recentadvances in the semantics of nonmonotonic logic and show that they can be used to unifythe foundations of active databases and deductive databases. Finally; we outline the designa new rule language for databases that integrates a deductive system with a trigger-basedDBMS.,British National Conference on Databases,1993,26
Swipe: searching wikipedia by example,Maurizio Atzori; Carlo Zaniolo,Abstract A novel method is demonstrated that allows semantic and well-structuredknowledge bases (such as DBpedia) to be easily queried directly from Wikipedia's pages.Using Swipe; naive users with no knowledge of RDF triples and SPARQL can easily queryDBpedia with powerful questions such as:" Who are the US presidents who took office whenthey were 55-year old or younger; during the last 60 years"; or" Find the town in Californiawith less than 10 thousand people". This is accomplished by a novel Search by Example(SBE) approach where a user can enter the query conditions directly on the Infobox of aWikipedia page. In fact; Swipe activates various fields of Wikipedia to allow users to enterquery conditions; and then uses these conditions to generate equivalent SPARQL queriesand execute them on DBpedia. Finally; Swipe returns the query results in a form that is …,Proceedings of the 21st International Conference on World Wide Web,2012,25
Supporting complex queries on multiversion XML documents,Shu-Yao Chien; Vassilis J Tsotras; Carlo Zaniolo; Donghui Zhang,Abstract Managing multiple versions of XML documents represents a critical requirement formany applications. Recently; there has been much work on supporting complex queries onXML data (eg; regular path expressions; structural projections; etc.). In this article; weexamine the problem of implementing efficiently such complex queries on multiversion XMLdocuments. Our approach relies on a numbering scheme; whereby durable node numbers(DNNs) are used to preserve the order among the nodes of the XML tree while remaininginvariant with respect to updates. Using the document's DNNs; we show that many complexqueries are reduced to combinations of range version retrieval queries. We thus examinethree alternative storage organizations/indexing schemes to efficiently evaluate rangeversion retrieval queries in this environment. A thorough performance analysis is then …,ACM Transactions on Internet Technology (TOIT),2006,25
Big data analytics with datalog queries on spark,Alexander Shkapsky; Mohan Yang; Matteo Interlandi; Hsuan Chiu; Tyson Condie; Carlo Zaniolo,Abstract There is great interest in exploiting the opportunity provided by cloud computingplatforms for large-scale analytics. Among these platforms; Apache Spark is growing inpopularity for machine learning and graph analytics. Developing efficient complex analyticsin Spark requires deep understanding of both the algorithm at hand and the Spark API orsubsystem APIs (eg; Spark SQL; GraphX). Our BigDatalog system addresses the problem byproviding concise declarative specification of complex queries amenable to efficientevaluation. Towards this goal; we propose compilation and optimization techniques thattackle the important problem of efficiently supporting recursion in Spark. We perform anexperimental comparison with other state-of-the-art large-scale Datalog systems and verifythe efficacy of our techniques and effectiveness of Spark in supporting Datalog-based …,Proceedings of the 2016 International Conference on Management of Data,2016,24
Scalable architecture and query optimization fortransaction-time DBs with evolving schemas,Hyun Jin Moon; Carlo A Curino; Carlo Zaniolo,Abstract The problem of archiving and querying the history of a database is made morecomplex by the fact that; along with the database content; the database schema also evolveswith time. Indeed; archival quality can only be guaranteed by storing past database contentsusing the schema versions under which they were originally created. This causes majorusability and scalability problems in preservation; retrieval and querying of databases withintense evolution histories; ie; hundreds of schema versions. This scenario is common inweb information systems and scientific databases that frequently accumulate that manyversions in just a few years. Our system; Archival Information Management System (AIMS);solves this usability issue by letting users write queries against a chosen schema versionand then performing for the users the rewriting and execution of queries on all …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,24
Managing the history of metadata in support for db archiving and schema evolution,Carlo A Curino; Hyun J Moon; Carlo Zaniolo,Abstract Modern information systems; and web information systems in particular; are facedwith frequent database schema changes; which generate the necessity to manage them andpreserve the schema evolution history. In this paper; we describe the Panta Rhei Frameworkdesigned to provide powerful tools that:(i) facilitate schema evolution and guide theDatabase Administrator in planning and evaluating changes;(ii) support automatic rewritingof legacy queries against the current schema version;(iii) enable efficient archiving of thehistories of data and metadata; and (iv) support complex temporal queries over suchhistories. We then introduce the Historical Metadata Manager (HMM); a tool designed tofacilitate the process of documenting and querying the schema evolution itself. We use theschema history of the Wikipedia database as a telling example of the many uses and …,International Conference on Conceptual Modeling,2008,24
An xml-based approach to publishing and querying the history of databases,Fusheng Wang; Carlo Zaniolo,Abstract There is much current interest in publishing and viewing databases as XMLdocuments. The general benefits of this approach follow from the popularity of XML and thetool set available for visualizing and processing information encoded in this universalstandard. In this paper; we explore the additional and unique benefits achieved by thisapproach on temporal database applications. We show that XML with XQuery can providesurprisingly effective solutions to the problem of supporting historical queries on past contentof database relations and their evolution. Indeed; using XML; the histories of databaserelations can be naturally represented by temporally grouped data models. Thus; we identifymappings from relations to XML that are most conducive to modeling and querying databasehistories; and show that temporal queries that would be difficult to express in SQL can be …,World Wide Web,2005,24
Deductive databases-theory meets practice,Carlo Zaniolo,Abstract Deductive Databases are coming of age with the emergence of efficient and easy touse systems that support queries; reasoning; and application development on databasesthrough declarative logic-based languages. Building on solid theoretical foundations; thefield has benefited in the recent years form dramatic advances in the enabling technology.This progress is demonstrated by the completion of prototype systems offering such levels ofgenerality; performance and robustness that they support well complex applicationdevelopment. Valuable know-how has emerged from the experience of building and usingthese systems: we have learned about algorithms and architectures for building powerfuldeductive database systems; and we begin to understand the programming environmentsand paradigms they are conducive to. Thus; several application areas have been …,International Conference on Extending Database Technology,1990,24
Designing an inductive data stream management system: the stream mill experience,Hetal Thakkar; Barzan Mozafari; Carlo Zaniolo,Abstract There has been much recent interest in on-line data mining. Existing miningalgorithms designed for stored data are either not applicable or not effective on datastreams; where real-time response is often needed and data characteristics changefrequently. Therefore; researchers have been focusing on designing new and improvedalgorithms for on-line mining tasks; such as classification; clustering; frequent itemsetsmining; pattern matching; etc. Relatively little attention has been paid to designing DSMSs;which facilitate and integrate the task of mining data streams---ie; stream systems thatprovide Inductive functionalities analogous to those provided by Weka and MS OLE DB forstored data. In this paper; we propose the notion of an Inductive DSMS---a system thatbesides providing a rich library of inter-operable functions to support the whole mining …,Proceedings of the 2nd international workshop on Scalable stream processing system,2008,23
Data cleaning using belief propagation,Fang Chu; Yizhou Wang; D Stott Parker; Carlo Zaniolo,Abstract Effective data cleaning is critical in many applications where the quality of data ispoor due to missing values or inaccurate values. Fortunately; a wide spectrum ofapplications exhibit strong dependencies between data samples; and such dependenciescan be used very effectively for cleaning the data. For example; the readings of nearbysensors are generally correlated; and proteins interact with each other when performingcrucial functions. We propose a data cleaning approach; based on modeling datadependencies with Markov networks. Belief propagation is used to efficiently compute themarginal or maximum posterior probabilities; so as to infer missing values or to correcterrors. To illustrate the benefits and generality of the technique; we discuss its use in severalapplications and report on the data quality and improvements so obtained.,Proceedings of the 2nd international workshop on Information quality in information systems,2005,23
Greedy algorithms in Datalog,Sergio Greco; Carlo Zaniolo,Abstract In the design of algorithms; the greedy paradigm provides a powerful tool forsolving efficiently classical computational problems; within the framework of procedurallanguages. However; expressing these algorithms within the declarative framework of logic-based languages has proven a difficult research challenge. In this paper; we extend theframework of Datalog-like languages to obtain simple declarative formulations for suchproblems; and propose effective implementation techniques to ensure computationalcomplexities comparable to those of procedural formulations. These advances are achievedthrough the use of the choice construct; extended with preference annotations to effect theselection of alternative stable-models and nondeterministic fixpoints. We show that; withsuitable storage structures; the differential fixpoint computation of our programs matches …,Theory and Practice of Logic Programming,2001,23
ABS: a system for scalable approximate queries with accuracy guarantees,Kai Zeng; Shi Gao; Jiaqi Gu; Barzan Mozafari; Carlo Zaniolo,Abstract Approximate Query Processing (AQP) based on sampling is critical for supportingtimely and cost-effective analytics over big data. To be applied successfully; AQP must beaccompanied by reliable estimates on the quality of sample-produced approximate answers;the two main techniques used in the past for this purpose are (i) closed-form analytic errorestimation; and (ii) the bootstrap method. Approach (i) is extremely efficient but lacksgenerality; whereas (ii) is general but suffers from high computational overhead. Ourrecently introduced Analytical Bootstrap method combines the strengths of both approachesand provides the basis for our ABS system; which will be demonstrated at the conference.The ABS system models bootstrap by a probabilistic relational model; and extends relationalalgebra with operations on probabilistic relations to predict the distributions of the AQP …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,22
Graph queries in a next-generation datalog system,Alexander Shkapsky; Kai Zeng; Carlo Zaniolo,Abstract Recent theoretical advances have enabled the use of special monotonicaggregates in recursion. These special aggregates make possible the concise expressionand efficient implementation of a rich new set of advanced applications. Among theseapplications; graph queries are particularly important because of their pervasiveness in dataintensive application areas. In this demonstration; we present our Deductive ApplicationLanguage (DeAL) System; the first of a new generation of Deductive Database Systems thatsupport applications that could not be expressed using regular stratification; or could beexpressed using XY-stratification (also supported in DeAL) but suffer from inefficientexecution. Using example queries; we will (i) show how complex graph queries can beconcisely expressed using DeAL and (ii) illustrate the formal semantics and efficient …,Proceedings of the VLDB Endowment,2013,22
Design and Implementation of a Temporal Extension of SQL,Cindy Xinmin Chen; Jiejun Kong; Carlo Zaniolo,We present a valid-time extension of SQL and investigate its efficient implementation on anobject-relational database system. We propose an approach; where temporal queries areexpressed using a point-based time model; which only requires minimal extensions to SQL:1999. Our prototype system called TENORS (for Temporal ENhanced Object-RelationalSystem) maps the external point-based temporal queries and data model into equivalentinternal representations based on time intervals. We describe the mapping of queries fromexternal views to internal relations; and the temporal clustering and indexing methods usedto support these queries on DB2.,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,22
Extrema predicates in deductive databases,Sumit Ganguly; Sergio Greco; Carlo Zaniolo,Abstract A novel approach is proposed for expressing and computing efficiently a large classof problems; including finding the shortest path in a graph; that were previously consideredimpervious to an efficient treatment in the declarative framework of logic-based languages.Our approach is based on the use of min and max predicates having a first-order semanticsdefined using rules with negation in their bodies. We show that; under certain monotonicityconditions;(1) there exists a total well-founded model for these programs expressed usingnegation;(2) this model can be computed efficiently using a procedure called greedy fixpoint;and (3) programs with min/max goals on recursively defined cost predicates can often berewritten into more efficient ones by pushing min and max predicates into recursion. Thegreedy fixpoint evaluation of the program expressing the shortest path problem coincides …,Journal of Computer and System Sciences,1995,22
Rewriting of rules containing set terms in a logic data language LDL,Oded Shmueli; Shalom Tsur; Carlo Zaniolo,Abstract We propose compilation methods for supporting set terms in Horn clause programs;without using general-purpose set matching algorithms; which tend to run in timesexponential in the size of the participating sets Instead; we take the approach of formulatingspecialized computation plans that; by taking advantage of information available in the givenrules; limit the number of alternatives explored. Our strategy is to employ compile timerewriting techniques and to transform the problem into an “ordinary” Horn clause compilationproblem; with minimal additional overhead. The execution cost of the rewritten rules issubstantially lower than that of the original rules and the additional cost of compilation canthus be amortized over many executions,Proceedings of the seventh ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1988,22
Representing and Querying the Evolution of Databases and their Schemas in XML,Fusheng Wang; Carlo Zaniolo,*,evolution,2003,21
High-performance complex event processing over hierarchical data,Barzan Mozafari; Kai Zeng; Loris D'antoni; Carlo Zaniolo,Abstract While Complex Event Processing (CEP) constitutes a considerable portion of the so-called Big Data analytics; current CEP systems can only process data having a simplestructure; and are otherwise limited in their ability to efficiently support complex continuousqueries on structured or semistructured information. However; XML-like streams represent avery popular form of data exchange; comprising large portions of social network and RSSfeeds; financial feeds; configuration files; and similar applications requiring advanced CEPqueries. In this article; we present the XSeq language and system that support CEP on XMLstreams; via an extension of XPath that is both powerful and amenable to an efficientimplementation. Specifically; the XSeq language extends XPath with natural operators toexpress sequential and Kleene-* patterns over XML streams; while remaining highly …,ACM Transactions on Database Systems (TODS),2013,20
Unifying the processing of xml streams and relational data streams,Xin Zhou; Hetal Thakkar; Carlo Zaniolo,Relational data streams and XML streams have previously provided two separate researchfoci; but their unified support by a single Data Stream Management System (DSMS) is verydesirable from an application viewpoint. In this paper; we propose a simple approach toextend relational DSMSs to support both kinds of streams efficiently. In our Stream Millsystem; XML streams expressed as SAX events; can be easily transformed into relationalstreams; and vice versa. This enables a close cooperation of their query languages;resulting in great power and flexibility. For instance; XQuery can call functions defined in ourSQLbased Expressive Stream Language (ESL) using the logical/physical windows that haveproved so useful on relational data streams. Many benefits are also gained at the systemlevel; since relational DSMS techniques for load shedding; memory management; query …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,20
Ibminer: A text mining tool for constructing and populating infobox databases and knowledge bases,Hamid Mousavi; Shi Gao; Carlo Zaniolo,Abstract Knowledge bases and structured summaries are playing a crucial role in manyapplications; such as text summarization; question answering; essay grading; and semanticsearch. Although; many systems (eg; DBpedia and YaGo2) provide massive knowledgebases of such summaries; they all suffer from incompleteness; inconsistencies; andinaccuracies. These problems can be addressed and much improved by combining andintegrating different knowledge bases; but their very large sizes and their reliance ondifferent terminologies and ontologies make the task very difficult. In this demo; we willdemonstrate a system that is achieving good success on this task by: i) employing availableinterlinks in the current knowledge bases (eg externalLink and redirect links in DBpedia) tocombine information on individual entities; and ii) using widely available text corpora (eg …,Proceedings of the VLDB Endowment,2013,19
Fast and accurate computation of equi-depth histograms over data streams,Hamid Mousavi; Carlo Zaniolo,Abstract Equi-depth histograms represent a fundamental synopsis widely used in bothdatabase and data stream applications; as they provide the cornerstone of many techniquessuch as query optimization; approximate query answering; distribution fitting; and paralleldatabase partitioning. Equi-depth histograms try to partition a sequence of data in a way thatevery part has the same number of data items. In this paper; we present a new algorithm toestimate equi-depth histograms for high speed data streams over sliding windows. Whilemany previous methods were based on quantile computations; we propose a new methodcalled BAr Splitting Histogram (BASH) that provides an expected ε-approximate solution tocompute the equi-depth histogram. Extensive experiments show that BASH is at least fourtimes faster than one of the best existing approaches; while achieving similar or better …,Proceedings of the 14th International Conference on Extending Database Technology,2011,19
PRIMA: archiving and querying historical data with evolving schemas,Hyun J Moon; Carlo A Curino; Myungwon Ham; Carlo Zaniolo,Abstract Schema evolution poses serious challenges in historical data management.Traditionally; historical data have been archived either by (i) migrating them into the currentschema version that is well-understood by users but compromising archival quality; or (ii) bymaintaining them under the original schema version in which the data was originallycreated; leading to perfect archival quality; but forcing users to formulate queries againstcomplex histories of evolving schemas. In the PRIMA system; we achieve the best of bothapproaches; by (i) archiving historical data under the schema version under which they wereoriginally created; and (ii) letting users express temporal queries using the current schemaversion. Thus; in PRIMA; the system rewrites the queries to the (potentially many) pertinentversions of the evolving schema. Moreover; the system o ers automatic documentation of …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,19
Nonmonotonic reasoning in ldl++,Haixun Wang; Carlo Zaniolo,Abstract Deductive database systems have made major advances on efficient support fornonmonotonic reasoning. A first generation of deductive database systems supported thenotion of stratification for programs with negation and set aggregates. Stratification is simpleto understand and efficient to implement but it is too restrictive; therefore; a secondgeneration of systems seeks efficient support for more powerful semantics based on notionssuch as well-founded models and stable models. In this respect; a particularly powerful setof constructs is provided by the recently enhanced LDL++ system that supports (i) monotonieuser-defined aggregates;(ii) XY-stratified programs; and (iii) the nondeterministic choiceconstructs under stable model semantics. This integrated set of primitives supports a terseformulation and efficient implementation for complex computations; such as greedy …,*,2000,19
Extending the power of datalog recursion,Mirjana Mazuran; Edoardo Serra; Carlo Zaniolo,Abstract Supporting aggregates in recursive logic rules represents a very important problemfor Datalog. To solve this problem; we propose a simple extension; called Datalog^FS\;(Datalog extended with frequency support goals); that supports queries and reasoningabout the number of distinct variable assignments satisfying given goals; or conjunctions ofgoals; in rules. This monotonic extension greatly enhances the power of Datalog; whilepreserving (i) its declarative semantics and (ii) its amenability to efficient implementation viadifferential fixpoint and other optimization techniques presented in the paper. Thus; Datalog^FS\; enables the efficient formulation of queries that could not be expressed efficiently orcould not be expressed at all in Datalog with stratified negation and aggregates. In fact;using a generalized notion of multiplicity called frequency; we show that diffusion models …,The VLDB Journal,2013,18
Efficient temporal coalescing query support in relational database systems,Xin Zhou; Fusheng Wang; Carlo Zaniolo,Abstract The interest in; and user demand for; temporal databases have only increased withtime; unfortunately; DBMS vendors and standard groups have not moved aggressively toextend their systems with support for transaction-time or valid-time. This can be partiallyattributed to the expected major R&D costs to add temporal support to RDBMS by directlyextending the database engine. The newly introduced SQL: 2003 standards have actuallysignificantly enhanced our ability to support temporal applications in commercial databasesystems. The long recognized problem of coalescing; which is difficult to support in theframework of SQL: 1992; can now be effectively supported in RDBMS. In this paper; weinvestigate alternatives of temporal coalescing queries under temporal data models inRDBMS. We provide an SQL: 2003-based query algorithm and a native relational user …,International Conference on Database and Expert Systems Applications,2006,18
Intelligent databases: old challenges and new opportunities,Carlo Zaniolo,Abstract The evolution of existing information systems and a new wave of data-intensiveapplications are creating a strong demand for database-centered programmingenvironments much more sophisticated and intelligent than those supported by currentdatabase systems. In this paper; we describe the contributions that deductive databasesoffer to the evolution of databases and information systems to satisfy said demands. Inaddition to all database essentials; deductive databases support rule-based logic-orientedlanguages that allow terse formulations of complete applications; along with reasoning andqueries. Thus; they support a rule-based interface that eliminates the impedance mismatchproblem (between programming language and query sublanguage) and elevates the designand development of database applications to the level of declarative; knowledge-based …,Journal of Intelligent Information Systems,1992,18
From regular expressions to nested words: Unifying languages and query execution for relational and xml sequences,Barzan Mozafari; Kai Zeng; Carlo Zaniolo,Abstract There is growing interest in query language extensions for pattern matching overevent streams and stored database sequences; due to the many important applications thatsuch extensions make possible. The push for such extensions has led DBMS vendors andDSMS venture companies to propose Kleene-closure extensions of SQL standards; buildingon seminal research that demonstrated the effectiveness and amenability to efficientimplementation of such constructs. These extensions; however powerful; suffer fromlimitations that severely impair their effectiveness in many real-world applications. Toovercome these problems; we have designed the K* SQL language and system; based onour investigation of the nested words; which are recent models that generalize both wordsand trees. K* SQL extends the existing relational sequence languages; and also enables …,Proceedings of the VLDB Endowment,2010,17
Temporal XML? SQL strikes back!,Fusheng Wang; Carlo Zaniolo; Xin Zhou,While the introduction of temporal extensions into database standards has proven difficult toachieve; the newly introduced SQL: 2003 and XML/XQuery standards have actuallyenhanced our ability to support temporal applications in commercial database systems. Weillustrate this point by discussing three approaches that use temporally groupedrepresentations. We first compare the approaches at the logical level using a common set ofqueries; then we turn to the physical level and discuss our ArchIS system that supports thethree different approaches efficiently in one unified physical implementation. We concludethat the approaches of managing transaction-time information using XML and SQL can beintegrated and supported efficiently within the current standards; and claim that theproposed approach can be extended to valid-time and bitemporal databases.,Temporal Representation and Reasoning; 2005. TIME 2005. 12th International Symposium on,2005,17
Greedy Algorithms in Datalog with Choice and Negation.,Sergio Greco; Carlo Zaniolo,Abstract In the design of algorithms; the greedy paradigm provides a powerful tool forsolving efficiently classical computational problems; within the framework of procedurallanguages. However; expressing these algorithms within the declarative framework of logic-based languages has proven a difficult research challenge. In this paper; we extend theframework of Datalog-like languages to obtain simple declarative formulations for suchproblems; and propose effective implementation techniques to ensure computationalcomplexities comparable to those of procedural formulations. These advances are achievedthrough the use of the choice construct; extended with preference annotations to effect theselection of alternative stable-models and nondeterministic fixpoints. We show that; withsuitable storage structures; the differential fixpoint computation of our programs matches …,IJCSLP,1998,17
Optimization of linear logic programs using counting methods,Sergio Greco; Carlo Zaniolo,Abstract We present a general solution to the problem of optimized execution of logicprograms containing linear recursive rules. Our solution is based on extensions of theclassical counting method; which is known to be efficient but of limited applicability. In fact;the range of applicability of the counting method; and its variants proposed by previousresearchers; suffer from one or more of the following limitations: the method can be appliedonly when (1) the adorned program contains one recursive rule;(2) the 'left part'and the 'rightpart'of the recursive rule do not have any common variable and (3) the relation associatedwith the left part of the recursive rule is 'acyclic'. In this paper; a simple and unified frameworkis presented; where those limitations are removed; and the counting method thus becomeapplicable to all programs with linear rules. This framework also allows a simple …,International Conference on Extending Database Technology,1992,17
The pushdown method to optimize chain logic programs,Sergio Greco; Domenico Saccà; Carlo Zaniolo,Abstract The critical problem of finding efficient implementations for recursive queries withbound arguments offers many open challenges of practical and theoretical import. Wepropose a novel approach that solves this problem for chain queries; ie; for queries wherebindings are propagated from arguments in the head to arguments in the tail of the rules; ina chain-like fashion. The method; called pushdown; is based on the fact that a chain querycan have associated a context-free language and a pushdown automaton recognizing thislanguage can be emulated by rewriting the query as a particular fectorized left-linearprogram. The proposed method generalizes and unifies previous techniques such as the'counting'and 'right-; left-; mixed-linear'methods. It also succeeds in reducing many non-linear programs to query-equivalent linear ones.,International Colloquium on Automata; Languages; and Programming,1995,16
Decomposition of relations and synthesis of entity-relationship diagrams,Michel A Melkanoff; Carlo Zaniolo,*,Proceedings of the 1st International Conference on the Entity-Relationship Approach to Systems Analysis and Design,1980,16
XML version detection,Deise de Brum Saccol; Nina Edelweiss; Renata de Matos Galante; Carlo Zaniolo,Abstract The problem of version detection is critical in many important application scenarios;including software clone identification; Web page ranking; plagiarism detection; and peer-to-peer searching. A natural and commonly used approach to version detection relies onanalyzing the similarity between files. Most of the techniques proposed so far rely on the useof hard thresholds for similarity measures. However; defining a threshold value isproblematic for several reasons: in particular (i) the threshold value is not the same whenconsidering different similarity functions; and (ii) it is not semantically meaningful for theuser. To overcome this problem; our work proposes a version detection mechanism for XMLdocuments based on Naïve Bayesian classifiers. Thus; our approach turns the detectionproblem into a classification problem. In this paper; we present the results of various …,Proceedings of the 2007 ACM symposium on Document engineering,2007,15
Preserving and querying histories of xml-published relational databases,Fusheng Wang; Carlo Zaniolo,Abstract There is much current interest in publishing and viewing database-resident data asXML documents. In fact; such XML views of the database can be easily visualized on webbrowsers and processed by web languages; including powerful query languages such asXQuery. As the database is updated; its external XML view also evolves. In this paper; weinvestigate the problem of representing the evolution history of such a view as yet anotherXML document; whereby the complete history of the database can also be visualized onweb browsers; processed by web languages; and queried using powerful query languagessuch as XQuery. We investigate various approaches used for publishing relational data; andidentify and select those which are best for representing and querying database histories.We show that the selected representations make it easy to formulate in XQuery temporal …,International Conference on Conceptual Modeling,2002,15
Dynamic programming optimization for logic queries with aggregates,Sergio Greco; Domenico Sacca; Carlo Zaniolo,*,Proceedings of the 1993 international symposium on Logic programming,1993,15
Load shedding for window joins on multiple data streams,Yan-Nei Law; Carlo Zaniolo,We consider the problem of semantic load shedding for continuous queries containingwindow joins on multiple data streams and propose a robust approach that is effective withthe different semantic accuracy criteria that are required in different applications. In fact; ourapproach can be used to (i) maximize the number of output tuples produced by joins; and (ii)optimize the accuracy of complex aggregates estimates under uniform random sampling.We first consider the problem of computing maximal subsets of approximate window joinsover multiple data streams. Previously proposed approaches are based on multiple pair-wise joins and; in their load-shedding decisions; disregard the content of streams outsidethe joined pairs. To overcome these limitations; we optimize our load-shedding policy usingvarious predictors of the productivity of each tuple in the window. To minimize processing …,Data Engineering Workshop; 2007 IEEE 23rd International Conference on,2007,14
Optimizing timestamp management in data stream management systems,Yijian Bai; Hetal Thakkar; Haixun Wang; Carlo Zaniolo,It has long been recognized that multi-stream operators; such as union and join; often haveto wait idly in a temporarily blocked state; as a result of skews between the timestamps oftheir input streams. It has been shown that the injection of heartbeat information throughpunctuation tuples can alleviate this problem. In this paper; we propose and investigatemore effective solutions that use timestamps generated on-demand to reactivate idle-waitingoperators. We thus introduce a simple execution model that efficiently supports on-demandpunctuation. Experiments show that response time and memory usage are reducedsubstantially by this approach.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,14
User-defined aggregates in database languages,Haixun Wang; Carlo Zaniolo,Abstract User-defined aggregates (UDAs) can be the linchpin of sophisticated data miningfunctions and other advanced database applications; but they find little support in currentdatabase systems. In this paper; we describe the SQL-AG prototype that overcomes theselimitations by supporting UDAs as originally proposed in Postgres and SQL 3. Then weextend the power and flexibility of UDAs by adding (i) early returns;(to express onlineaggregation) and (ii) syntactically recognizable monotonic UDAs that can be used inrecursive queries to support applications; such as Bill of Materials (BoM) and greedyalgorithms for graph optimization; that cannot be expressed under stratified aggregation.This paper proposes a unified solution to both the theoretical and practical problems ofUDAs; and demonstrates the power of UDAs in dealing with advanced database …,International Symposium on Database Programming Languages,1999,14
The logic of totally and partially ordered plans: A deductive database approach,Antonio Brogi; VS Subrahmanian; Carlo Zaniolo,Abstract The problem of finding effective logic-based formalizations for problems involvingactions remains one of the main application challenges of non-monotonic knowledgerepresentation. In this paper; we show that complex planning strategies find natural logic-based formulations and efficient implementations in the framework of deductive databaselanguages. We begin by modeling classical STRIPS-like totally ordered plans by means ofDatalog 1 S programs; and show that these programs have a stable model semantics that isalso amenable to efficient computation. We then show that the proposed approach is quiteexpressive and flexible; and can also model partially ordered plans; which are abstractplans whereby each plan stands for a whole class of totally ordered plans. This results in areduction of the search space and a subsequent improvement in efficiency.,Annals of Mathematics and Artificial Intelligence,1997,14
Differential fixpoint methods and stratification of logic programs,Domenico Saccà; Carlo Zaniolo,Recent work has proved the merits of rule transformation methods in the compilation ofrecursive Horn Clauses. For instance; the Magic Set and Counting Methods support theefficient implementation of bound recursive queries using fixpoint operators. This paperconcentrates on the efficient implementation of the fixpoint operator and proves that thedifferential improvements to the fixpoint algorithm discussed by previous authors (SeminaiveFixpoint) can be expressed as rule rewriting scripts. Thanks to the simplicity and generalityof these scripts; various improvements and extensions are then developed for the method.Since the transformed rules contain negation; the concepts of locally stratified programs andperfect models are used to prove the correnctness of the transformations. We have thusidentified a class of locally stratified programs that is amenable to efficient computation …,*,1988,14
Mining semantic structures from syntactic structures in free text documents,Hamid Mousavi; Deirdre Kerr; Markus Iseli; Carlo Zaniolo,The Web has made possible many advanced text-mining applications; such as newssummarization; essay grading; question answering; and semantic search. For many of suchapplications; statistical text-mining techniques are ineffective since they do not utilize themorphological structure of the text. Thus; many approaches use NLP-based techniques; thatparse the text and use patterns to mine and analyze the parse trees which are oftenunnecessarily complex. Therefore; we propose a weighted-graph representation of text;called Text Graphs; which captures the grammatical and semantic relations between wordsand terms in the text. Text Graphs are generated using a new text mining framework which isthe main focus of this paper. Our framework; SemScape; uses a statistical parser to generatefew of the most probable parse trees for each sentence and employs a novel two-step …,Semantic Computing (ICSC); 2014 IEEE International Conference on,2014,13
Event-oriented data models and temporal queries in transaction-time databases,Carlo Zaniolo,Past research on temporal databases has primarily focused on state-based representationsand on relational query language extensions for such representations. This led to manydifferent proposals that had did not succeed in making a significant impact on SQL-compliant DBMS. More recently however; there has been significant interest and progresson event sequences; leading to vendor-proposed extensions of SQL standards for patternqueries based on Kleene-closure expressions. In this paper; we first outline theseextensions and their uses in dealing with sequence of events; and then show that they canalso be used effectively to express more traditional temporal queries; such as coalescingand joins; on state-based representations. Thus; we propose an approach that takes fulladvantage of the fact that every state-based representation also has a dual …,Temporal Representation and Reasoning; 2009. TIME 2009. 16th International Symposium on,2009,13
ATLaS: A Turing-complete extension of SQL for data mining applications and streams,Haixun Wang; Carlo Zaniolo; Richard C Luo,ATLaS is a powerful database language and system that enables users to develop completedata-intensive applications in SQL—by writing new table functions and aggregates in SQL;rather than in procedural languages as in current OR systems. As a result; ATLaS is Turing-complete [12]; and very suitable for advanced data-intensive applications; such as datamining and stream queries [10]. The first ATLaS system and application suite is available fordownloading from [1]. The suite includes several data mining functions coded in ATLaS'SQLthat run with only a modest (20–40%) performance overhead with respect to the sameapplications written in C/C++. We are now developing an extension of ATLaS for streams.Our proposed ACM SIGMOD 2003 demo will illustrate the key features and applications ofAT-LaS. In particular; we will demonstrate how to:• write new aggregates and table …,Manuscript available at http://wis. cs. ucla. edu/publications. html,2002,13
Optimizing recursive queries with monotonic aggregates in deals,Alexander Shkapsky; Mohan Yang; Carlo Zaniolo,The exploding demand for analytics has refocused the attention of data scientists onapplications requiring aggregation in recursion. After resisting the efforts of researchers formore than twenty years; this problem is being addressed by innovative systems that areraising logic-oriented data languages to the levels of generality and performance that areneeded to support efficiently a broad range of applications. Foremost among these newsystems; the Deductive Application Language System (DeALS) achieves superior generalityand performance via new constructs and optimization techniques for monotonic aggregateswhich are described in the paper. The use of a special class of monotonic aggregates inrecursion was made possible by recent theoretical results that proved that they preserve therigorous least-fixpoint semantics of core Datalog programs. This paper thus describes …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,12
Ontoharvester: An unsupervised ontology generator from free text,Hamid Mousavi; Deirdre Kerr; Markus Iseli; Carlo Zaniolo,ABSTRACT Ontologies are a vital component of most knowledge-based applications; suchas semantic web search; intelligent information integration; and natural languageprocessing. In the last two decades; many manual or highly supervised techniques wereproposed to generate domain-specific ontologies. However; these approaches are usuallyvery time-consuming and resource-demanding; and thus not scalable. To address this issue;numerous approaches have been recently proposed to automatically generate ontologies.These approaches mostly employ statistical or shallow NLP-based methods which usuallylimit the quality of their results. In this paper; we introduce OntoHarvester; a deep NLP-basedsystem to automatically extract and populate domain-specific ontologies from morphologicalstructures in the free text. Using our text mining framework; called SemScape …,UCLA,2013,12
Bridging relational database history and the web: the XML approach,Fusheng Wang; Xin Zhou; Carlo Zaniolo,Abstract The preservation of digital artifacts represents an unanswered challenge for themodern information society: XML and its query languages provide an effective environmentto address this challenge because of their ability to support temporal information andqueries; and make it easy to publish database history to the Web. In this paper; we focus onthe problem of preserving; publishing; and querying efficiently the history of a relationaldatabase. Past research on temporal databases revealed the difficulty of achievingsatisfactory solutions using flat relational tables and SQL. Here we show that the problemcan be solved using (a) XML to support temporally grouped representations of the databasehistory; and (b) XQuery to express powerful temporal queries on such representations.Furthermore; the approach is quite general and it can be used to preserve and query the …,Proceedings of the 8th annual ACM international workshop on Web information and data management,2006,12
Key constraints and monotonic aggregates in deductive databases,Carlo Zaniolo,Abstract We extend the fixpoint and model-theoretic semantics of logic programs to includeunique key constraints in derived relations. This extension increases the expressive powerof Datalog programs; while preserving their declarative semantics and efficientimplementation. The greater expressive power yields a simple characterization for the notionof set aggregates; including the identification of aggregates that are monotonic with respectto set containment and can thus be used in recursive logic programs. These new constructsare critical in many applications; and produce simple logic-based formulations for complexalgorithms that were previously believed to be beyond the realm of declarative logic.,*,2002,12
Database System Extensions for Decision Support: the AXL Approach.,Haixun Wang; Carlo Zaniolo,Abstract Research on database-centric data mining is seeking to improve the effectivenessof database systems in decision support applications. Different solutions are now used fordifferent problems; including (i) SQL extensions for more complex OLAP queries;(ii) newdatablades for special data types such as time-series; and (iii) architectural extensions tosupport data mining functions. Here; we proposed a unified solution for all these problemsthe solution is based on User-Defined Aggregates (UDAs) expressed in an SQL-likelanguage called AXL. In this paper; we discuss the architecture and implementation of theAXL prototype and its use and performance in expressing data mining functions andcomplex OLAP queries.,ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery,2000,12
User-Defined Aggregates for Datamining.,Haixun Wang; Carlo Zaniolo,Abstract User-de ned aggregates can be the linchpin of sophisticated datamining functionsand other advanced database applications. This is demonstrated by our e cientimplementation on DB2 of SQL3 user-de ned aggregates extended with early returns; whichwe have used to implement several data mining algorithms. Aggregates with early returnsare monotonic and can thus be used freely in recursive queries.,1999 ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery,1999,12
System Analysis for Deductive Database Environments: An Enhanced Role for Aggregate Entities.,D Ackley; RP Carasik; T Soon; D Tyron; E Tsou; Shalom Tsur; Carlo Zaniolo,*,ER,1990,12
Rule rewriting methods for efficient implementations of horn logic,Carlo Zaniolo; Domenico Saccà,Abstract This paper treats the problem of implementing efficiently queries expressed in aHorn clause based language when recursive predicates; possibly with function symbols; arepresent. The proposed approach takes the naive fixpoint computation that realizes theminimum model semantics of the given program and transforms it into a computation thatterminates and executes efficiently. Rule rewriting scripts; called methods; are used for thesetransformations; different methods are selected according to the patterns of free and boundvariables appearing in the given recursive goal. One method; known as Seminaive Fixpoint;applies to the situation where no argument is bound in the recursive goal. The remainingmethods are meant for queries where the initial bindings of the arguments in the recursivegoal can be used to improve the efficiency and; often; to guarantee the termination of the …,*,1988,12
Supporting semantic web search and structured queries on mobile devices,Andrea Dessi; Andrea Maxia; Maurizio Atzori; Carlo Zaniolo,Abstract There has been much recent interest in user-friendly interfaces that support queriesand searching the Semantic Web; without requiring knowledge of sparql and the internalstructure used by DBpedia or other knowledge bases. Although powerful; the existingproposals assume the use of desktop computers featuring rather large displays and pointingdevices such as a mouse or trackpad. In this paper we tackle the problem of querying andsearching the Semantic Web from mobile devices; by taking full advantage of their smalltouch-enabled screens. We focus on a user-friendly interface that can be used fromsmartphones; mini tablets; smart watches and possibly other wearable computers such asGoogle Glass. Indeed existing approaches become much less usable and effective onmobile since these support and require different modalities of user interaction. Our …,Proceedings of the 3rd International Workshop on Semantic Search Over the Web,2013,11
K* sql: A unifying engine for sequence patterns and xml,Barzan Mozafari; Kai Zeng; Carlo Zaniolo,Abstract A strong interest is emerging in SQL extensions for sequence patterns using Kleene-closure expressions. This burst of interest from both the research community and thecommercial world is due to the many database and data stream applications made possibleby these extensions; including financial services; RFID-based inventory management; andelectronic health systems. In this demo we will present the K* SQL system that represents amajor step forward in this area. K* SQL supports a more expressive language that allows forgeneralized Kleene-closure queries and also achieves the expressive power of the nestedword model; which greatly expands the application domain to include XML queries; softwaretrace analysis; and genomics. In this demo; we first introduce the core features of ourlanguage in expressing complex pattern queries over both relational and XML data. We …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,11
A data stream mining system,Hetal Thakkar; Barzan Mozafari; Carlo Zaniolo,On-line data stream mining has attracted much research interest; but systems that can beused as a workbench for online mining have not been researched; since they pose manydifficult research challenges. The proposed system addresses these challenges by anarchitecture based on three main technical advances;(i) introduction of new constructs andsynoptic data structures whereby complex KDD queries can be easily expressed andefficiently supported;(ii) an integrated library of mining algorithms that are fast & light enoughto be effective on data streams; and (iii) support for Mining Model Definition Language(MMDL) that allows users to define new mining algorithms as a set of tasks and flows. Thus;the proposed system provides an extensible workbench for online mining; which is beyondthe existing proposals for even static mining.,Data Mining Workshops; 2008. ICDMW'08. IEEE International Conference on,2008,11
Load shedding in classifying multi-source streaming data: A Bayes Risk approach,Yijian Bai; Haixun Wang; Carlo Zaniolo,Abstract Monitoring multiple streaming sources for collective decision making presentsseveral challenges. First; streaming data are often of large volume; fast speed; and highlybursty nature. Second; it is impossible to offload classification decisions to individual datasources; each of which lacks full knowledge for the decision making. Hence; the centralclassifier responsible for decision making may be frequently overloaded. In this paper; westudy intelligent load shedding for classifying multi-source data. We aim at maximizingclassification quality under resource (CPU and bandwidth) constraints. We use a Markovmodel to predict the distribution of feature values over time. Then; leveraging Bayesiandecision theory; we use Bayes risk analysis to model the variances among different datasources in their contributions to the classification quality. We adopt an Expected …,*,2007,11
Efficient support for time series queries in data stream management systems,Yijian Bai; Chang R Luo; Hetal Thakkar; Carlo Zaniolo,Abstract There is much current interest in supporting continuous queries on data streamsusing generalizations of database query languages; such as SQL. The research challengesfaced by this approach include (i) overcoming the expressive power limitations of databaselanguages on data stream applications; and (ii) providing query processing and optimizationtechniques for the data stream execution environment that is so different from that oftraditional databases. In particular; SQL must be extended to support sequence queries ontime series; and to overcome the loss of expressive power due to the exclusion of blockingquery operators. Furthermore; the query processing techniques of relational databases mustbe replaced with techniques that optimize execution of time-series queries and the utilizationof main memory. The Expressive Stream Language for Time Series (ESL-TS) and its …,*,2005,11
User-De ned Aggregates for Logical Data Languages,Haixun Wang; Carlo Zaniolo,Abstract. A new wave of data-intensive and knowledge-based applications| such as datamining and decision support| require the introduction of complex application-speci caggregate functions. In this paper; we propose extensions for deductive database systems tosupport these new applications. We develop constructs; formal semantics; andimplementation techniques for user-de ned aggregates; and describe their realization in anextended,GMD–Forschungszentrum Informationstechnik GmbH,1998,11
LDL++: A second-generation deductive database system,Carlo Zaniolo,Abstract) Carlo Zaniolo University of California; Los Angeles; CA.; 90024; zaniolo@ cs. ucla.edu Abstract The expanding role of intelligent information systems and a new wave ofdatabase applications are creating a strong demand for database-centered programmingenvironments. In response to this demand; deductive database systems support complexqueries and reasoning; rule-based programming and the integration of databases withknowledge bases. The LDL and LDL++ systems are the two prototypes produced by aleading research project in this area. This paper focuses on the second system that wasshaped by the experience gained with its predecessor and recent research advances in thefield. The LDL++ system features a better integration with external databases; a newexecution model; and various language extensions to support non-deterministic and non …,ComputationalLogic; Vol,1996,11
Using Metagueries to Integrate Inductive Learning and Deductive Database Technology.,Wei-Min Shen; Bharat G Mitbander; KayLiang Ong; Carlo Zaniolo,*,KDD Workshop,1994,11
Logic programming semantics made easy,Els Laenens; Dirk Vermeir; Carlo Zaniolo,Abstract We propose a new model-theoretic semantics for logic programs; called puresemantics; based on the notions of unfounded set and assumption set. The pure semanticsemerges from the observation that major logic programming semantics have the followingfeature in common: given an'intended model'M; the set of negative literals in M correspondsexactly with the greatest unfounded set wrt the set of positive literals in M. In other words; amodel contains redundant information as its negative part can be described in function of itspositive part. Thus; the total models and the partial models of programs can now becharacterized by a set of positive literals. Based on this idea; we develop the pure semanticsfor logic programs. The result is a remarkably simple semantics that unifies previousapproaches and explains how partial model semantics follows from a weaker closed …,International Colloquium on Automata; Languages; and Programming,1992,11
Main memory evaluation of recursive queries on multicore machines,Mohan Yang; Carlo Zaniolo,Supporting iteration and/or recursion for advanced big data analytics requires reexaminationof classical algorithms on modern computing environments. Several recent studies havefocused on the implementation of transitive closure in multi-node clusters. Algorithms thatdeliver optimal performance on multi-node clusters are hardly optimal on multicoremachines. We present an experimental study on finding efficient main memory recursivequery evaluation algorithms on modern multi-core machines. We review SEMINAIVE;SMART and a pair of single-source closure (SSC) algorithms. We also propose a new hybridSSC algorithm; named SSC12; which combines two previously known SSC algorithms. Weimplement these algorithms on a multicore shared memory machine; and compare theirmemory utilization; speed and scalability on synthetic and real-life datasets. Our …,Big Data (Big Data); 2014 IEEE International Conference on,2014,10
Analysing microarray expression data through effective clustering,Elio Masciari; Giuseppe Massimiliano Mazzeo; Carlo Zaniolo,Abstract The recent advances in genomic technologies and the availability of large-scalemicroarray datasets call for the development of advanced data analysis techniques; such asdata mining and statistical analysis to cite a few. Among the mining techniques proposed sofar; cluster analysis has become a standard method for the analysis of microarrayexpression data. It can be used both for initial screening of patients and for extraction ofdisease molecular signatures. Moreover; clustering can be profitably exploited tocharacterize genes of unknown function and uncover patterns that can be interpreted asindications of the status of cellular processes. Finally; clustering biological data would beuseful not only for exploring the data but also for discovering implicit links between theobjects. To this end; several clustering approaches have been proposed in order to …,Information Sciences,2014,10
Minimizing latency and memory in dsms: a unified approach to quasi-optimal scheduling,Yijian Bai; Carlo Zaniolo,Abstract Data Stream Management Systems (DSMSs) must support optimized executionscheduling of multiple continuous queries on massive; and frequently bursty; data streams.Previous approaches on optimizing memory consumption or response time (ie; latency)usually produce very different algorithms. In this paper; we extend the popular chart-partitioning procedure; which was previously used for memory optimization on simpleoperator paths; to minimize latency as well as memory on complex query-graphs with tuple-sharing forks. Furthermore; we test the performance of algorithms that only assumeknowledge of the average behavior of tuples and operators; against a theoretical one thatassumes detailed knowledge on the behavior of individual tuples. These experiments showthat the practical algorithms closely approximate the performance of the optimal ones.,Proceedings of the 2nd international workshop on Scalable stream processing system,2008,10
Universal Temporal Data Languages.,Cindy Xinmin Chen; Carlo Zaniolo,Abstract. Temporal reasoning and temporal query languages present di cult researchchallenges; which are slowly yielding to the combined attack of many investigationsmotivated by the theoretical interest and practical import of the problem. In this paper; wesubscribe to TSQL2 insofar as practical requirements for a query language are concerned;but we propose a solution that overcomes its shortcomings; particularly the lack ofuniversality whereby TSQL2 temporal extensions can not be easily applied to other querylanguages; such as QBE and Datalog. In this paper; we use Datalog as a framework todevelop a new language| Temporal Data Language (TDL). To support our claim ofuniversality; we argue that TDL constructs and semantics can be directly applied to derivetemporal extensions of languages; such as QBE and SQL. Finally; we evaluate alternative …,DDLP,1998,10
Answering Controlled Natural Language Questions on RDF Knowledge Bases.,Giuseppe M Mazzeo; Carlo Zaniolo,ABSTRACT The fast growth in number; size and availability of rdf knowledge bases (kb) iscreating a pressing need for research advances that will let people consult them withouthaving to learn structured query languages; such as sparql; and the internal organization ofthe kbs. In this demo; we present our Question Answering (QA) system that acceptsquestions posed in a Controlled Natural Language. The questions entered by the users areannotated on the fly; and an ontology driven autocompletion system displays suggestedpatterns computed in real time from the partially completed sentence the person is typing. Byfollowing these patterns; users can enter only semantically correct questions which areunambiguously interpreted by the system. This approach assures high levels of usability andgenerality; which will be demonstrated by (i) the superior performance of our system on …,EDBT,2016,9
Expressivity and accuracy of by-example structured queries on wikipedia,Maurizio Atzori; Carlo Zaniolo,This paper discusses expressivity and accuracy of the By-Example Structured (BESt) Queryparadigm implemented on the SWiPE system through the Wikipedia interface. We define anexperimental setting based on the natural language questions made available by the QALD-4 challenge; in which we compare SWiPE against Xser; a state-of-the-art QuestionAnswering system; and plain keyword search provided by the Wikipedia Search Engine. Theexperiments show that SWiPE outperforms the results provided by Wikipedia; and it alsoperforms sensibly better than Xser; obtaining an overall 85% of totally correct answers vs.68% of Xser. Among all answered questions; we obtain a precision of 100% and recall 96%.SWiPE is also able to answer more questions than the other systems. A formalcharacterization of the set of SPARQL queries supported by the BESt Query paradigm is …,Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE); 2015 IEEE 24th International Conference on,2015,9
Discovering attribute and entity synonyms for knowledge integration and semantic web search,Hamid Mousavi; Shi Gao; Carlo Zaniolo,Abstract We propose the Context-aware Synonym Suggestion System (CS 3) which learnssynonyms from text by using our NLP-based text mining framework; called SemScape; andalso from existing evidence in the current knowledge bases (KBs). Using CS 3 and ourpreviously proposed knowledge extraction system IBminer; we integrate some of the publiclyavailable knowledge bases into one of the superior quality and coverage; called IKBstore.,Proceedings of the 3rd International Workshop on Semantic Search Over the Web,2013,9
A declarative extension of horn clauses; and its significance for datalog and its applications,Mirjana Mazuran; Edoardo Serra; Carlo Zaniolo,Abstract FS-rules provide a powerful monotonic extension for Horn clauses that supportsmonotonic aggregates in recursion by reasoning on the multiplicity of occurrences satisfyingexistential goals. The least fixpoint semantics; and its equivalent least model semantics; holdfor logic programs with FS-rules; moreover; generalized notions of stratification and stablemodels are easily derived when negated goals are allowed. Finally; the generalization oftechniques such as seminaive fixpoint and magic sets; make possible the efficientimplementation of Datalog FS; ie; Datalog with rules with Frequency Support (FS-rules) andstratified negation. A large number of applications that could not be supported efficiently; orcould not be expressed at all in stratified Datalog can now be easily expressed andefficiently supported in Datalog FS and a powerful Datalog FS system is now being …,Theory and Practice of Logic Programming,2013,9
A new; fast and accurate algorithm for hierarchical clustering on Euclidean distances,Elio Masciari; Giuseppe Massimiliano Mazzeo; Carlo Zaniolo,Abstract A simple hierarchical clustering algorithm called CLUBS (for CLustering UsingBinary Splitting) is proposed. CLUBS is faster and more accurate than existing algorithms;including k-means and its recently proposed refinements. The algorithm consists of adivisive phase and an agglomerative phase; during these two phases; the samples arerepartitioned using a least quadratic distance criterion possessing unique analyticalproperties that we exploit to achieve a very fast computation. CLUBS derives good clusterswithout requiring input from users; and it is robust and impervious to noise; while providingbetter speed and accuracy than methods; such as BIRCH; that are endowed with the samecritical properties.,Pacific-Asia Conference on Knowledge Discovery and Data Mining,2013,9
Deducing infoboxes from unstructured text in wikipedia pages,Hamid Mousavi; Deirdre Kerr; Markus Iseli; Carlo Zaniolo,Abstract—InfoBoxes in Wikipedia pages were originally meant as quick references forreaders. However; an assortment of knowledge bases built from such InfoBoxes now play acrucial role in a variety of important applications; including review summarization; documentcategorization; question answering; and semantic search. Unfortunately; current InfoBoxessuffer from incompleteness; inconsistencies; and inaccuracies; largely due to the fact thatthey are created manually. Previous research attempts to correct these problems have reliedon text mining approaches that mostly exploit structured information in Wikipedia such asinternal links; redirects; or disambiguation pages; but not the morphological information inthe text. In this paper; we present a novel system; IBminer; to derive structured information(in form of InfoBoxes) from the free text in Wikipedia pages using Natural Language …,CSD Technical Report# 130001); UCLA,2013,9
Improving the accuracy of continuous aggregates and mining queries on data streams under load shedding,Yan-Nei Law; Carlo Zaniolo,Random samples are common in data streams applications due to limitations in datasources and transmission lines; or to load-shedding policies. Here we introduce a formalerror model and show that; besides providing accurate estimates; it improves query answeraccuracy by exploiting past statistics. The method is general; robust in the presence ofconcept drift; and minimises uncertainties due to sampling with negligible time and spaceoverhead. We describe the application of the method; and the results obtained for SQLwindow aggregates; statistical aggregates such as quantiles; and data mining functionssuch as k-means clustering and naive Bayesian classifiers.,International Journal of Business Intelligence and Data Mining,2008,9
Mining noisy data streams via a discriminative model,Fang Chu; Yizhou Wang; Carlo Zaniolo,Abstract The two main challenges typically associated with mining data streams are conceptdrift and data contamination. To address these challenges; we seek learning techniques andmodels that are robust to noise and can adapt to changes in timely fashion. In this paper; weapproach the stream-mining problem using a statistical estimation framework; and proposea discriminative model for fast mining of noisy data streams. We build an ensemble ofclassifiers to achieve adaptation by weighting classifiers in a way that maximizes thelikelihood of the data. We further employ robust statistical techniques to alleviate theproblem of noise sensitivity. Experimental results on both synthetic and real-life data setsdemonstrate the effectiveness of this new discriminative model.,International Conference on Discovery Science,2004,9
Grammars and automata to optimize chain logic queries,Sergio Greco; Domenico Sacca; Carlo Zaniolo,The critical problem of finding efficient implementations for recursive queries with boundarguments offers many open challenges of practical and theoretical import. In particular;there is a need to find methods that are effective for the general case; such as non-linearprograms; as well as for specialized cases; such as left-recursive linear programs. In thispaper; we propose a novel approach that solves this problem for chain queries; ie; forqueries where bindings are propagated from arguments in the head to arguments in the tailof the rules; in a chain-like fashion. The method; called pushdown method; is based on thefact that a chain query can have associated a context-free language and a pushdownautomaton recognizing this language can be emulated by rewriting the query as a particularfactorized left-linear program. The proposed method generalizes and unifies previous …,International Journal of Foundations of Computer Science,1999,9
Multilingual knowledge graph embeddings for cross-lingual knowledge alignment,Muhao Chen; Yingtao Tian; Mohan Yang; Carlo Zaniolo,Abstract: Many recent works have demonstrated the benefits of knowledge graphembeddings in completing monolingual knowledge graphs. Inasmuch as related knowledgebases are built in several different languages; achieving cross-lingual knowledge alignmentwill help people in constructing a coherent knowledge base; and assist machines in dealingwith different expressions of entity relationships across diverse human languages.Unfortunately; achieving this highly desirable crosslingual alignment by human labor is verycostly and errorprone. Thus; we propose MTransE; a translation-based model formultilingual knowledge graph embeddings; to provide a simple and automated solution. Byencoding entities and relations of each language in a separated embedding space;MTransE provides transitions for each embedding vector to its cross-lingual counterparts …,arXiv preprint arXiv:1611.03954,2016,8
Parallel Bottom-Up Evaluation of Logic Programs: DeALS on Shared-Memory Multicore Machines.,Mohan Yang; Alexander Shkapsky; Carlo Zaniolo,Abstract Delivering superior expressive power over RDBMS; while maintaining competitiveperformance; has represented the main goal and technical challenge for deductivedatabase research since its inception forty years ago. Significant progress toward thisambitious goal is being achieved by the DeALS system through the parallel bottom-upevaluation of logic programs; including recursive programs with monotonic aggregates; on ashared-memory multicore machine. In DeALS; a program is represented as an AND/OR tree;where the parallel evaluation instantiates multiple copies of the same AND/OR tree thataccess the tables in the database concurrently. Synchronization methods such as locks areused to ensure the correctness of the evaluation. We describe a technique which finds anefficient hash partitioning strategy of the tables that minimizes the use of locks during the …,ICLP (Technical Communications),2015,8
Sequential pattern mining from trajectory data,Elio Masciari; Gao Shi; Carlo Zaniolo,Abstract In this paper; we study the problem of mining for frequent trajectories; which iscrucial in many application scenarios; such as vehicle traffic management; hand-off incellular networks; supply chain management. We approach this problem as that of mining forfrequent sequential patterns. Our approach consists of a partitioning strategy for incomingstreams of trajectories in order to reduce the trajectory size and represent trajectories asstrings. We mine frequent trajectories using a sliding windows approach combined with acounting algorithm that allows us to promptly update the frequency of patterns. In order tomake counting really efficient; we represent frequent trajectories by prime numbers; wherebythe Chinese reminder theorem can then be used to expedite the computation.,Proceedings of the 17th International Database Engineering & Applications Symposium,2013,8
Time-stamp management and query execution in data stream management systems,Yijian Bai; Hetal Thakkar; Haixun Wang; Carlo Zaniolo,Relational query languages can effectively express continuous queries on data streamsafter modest extensions. However; implementing such queries efficiently in data streammanagement systems requires major changes in execution models and optimizationtechniques. In particular; finer-granularity execution models that are conducive to effectivetime-stamp management and response-time optimization must replace databases' relationalalgebra schemes. This article introduces such a model and uses it to solve the idle-waitingproblems of data stream operators; such as unions; joins; and aggregates over windowswith slides.,IEEE Internet Computing,2008,8
Managing Multiversion Documents & Historical Databases: a Unified Solution Based on XML.,Fusheng Wang; Carlo Zaniolo; Xin Zhou; Hyun Jin Moon,ABSTRACT XML can provide a very effective environment for the preservation of digitalinformation whereby historical information can be easily preserved and searched throughpowerful historical queries. We propose a unified approach to represent multiversion XMLdocuments and transaction-time databases in XML; and show that temporal queries canthen be expressed in standard XQuery. In our demo we demonstrate the benefits of thisapproach on several examples; including the UCLA course catalog; W3C XLink standards;the CIA WorldFact Book; and a database of company employees. We will also demonstratethe ICAP and ArchIS system that have explored two alternative implementationarchitectures; one based on native XML DBMS; and the other on mapping the historical XMLviews back into a relational DBMS.,WebDB,2005,8
Pushing extrema aggregates to optimize logic queries,Filippo Furfaro; Sergio Greco; Sumit Ganguly; Carlo Zaniolo,Abstract In this paper; we explore the possibility of transforming queries with minimum andmaximum predicates into equivalent queries that can be computed more efficiently. Themain contribution of the paper is an algorithm for propagating min and max predicates whilepreserving query equivalence under certain monotonicity constraints. We show that thealgorithm is correct and that the transformed query is often safe when the original one is not.Although in this paper we use logic rules; the technique presented can be used to optimize(possibly recursive) queries expressed using SQL3.,Information Systems,2002,8
Extending stratified datalog to capture complexity classes ranging from ${\cal P} to {\cal QH} $,Sergio Greco; Domenico Sacca; Carlo Zaniolo,Abstract. This paper presents a unified solution to the problem of extending stratifiedDATALOG to express database complexity classes ranging from \calPto\calQH;\calQH is thequery hierarchy containing the decision problems that can be solved in polynomial time by adeterministic Turing machine using a constant number of calls to an \calNP-oracle. Thesolution is based on (i) stratified negation as the core of a simple; declarative semantics fornegation;(ii) the use of a “choice” construct to capture the nondeterminism of stable modelsin a disciplined fashion;(iii) the ability to bind a query to the lowest complexity level thatincludes the problem at hand; and (iv) a general algorithm that adapts its behavior to thedesired level of complexity required by the query so that exponential time computation isonly required for hard problems.,Acta Informatica,2001,8
Using LDL++ For Spatio-Temporal Reasoning in Atmospheric Science Databases,Richard Muntz; Eddie Shek; Carlo Zaniolo,Abstract We describe a system being built at UCLA to support spatio-temporal analysis andqueries on very large databases of atmospheric science data. The LDL++ system isintegrated into a prototype used for the detection; analysis and visualization of evolvingphysical phenomena involving massive data sets produced by combined atmospheric andocean global models. We briefly describe how the architecture of our system is evolving tohandle massive data sets; and then we focus on the uses of LDL++ in supporting spatial andtemporal reasoning.,*,1995,8
Declarative Semantics for Pruning Operators in Logic Programming.,Fosca Giannotti; Dino Pedreschi; Carlo Zaniolo,ABSTRACT Stable models; a concept from autoepisternic logic; have been recentlyproposed to define the semantics of logic programs with negation. A program may haveseveral stable models; and this multiplicity can be exploited to characterize “don't care"nondeterminism; such as that arising from pruning operators. in the (bottom-up) context ofdeductive databases; a semantics for the nondeterministic choice construct was given bySaccà and Zaniolo; ln this paper; we extend their approach to handle the cut-like pruningoperators in the top-down evaluation context; inciuding the one-of and commit operators.The goal is accomplished by means of a series of program transformations. The transformedprogram is shown to have several stable models; each corresponding to a set of solutionsobtained by top-down evaluation augmented with a nondeterministic pruning operator.,Meth. of Logic in CS,1994,8
Data streams and data stream management systems and languages,Emanuele Panigati; Fabio A Schreiber; Carlo Zaniolo,Abstract The massive usage of data streams dates back to artificial satellite informationprocessing systems and to their commercial application in the early 1970s; such as intelecommunications switching; land monitoring; meteorological surveillance; etc. Today theyare extensively used in monitoring systems applications based on wired and wirelesssensor networks; in social networks; and in the Internet of Things [20]. The main functionalgoals of data stream management systems (DSMSs) are as follows:(a) results must bepushed to the output promptly and eagerly while input tuples continue to arrive and (b)because of the unbounded and massive nature of data streams; all past tuples cannot bememorized for future use. Only synopses can be kept in memory and the rest must bediscarded.,*,2015,7
Text-mining; structured queries; and knowledge management on web document corpora,Hamid Mousavi; Maurizio Atzori; Shi Gao; Carlo Zaniolo,Abstract Wikipedia's InfoBoxes play a crucial role in advanced applications and provide themain knowledge source for DBpedia and the powerful structured queries it supports.However; InfoBoxes; which were created by crowdsourcing for human rather than computerconsumption; suffer from incompleteness; inconsistencies; and inaccuracies. To overcomethese problems; we have developed (i) the IBminer system that extracts InfoBox informationby text-mining Wikipedia pages;(ii) the IKBStore system that integrates the informationderived by IBminer with that of DBpedia; YAGO2; WikiData; WordNet; and other sources;and (iii) SWiPE and InfoBox Editor (IBE) that provide a user-friendly interfaces for queryingand revising the knowledge base. Thus; IBminer uses a deep NLP-based approach toextract from text a semantic representation structure called TextGraph from which the …,ACM SIGMOD Record,2014,7
Harvesting domain specific ontologies from text,Hamid Mousavi; Deirdre Kerr; Markus Iseli; Carlo Zaniolo,Ontologies are a vital component of most knowledge-based applications; including semanticweb search; intelligent information integration; and natural language processing. Inparticular; we need effective tools for generating in-depth ontologies that achievecomprehensive converge of specific application domains of interest; while minimizing thetime and cost of this process. Therefore we cannot rely on the manual or highly supervisedapproaches often used in the past; since they do not scale well. We instead propose a newapproach that automatically generates domain-specific ontologies from a small corpus ofdocuments using deep NLP-based text-mining. Starting from an initial small seed of domainconcepts; our Onto Harvester system iteratively extracts ontological relations connectingexisting concepts to other terms in the text; and adds strongly connected terms to the …,Semantic Computing (ICSC); 2014 IEEE International Conference on,2014,7
Publishing naive Bayesian Classifiers: Privacy without accuracy loss,Barzan Mozafari; Carlo Zaniolo,Abstract We address the problem of publishing a Naïve Bayesian Classifier (NBC) or;equivalently; publishing the necessary views for building an NBC; while protecting privacy ofthe individuals who provided the training data. Our approach completely preserves theaccuracy of the original classifier; and thus significantly improves on current approaches;such as randomization or anonymization; which typically degrade accuracy to preserveprivacy. Current query-view security checkers address the question of'Is the view safe topublish?'and are computationally expensive (often Π p 2-complete). Here instead; we tacklethe question of'How to make a view safe to publish?'and propose a linear-time algorithm topublish safe NBC-enabling views. We first show that a simple measure that restricts theratios between the published NBC statistics is sufficient to prevent any breach of privacy …,Proceedings of the VLDB Endowment,2009,7
Extending SQL for Decision Support Applications.,Haixun Wang; Carlo Zaniolo,The challenge of extending database systems for decision support applications has beenthe topic of much recent research—a very incomplete list of previous work includes [11; 8;12; 4; 10; 5]. Yet; there is no generally accepted solution for the problem; which remains acritical one; since the inability of current DBMSs to support data mining applications is well-tested and clearly documented [12]. Our research approach in addressing this difficultproblem is motivated by the observation that aggregate functions provide the linchpin formost decision support computations; moreover inductive discovery from large data sets canbe viewed as the process of aggregating low level data into statistical summaries ofsemantic significance. Therefore; the ATLaS system designed at UCLA [2] allows end-usersto define new powerful aggregate functions by writing them in SQL. The same mechanism …,DMDW,2002,7
A Simple Model for Active Rules and their Behavior in Deductive Databases.,Carlo Zaniolo; Reza Sadri,Abstract Recent advances in non-monotonic semantics of deductive databases provide asimple framework for modeling the even-condition-action rules of active databases. Thisapproach unifies the semantics of active and deductive databases and yields severalbenefits. In particular it can be used to model the semantics of different active databases andto perform termination analysis for active rules.,Workshop on Deductive Databases and Logic Programming,1994,7
Safety and Optimization of Horn Clause Queries.,Ravi Krishnamurthy; Carlo Zaniolo,*,XP7. 52 Workshop on Database Theory,1986,7
Mixed transitivity for functional and multivalued dependencies in database relations,Carlo Zaniolo,In the framework of the relational approach to database systems [5; 7] functional andmultivalued dependencies have supplied the basic tools for a formal analysis of databasesemantics and the definition and design of database schémas [3]. Functional dependieshave been part of the relational model since its inception [6]. Multivalued dependencies;more recently introduced by Zaniolo [10] and; independently; by Fagin [8]; can be regardedas a generalization of functional dependencies with which they share many formalproperties. Much of the work in relational database schema analysis; definition and design isbased on these dependencies and their inference rules (also called axioms)[3]. Of particularimportance is the availability of a complete system of inference rules (completeaxiomatization). A complete axiomatization for the combined functional and multivalued …,Inf. Process. Lett.,1980,7
Relational Schemas for Database Systems,Carlo Zaniolo; Michel A Melkanoff,*,*,1978,7
Logic and databases: A history of deductive databases,Jack Minker; Dietmar Seipel; Carlo Zaniolo,*,*,2014,6
Fast computation of approximate biased histograms on sliding windows over data streams,Hamid Mousavi; Carlo Zaniolo,Abstract Histograms provide effective synopses of large data sets; and are thus used in awide variety of applications; including query optimization; approximate query answering;distribution fitting; parallel database partitioning; and data mining. Moreover; very fastapproximate algorithms are needed to compute accurate histograms on fast-arriving datastreams; whereby online queries can be supported within the given memory and computingresources. Many real-life applications require that the data distribution in certain regionsmust be modeled with greater accuracy; and Biased Histograms are designed to addressthis need. In this paper; we define biased histograms over data streams and sliding windowson data streams; and propose the Bar Splitting Biased Histogram (BSBH) algorithm toconstruct them efficiently and accurately. We prove that BSBH generates expected∈ …,Proceedings of the 25th International Conference on Scientific and Statistical Database Management,2013,6
Supporting database provenance under schema evolution,Shi Gao; Carlo Zaniolo,Abstract Database schema upgrades are common in modern information systems; wherethe provenance of the schema is of much interest; and actually required to explain theprovenance of contents generated by the database conversion that is part of such upgrades.Thus; an integrated management for data and metadata is needed; and the ArchivedMetadata and Provenance Manager (AM&PM) system is the first to address this requirementby building on recent advances in schema mappings and database upgrade automation.Therefore AM&PM (i) extends the Information Schema with the capability of archiving theprovenance of the schema and other metadata;(ii) provides a timestamp basedrepresentation for the provenance of the actual data; and (iii) supports powerful queries onthe provenance of the data and on the history of the metadata. In this paper; we present …,International Conference on Conceptual Modeling,2012,6
Information systems integration and evolution: Ontologies at rescue,Carlo A Curino; Letizia Tanca; Carlo Zaniolo,Abstract The life of a modern Information System is often characterized by (i) a push towardintegration with other systems; and (ii) the evolution of its data management core inresponse to continuously changing application requirements. Most of the current proposalsdealing with these issues from a database perspective rely on the formal notions of mappingand query rewriting. This paper presents the research agenda of ADAM (Advanced DataAnd Metadata Manager); by harvesting the recent theoretical advances in this area into aunified framework; ADAM seeks to deliver practical solutions to the problems of automaticschema mapping and assisted schema evolution. The evolution of an Information System(IS) reflects the changes occurring in the application reality that the IS is modelling: thus;ADAM exploits ontologies to capture such changes and provide traceability and …,International Workshop on Semantic Technologies in System Maintenance (STSM),2008,6
Mining databases and data streams with query languages and rules,Carlo Zaniolo,Abstract Among data-intensive applications that are beyond the reach of traditional DataBase Management Systems (DBMS); data mining stands out because of practicalimportance and the complexity of the research problems that must be solved before thevision of Inductive DBMS can become a reality. In this paper; we first discuss technicaldevelopments that have occurred since the very notion of Inductive DBMS emerged as aresult of the seminal papers authored by Imielinski and Mannila a decade ago. The researchprogress achieved since then can be subdivided into three main problem subareas asfollows:(i) language (ii) optimization; and (iii) representation. We discuss the problems inthese three areas and the different approaches to Inductive DBMS that are made possible byrecent technical advances. Then; we pursue a language-centric solution; and introduce …,International Workshop on Knowledge Discovery in Inductive Databases,2005,6
Polynomial-time computable stable models,Luigi Palopoli; Carlo Zaniolo,Abstract We study the relations between the expressive power of non-monotonic formalismsand polynomial-time computability in the framework of stable models semantics. While theproblem of deciding whether a logic program has a total stable model is NP-complete; weintroduce a polynomial-time algorithm that generates such a model for several importantclasses of programs; that are discussed in this paper. In the general case; the algorithmgenerates a (not necessarily total) p-stable model of the input program.,Annals of Mathematics and Artificial Intelligence,1996,6
Architecture of deductive database systems,Carlo Zaniolo,Deductive databases are coming of age with the emergence of efficient and easy-to-usesystems that support queries; reasoning; and application development on databasesthrough declarative logic-based languages. The progress is demonstrated by the completionof prototype systems offering such levels of generality; performance; and robustness thatthey support well complex application development. An overview of the architectures andtechniques of these systems is provided. The main motivations for development of thedeductive database systems (DDSs) are:(1) to provide support for advanced databaseapplications; with a focus on expert systems and knowledge-based applications; and (2) toprovide better support for traditional database applications by integrating the applicationdevelopment and database queries into one language; thus solving the impedance …,Compcon Spring'90. Intellectual Leverage. Digest of Papers. Thirty-Fifth IEEE Computer Society International Conference.,1990,6
Expert database systems (workshop review),Michael Brodie; Charles Kellogg; D Stott Parker; Gio Wiederhold; Carlo Zaniolo; Larry Chairman-Kerschberg,University of South Carolina; in cooperation with the ACM Special Interest Groups SIGMOD andSIGART and the IEEE Technical CommIttee on Data Base Engineering The Working Groupswere charged with explormg research Issues related to the various facets of Expert DatabaseSystems The Panel Chair served as Program Chairman for the EDS Workshop and the PanelMembers organized and coordmated the Working Groups The goal of this panel 1s to reviewthe Working Group 1' scussions and conclusions; and to explore research topics related to ExpertDatabase Systems that span the boundarles of the various Groups 1 INTRODUCTION TheFu-st InternatIonal Workshop on Expert Data- base Systems provided a forum to address thetheoretl- cal and practical Issues involved in making database sys- tems more knowledgeableand supportive of Artificial Intelligence applications Although the Aelds of Expert Systems …,ACM SIGMOD Record,1985,6
BOOT-TS: A Scalable Bootstrap for Massive Time-Series Data,Nikolay Laptev; Carlo Zaniolo; Tsai-Ching Lu; CA Malibu,Abstract We propose a scalable method of assessing the quality of machine learningalgorithms over sampled time-series data. While bootstrap provides a simple and powerfulmeans of estimating accuracy; its application to large time-series data still suffers fromscalability issues. As an alternative we introduce BOOT-TS; a scalable extension ofbootstrap for time-series which utilizes the recent advances in bootstrap and time-seriestheory to provide a practical implementation for assessing a time-series sample quality usingHadoop. For instance; our new procedure yields a robust and computationally efficientmeans of assessing the quality of our Twitter analytics workflow over large; real-world; time-series data.,NIPS 2012 Workshop on Parallel and Large Scale Machine Learning (Big Learning); Lake Tahoe; Nevada; CA,2012,5
A Logic-based Language for Data Streams.,Carlo Zaniolo,Abstract. Data Stream Management Systems (DSMS) have attracted much interest; andvarious extensions of relational-database query languages have been proposed for datastreams. However; relational query languages were built on the solid bedrock of logic; whilecurrent DSMS languages and their computation models are missing such foundations. Inthis paper; we show that continuous queries can be characterized using the familiarconcepts of closed-world and local stratification; leading to Streamlog that allows a freer andmore natural usage of nonmonotonic constructs than Datalog. Thus; Streamlog takes thequery languages of DSMS to new levels of expressive power and removes the limitationsthat severely impair current commercial systems and research prototypes.,SEBD,2012,5
Provenance Management in Databases Under Schema Evolution.,Shi Gao; Carlo Zaniolo,Abstract Since changes caused by database updates combine with the internal changescaused by database schema evolution; an integrated provenance management for data andmetadata represents a key requirement for modern information systems. In this paper; weintroduce the Archived Metadata and Provenance Manager (AM&PM) system whichaddresses this requirement by (i) extending the Information Schema with the capability ofrepresenting the provenance of the schema and other metadata;(ii) providing a simple time-stamp based representation of the provenance of the actual data; and (iii) supportingpowerful queries on the provenance of the data and the history of the metadata.,TaPP,2012,5
Continuous post-mining of association rules in a data stream management system,Hetal Thakkar; Barzan Mozafari; Carlo Zaniolo,The real-time (or just-on-time) requirement associated with online association rule miningimplies the need to expedite the analysis and validation of the many candidate rules; whichare typically created from the discovered frequent patterns. Moreover; the mining process;from data cleaning to post-mining; can no longer be structured as a sequence of stepsperformed by the analyst; but must be streamlined into a workflow supported by an efficientsystem providing quality of service guarantees that are expected from modern Data StreamManagement Systems (DSMSs). This chapter describes the architecture and techniquesused to achieve this advanced functionality in the Stream Mill Miner (SMM) prototype; anSQL-based DSMS designed to support continuous mining queries. introduction systems;much ofrecentresearch work has focused,Post-Mining of Association Rules: Techniques for Effective Knowledge Extraction,2009,5
The nonmonotonic semantics of active rules in deductive databases,Carlo Zaniolo,Abstract We provide a logic-based semantics for active database rules under theimmediately-after; tuple-level activation policy of SQL3. This result allows us to characterizeand analyze the behavior of complex rule systems using the declarative framework of stablemodels; and to lay the theoretical foundation for a very desirable integration of activedatabases and deductive databases.,International Conference on Deductive and Object-Oriented Databases,1997,5
Database Transactions in LDL.,Ravi Krishnamurthy; Shamim A Naqvi; Carlo Zaniolo,*,NACLP,1989,5
SPARQLT and its User-Friendly Interface for Managing and Querying the History of RDF Knowledge Bases,Shi Gao; Muhao Chen; Maurizio Atzori; Jiaqi Gu; Carlo Zaniolo,Segnalazioni con codici 20501/20503/20504: Alcuni dati obbligatori per il sito CINECA nonsono presenti o invalidi; oppure il sito CINECA non è riuscito ad individuare una rivista con idati forniti; è necessario controllare la correttezza dell'ISSN e/o EISSN dove applicabili e iltitolo della rivista.Segnalazioni con codici 20201/20202: La pubblicazione non è statatrasferita SOLO per i docenti segnalati nel messaggio a causa di problemi nell'anagraficaCINECA e/o di Ateneo (pe i codici fiscali non sono gli stessi) oppure perché si tratta didocenti che; pur essendo abilitati all'inserimento nel sistema di Ateneo; non hanno facoltà disincronizzare le proprie pubblicazioni in CINECA. Tutti gli altri eventuali coautori senzasegnalazione troveranno la pubblicazione correttamente nel proprio spazio personaleCINECA.,14th International Semantic Web Conference (ISWC 2015),2015,4
Optimization of massive pattern queries by dynamic configuration morphing,Nikolay Laptev; Carlo Zaniolo,Complex pattern queries play a critical role in many applications that must efficiently searchdatabases and data streams. Current techniques support the search for multiple patternsusing deterministic or non-deterministic automata. In practice however; the static patternrepresentation does not fully utilize available system resources; subsequently suffering frompoor performance. Therefore a low overhead auto-reconfigurable automaton is needed thatoptimizes pattern matching performance. In this paper; we propose a dynamic system thatentails the efficient and reliable evaluation of a very large number of pattern queries on aresource constrained system under changing stress-load. Our system prototype; Morpheus;pre-computes several query pattern representations; named templates; which are thenmorphed into a required form during run-time. Morpheus uses templates to speed up …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,4
The logic of query languages for data streams,Carlo Zaniolo,Abstract Data Stream Management Systems (DSMS) represent a vibrant research area thatis rich in technical challenges; which many projects have approached by extendingdatabase query languages and models for continuous queries on data streams [1; 3; 4; 9; 5].These database-inspired approaches have delivered remarkable systems and applications;but have yet to produce solid conceptual foundations for DSMS data models and querylanguages---particularly if we compare with the extraordinary ones of relational databases. Acornerstone of the success of relational databases was the tight coupling between their datamodel and their logic-based query languages. In this paper; we show that a similarapproach can succeed for data streams and propose a tight-coupled design for DSMS datamodels and query languages. To express more naturally the behavior of a data stream …,Proceedings of the 4th International Workshop on Logic in Databases,2011,4
A Flexible Query Graph Based Model for the Efficient Execution of Continuous Queries,Yijian Bai; Hetal Thakkar; Haixun Wang; Carlo Zaniolo,In this paper; we propose a simple and flexible execution model that (i) supports a widespectrum of alternative optimization and execution strategies and their mixtures;(ii) providesfor dynamic reconfiguration when adding/deleting queries and changing optimizationgoals;(iii) optimizes response time in idle-waiting prone operators; such as union; joins; andoperators used in time series and temporal sequence queries. Thus; we introduce a flexibleand concrete model of execution semantics for continuous DSMS queries; and demonstrateits many applications. Our tuple-oriented model dovetails and complements the abstract set-oriented semantics of current DSMS constructs and operators; which are often based onrelational algebra and SQL enhanced with windows.,Data Engineering Workshop; 2007 IEEE 23rd International Conference on,2007,4
Version management and historical queries in digital libraries,Fusheng Wang; Carlo Zaniolo; Xin Zhou; Hyun Jin Moon,Historical information can be effectively preserved using XML and searched throughpowerful historical queries written in XQuery. Indeed; by storing the successive versions of adocument in an incremental fashion; XML repositories and data warehouses can achieve (i)the efficient preservation of critical information;(ii) its representation using a temporallygrouped data model and (iii) the ability of supporting historical queries on the evolution ofdocuments and their contents using XQuery. The proposed approach can be applieduniformly to (a) XML document archives and (b) transaction time databases published inXML and queried in XQuery. Our case studies include the UCLA course catalog; W3C Xlinkstandards; and the CIA WorldFact Book; besides relational databases. The experiencedescribed here and in the work of Wang et al.(2005 and 2003) suggests that current …,Temporal Representation and Reasoning; 2005. TIME 2005. 12th International Symposium on,2005,4
Temporal information management using XML,Fusheng Wang; Xin Zhou; Carlo Zaniolo,Abstract A closer integration of XML and database systems is actively pursued byresearchers and vendors because of the many practical benefits it offers. Additional specialbenefits can be achieved on temporal information management–an important applicationarea that represents an unsolved challenge for relational databases [1]. Indeed; XML datamodel and query languages support: Temporally grouped representations that have longbeen recognized as a natural data model for historical information [2]; and Turing-completequery languages; such as XQuery [3]; where all the constructs needed for temporal queriescan be introduced as user-defined libraries; without requiring extensions to existingstandards. By contrast; the flat relational tables of traditional DBMSs are not well-suited fortemporally grouped representations [4]; moreover; significant extensions are required to …,International Conference on Conceptual Modeling,2004,4
Blocking; monotonicity; and turing completeness in a database language for sequences and streams,Yan-Nei Law; Haixun Wang; Carlo Zaniolo,Page 1. Blocking; Monotonicity; and Turing Completeness in a Database Language forSequences and Streams Yan-Nei Law Computer Science Dept; UCLA Los Angeles; CA 90095ynlaw@cs.ucla.edu Haixun Wang IBM TJ Watson Research Hawthorne; NY 10532haixun@us.ibm.com Carlo Zaniolo Computer Science Dept; UCLA Los Angeles; CA 90095zaniolo@cs.ucla.edu Abstract We propose a database language; based on simple extensionsof the relational data model and SQL; and show that it is Turing-complete and supportive ofsequences and data streams. In fact; we obtain a simple characterization of theblocking/non-blocking behavior of queries and operators on streams: we show that non-blockingoperators can be characterized algebraically by the monotonic behavior of the functions theyrealize; and syntactically; by the structure of the programs used to define them …,Submitted for publication,2003,4
Deductive Databases: Challenges; opportunities and future directions,Arno Siebes; Shalom Tsur; Jeff Ullman; Laurent Vieille; Carlo Zaniolo,-The lack of simple and effective solutions to problems such as non-stratifiednegation/aggregates; and support for updates or objects in logic; has reduced theeffectiveness of deductive databases in application domains ranging from Bill-of-Materials totemporal queries; and from GIS applications to concreteview maintenance. How much havethese problems hampered the field in the past; and what is your forecast for the future?-Many prototypes developed in the past lacked in functionality; robustness; availability andperformance. How seriously have these problems impacted the deployment of the newtechnology; and will the situation change in the future?,*,1996,4
Expressive Power of Non-Deterministic Operators for Logic-based Languages.,Luca Corciulo; Fosca Giannotti; Dino Pedreschi; Carlo Zaniolo,Abstract Non-deterministic operators are needed in First-Order relational languages andDatalog to extend the expressive power of such languages and support efficientformulations of lowcomplexity problems. In this paper; we study the operators proposed inthe literature; including witness; lazy choice and dynamic choice; and compare their powerof expressing deterministic and non-deterministic queries. We obtain a simple hierarchy thatrelates these operators with each other and with other constructs; such as negation andfixpoint.,Workshop on Deductive Databases and Logic Programming,1994,4
Database updates and AI planning domains,VS Subrahmanian; Carlo Zaniolo,Abstract In this paper; we show that there is a simple connection between logicprogramming and planning. The main result of this paper is the following: given anyplanning domain consisting of an initial state; and a set of operation definitions; this domaincan be translated; in linear-time; to a logic program such that a given goal G is achievable inthe planning domain iff a related goal G? is true in some stable model of the logic programobtained by the translation. We show that this translation yields at least two interestingconsequences:(1) methods to update databases can be used to handle surprises whenexecuting plans (ie a surprise occurs when an initial plan is partly executed; but one of theresulting intermediate states differs; perhaps due to external reasons; from what ispredicted).(2) rigid actions; which are actions that must be executed when their pre …,*,1993,4
Scaling up the performance of more powerful Datalog systems on multicore machines,Mohan Yang; Alexander Shkapsky; Carlo Zaniolo,Abstract Extending RDBMS technology to achieve performance and scalability for queriesthat are much more powerful than those of SQL-2 has been the goal of deductive databaseresearch for more than thirty years. The D e ALSD e ALS system has made major progresstoward this goal; by (1) Datalog extensions that support the more powerful recursive queriesneeded in advanced applications; and (2) superior performance for both traditional recursivequeries and those made possible by the new extensions; while (3) delivering competitiveperformance with commercial RDBMSs on non-recursive queries. In this paper; we focus onthe techniques used to support the in-memory evaluation of Datalog programs on multicoremachines. In D e ALSD e ALS; a Datalog program is represented as an AND/OR tree; andmultiple copies of the same AND/OR tree are used to access the tables in the database …,The VLDB Journal,2017,3
A discussion on the biological relevance of clustering results,Pietro Hiram Guzzi; Elio Masciari; Giuseppe Massimiliano Mazzeo; Carlo Zaniolo,Abstract The recent advances in genomic technologies and the availability of large-scaledatasets call for the development of advanced data analysis techniques; such as datamining and statistical analysis to cite a few. A main goal in understanding cell mechanismsis to explain the relationship among genes and related molecular processes through thecombined use of technological platforms and bioinformatics analysis. High throughputplatforms; such as microarrays; enable the investigation of the whole genome in a singleexperiment. Among the mining techniques proposed so far; cluster analysis has become astandard method for the analysis of microarray expression data. It can be used both for initialscreening of patients and for extraction of disease molecular signatures. Moreover;clustering can be profitably exploited to characterize genes of unknown function and …,International Conference on Information Technology in Bio-and Medical Informatics,2014,3
Mining Data Bases and Data Streams,Carlo Zaniolo; Hetal Thakkar,Abstract. Data mining represents an emerging technology area of great importance tohomeland security. Data mining enables knowledge discovery on databases by identifyingpatterns that are novel; useful; and actionable. It has proven successful in many domains;such as banking; ecommerce; genomic; investment; telecom; web analysis; link analysis;and security applications. In this chapter; we will survey the main methods and applicationsof data mining and the information systems recently developed for supporting the miningprocess. We then overview the key areas of data mining research; in particular; on-linemining of massive data streams; such as those that flow continuously on the Internet andother communication channels. We show that the traditional store-now & mine-latertechniques are no longer effective either because of the size of the data stream or …,University of California; Los Angeles,2010,3
Succinct sampling on streams,Vladimir Braverman; Rafail Ostrovsky; Carlo Zaniolo,Abstract: A streaming model is one where data items arrive over long period of time; eitherone item at a time or in bursts. Typical tasks include computing various statistics over asliding window of some fixed time-horizon. What makes the streaming model interesting isthat as the time progresses; old items expire and new ones arrive. One of the simplest andcentral tasks in this model is sampling. That is; the task of maintaining up to $ k $ uniformlydistributed items from a current time-window as old items expire and new ones arrive. Wecall sampling algorithms {\bf succinct} if they use provably optimal (up to constant factors){\bfworst-case} memory to maintain $ k $ items (either with or without replacement). We stressthat in many applications structures that have {\em expected} succinct representation as thetime progresses are not sufficient; as small probability events eventually happen with …,arXiv preprint cs/0702151,2007,3
Managing XML Versions and Replicas in a P2P Context,Deise De Brum Saccol; Nina Edelweiss; Renata De Matos Galante; Carlo Zaniolo,Abstract Peer-to-Peer (P2P) systems seek to provide sharing of computational resources;which may be duplicated or versioned over several peers. Duplicate resources (ie replicas)are the key to better query performance and availability. On the other hand; multiple versionscan be used to support queries on the lineage of resources and the evolution of history.However; traditional P2P systems are not aware of replicas and versions; which causecomplexity at the logical level and inefficiency at the physical level. To solve these problems;we propose an environment for detecting; managing and querying replicas and versions ofXML documents in a P2P context. We also show that the proposed environment can also beused for plagiarism detection; web page ranking; and software clone identification. 1.,*,2007,3
Toward extensible spatio-temporal databases: an approach based on user-defined aggregates,Cindy X Chen; Haixun Wang; Carlo Zaniolo,Abstract The need for spatial; temporal; and spatio-temporal databases is ubiquitous andtouches several application domains; including geographical information systems;autonomous navigation; tracking; medical imaging; and many others. To answer this need;database researchers have proposed many approaches that cover a wide spectrum of datamodels and query languages: a very incomplete list include [7; 15; 3; 11]. The number andvariety of alternative solutions proposed reflect the complexity of technical problems and thediversity of applications and requirements encountered in the field; whereby a solutionproposed for a certain spatio-temporal application might not satisfy the requirements ofanother. To solve this problem; we propose an approach whereby the primitives built in thesystem can be further customized and extended by the end-users via the query language …,*,2004,3
A deductive database approach to ai planning,Antonio Brogi; VS Subrahmanian; Carlo Zaniolo,Abstract In this paper; we show that the classical AI planning problem can be modelledusing simple database constructs with logic-based semantics. The approach is similar to thatused to model updates and nondeterminism in active database rules. We begin by showingthat planning problems can be automatically converted to Datalog 1 S programs withnondeterministic choice constructs; for which we provide a formal semantics using theconcept of stable models. The resulting programs are characterized by a syntactic structure(XY-stratification) that makes them amenable to efficient implementation using compilationand fixpoint computation techniques developed for deductive database systems. We firstdevelop the approach for sequential plans; and then we illustrate its flexibility andexpressiveness by formalizing a model for parallel plans; where several actions can be …,Journal of Intelligent Information Systems,2003,3
Optimization of logic queries with MIN and MAX predicates,Sergio Greco; Carlo Zaniolo; Sumit Ganguly,Abstract We propose an algorithm for pushing min and max aggregates into recursivepredicates; while preserving query equivalence under certain monotonicity constraints. Thetransformed query is often safe when the original one is not; and more efficient than theoriginal query when this is safe.,International Conference on Flexible Query Answering Systems,1998,3
A fast and accurate algorithm for unsupervised clustering around centroids,Giuseppe M Mazzeo; Elio Masciari; Carlo Zaniolo,Abstract A centroid-based clustering algorithm is proposed that works in a totallyunsupervised fashion and is significantly faster and more accurate than existing algorithms.The algorithm; named CLUBS+(for CLustering Using Binary Splitting); achieves theseresults by combining features of hierarchical and partition-based algorithms. Thus; CLUBS+consists of two major phases; ie; a divisive phase and an agglomerative phase; eachfollowed by a refinement phase. Each major phase consists of successive steps in which thesamples are repartitioned using a criterion based on least quadratic distance. This criterionpossesses unique analytical properties that are elucidated in the paper and exploited by thealgorithm to achieve a very fast computation. The paper presents the results of the extensiveexperiments performed: these confirm that the new algorithm is fast; impervious to noise …,Information Sciences,2017,2
Answering End-User Questions; Queries and Searches on Wikipedia and its History.,Maurizio Atzori; Shi Gao; Giuseppe M Mazzeo; Carlo Zaniolo,Abstract Knowledge bases (KBs) encoded using RDF triples deliver many benefits toapplications and programmers that access the KBs on the web via SPARQL endpoints. Inthis paper; we describe and compare two user-friendly systems that seek to make theuniversal knowledge of Web KBs available to users who neither know SPARQL; nor theinternals of the KBs. We first describe CANaLI; that lets people enter Natural Language (NL)questions and translates them into SPARQL queries executed on DBpedia. CANaLIremoves the ambiguities that are often present in NL communication by requiring the use ofa Controlled NL and providing on-line knowledge-driven question-completion that showsalternate correct interpretations. While CANaLI is a very powerful NL system; which placedfirst in the 2016 competition on Question Answering over Linked Data QALD-6; even …,IEEE Data Eng. Bull.,2016,2
Mining Semantics Structures from Syntactic Structures in Web Document Corpora,Hamid Mousavi; Shi Gao; Deirdre Kerr; Markus Iseli; Carlo Zaniolo,The Web is making possible many advanced text-mining applications; such as newssummarization; essay grading; question answering; semantic search and structured querieson corpora of Web documents. For many of such applications; statistical text-miningtechniques are of limited effectiveness since they do not utilize the morphological structureof the text. On the other hand; many approaches use NLP-based techniques that parse thetext into parse trees; and then use patterns to mine and analyze parse trees which are oftenunnecessarily complex. To reduce this complexity and ease the entire process of textmining; we propose a weighted-graph representation of text; called TextGraphs; whichcaptures the grammatical and semantic relations between words and terms in the text.TextGraphs are generated using a new text mining framework which is the main focus of …,International Journal of Semantic Computing,2014,2
Complex pattern matching in complex structures: the xseq approach,Kai Zeng; Mohan Yang; Barzan Mozafari; Carlo Zaniolo,There is much current interest in applications of complex event processing over datastreams and of complex pattern matching over stored sequences. While some applicationsuse streams of flat records; XML and various semi-structured information formats arepreferred by many others-in particular; applications that deal with domain science; socialnetworks; RSS feeds; and finance. XSeq and its system improve complex pattern matchingtechnology significantly; both in terms of expressive power and efficient implementation.XSeq achieves higher expressiveness through an extension of XPath based on Kleene-*pattern constructs; and achieves very efficient execution; on both stored and streaming data;using Visibly Pushdown Automata (VPA). In our demo; we will (i) show examples of XSeq indifferent application domains;(ii) explain its compilation/query optimization techniques …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,2
Fast and space-efficient computation of equi-depth histograms for data streams,Hamid Mousavi; Carlo Zaniolo,Abstract—Equi-depth histograms represent a fundamental synopsis widely used in bothdatabase and data stream applications; as they provide the cornerstone of many techniquessuch as query optimization; approximate query answering; distribution fitting; and paralleldatabase partitioning. Equi-depth histograms try to partition a sequence of data in a way thatevery part has the same number of data items. In this paper; we present a new algorithm toestimate equi-depth histograms for high speed data streams over sliding windows. Whilemany previous methods were based on quantile computations; we propose a new methodcalled BAr Splitting Histogram (BASH) that provides an expected ϵ-approximate solution tocompute the equi-depth histogram. Extensive experiments show that BASH is at least fourtimes faster than one of the best existing approaches; while achieving the same accuracy …,*,2010,2
Improving mining quality by exploiting data dependency,Fang Chu; Yizhou Wang; Carlo Zaniolo; D Stott Parker,Abstract The usefulness of the results produced by data mining methods can be criticallyimpaired by several factors such as (1) low quality of data; including errors due tocontamination; or incompleteness due to limited bandwidth for data acquisition; and (2)inadequacy of the data model for capturing complex probabilistic relationships in data.Fortunately; a wide spectrum of applications exhibit strong dependencies between datasamples. For example; the readings of nearby sensors are generally correlated; andproteins interact with each other when performing crucial functions. Therefore;dependencies among data can be successfully exploited to remedy the problems mentionedabove. In this paper; we propose a unified approach to improving mining quality usingMarkov networks as the data model to exploit local dependencies. Belief propagation is …,Pacific-Asia Conference on Knowledge Discovery and Data Mining,2005,2
Incompleteness of Database Languages for Data Streams and Data Mining: the Problem and the Cure.,Carlo Zaniolo; Chang Luo; Y Law; Haixun Wang,*,SEBD,2003,2
Extending SQL for decision support applications,Carlo Zaniolo,1. SQL–AG: extending SQL3 proposal for Aggregates to support 'early returns'2. LDL++ 5.1:Logic Database Language Monotonic Aggregates: used freely in recursive queries for BoMand greedy algorithms 3. SADL: Simple Aggregate Definition Language based on SQL.easy to use; but with limited performance and power 4. AXL: Aggregate eXtensionLanguage: Much more powerful and efficient,Presentation slides of keynote address at DMDW; Toronto; Ontario; Canada,2002,2
Spatiotemporal Queries in Database Systems,Carlo Zaniolo,My research focuses on how to extend database technology to better support spatiotemporalapplications and queries. There is now a gulf between the spatio-temporal concepts used bydomain scientists and the tools provided by database systems and other commercialsystems. Bridging; or at at least narrowing; this gap is universally viewed as a criticalobjective; but progress toward this goal has been painfully slow. This lack of progress canhardly be ascribed to a lack of interest or research efforts; in fact; many interesting solutionshave already been proposed and most alternatives in the design space have been exploredand advocated. But I do not see widely accepted solutions emerging that are likely besupported by major commercial database systems (also known as SQL DBMSs) andeventually incorporated into standards. These problems can be vividly illustrated by the …,Position Paper for the NSF Workshop on Spatio-temporal Data Model for Biogeophysical Fields,2002,2
LDL++ Tutorial,Carlo Zaniolo,PART COST BASIC PART SUPPLIER COST TIME top tube cinelli 20.00 14 top tubecolumbus 15.00 6 down tube columbus 10.00 6 head tube cinelli 20.00 14 head tubecolumbus 15.00 6 seat mast cinelli 20.00 14 seat mast columbus 15.00 6 seat stay cinelli15.00 14 seat stay columbus 10.00 6 chain stay cinelli 15.00 14 chain stay columbus 10.006 fork cinelli 40.00 14 fork columbus 30.00 6 spoke performance 0.50 3 spoke campagnolo0.60 15 nipple performance 0.10 3 hub shimano 35.00 7 hub campagnolo 31.00 5 hubsuntour 18.00 14 rim mavic 50.00 3 rim araya 70.00 1 ASSEMBLY PART SUBPART QTYframe top tube 1 frame down tube 1 frame head tube 1 frame seat mast 1 frame seat stay 2frame chain stay 2 frame fork 1 wheel spoke 36 wheel nipple 36 wheel rim 1 wheel hub 1wheel tire 1,UCLA; Dec,1998,2
Semantics and expressive power of non-deterministic constructs in deductive databases,Fosca Giannotti; Dino Pedreschi; Carlo Zaniolo,CiteSeerX - Document Details (Isaac Councill; Lee Giles; Pradeep Teregowda): this paper;.,*,1996,2
The Dynamics of Active Database Rules: Models and Refinements.,Carlo Zaniolo,Abstract Semantics represents a major problem area for active databases inasmuch as (i)there is no formal framework for defining the abstract semantics of active rules; and (ii) thevarious systems developed so far have ad-hoc operational semantics that are widelydifferent from each other. This situation contributes to the difficulty of predicting the run-timebehavior of sets of rules: thus; ensuring the termination of a given set of rules is currentlyrecognized as a major research issue. This situation hampers the applicability of thispowerful technology in critical application areas. In this paper; we introduce a durablechange semantics for active database rules; this semantics improves Starburst's deferredactivation notion with concepts taken from Postgres and Heraclitus and the semanticfoundations of deductive databases. We provide a formal logicbased model for this …,NGITS,1995,2
Efficient execution of recursive queries through controlled binding propagation,Sergio Greco; Carlo Zaniolo,Abstract This paper presents a new method for the computation of bound linear Datalogqueries and compares its performance to that of other bottom-up execution strategies. Thetechnique; called pushdown method; uses virtual stacks to store information on the currentstate of the evaluation thus ensuring both termination and efficient execution. The asymptoticworst-case behavior and experimental results for the new method compare favorablyagainst those of previous methods.,International Symposium on Methodologies for Intelligent Systems,1994,2
ACM forum,Albert D'Andrea; Carlo Zaniolo; Patrick J Brennan; Francois Bancihon; Philip A Bernstein; Michael Carey; David DeWitt; Ronald Fagin,In his" Forum" letter (Apr. 1992; pp. 16; 18) concerning the article;" Database Systems:Achievements and Opportunities"(Oct. 1991; p. 110); Henry Baker rightly asserts the well-known problems of relational database systems. Notwithstanding; Baker's amusing portrayalof the relational era as the" Dark Ages of commercial data processing" is simply not correct.In fact; future historians may well view relational technology as the primordial soup fromwhich a far superior class of database systems evolved.Despite Baker's failure to includethem with his rather manufacturing-centric views; the relational technology brought forthmany well-acknowledged benefits to the data-processing world at large. Clearly; relationaldatabase systems are being used today by a far wider audience of users than would haveever been possible with CODASYL and hierarchical database systems. And; as Ted …,Communications of the ACM,1992,2
Pure models for logic programs: a simplification and unification of logic programming semantics,E Laenens; D Vermeir; C Zaniolo,*,*,1991,2
The s System Prototype,D Chimenti; R Gamboaa; R Krishnamurthy; S Naqvi; C Zaniolo,*,tEEE Journal on Data and Knowledge,1990,2
The Database Language GEM. Readings in Database Systems,Carlo Zaniolo,*,*,1988,2
On the Design of Relational Schemata,Carlo Zaniolo; Michel A Melkanoff,CiteSeerX - Document Details (Isaac Councill; Lee Giles; Pradeep Teregowda):this paper is to present a new approach to the conceptual design of relationaldatabases based on the complete relatability conditions (CRCs).,ACM Trans. Database Syst,1981,2
A simple method for determining static parameters of large signal semiconductor diode and transistor models,C Zaniolo; LP McNamee,Abstract A simple on-line interactive computing methodology for determining staticparameters of large-signal semiconductor models is described. The procedure makes useof:(1) an automatic data collection scheme;(2) a single search; three-parameter optimizationmethod for computing diode parameters; and (3) a partitioning scheme to separate thedefining transistor equations into two single search; three-parameter problems which aresolved by the diode optimization method described in (2). The method is described bymeans of the CIRCUS diode and transistor models; and is compared experimentally withSokal's method [1; 2]. The new approach is shown to be superior.,Solid-State Electronics,1972,2
Learning multi-faceted knowledge graph embeddings for natural language processing,Muhao Chen; Carlo Zaniolo,Abstract Knowledge graphs have challenged the existing embedding-based approaches forrepresenting their multifacetedness. To address some of the issues; we have investigatedsome novel approaches that (i) capture the multilingual transitions on different language-specific versions of knowledge; and (ii) encode the commonly existing monolingualknowledge with important relational properties and hierarchies. In addition; we propose theuse of our approaches in a wide spectrum of NLP tasks that have not been well explored byrelated works.,Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),2017,1
Fast lossless frequent itemset mining in data streams using crucial patterns,Ariyam Das; Carlo Zaniolo,Abstract We study the problem of mining exact frequent itemsets from data streams. Sincethe number of frequent patterns is often quite large; concise representations that saveresources by avoiding redundancy are critical for an efficient lossless extraction of frequentpatterns. In this paper; we introduce the novel concept of crucial patterns; and formally provethem to be an effective subset of closed frequent itemsets that assures lossless extraction.Extensive experiments confirm that crucial patterns provide a significantly better losslesscompression for the frequent itemsets than other condensed representations. Lastly; wepropose our new Crucial Pattern Mining (CPM) algorithm for data streams that includes thesignificant optimization strategies described in the paper. The performance study show thatCPM consistently outperforms other state-of-the-art methods by orders of magnitude; on …,*,2016,1
Improving the power; performance and usability of datalog by pushing constraints into recursion,Carlo Zaniolo; Mohan Yang; Ariyam Das; Matteo Interlandi,Abstract. We introduce a novel query optimization method based on pushing extrema andother integrity constraints into recursion. This optimization produces a more efficientcomputation that preserves the soundness and completeness for the predicates specified bythe integrity constraints. Complex algorithms; including greedy algorithms; can now bespecified via simpler programs that are stratified with respect to min and max; and thenimplemented via optimized rules that use these aggregates in the seminaive fixpointcomputation to achieve more efficient execution. This rewriting is not limited to extrema; butin many situations a similar optimization is applicable to other constraints as well. Therelationships of this optimization technique with recent works on monotonic aggregates andtheir combined efficient implementation in the seminaive fixpoint method are also …,Submitted for publication,2016,1
Question answering on RDF KBs using controlled natural language and semantic auto completion,Giuseppe M Mazzeo; Carlo Zaniolo,Abstract. The fast growth in number; size and availability of RDF knowledge bases (KBs) iscreating a pressing need for research advances that will let people consult them withouthaving to learn structured query languages; such as SPARQL; and the internal organizationof the KBs. In this paper; we present our Question Answering (QA) system; that acceptsquestions posed in a Controlled Natural Language. The questions entered by the user areannotated on the fly; and a KB-driven autocompletion system displays suggestionscomputed in real time from the partially completed sentence the person is typing. Byfollowing these patterns; users can enter only semantically correct questions which areunambiguously interpreted by the system. This approach assures high levels of usability andgenerality. Experiments conducted on well-known QA benchmarks; including questions …,Semantic Web Journal (under review),2016,1
Extending relational query languages for data streams,Nikolay Laptev; Barzan Mozafari; Hamid Mousavi; Hetal Thakkar; Haixun Wang; Kai Zeng; Carlo Zaniolo,Abstract The design of continuous query languages for data streams and the extent to whichthese should rely on database query languages represent pivotal issues for data streammanagement systems (DSMSs). The Expressive Stream Language (ESL) of our Stream Millsystem is designed to maximize the spectrum of applications a DSMS can support efficiently;while retaining compatibility with the SQL: 2003 standards. This approach offers significantadvantages; particularly for the many applications that span both data streams anddatabases. Therefore; ESL supports minimal extensions required to overcome SQL'sexpressive power limitations—a critical enhancement since said limitations are quite severeon database applications and are further exacerbated on data stream applications; where;eg; only nonblocking query operators can be used. Thus; ESL builds on user-defined …,*,2016,1
CANaLI: A System for Answering Controlled Natural Language Questions on RDF Knowledge Bases,Giuseppe M Mazzeo; Carlo Zaniolo,ABSTRACT The fast growth in number; size and availability of rdf knowledge bases (kbs) iscreating a pressing need for research advances that will let people consult them withouthaving to learn structured query languages; such as sparql; and the internal organization ofthe kbs. In this paper we present our Question Answering (QA) system; that acceptsquestions posed in a Controlled Natural Language. The questions entered by the user areannotated on the fly; and an ontology driven auto-completion system displays suggestedpatterns computed in real time from the partially completed sentence the person is typing. Byfollowing these patterns; users can enter only semantically correct questions which areunambiguously interpreted by the system. This approach assures high levels of usability andgenerality. Experiments conducted on well-known QA benchmarks; including questions …,*,2016,1
The Magic of Pushing Extrema into Recursion: Simple; Powerful Datalog Programs.,Carlo Zaniolo; Mohan Yang; Ariyam Das; Matteo Interlandi,Abstract. We introduce a novel query optimization method based on pushing extrema andother integrity constraints into recursion. This optimization produces an efficient operationalsemantics for the goals declaratively specified by the original program that is stratified withrespect to negation and aggregates. Complex algorithms can now be specified via simplerprograms; and then implemented via optimized rules that use min and max aggregates inthe seminaive fixpoint computation to achieve safe and efficient execution. This optimizationdovetails with recent advances that entail the use of monotonic aggregates in recursion andleads to efficient implementation; as demonstrated by the scalable Datalog systems we haverecently developed.,AMW,2016,1
Max-Intensity: Detecting Competitive Advertiser Communities in Sponsored Search Market,Wenchao Yu; Ariyam Das; Justin Wood; Wei Wang; Carlo Zaniolo; Ping Luo,In a sponsored search market; the problem of measuring the intensity of competition amongadvertisers is increasingly gaining prominence today. Usually; search providers want tomonitor the advertiser communities that share common bidding keywords; so that they canintervene when competition slackens. However; to the best of our knowledge; not muchresearch has been conducted in identifying advertiser communities and understandingcompetition within these communities. In this paper we introduce a novel approach to detectcompetitive communities in a weighted bi-partite network formed by advertisers and theirbidding keywords. The proposed approach is based on an advertiser vertex metric calledintensity score; which takes the following two factors into consideration: the competitors thatbid on the same keywords; and the advertisers' consumption proportion within the …,Data Mining (ICDM); 2015 IEEE International Conference on,2015,1
Expressing and Supporting E ciently Greedy Algorithms as Locally Stratified Logic Programs,Carlo Zaniolo,Abstract The problem of expressing and supporting classical greedy algorithms in Dataloghas been the focus of many significant research efforts that have produced very interestingsolutions for particular algorithms. But we still lack a general treatment that characterizes therelationship of greedy algorithms to non-monotonic theories and leads to asymptoticallyoptimal implementations. In this paper; we propose a general solution to this problem. Ourapproach begins by identifying a class of locally stratified programs that subsumes XY-stratified programs and is formally characterized using the Datalog1S representation ofnumbers. Then; we propose a simple specialization of the iterated fixpoint procedure thatcomputes e ciently the perfect model for these programs; achieving optimal asymptoticcomplexities for well-known greedy algorithms. This makes possible their e cient support …,*,2015,1
Trajectory data pattern mining,Elio Masciari; Gao Shi; Carlo Zaniolo,Abstract In this paper; we study the problem of mining for frequent trajectories; which iscrucial in many application scenarios; such as vehicle traffic management; hand-off incellular networks; supply chain management. We approach this problem as that of mining forfrequent sequential patterns. Our approach consists of a partitioning strategy for incomingstreams of trajectories in order to reduce the trajectory size and represent trajectories asstrings. We mine frequent trajectories using a sliding windows approach combined with acounting algorithm that allows us to promptly update the frequency of patterns. In order tomake counting really efficient; we represent frequent trajectories by prime numbers; wherebythe Chinese reminder theorem can then be used to expedite the computation.,International Workshop on New Frontiers in Mining Complex Patterns,2013,1
Mining frequent itemsets over tuple-evolving data streams,Chongsheng Zhang; Mirjana Mazuran; Hamid Mousavi; Yuan Hao; Carlo Zaniolo; Florent Masseglia,Abstract In many data streaming applications today; tuples inside the streams may getrevised over time. This type of data stream brings new issues and challenges to the datamining tasks. We present a theoretical analysis for mining frequent itemsets from slidingwindows over such data. We define conditions that determine whether an infrequent itemsetwill become frequent when some existing tuples inside the streams have been updated. Wedesign simple but effective structures for managing both the evolving tuples and thecandidate frequent itemsets. Moreover; we provide a novel verification method that efficientlycomputes the counts of candidate itemsets. Experiments on real-world datasets show theefficiency and effectiveness of our proposed method.,Proceedings of the 28th Annual ACM Symposium on Applied Computing,2013,1
Data stream management systems for processing and mining RFID streams,Carlo Zaniolo,*,Engineering Intelligent Systems,2012,1
An Extension of Datalog for Graph Queries.,Mirjana Mazuran; Edoardo Serra; Carlo Zaniolo,Abstract. Supporting aggregates in recursive logic rules is a crucial long-standing problemfor Datalog. To solve this problem; we propose DatalogF S that supports queries andreasoning on the number of distinct occurrences satisfying given goals; or conjunction ofgoals; in rules. By using a generalized notion of multiplicity called frequency; we show thatgraph queries can be easily expressed in DatalogF S. This simple extension preserves allthe desirable semantic and computational properties of logic-based languages; whilesignificantly extending their application range to support efficiently page-rank; and social-network queries.,SEBD,2012,1
Optimizing Regular Expression Clustering for Massive Pattern Search,Nikolay Laptev; Hamid Mousavi; Alexander Shkapsky; Carlo Zaniolo,Abstract—Optimizing the search for a large sets of patterns; due to their prevalence; isbecoming critical in a variety of applications including financial services; healthcaremonitoring; RFID-based inventory management; publish subscribe and network intrusiondetection systems (NIDS). For example in NIDS; to identify particular packets in a packetstream; modern network devices need to perform deep packet inspection at high rates givena limited memory and a large number of signatures. It is often proposed to group togetherregular expressions (REs) denoting similar patterns into a DFA or an NFA to decrease thememory usage and increase the average throughput. In this paper; we propose a frameworkcomprised of three novel distance metrics used to cluster REs and an execution techniquethat uses the information aggregated by the distance metrics to optimize the number of …,*,2012,1
Schema Evolution In Wikipedia,Carlo A Curino; Hyun J Moon; Letizia Tanca; Carlo Zaniolo,Abstract: Evolving the database that is at the core of an Information System represents adifficult maintenance problem that has only been studied in the framework of traditionalinformation systems. However; the problem is likely to be even more severe in webinformation systems; where open-source software is often developed through thecontributions and collaboration of many groups and individuals. Therefore; in this paper; wepresent an indepth analysis of the evolution history of the Wikipedia database and itsschema; Wikipedia is the best-known example of a large family of web information systemsbuilt using the open-source software MediaWiki. Our study is based on:(i) a set of SchemaModification Operators that provide a simple conceptual representation for complex schemachanges; and (ii) simple software tools to automate the analysis. This framework allowed …,*,2008,1
Managing XML Versions and Replicas in a P2P Context.,Deise de Brum Saccol; Nina Edelweiss; Renata de Matos Galante; Carlo Zaniolo,Abstract Peer-to-Peer (P2P) systems seek to provide sharing of computational resources;which may be duplicated or versioned over several peers. Duplicate resources (ie replicas)are the key to better query performance and availability. On the other hand; multiple versionscan be used to support queries on the lineage of resources and the evolution of history.However; traditional P2P systems are not aware of replicas and versions; which causecomplexity at the logical level and inefficiency at the physical level. To solve these problems;we propose an environment for detecting; managing and querying replicas and versions ofXML documents in a P2P context. We also show that the proposed environment can also beused for plagiarism detection; web page ranking; and software clone identification.,SEKE,2007,1
Advances in Database Technology-EDBT 2000: 7th International Conference on Extending Database Technology Konstanz; Germany; March 27-31; 2000 Proceedi...,Carlo Zaniolo; Peter C Lockemann; Marc H Scholl; Torsten Grust,EDBT 2000 is the seventh conference in a series dedicated to the advancement of databasetechnology. This year's conference special theme;\Connect Millions of Users and DataSources;" underscores the importance of databases for the information age that is dawningwith the new millennium. The importance-rives not just from the observation that theinformation age essentially rests on theconvergenceofcommunications; computing;andstorage. Equallyimportant; many of the concepts and techniques underlying the successof databasesystems have independent meaning and impact for today's distributedinformation s-tems. The papers in the volume should also be seen in this light. The EDBT2000 conference program includes 30 research papers selected by the program committeeout of 187 submissions; covering advances in research; development; and applications of …,*,2003,1
The ATLaS system and its powerful database language based on simple extensions of SQL,Haixun Wang; Carlo Zaniolo,A lack of power and extensibility in their query languages has seriously limited the generalityof DBMSs and hampered their ability to support new applications domains; such as datamining. In this paper; we solve this problem by stream-oriented aggregate functions andgeneralized table functions which are definable by users in the SQL language itself; ratherthan in an external programming language. These simple extensions turn SQL into apowerful database language; which can express a wide range of applications; includingrecursive queries; ROLAP (relational online analytical processing) aggregates; time-seriesqueries; stream-oriented processing and data-mining functions. The SQL extensions areimplemented in ATLaS (Aggregate and Table Language and System).,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,1
Greedy Algorithms in Deductive Databases,Sergio Greco; Carlo Zaniolo,Abstract In the design of algorithms; the greedy paradigm provides a powerful tool forsolving efficiently classical computational problems; within the framework of procedurallanguages. However; expressing these algorithms within the declarative framework of logic-based languages has proven a difficult research challenge. In this paper; we extend theframework of Datalog-like languages to obtain simple declarative formulations for suchproblems; and propose effective implementation techniques to ensure computationalcomplexities comparable to those of procedural formulations. These advances are achievedthrough the use of the choice construct; extended with preference annotations to effect theselection of alternative stable-models and nondeterministic fixpoints. We show that; withsuitable storage structures; the differential fixpoint computation of our programs matches …,*,2000,1
Logic in Databases,Carlo Zaniolo,Page 1. Dino Pedreschi Carlo Zaniolo (Eds.) Logic in Databases International Workshop LID'96 San Miniato; Italy; July 1-2; 1996 Proceedings Springer Page 2. Table of Contents KeynoteLecture 1 Logic and Databases: A 20 Year Retrospective 3 J. Minker Uncertainty 59 AParametric Approach to Deductive Databases with Uncertainty 61 LVS Lakshmanan; N. ShiriA Deductive Database Approach to Planning in Uncertain Environments . 83 VS Subrahmanian;C. Ward Temporal and Spatial Reasoning 99 Termination Properties of Spatial DatalogPrograms 101 B. Kuijpers; J. Paredaens; M. Smits; J. Van den Bussche Applying TransitionRules to Bitemporal Deductive Databases for Integrity Constraint Checking 117 C. Martin;J. Sistac Invited Lecture 135 Towards a Unified Agent Architecture that Combines Rationalitywith Reactivity 137 R. Kowalski; F. Sadri Updates 151 …,*,1996,1
VLDB'95: Proceedings of the 21st International Conference on Very Large Data Bases; Zurich; Switzerland; Sept. 11-15; 1995,Umeshwar Dayal; Peter MD Gray; Shojiro Nishio,*,*,1995,1
The design and implementation of EPL: An event pattern language for active databases,G Giuffrida; C Zaniolo,Abstract: The growing demand for intelligent information systems requires closer coupling ofrule-based reasoning engines; such as CLIPS; with advanced data base managementsystems (DBMS). For instance; several commercial DBMS now support the notion of triggersthat monitor events and transactions occurring in the database and fire induced actions;which perform a variety of critical functions; including safeguarding the integrity of data;monitoring access; and recording volatile information needed by administrators; analysts;and expert systems to perform assorted tasks; examples of these tasks include securityenforcement; market studies; knowledge discovery; and link analysis. At UCLA; we designedand implemented the event pattern language (EPL) which is capable of detecting and actingupon complex patterns of events which are temporally related to each other. For instance …,*,1994,1
Rule Transformation Methods in the Implementation of Logic Based Languages,DOMENICO SACCA; CARLO ZANIOLO,This chapter discusses a comprehensive rule rewriting approach and supporting algorithmson which the actual implementation of LDL is based. It presents the differential Fixpointalgorithm. The chapter describes the magic set method and the counting method. Thesemethods rewrite the original rules; which; as such; cannot be implemented safely andefficiently as a single Fixpoint; into equivalent ones that can be implemented by a pair ofFixpoint computations safely and efficiently. The chapter presents the seminaive formulationfor the Fixpoint algorithm. The counting method eliminates the duplicate computation of themagic set method by using counting indices. For instance; the humans of the samegeneration as Adam can be found by first computing the ancestors of Adam and then thedescendants of these ancestors—provided that the levels of these ancestors and their …,*,1989,1
ATLaS User Manual,Haixun Wang; Carlo Zaniolo; Richard Luo,*,*,*,1
0 ptimization of Li. near P rograms Using Counting Methods,Sergio Greco; Carlo Zaniolo,Abstract We present a general solution to the problem of optimized execution of logicprogmms containing linear recursive rules. Our solution is based on extension8 of theclassical counting method; which ia known to be eficient but of limited applicability. In fact;the range of applicability of the counting method; and its variants proposed by previousresearchers; sufier from one or more of the following limitationa: the method can be appliedonly when (I) the adorned program contains one recursive rule;(2) the 'left part'and the 'rightpart'of the recursive rule do not have any common variable and (3) the relation associatedwith the left part of the recursive rule is 'acyclic'. In this paper; a simple and unified frameworkia presented; where those limitations are removed; and the counting method thua becomeapplicable to all programs with linear rules. This framework abo allows a simple treatment …,Pro. 3rd Int'l Conf. on Extending Database Technology,*,1
User-friendly temporal queries on historical knowledge bases,Carlo Zaniolo; Shi Gao; Maurizio Atzori; Muhao Chen; Jiaqi Gu,Abstract DBpedia and other RFD-encoded Knowledge Bases (KB) s give users access toencyclopedic knowledge via SPARQL queries. As the world evolves; the KBs are updated;and the history of entities and their properties becomes of great interest. Thus; we needpowerful tools and friendly interfaces to query histories and flash-back to the past. Here; wepropose (i) a point-based temporal extension of SPARQL; called SPARQL T; which enablessimple and concise expression of temporal queries; and (ii) an extension of WikipediaInfoboxes to support user-friendly by-example temporal queries implemented by mappingthem into SPARQL T. Our main-memory RDF-TX system supports such queries efficientlyusing Multi-Version B+ trees; compressed indexes; and query optimization techniques;which achieve performance and scalability; as demonstrated by experiments on historical …,Information and Computation,2017,*
Fixpoint semantics and optimization of recursive Datalog programs with aggregates,Carlo Zaniolo; Mohan Yang; Ariyam Das; Alexander Shkapsky; Tyson Condie; Matteo Interlandi,Abstract A very desirable Datalog extension investigated by many researchers in the last 30years consists in allowing the use of the basic SQL aggregates min; max; count and sum inrecursive rules. In this paper; we propose a simple comprehensive solution that extends thedeclarative least-fixpoint semantics of Horn Clauses; along with the optimization techniquesused in the bottom-up implementation approach adopted by many Datalog systems. We startby identifying a large class of programs of great practical interest in which the use of min ormax in recursive rules does not compromise the declarative fixpoint semantics of theprograms using those rules. Then; we revisit the monotonic versions of count and sumaggregates proposed by Mazuran et al.(2013b; The VLDB Journal 22; 4; 471–493) andnamed; respectively; mcount and msum. Since mcount; and also msum on positive …,Theory and Practice of Logic Programming,2017,*
Carlo Zaniolo Speaks Out on his Passion for Relational Databases and Logic,Carlo Zaniolo,Welcome to ACM SIGMOD Record's series of interviews with distinguished members of thedatabase community. I'm Marianne Winslett; and today we are in Phoenix; site of the 2012SIGMOD and PODS conference. I have here with me Carlo Zaniolo; who is the NEFriedmann Professor in Knowledge Science at UCLA. Before that; Carlo was a researcher atBell Labs and MCC. His PhD is from UCLA. So; Carlo; welcome!,SIGMOD Record,2016,*
Ranking support for matched patterns over complex event streams: The CEPR system,Jiaqi Gu; Jin Wang; Carlo Zaniolo,There is a growing interest in pattern matching over complex event streams. While manybodies of techniques were proposed to search complex patterns and enhance theexpressive power of query language; no previous work focused on supporting a well-defined ranking mechanism over answers using semantic ordering. To satisfy this need; weproposed CEPR; a CEP system capable of ranking matchings and emitting ordered resultsbased on users' intentions via a novel query language. In this demo; we will (i) demonstratelanguage features; system architecture and functionalities;(ii) show examples of CEPR invarious application domains and (iii) present a user-friendly interface to monitor queryresults and interact with the system in real time.,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,*
The Parallelization of a Complex Hierarchical Clustering Algorithm: faster unsupervised learning on larger data sets,Giuseppe M Mazzeo; Carlo Zaniolo,The need to support advanced analytics on Big Data is driving data scientist's interesttoward massively parallel distributed systems and software platforms; such as Map-Reduceand Spark; that make possible their scalable utilization. However; when complex datamining algorithms are required; their fully scalable deployment on such platforms is facing anumber of technical challenges that grow with the complexity of the algorithms involved.Thus algorithms; that were originally designed for a sequential nature; must often beredesigned in order to effectively use the distributed computational resources. In this paper;we explore these problems; and then propose a solution which has proven to be veryeffective on the complex hierarchical clustering algorithm CLUBS+. By using four stages ofsuccessive refinements; CLUBS+ delivers high-quality clusters of data grouped around …,*,2016,*
Historical Queries on Wikipedia: A Usability-Driven Approach,Carlo Zaniolo,The extraordinary success of Wikipedia shows that major advances in Web documentsearching and knowledge retrieval can be achieved once powerful structured queries onRDF knowledge bases will be supported by semantic-web information systems. Significantresearch progress on the usability front is being made by two main approaches. One is theSwipe approach; where casual users can pose powerful structured queries by entering QBE-like conditions on the system-activated infoboxes of pages of entities that exemplify thosethat are the subject of the query. This approach allows users to ask structured queries ofsignificant complexity combined with free-text and keyword search conditions; and cantherefore return structured answers along with the snippets of pages of interest. The otherapproach that has seen much progress is Question Answering (QA) systems. QA provides …,Temporal Representation and Reasoning (TIME); 2015 22nd International Symposium on,2015,*
Beyond text-based web search: Question answering and by-example structured queries compared on wikipedia,Maurizio Atzori; Carlo Zaniolo,Segnalazioni con codici 20501/20503/20504: Alcuni dati obbligatori per il sito CINECA nonsono presenti o invalidi; oppure il sito CINECA non è riuscito ad individuare una rivista con idati forniti; è necessario controllare la correttezza dell'ISSN e/o EISSN dove applicabili e iltitolo della rivista.Segnalazioni con codici 20201/20202: La pubblicazione non è statatrasferita SOLO per i docenti segnalati nel messaggio a causa di problemi nell'anagraficaCINECA e/o di Ateneo (pe i codici fiscali non sono gli stessi) oppure perché si tratta didocenti che; pur essendo abilitati all'inserimento nel sistema di Ateneo; non hanno facoltà disincronizzare le proprie pubblicazioni in CINECA. Tutti gli altri eventuali coautori senzasegnalazione troveranno la pubblicazione correttamente nel proprio spazio personaleCINECA.,23rd Italian Symposium on Advanced Database Systems; SEBD 2015,2015,*
SWiPE Query Engine,M Atzori; C Zaniolo,Segnalazioni con codici 20501/20503/20504: Alcuni dati obbligatori per il sito CINECA nonsono presenti o invalidi; oppure il sito CINECA non è riuscito ad individuare una rivista con idati forniti; è necessario controllare la correttezza dell'ISSN e/o EISSN dove applicabili e iltitolo della rivista.Segnalazioni con codici 20201/20202: La pubblicazione non è statatrasferita SOLO per i docenti segnalati nel messaggio a causa di problemi nell'anagraficaCINECA e/o di Ateneo (pe i codici fiscali non sono gli stessi) oppure perché si tratta didocenti che; pur essendo abilitati all'inserimento nel sistema di Ateneo; non hanno facoltà disincronizzare le proprie pubblicazioni in CINECA. Tutti gli altri eventuali coautori senzasegnalazione troveranno la pubblicazione correttamente nel proprio spazio personaleCINECA.,*,2014,*
User-Friendly Structured Queries on Wikipedia: the SWiPE System,M Atzori; C Zaniolo,Segnalazioni con codici 20501/20503/20504: Alcuni dati obbligatori per il sito CINECA nonsono presenti o invalidi; oppure il sito CINECA non è riuscito ad individuare una rivista con idati forniti; è necessario controllare la correttezza dell'ISSN e/o EISSN dove applicabili e iltitolo della rivista.Segnalazioni con codici 20201/20202: La pubblicazione non è statatrasferita SOLO per i docenti segnalati nel messaggio a causa di problemi nell'anagraficaCINECA e/o di Ateneo (pe i codici fiscali non sono gli stessi) oppure perché si tratta didocenti che; pur essendo abilitati all'inserimento nel sistema di Ateneo; non hanno facoltà disincronizzare le proprie pubblicazioni in CINECA. Tutti gli altri eventuali coautori senzasegnalazione troveranno la pubblicazione correttamente nel proprio spazio personaleCINECA.,SEBD 2014,2013,*
Supporting database schema evolution represents a long-standing challenge of practical and theoretical importance for modern information systems. In this paper; w...,Wolfgang Lehner; Sunita Sarawagi; Carlo Curino; Hyun Jin Moon; Alin Deutsch; Carlo Zaniolo,In this paper; we present a technique for building a high-availability (HA) databasemanagement system (DBMS). The proposed technique can be applied to any DBMS withlittle or no customization; and with reasonable performance overhead. Our approach isbased on Remus; a commodity HA solution implemented in the virtualization layer; that usesasynchronous virtual machine state replication to provide...,The VLDB Journal,2013,*
A Monotonic Extension for Horn-Clauses and its Significance in Datalog's Renaissance.,Mirjana Mazuran; Edoardo Serra; Carlo Zaniolo,Abstract. FS-rules provide a powerful monotonic extension for Horn clauses that supportsmonotonic aggregates in recursion by reasoning on the multiplicity of occurrences satisfyingexistential goals. The least fixpoint semantics; and its equivalent least model semantics; holdfor logic programs with FS-rules; moreover; generalized notions of stratification and stablemodels are easily derived once negated goals are also allowed. Finally; the generalizationof techniques such as seminaive fixpoint and magic sets; make possible the efficientimplementation of DatalogF S; ie; Datalog with FS-rules and stratified negation. A largenumber of applications that could not be supported efficiently; or could not be expressed atall in stratified Datalog can now be easily expressed and efficiently supported in DatalogF Sand a powerful DatalogF S system is now being developed at UCLA.,AMW,2013,*
Composite Temporal Events in Active,Iakovos Motakis; Carlo Zaniolo,Abstract Active databases must support rules triggered by complex patterns of compositetemporal events. This paper proposes a general method for specifying the semantics ofcomposite event specification languages. The method is based on a syntax-directedtranslation of the composite event expressions into Datalog1s; whose formal semantics isthen used to define the meaning of the original event expressions. We show that the methodis applicable to languages such as ODE; Snoop and SAMOS that are based respectively onthe formalisms of Finite State Machines; Event Graphs and Petri Nets. The proposed methodovercomes various prob-lems and limitations affecting such formalisms.,Recent Advances in Temporal Databases: Proceedings of the International Workshop on Temporal Databases; Zurich; Switzerland; 17–18 September 1995,2012,*
Master Seminar Topics,Jiakui Zhao; Dongqing Yang; Bin Cui; Lijun Chen; Jun Gao; Hailong Liu; Zhanhuai Li; Qun Chen; Shanglian Peng; Donghui Zhang; Alexander Markowetz; Vassilis J Tsotras; Dimitrios Gunopulos; Bernhard Seeger; Sudipto Das; Shyam Antony; Divyakant Agrawal; Amr El Abbadi; Vagelis Hristidis; Oscar Valdivia; Michail Vlachos; Philip S Yu; Yan-Nei Law; Haixun Wang; Carlo Zaniolo; Thanh TL Tran; Liping Peng; Yanlei Diao; Andrew McGregor; Anna Liu,Sorting is another example of a blocking operator. Data streams are characterised by theirenormous size (potentially infinite); rapid arrival rate and their transient nature (in terms ofdata distributions). Since the data streams are open-ended by definition (ie; they have andinfinite size as data keeps arriving); these operators would go into a perpetual wait state.Moreover; these conventional operators are designed to give the exact answers and alwaysmake a complete iteration over all the available data whenever they are invoked; hence; areextremely resource intensive; not in terms of time and space. In the context of stream mining;where resources are limited; the algorithms make a trade off of accuracy for efficiency. Theanswers provided by the streaming algorithms are approximation of the original answers(with error bounds). Please find a list of articles that proposes new techniques that aim at …,*,2012,*
High-performance complex event processing over {XML} streams and other hierarchical data,Barzan Mozafari; Kai Zeng; Carlo Zaniolo; Loris D’Antoni,Search all the public and authenticated articles in CiteULike. Include unauthenticated resultstoo (may include "spam") Enter a search phrase. You can also specify a CiteULike article id(123456);. a DOI (doi:10.1234/12345678). or a PubMed ID (pmid:12345678). Click Help foradvanced usage. CiteULike; Group: Mostrare; Search; Register; Log in …,*,2012,*
Proceedings-International Conference on Data Engineering: Foreword,Michael Hicks; Rida A Bazzi; Carlo Zaniolo,Abstract It is our pleasure to present this proceedings of the Third Workshop on Hot Topics inSoftware Upgrades (HotSWUp III); which took place on April 16; 2011 in Hannover;Germany. The workshop was held as a satellite event of the International Conference onData Engineering (ICDE 2011).,2011 IEEE 27th International Conference on Data Engineering Workshops; ICDE 2011,2011,*
Ion channel profiles in murine spiral ganglion neurons: Characterization of two-pore-domain potassium channels; voltage-gated calcium channels; and calcium-activ...,Wei Chun Chen,Abstract Spiral ganglion neurons possess a rich repertoire of ion channels that underliesdifferential firing properties. Previous studies have shown that voltage-gated K+ channels(Kv1. 1; Kv3. 1; Kv4. 2; and BK) are arranged in a tonotopic manner along the length ofcochlea. The present study will expand the analysis to three additional classes of ionchannels: two-pore-domain K+ channels (K2Ps); voltage-gated Ca 2+ channels (VGCCs);and Ca 2+-activated K+ channels; in order to determine their contribution to the tonotopicorganization in these primary afferent neurons.,*,2011,*
Introducing Stream Mill,Hetal Thakkar; Nikolay Laptev; Vincenzo Russo; Carlo Zaniolo,Stream Mill is a Data Stream Management System (DSMS) designed to support complexapplications; along with the simpler applications serviced by other DSMS. In particular;Stream Mill is the first DSMS to support stream mining applications; this is achieved by theStream Mill Miner (SMM) workbench that is discussed later in this document. For now; wewill focus on providing a user-friendly introduction to the system and its powerful querylanguage Expressive Stream Language (ESL); which extends SQL into a Turing-completelanguage by means of User Defined Aggregates (UDAs). Most of the ESL constructs areapplicable to both database tables and data streams. This simplifies the task of ESLprogrammers who are likely to develop and test their ESL queries on tables before applyingthem to streams. It is recommended that new users learning the system follow this …,*,2009,*
Windows,Carlo Zaniolo,W3C was founded in 1994 by the inventor of the World Wide Web Tim Berners-Lee as avendor-neutral forum for building consensus around Web technologies. The consortiumconsists of member organization and dedicated staff of technical experts. Membership isopen to any organization or individual whose application is reviewed and approved by theW3C. Usually W3C members invest significant resources into the Web technologies. W3Cfulfils its mission by creation of recommendations enjoying status of international standards.In the first 10 years of existence; it produced over eighty W3C recommendations. W3C isresponsible for such technologies as HTML; XHTML; XML; XML Schema; CSS; SOAP;WSDL and others. W3C members play a leading role in the development of therecommendations. W3C initiatives involve international; national; and regional …,Encyclopedia of Database Systems,2009,*
Time Versus Standards: A Tale of Temporal Databases,Carlo Zaniolo,Abstract Because of the importance of time in Information Systems; there is a wideconsensus on the need for having temporal data types and temporal queries supported aspart of database standards. Actually; in the mid 90s; a forward-looking group of databaseresearchers proposed an ambitious temporal extension of SQL-2 named TSQL2.Unfortunately; this proposal was ahead of its time and was not well-received by standardcommittees and DBMS vendors represented in those committees. Since then;standardization groups have not spent much effort on temporal issues; and; hence; it isnatural to assume that no progress was made; and the gap between database standardsand temporal applications remains as wide as it was in the mid-90s. This would representvery bad news for temporal application developers who suffer from inadequacies of …,International Conference on Conceptual Modeling,2008,*
display. cgi? uids= 12519993.[3] GD Bader and CW Hogue. Analyzing yeast protein-protein interaction data obtained from different sources. Nat Biotechnol; 20 (10):...,Jorge B Bocca; Matthias Jarke; Carlo Zaniolo,[1] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules.In Jorge B. Bocca; Matthias Jarke; and Carlo Zaniolo; editors; Proc. 20th Int. Conf. Very LargeData Bases; VLDB; pages 487–499. Morgan Kaufmann; 12–15 1994. ISBN 1-55860-153-8 …[2] GD Bader; D Betel; and CW Hogue. Bind: the biomolecular interaction network database.Nucleic Acids Res; 31(1):248–250; Jan 2003. URL http://www.hubmed.org/display.cgi?uids=12519993 … [3] GD Bader and CW Hogue. Analyzing yeast protein-proteininteraction data obtained from different sources. Nat Biotechnol; 20(10):991–997; Oct 2002 …[4] GD Bader and CW Hogue. An automated method for finding molecular complexes in largeprotein interaction networks. BMC Bioinformatics; 4(1):2–2; Jan 2003 … [5] Béla Bollobás. RandomGraphs. Cambridge University Press; second edition; 2001 … [6] E Bolten; A Schliep; S …,Cell Biol,2004,*
Efficient Complex Query Support,Shu-Yao Chien; Vassilis J Tsotras; Carlo Zaniolo; Donghui Zhang,Abstract. Managing multiple versions of XML documents represents a critical requirement formany applications. Also; there has been much recent interest in supporting complex querieson XML data (eg; regular path expressions; structural projections; DIFF queries). In thispaper; we examine the problem of supporting efficiently complex queries on multiversionedXML documents. Our approach relies on a scheme based on durable node numbers (DNNs)that preserve the order among the XML tree nodes and are invariant with respect to updates.Using the document's DNNs various complex queries are reduced to combinations of partialversion retrieval queries. We examine three indexing schemes to efficiently evaluate partialversion retrieval queries in this environment. A thorough performance analysis is thenpresented to reveal the advantages of each scheme.,Advances in Database Technology-EDBT 2002: 8th International Conference on Extending Database Technology; Prague; Czech Republic; March 25-27; Proceedings,2003,*
Second International Workshop on Evolution and Change in Data Management (ECDM 2002)-Management of Time and Changes in Information Systems-Preservin...,Fhusheng Wang; Carlo Zaniolo,*,Lecture Notes in Computer Science,2003,*
Data and knowledge in database systems: deductive databases,Carlo Zaniolo,Abstract The objective of deductive databases is to provide efficient support for sophisticatedqueries and reasoning on large databases; toward this goal; they combine the technology oflogic programming with that of relational databases. Deductive database research hasproduced methods and techniques for implementing the declarative semantics of logicalrules via efficient computation of fixpoints. Also; advances in language design andnonmonotonic semantics were made to allow the use of negation and set-aggregates inrecursive programs; these yield greater expressive power while retaining polynomial datacomplexity and semantic well-formedness. Deductive database systems have been used indata mining and other advanced applications; and their techniques have been incorporatedinto a new generation of commercial databases.,Handbook of data mining and knowledge discovery,2002,*
Pushing Extrema Aggregates to Optimize Logic Queries⋆,Sergio Greco; Sumit Ganguly; Carlo Zaniolo,Abstract In this paper; we explore the possibility of transforming queries with minimum andmaximum predicates into equivalent queries that can be computed more efficiently. Themain contribution of the paper is an algorithm for propagating min and max predicates whilepreserving query equivalence under certain monotonicity constraints. We show that thealgorithm is correct and that the transformed query is often safe when the original one is not.Although in this paper we use logic rules; the technique presented can be used to optimize(possibly recursive) queries expressed by means of SQL3.,*,2001,*
User-De ned Aggregates for Advanced Database Applications,Haixun Wang; Carlo Zaniolo,The explosive growth of new database applications has; in several cases; outpaced thealbeit dramatic progress made by database technology. In fact; the great success of newapplication areas often serves as a grim reminder of the limitations from which DBMSs stillsu er in terms of power and extensibility. For instance; the newly introduced Object-Relational (OR) systems o er great improvements in generality; extensibility; and querypower; yet OR systems do not support well data mining applications| a leading growth areafor data-intensive applications. Problems also occur in many other application areas; wheredatablades and similar function libraries need better exibility and integration with SQL.,*,2000,*
Aggregates in Recursive Datalog and SQL3 Queries,Haixun Wang; Carlo Zaniolo,Abstract Advanced database applications require user-de ned aggregates that; along withthe nal returns provided by traditional aggregates; also support early returns as needed inonline aggregation and many data mining applications. In this paper; we (i) de ne the formalsemantics of user-de ned aggregates in Datalog;(ii) introduce aggregates with early returnsin LDL++ and SQL3; and (iii) provide a simple syntactic characterization of aggregates thatare monotonic under set-containment. Thus; we nd that aggregates functions based on earlyreturns are monotone and; therefore; can be freely used in recursive queries. This allow usto succinctly express and e ciently compute queries; such as Bill-of-Materials orleastdistance graph traversals; that have traditionally been recalcitrant to relational querylanguages| and to SQL3's recursive queries as well; since these are restricted to a strati …,*,1998,*
Logic in Databases: International Workshop LID'96; San Miniato; Italy; July 1-2; 1996. Proceedings,Dino Pedreschi; Carlo Zaniolo,This book constitutes the strictly refereed post-workshop proceedings of the InternationalWorkshop on Logic in Databases; LID'96; held in San Miniato; Italy; in July 1996; as the finalmeeting of an EC-US cooperative activity. The volume presents 21 revised full papersselected from 49 submissions as well as 3 invited contributions and a summary of a paneldiscussion on deductive databases: challenges; opportunities and future directions. Theretrospective survey on logic and databases by Jack Minker deserves a special mention: it isa 56-page overview and lists 357 references. The papers are organized in sections onuncertainty; temporal and spatial reasoning; updates; active databases; semantics;advanced applications; query evaluation; language extensions; and logic constructs andexpressive power.,*,1996,*
Foreword by the VLDB'94 PC-Chairmen,M Jarke; J Bocca; C Zaniolo,*,VLDB Journal,1996,*
VLDB: Very Large Data Bases: International Conference: Proceedings; 20th; September 12-15; 1994; Santiago; Chile,Jorge Bocca; Matthias Jarke; Carlo Zaniolo,*,*,1994,*
20th VLDB Conference: September 12-15; 1994; Santiago; Chile,Carlo Zaniolo; Matthias Jarke; Jorge Bocca,*,*,1994,*
Very Large Data Bases: Proceeding of the 20th International Conference,Carlo Zaniolo; Matthias Jarke; Jorge Bocca,*,*,1994,*
METHODS OF LOGIC IN COMPUTER SCIENCE 1; 61-76 (1994),Fosca Giannotti; Dino Pedreschi; Carlo Zaniolo,*,Methods of Logic in Computer Science,1994,*
CNUCE-CNR,Dino Pedreschi; Domenico Sacca; Carlo Zaniolo,*,Deductive and Object-Oriented Databases: Second International Conference; DOOD'91; Munich; Germany; December 16-18; 1991. Proceedings,1991,*
Efficient processing of declarative rule-based languages for Databases,Carlo Zaniolo,Abstract In recent years; Deductive Databases have progressed from a subject of theoreticalinterest to an emerging technology area of significant commercial potential. The two maincatalysts for progress have been a demand for advanced database applications and a rapidmaturation of the enabling technology. Thus; Deductive Databases have now progressedbeyond their initial Prolog-oriented beginnings and produced logic-based language;architectures and systems that support a declarative expression of knowledge through rulesand their efficient processing on large databases. In this paper; we review the key conceptsbehind deductive databases; including language constructs; semantics issues;implementation techniques; architectures and prototypes. Then; we discuss key applicationareas driving the development of this technology; and current research directions in …,*,1991,*
Deductive database systems: applications and programming,Carlo Zaniolo,*,Proceedings of the 1990 North American conference on Logic programming,1990,*
Relational algebra and fixpoint computation for logic programming implementation,Domenico Saccà; Carlo Zaniolo,Various techniques are devised to use logic in actual programming applications. The mostpopular example is Prolog that has proven the effective amenability of logic programming(LP) to efficient implementation and its applicability to a variety of realms includingdatabases. A logic program can be compiled into a number of relational algebra equationshaving database relations as constants or unknowns; the execution of a logic program; then;corresponds to computing the least fix-point of the equations. This chapter discusses a classof logic program with negated predicates in the bodies of rules for which there exists a clearsemantics; stratified programs. It presents the techniques for executing stratified logicprograms using relational algebra and fix-point computation. The chapter also discussesabout the semi-naive technique as rule rewrite method which; in turn; provides a new …,*,1989,*
DOMENICO SACCA1,CARLO ZANIOLO,*,Resolution of Equations in Algebraic Structures: Algebraic techniques,1989,*
Control and Optimization Strategies in the Implementation of LDL.,Ravi Krishnamurthy; Carlo Zaniolo,The Logic Data Language; LDL; combines the expressive power of a highlevel; logic-basedlanguage (such as Prolog) with the nonnavigational style of relational query languages;where the user need only supply a correct query; and the system (ie; the compiler/optimizer)is expected to devise an efficient execution strategy for it. Consequently; the optimizer isgiven the responsibility of choosing an optimal execution--a function similar to that of thequery optimizer in a relational database system. A relational system uses knowledge ofstorage structures; information about database statistics; and various estimates to predict thecost of execution schemes chosen from a predefined search space and to select a minimumcost execution in such a space. An LDL system offers to a user all the benefits of a databaselanguage--including the elimination of the impedance mismatch between the language …,DBPL,1987,*
ACM SIGMOD Record Volume 15 Issue 2,Carlo Zaniolo,*,*,1986,*
Proceedings of ACM SIGMOD'86: International Conference on Management of Data; Washington; DC May 28-30; 1986,Carlo Zaniolo,*,*,1986,*
Logic programming and databases,Carlo Zaniolo,*,Proceedings of the 1985 ACM annual conference on The range of computing: mid-80's perspective: mid-80's perspective,1985,*
Architectures for expert-DBMS (panel session),Larry Kerschberg; Carlo Zaniolo; Michael Stonebraker; D Stott Parker; Matthias Jarke; B Chairman-Muthuswamy,The First International Workshop on Expert Database Systems clearly showed that disk-based management of rules; schemas; and database instances will be needed to managelarge knowledge-based applications. How can we store and manage such diverse objectson the disk? How can we effectively bring together searching techniques from ES and DB forpattern matching; inference; query optimization and associative retrieval? Can we hope tohave a uniform representation for data and knowledge? Even if we do achieve thisrepresentation; will we require different tools and techniques to manage knowledge anddata? As we begin to develop large rule bases; how can we index rules for efficientretrieval? How to measure the impact of changing a rule in the knowledge base? What toolsand techniques do we need to manage the creation and maintenance of large data …,Proceedings of the 1985 ACM annual conference on The range of computing: mid-80's perspective: mid-80's perspective,1985,*
Non-members-$25.00 Members-$18.75,Carlo Zaniolo; Claude Delobel,369: Tutorial-Data-Base Management in the 1980's; edited by James A. Larson and HarveyA. Freeman; 1981-497 pp. This tutorial presents current material on what data-base managementwill be like in the 80's. For example; traditional navigational styles of data-manipulationcommands-ie; single-record-at-a-time-will be replaced by multiple-record-at-a-time; high-levelstyles. The 40 articles reprinted in this volume fall in- to such categories as tools for data-baseac- cess; data-base design; and hardware aids. Each of the topics includes an informativeoverview. Non-members-$25.00 Members-$18.75 … 371: Proceedings-Seventh InternationalConference on Very Large Data Bases; edited by Carlo Zaniolo and Claude Delobel; Sep- tember9-11; 1981-570 pp. Held in Cannes; France; this conference heard presentations from over 75participants on such topics as data modeling; concurrency control; data compression …,Computer,1981,*
Proceedings: sept. 9-11; 1981,Carlo Zaniolo,*,*,1981,*
Very large data bases: proceedings,Carlo Zaniolo; C Delobel,*,*,1981,*
Analysis and design of relational schemata for database systems[Ph. D. Thesis],CA ZANIOLO,*,*,1976,*
A FOREWORD,Richard Hocks,Richard Hocks A Foreword The following article by John Reuer is written with conviction.The author's frustration punctuates his argument from beginning to end. There is a sense ofurgency to his statements which demands audience by those readers who; for mythicreasons; have never troubled to look closely at the newest architectural egg-dances on thiscampus. Timeliness is not a unique virtue of the article. It would have been timely ten yearsago or ten before that. Blunders have a veritable genius for reproduction when bred in theclimate of sumption. For it is presumption of the most devious kind for any layman to dictatethe precise architectural motif of a public ing; especially when the talent involved isuniversally recognized to be a form of art. This kind of action contradicts the essence of auniversity; in which the unlettered are instructed by professionals; men who possess …,Carolina Quarterly,1960,*
MCC; Austin; Texas,Shalom Tsar Oded Shmueii; Carlo Zaniolo,*,*,*,*
Literatura do kursu: Zaawansowane systemy baz danych,Carlo Zaniolo; Stefano Ceri; Christos Faloutsos; Richard T Snodgrass; VS Subrahmanian,2. Connolly T.; Begg C.; Database Systems - A Practical Approach to Design;Implementation; and Management (third edition). Addison-Wesley; 2002; ISBN 0 201 708574 … 3. Date CJ; Wprowadzenie do systemów baz danych; WNT; Seria „KlasykaInformatyki”; W- wa; 2000 … 4. Elmasri R.; Navathe SB; Wprowadzenie do systemów bazdanych; Wyd. Helion; Seria „Kanon Informatyki”; Gliwice; 2005 … 5. Garcia-Molina H.; UllmanJD; Widom J.; Systemy baz danych. Pełny wykład; WNT; Seria „Klasyka Informatyki”; W-wa; 2006… 6. Jarke M.; Lenzerini M.; Vassiliou Y.; Vassiliadis P.; Fundamentals of Data Warehouses.Springer-Verlag; 2003; ISBN 3-540-42089-4 … 7. Subrahmanian; VS; Principles of MultimediaDatabase Systems; Morgan Kaufmann; 1998 … 8. Wrembel R.; Bębel B.; Oracle - Projektowanierozproszonych baz danych; HELION Publisher; 2003; ISBN 83-7197-951-7.,*,*,*
Multi-graph Affinity Embeddings for Multilingual Knowledge Graphs,Muhao Chen; Tao Zhou; Pei Zhou; Carlo Zaniolo,Abstract Multilingual knowledge graph embeddings provide important latent semanticrepresentations for knowledge-driven cross-lingual NLP tasks. Learning such embeddingsis currently based on the training of a weakly supervised alignment model in joint withmonolingual knowledge models. However; existing techniques for alignment model suffersignificantly from the multilingual inconsistency of knowledge graphs. In this paper; wepropose an improved model by learning a generalized affine-map-based alignment model.We find that the proposed approach effectively addresses the limitations of existingapproaches; especially in handling the incoherence of embedding spaces on differentlanguages. Experimental results show that the proposed approach offers better performancefor both entity and triple-wise knowledge alignment tasks.,*,*,*
Data Streams and Time Series Analysis,Rebecca Ong; Mirco Nanni; Chiara Renso; Monica Wachowicz; Dino Pedreschi; Dominique Gay; Romain Guigourès; Marc Boullé; Fabrice Clérot; Daniele Codecasa; Fabio Stella; Trajectory Data Pattern Mining; Elio Masciari; Gao Shi; Carlo Zaniolo; Sonja Pravilovic; Annalisa Appice; Donato Malerba; Vladimer Kobayashi; Rui Henriques; Cláudia Antunes; Sara C Madeira; Takashi Katoh; Shin-ichiro Tago; Tatsuya Asai; Hiroaki Morikawa; Junichi Shigezumi; Hiroya Inakoshi; Jan Górecki; Martin Holena; Ivica Slavkov; Jana Karcheska; Dragi Kocev; Slobodan Kalajdziski; Sašo Dzeroski,Parameter Estimation and Pattern Validation in Flock Mining … Rebecca Ong; MircoNanni; Chiara Renso; Monica Wachowicz; and Dino Pedreschi … Feature Extraction over MultipleRepresentations for Time Series Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .… Dominique Gay; Romain Guigourès; Marc Boullé; and Fabrice Clérot … A ClassificationBased Scoring Function for Continuous Time Bayesian Network Classifiers. . . . . . . . . . . . . . . .… Elio Masciari; Gao Shi; and Carlo Zaniolo … Process Miningto Forecast the Future of Running Cases … Sonja Pravilovic; AnnalisaAppice; and Donato Malerba … A Hybrid Distance-Based Method and Support Vector Machinesfor Emotional Speech Detection … Methods for the EfficientDiscovery of Large Item-Indexable Sequential Patterns …,*,*,*
Efficient Support for Complex Queries on Multiversion XML Documents,Shu-Yao Chien; Vassilis J Tsotras; Carlo Zaniolo; Donghui Zhang,Abstract Managing multiple versions of XML documents represents a critical requirement formany applications. Also; there has been much recent work on supporting complex querieson XML data (eg; regular path expressions; structural projections; DIFF queries). In thispaper; we examine the problem of implementing efficiently complex queries onmultiversioned XML documents. Our approach relies on a scheme based on durable nodenumbers (DNNs) that preserve the order among the XML tree nodes and* This work waspartially supported by NSF grants IIS-9907477; EIA-9983445; and the Department ofDefense.,*,*,*
Partial stable models for deductive databases; ie; normal function-free logic programs (also called datalog programs); have two equivalent definitions: one based on...,Fosca Giannotti; Sergio Greco; Domenico Saccà; Carlo Zaniolo,While non-determinism has long been established as a key concept in logic pro-gramming;its importance in the context of deductive databases was recognized only recently. Thispaper provides an overview of recent results on this topic with the aim of providing anintroduction to the theory and practice of non-determinism in deductive databases. Inparticular we (i) recall the main results linking...,*,*,*
Time Versus Standards,Carlo Zaniolo,October 2008 C. Zaniolo 9< db ts= T1 te= now>< empacct ts= T1 te= now>< row ts= T1 te=now>< empno ts= T1 te= now> 1001</empno>< title ts= T1 te= T3> Engineer</title>< title ts=T3 te= T4> Sr Engineer</title>< title ts= T4 te= now> Tech Leader</title>< deptno ts= T1 te=T3> d01</deptno>< deptno ts= T3 te= now> d02</deptno></row></empacct></db>,*,*,*
2015 22nd International Symposium on Temporal Representation and Reasoning,Carlo Zaniolo,Historical Queries on Wikipedia: A Usability-Driven Approach .....................................................................1 Carlo Zaniolo … Temporal Reasoning in Bounded Situation Calculus ...................................................................................2 Giuseppe De Giacomo … Towards Formal Verification ofDistributed Algorithms .................................................................................3 Benedikt Bollig … ASound-and-Complete Propagation-Based Algorithm for Checking the Dynamic Consistencyof Conditional Simple Temporal Networks ..........................................................4 LukeHunsberger; Roberto Posenato; and Carlo Combi … Dynamic Consistency of Conditional SimpleTemporal Networks via Mean Payoff Games: A Singly-Exponential Time DC-checking ...........................................................................19 Carlo Comin and Romeo Rizzi … Fast Feature Extractionfor Time Series Analysis Using Least-Squares Approximations with Orthogonal Basis …,*,*,*
SPARQLT and its User-Friendly Interface for Querying the History of RDF Knowledge Bases,Shi Gao; Muhao Chen; Maurizio Atzori; Jiaqi Gu; Carlo Zaniolo,Abstract. As the real world evolves; the information contained in the knowledge bases isupdated accordingly and the evolution history of entities and their properties becomes ofgreat interest to users; who thus need effective query tools to explore it. In this paper; we (i)introduce SPARQLT and its user-friendly interface for expressing powerful structuredqueries on the history of RDF knowledge bases; and (ii) overview its underlying queryengine that supports efficient evaluation of such queries and the management of historicalinformation. To ensure ease of use; we take a by-example structured query approach thatallows users to specify queries by entering simple conditions into the Infoboxes of Wikipediapages extended with temporal fields. From this wysiwyg interface; the system derives andexecutes equivalent SPARQLT queries; sparing users from having to learn the query …,*,*,*
A DATA LANGUAGE BASED ON PURE HORN LOGIC,Carlo Zaniolo,Abstract This paper treats the problem of implementing eﬁiciently queries expressed by Hornclauses containing recursive predicates; including those with function symbols. In particular;the situation is studied where the initial bindings of the arguments in the recursive querygoal can be used in the top-down (as in backward chaining) execu-tion phase to improvethe eﬁiciency and; often; to guarantee the termination of the forward chaining executionphase that imple-ments the ﬁxpoint computation for a recursive query.(To ensure eﬁicientsupport for database applications this ﬁxpoint computation is actually carried out byrelational algebra operators.) A gen-eral approach is given for solving these queries; theapproach per—forms an analysis of the binding passing behavior of the query; and thenreschedules the overall execution as two ﬁxpoint computations derived as results of this …,*,*,*
2013 IEEE International Conference on Data Engineering (ICDE 2013),Haibo Hu; Qian Chen; Jianliang Xu,We demonstrate VERDICT; a location-based range query service featuring the privacy-preserving authentication capability. VERDICT adopts the common data-as-a-service(DaaS) model; which consists of the data owner (a location registry or a mobile operator)who provides the querying data; the service provider who executes the query; and thequerying users. The system features a privacy-preserving query...,*,*,*
Data ri,C Zaniolo,Contribution to the Bulletin is hereby solicited. News Items; letters; technicalpapers; bookreviews; meeting previews; summaries; case studies; etc.; should be sent to the Editor. Allletters to the Editor will be considered for publication unless accompanied by a request tothe contrary. Technical papers are unrefereed.,*,*,*
Mining Association Rules over SQL,Carlo Zaniolo,*,*,*,*
MCC; Austin; Texas,Oded Shmuelr; Shalom Tsar; Carlo Zaniolo,ABSTRACT We propose compilation methods for the efficient support of set term matching inHorn clause programs. Rather than using general-purpose set matching algorithms; we takethe approach of formulating at compile time specialized computation plans that; by takingadvantage of information available in the given rules; limit the number of alternativesexplored. Our strategy relies on rewriting techniques to transform the problem into an"ordinarf'Horn clause compilation problem; with minimal additional overhead. The executioncost of the rewritten rules is substantially lower than that of the original rules and theadditional cost of compilation can thus be amortized over many executions.,*,*,*
FOR RECURSIVE LOGIC QUERIES,Carlo ZANIOLO,*,*,*,*
l 2 Rule Transformation Methods in the,DOMENICO SACCA; CARLO ZANIOLO,Several novel techniques have been developed to support the efﬁcient implementation ofLogic based languages oriented towards data in-tensive applications; such as LDL (Tsurand Zaniolo; 1986; Beeri et al.; 1987) and NAIL!(Morris et al.; 1987) and several others(Zaniolo; 1987). A crucial problem in the implementation of these languages is the efﬁcientsupport for recursion. Among the most signiﬁcant tech-niques developed for the solution ofthis problem; we ﬁnd the Differ-ential (Seminaive) Fixpoint Method (Balbin andRamamohanarao; 1987; Sacca and Zaniolo; 1988); the Magic Set Method (Bancilhon et al.;1986); the Minimagic Method (Sac-ca and Zaniolo; 1987); the Counting Method (Sacca andZaniolo; To appear) and the Magic Counting Method (Sacca and Zaniolo; 1986; 1987). lPartof this work was done while the author was visiting at MCC. This author was also …,*,*,*
in Logic Programs with Negation,Domenico Sacca'r,Abstract Much of the current work in non-monotonic logic pursues the generalization ofconcepts such as well-founded models and stable models using three-valued logic. Thisapproach is also efﬁctive in dealing with incomplete and undeﬁned information that isfrequently found in knowledge bases. However. it also suffers from drawbacks; including thefact that; in multi-valued logic; there is more than one meaningful way to assign a meaning torules in a program. In this paper; we present a reconstruction of theory of negation in logicrules which deals with incompleteness and undeﬁnedness using the standard two-valuedlogic. Simple extensions of the notion of unfounded sets are used to deﬁne the concept ofpartial models and the notions of partial well-founded models and partial stable models. Weprove that the partial stable models so deﬁned are equivalent to the three-valued stable …,*,*,*
DESIGN AND IMIPLEIVIENTATION OF A LOGIC BASED LANGUAGE FOR DATA INTENSIVE APPLICATIONS,Carlo Zaniolo,*,*,*,*
LID'96: logic in databases(San Miniato; July 1-2; 1996),Dino Pedreschi; Carlo Zaniolo,*,Lecture notes in computer science,*,*
Advances in database technology(Konstanz; 27-31 March 2000),Carlo Zaniolo; Peter C Lockemann; Marc H Scholl; Torsten Grust,*,Lecture notes in computer science,*,*
A Comparative Study of Version Management Schemes for XML Docu,Shu-Yao Chien; Vassilis J Tsotras; Carlo Zaniolo,Abstract The problem of managing multiple versions for XML documents and semistructureddata is of significant interest in many DB applications and web-related services. Traditionaldocument version control schemes; such as RCS; suffer from the following two problems. Atthe logical level; they conceal the structure of the documents by modeling them assequences of text lines; and storing a document's evolution as a line-edit script. At thephysical level; they can incur in severe storage or processing costs because of their inabilityto trade-off storage with computation. To solve these problems; we propose versionmanagement strategies that preserve the structure of the original document; and apply andextend DB techniques to minimize storage and processing costs. Therefore; we propose andcompare three schemes for XML version management; namely; the Usefulness-Based …,History,*,*
Daniel A. Ford; IBM Almaden,Michael Franklin; Ophir Frieder; Norbert Fuhr; Dimitrios Georgakopoulos; Carol Goble; Theo Haerder; Willem Jonker; Yahiko Kambayashi; Jessie Kennedy; Martin Kersten; Hiroyuki Kitagawa; Masaru Kitsuregawa; Matthias Klusch; Mong Li Lee; Qing Li; Xiaoming Li; Ling Liu; Mengchi Liu; Frederick H Lochovsky; Hongjun Lu; Heiko Schuldt; Timos Sellis; Ming-Chien Shan; Timothy K Shih; Il-Yeol Song; Kian Lee Tan; Katsumi Tanaka; David Toman; Frank W Tompa; Shan Wang; Gerhard Weikum; Kam-Fai Wong; Masatoshi Yoshikawa; S Yu Philip; Osmar Zaiane; Carlo Zaniolo; Stan Zdonik; Aoying Zhou; Lizhu Zhou; Shuigeng Zhou,*,*,*,*
Efﬁcient Processing of Declarative Rule-Based Languages for Databases,Carlo Zaniolo,Abstract In recent years; Deductive Databases have progressed from a subject of theoreticalinterest to an emerging technology area of signiﬁcant commercial potential. The two maincatalysts for progress have been a demand for advanced database applications and a rapidmaturation of enabling technology. Thus; the area of Deductive Databases has nowprogressed beyond its initial Prolog-oriented beginnings and produced logic-basedlanguages; architectures and systems that support a declarative expression on knowledgethrough rules and their efﬁcient processing on large databases. In this paper; we review thekey concepts behind deductive databases; including language constructs; semantics issues;implementation techniques; architectures and prototypes. Then; we discuss key applicationareas driving the development of this technology; and current research directions on …,*,*,*
‘Composite Temporal Events in Active,Carlo Zaniolo,Abstract Active databases must support rules triggered by complex patterns of compositetemporal events. This paper proposes a general method for specifying the semantics ofcomposite event speciﬁcation languages. The method is based on a syntax-directedtranslation of the composite event expressions into Dataloglg; whose formal semantics isthen used to define the meaning of the original event expressions. We show that the methodis applicable to languages such as ODE; Snoop and SAMOS that are based respectively onthe formalisms of Finite State Machines; Event Graphs and Petri Nets. The proposed methodovercomes various problems and limitations affecting such formalisms.,*,*,*
Efficient Structural Joins on Indexed XML Documents,Vassilis J Tsotras; Carlo Zaniolo,Abstract Queries on XML documents typically combine selections on element contents; and;via path expressions; the structural relationships between tagged elements. Structural joinsare used to find all pairs of elements satisfying the primitive structural relationships specifiedin the query; namely; parent-child and ancestordescendant relationships. Efficient supportfor structural joins is thus the key to efficient implementations of XML queries. Recentlyproposed node numbering schemes enable the capturing of the XML document structureusing traditional indices (such as B+-trees or R-trees). This paper proposes efficientstructural join algorithms in the presence of tag indices. We first concentrate on using B+-trees and show how to expedite a structural join by avoiding collections of elements that donot participate in the join. We then introduce an enhancement (based on sibling pointers) …,*,*,*
E cient Management of Multiversion Documents by Object Referencing,Shu-Yao Chien Vassilis J Tsotras; Carlo Zaniolo,Abstract Traditional approaches to versioning documents are edit-based; and representsuccessive versions using edit scripts. This paper proposes a reference-based versioningscheme that preserves the rich logical structure of the evolving document via objectreferences. This approach produces better support for queries; and reconciles the storage-level and transport-level representations of multiversioned XML documents. In particular; wepresent e cient algorithms for supporting projection and selection queries; and for queryingthe document evolution history. Then; we show that our representation is also e cient at thetransport level; where XML documents are exchanged between remote parties. In fact; withthe reference-based scheme; an XML document's history can also be viewed and processedas yet another XML document. Finally; we demonstrate the e ectiveness of the new …,Change,*,*
ESL: a Very Powerful SQL-Compliant Data Stream Language,Yijian Bai; Chang R Luo; Hetal Thakkar; Haixun Wang; Carlo Zaniolo,Abstract Compliance with SQL standards is very desirable for a data stream query languagebecause of practical considerations; and it is also very beneficial for applications that spanboth data streams and data bases. However; SQL suffers from expressive powerimpairments that; on streaming data; are even more serious than those it suffers on storeddata. Our Expressive Stream Language (ESL) solves these problems; and achieves power;flexibility; and adherence to SQL: 2003 standards by using:(i) table expressions andconcrete views on data streams;(ii) non-blocking aggregates (UDAs); and (iii) efficient delta-based maintenance for UDAs on windows. ESL is fully supported in the UCLA Stream MillDSMS and has proven very effective on a wide spectrum of applications that includeapproximate computations; data stream mining; time-series queries; and XML streams.,*,*,*
RelatingStableModelsandAIPlan-ning Domains,VS Subrahmanian; Carlo Zaniolo,*,*,*,*
FAR EAST,William Armstrong; Christos Faloutsos; Vassos Hadzilacos; HV Jagadish; Ravi Krishnamurthy; David Lomet; Dennis McLeod; Shamkant Navathe; Ekow Otoo; Raghu Ramakrishnan; Betty Salzberg; Jacob Slonim; Irving Traiger; Carlo Zaniolo; Serge Abiteboul; Peter Apers; Horst Biller; Peter Dadam; Robert Demolombe; Frank Eliassen; Georg Gottlob; Peter Lockmann; Robert Meersman; Andreas Reuter; Hans-Joerg Schek; Nicolas Spyratos; Yannis Vasiliou; Bruce Croft; Hector Garcia-Molina; Paula Hawthorn; Randy Katz; Bruce Lindsay; Wo-Shun Luk; Albert Mendelzon; Jack Orenptein; Z Meral Ozsoyoglu; Arnon Rosenthal; Timos Sellis; Toby Teorey; Grant Weddell; Stanley Zdonik; Michele Adiba; Janis Bubenko; Klaus Dittrich; Norbert Fuhr; Theo Haerder; Heikki Mannila; Antoni Olive; Yehoshua Sagiv; Gunter Schlageter; Bernd Walter; Umeshwar Dayal; Goetz Graefe; Yannis Ioannidis; Roger King; Guy Lohman; Ian McLeod; Tim Merrett; Sylvia Osborn; M Tamer Ozsu; Nick Roussopoulos; Kenneth Sevcik; Frank Tompa; Clement Yu; Antonio Albano; Francois Bancilhon; Stefano Ceri; Hartmut Ehrig; Georges Gardarin; Witold Litwin; Rainer Manthey; Peter Pistor; Felix Saltor; Amilcar Sernadas; Henry Tirri; Roberto Zicari,William Armstrong (Canada) Christos Faloutsos (USA) Vassos Hadzilacos (Canada) HV Jagadish(USA) Ravi Krishnamurthy (USA) David Lomet (USA) Dennis McLeod (USA) Shamkant Navathe(USA) Ekow Otoo (Canada) Raghu Ramakrishnan (USA) Betty Salzberg (USA) Jacob Slonim(Canada) Irving Traiger (USA) Carlo Zaniolo (USA) … Serge Abiteboul (France) Peter Apers(Netherlands) Horst Biller (Germany) Peter Dadam (Germany) Robert Demolombe (France) FrankEliassen (Norway) Georg Gottlob (Austria) Peter Lockmann (Germany) Robert Meersman(Netherlands) Andreas Reuter (Germany) Hans-Joerg Schek (Switzerland) Nicolas Spyratos(France) Yannis Vasiliou (Greece) … Ching-Chen Chang (Taiwan; China) Yahiko Kambayashi(Japan) Sukho Lee (Korea) Leszek Maciaszek (Australia) Maria Orlowska (Australia) RonSacks-Davis (Australis) KP Tan (Singapore) KY Whang (Korea),*,*,*
Aggregates for the Next Generation of Database Systems,Carlo Zaniolo; Haixun Wang,*,*,*,*
Clustering Algorithms Implementation on ATLaS--CS240B Project Report,Richard Luo; Carlo Zaniolo,*,*,*,*
ICDM 2008,Franco Turini; Carlo Zaniolo; Naren Ramakrishnan,Pisa is a University town with less than 100;000 inhabitants and more than 50;000 students.One of the most appealing slogans used in the past for presenting the University to potentialapplicants was “University of Pisa: a campus as large as a city” to denote the closeintegration between the city and its university. Pisa has a long history in science and thenames of Galileo Galilei and Fibonacci are there to highlight it. This attitude for science isstill well alive nowadays. Pisa; besides the University and two postgraduate Universities–Scuola Superiore S. Anna and Scuola Normale Superiore-hosts the second largest campusof the National Research Council in Italy with more than 1100 permanent staff. Thepercentage of gross product of the area devoted to research is 3 times Italy's average andcompares favorably to the other European countries. If we look just at computer science …,*,*,*
Version Management of XML Documents,Vassilis J Tsotras; Carlo Zaniolo,*,*,*,*
The Generalized Counting Method for Recursive Logic Queries,Carlo Zaniolo,Abstract. This paper treats the problem ofimplementing efﬁciently recursive Horn clausesqueries; including those with function symbols. In particular; the situation is studied wherethe initial bindings of the arguments in the recursive query goal can be used in the top-down(as in backward chaining) execution phase to improve the efﬁciency and; often; to guaranteethe termination of the forward chaining execution phase that implements the ﬁxpointcomputation for the recursive query. A general method is given for solving these queries; themethod performs an analysis of the binding-passing behavior of the query; and thenreschedules the overall execution as two ﬁxpoim computations derived as results of thisanalysis. The ﬁrst such computation emulates the propagation of bindings in the top-downphase; the second generates the desired answer by proving the goals left unsolved …,*,*,*
Power and Extensibility in a Data Stream Query Language and System,Yijian Bai; Hetal Thakkar; Chang Luo; Haixun Wang; Carlo Zaniolo,ABSTRACT Through its Expressive Stream Language (ESL); our Stream Mill systemsupports efficiently a wide range of applications; including very advanced ones such as datastream mining; streaming XML processing; time-series queries; and RFID event processing.Such superior generality and power are achieved by simple SQL extensions that bringwindows and User-Defined Aggregates (UDAs) to levels of support and integration neverachieved in the past. Thus; ESL supports physical and logical windows (with optional slidesand tumbles) on both built-in aggregates and UDAs; using a simple framework that appliesuniformly to both aggregate functions written in a external procedural languages and thosenatively written in ESL. In this paper; we first introduce the main ESL concepts andconstructs and demonstrate their effectiveness in advanced applications; such as data …,*,*,*
TIME 2010 Committees,Ian Pratt-Hartmann; Nicolas Markey; Jef Wijsen; Alessandro Artale; Howard Barringer; Béatrice Bérard; Claudio Bettini; Davide Bresolin; Thomas Brihaye; Jan Chomicki; Stéphane Demri; Clare Dixon; Michael Fisher; Nissim Francez; Roman Kontchakov; Savas Konur; Martin Lange; Ranko Lazić; Inderjeet Mani; Angelo Montanari; Bernhard Nebel; Hans-Jürgen Ohlbach; Paritosh K Pandya; James Pustejovsky; Jean-François Raskin; Peter Revesz; David Toman; X Sean Wang; Michael Winter; Michael Zakharyaschev; Carlo Zaniolo,General Chair Ian Pratt-Hartmann; University of Manchester; UK … Program Committee ChairsNicolas Markey; CNRS & ENS Cachan; France Jef Wijsen; University of Mons; Belgium … OrganizationChair Nicolas Markey; CNRS & ENS Cachan; France … Program Committee AlessandroArtale; Free University of Bozen-Bolzano; Italy Howard Barringer; University of Manchester; UKBéatrice Bérard; University Paris 6-Pierre & Marie Curie; France Claudio Bettini; University ofMilan; Italy Davide Bresolin; University of Verona; Italy Thomas Brihaye; University of Mons; BelgiumJan Chomicki; SUNY at Buffalo; NY; USA Stéphane Demri; CNRS & ENS Cachan; France ClareDixon; University of Liverpool; UK Michael Fisher; University of Liverpool; UK NissimFrancez; The Technion; Israel Roman Kontchakov; Birkbeck College; London; UK SavasKonur; University of Liverpool; UK Martin Lange; University of Munich; Germany Ranko …,*,*,*
Supplementary List of Referees,J Banerjee; M Carey; Hong-Tai Chou; J Clifford; T Connors; G Copeland; A Croker; U Dayal; D Denning; R Dewan; A Dutta; M Erradi; R Floyd; J Garza; PS Giridharan; P Goes; G Hallmark; A Hevner; J Hoffer; R Hull; Y Ioannidis; M Kilian; Hyoung-Joo Kim; Kyung-Chang Kim; R Krishnamurthy; J Langford; B Mahbod; Yasushi Masuda; MA Neimat; P O’Brien; J Orenstein; Gultekin Ozsoyoglu; R Pi&ii; H Pirkul; A Segev; Ming-Chien Shan; D Shasha; Qing Li; A Shepherd; A Shoshani; V Storey; Myung W Suh; A Swami; AU Tansel; P Valduriez; D Van Gucht; G Vossen; S Whang; M Winslett; D Woelk; Xinhua Wu; C Zaniolo,The following additional referees gave invaluable help to the US Programme Committee …J. Banerjee M. Carey Hong-Tai Chou J. Clifford T. Connors G. Copeland A. Croker U. DayalD. Denning R. Dewan A. Dutta M. Erradi R. Floyd J. Garza PS Giridharan P. Goes G. HallmarkA. Hevner J. Hoffer * R. Hull Y. Ioannidis M. Kilian Hyoung-Joo Kim Kyung-Chang Kim R. KrishnamurthyJ. Langford B. Mahbod F. Manola … Yasushi Masuda M.-A. Neimat P. O'Brien J. Orenstein GultekinOzsoyoglu R. Pi&ii H Pirkul s. sarkar A. Segev Ming-Chien Shan D. Shasha Qing Li 0. R. LiuSheng A. Shepherd A. Shoshani V. Storey Myung W. Suh A. Swami AU Tansel P. ValduriezD. Van Gucht G. Vossen S. Whang M. Winslett D. Woelk Xinhua Wu C. Zaniolo,*,*,*
VLDB’04 submission# 268,Fusheng Wang; Xin Zhou; Carlo Zaniolo; Semi-structured Data,*,*,*,*
Deductive Databases,Carlo Zaniolo,Relational databases; which support simple logic-based query languages 13] wereproposed in early 70s 12]. Initially considered too theoretical and not conducive to e cientimplementation or commercial payo; by mid 80s they had become a stellar successmotivating research into more powerful query and data manipulation languages. A Prolog-like language; called Datalog; became the focus of this research; owing to the kinship ofrelational calculi and Prolog's Horn clauses 53; 60]; and the surge of interest in rule-basedlanguages for expert systems of the mid-80s. The main objectives of deductive databasesare: i) logic-based extensions of database languages to support more powerful queries andreasoning; rule-based programming; and (ultimately) achieving Turing completeness; and ii)the e cient and scalable implementation of these declarative languages on large …,*,*,*
User De ned Aggregates in Database Languages,Haixun Wang; Carlo Zaniolo,Abstract User-de ned aggregates can be the linchpin of sophisticated datamining functionsand other advanced database applications; but they nd little support in current databasesystems including Object-Relational databases. Three serious limitations of currentdatabases are (i) the inability of introducing new aggregates (eg; by coding them in aprocedural language as proposed in SQL3);(ii) the inability of returning partial results duringcomputation (eg; to support online aggregation); and (iii) the inability of using aggregates inrecursive queries| eg; to express Bill of Materials (BoM) applications and optimized graphsearches. This paper propose a uni ed solution to these problems; and discusses practicaland theoretical aspects of this powerful addition to database languages.,*,*,*
ESL: a Data Stream Query Language and System Designed for Power and Extensibility,Chang Luo; Haixun Wang; Carlo Zaniolo,Abstract Applications that span both data streams and databases provide a sound reason forusing SQL for continuous queries on data streams as well as for ad-hoc queries ondatabase tables. However; SQL was designed for persistent data on secondary store; andnot for transient data on the wire; therefore; SQL is facing major problems in this new role—particularly in the domains of expressive power and extensibility that constitute notoriousweakness areas for relational languages. For instance; SQL cannot support sequencequeries; approximate queries; and mining queries that represent important applications fordata stream management systems (DSMS). The fact that blocking operators cannot be usedon continuous queries further impairs the effectiveness of SQL on data streams applications.Our Stream Mill solves these problems with the Expressive Query Language (ESL) …,*,*,*
Storing and Querying Multiversion XML Documents using Durable Node Numbers,Shu-Yao Chien Vassilis J Tsotras; Carlo Zaniolo; Donghui Zhang,*,*,*,*
USA USA,Lars Soderlund; Yoshifumi Masunaga; David J Dewitt; Francois Bancilhon; Randy Katz; Carlo Zaniolo; Harry KT Wong; Walter Bond; Hamideh Afsarmanesh; Doron Rotem; Stuart Madnick,Page 1. ORGANIZATION General Conference Chair North American Chair Arie Shoshani JackShemer Lawrence Berkeley Laboratory Teradata Corporation USA USA European Chair LarsSoderlund University of Stockholm Sweden Far East Chair Yoshifumi Masunaga University ofLibrary and Information Science; Tsukuba-Tun Japan Program Committee Co-Chair David J. DewittUniversity of Wisconsin; Madison USA Program Committee Co-Chair Francois Bancilhon ALTAIRFrance Tutorial Chair Randy Katz University of California; Berkeley USA Panel Chair Carlo ZanioloMCC USA Organizing Committee Chair Harry KT Wong Ashton-Tate USA Local ArrangementsChair Walter Bond The Aerospace Corporation and Cal State University USA RegistrationCoordinator Hamideh Afsarmanesh Cal State University; Domingo Hills USA US PublicityCoordinator Doron Rotem Lawrence Berkeley Laboratory USA …,*,*,*
Effi cient Support for W indow Aggregates,Chang Luo; Carlo Zaniolo,*,*,*,*
ATLaS: a Powerful Database Language and System Based on Simple Extensions of SQL,Haixun Wang; Carlo Zaniolo,*,*,*,*
Design and Implementation of a Spatio-Temporal Data Models and Query Language,Cindy X Chen; Carlo Zaniolo,Abstract This paper presents the design and implementation of SQLST; an extensible spatio-temporal data model and query language. We use counterclockwise directed triangles tomodel spatial objects and intervals to model time. In addition to providing the spatio-temporal primitives as built-ins; SQLST allows the user to introduce additional extensions tothe data model and query language. The two main mechanisms used for this purpose areuser-defined aggregates and table expressions. This approach minimizes the extensionsrequired in SQL; supports the orthogonality of spatial and temporal constructs; and thecustomization of spatio-temporal extensions to meet the different needs of differentapplications. We also show the performance result of SQLST implemented using Oracle 11g[22].,*,*,*
Technology Based Assessments of Language and Literacy Project,Margaret Heritage; Abeer Alwan; Alison Bailey; Christy Kim Boscardin; Eva L Baker; Richard Muntz; Carlo Zaniolo; Barbara Ann Jones; Larry Casey; Kimberly Reynolds; Markus Iseli; Hong You; Xiaodong Cui; Yijian Bai; P David Pearson; Thao Michelle Duong; Maria Callahan; Shri Narayanan; Elaine Andersen; Sungbok Yee; Jorge Silva; Abe Kazemadeh; Joe Tepperman; Patti Price,*,*,*,*
Data Mining Applications in ATLAS,Carlo Zaniolo,• Many attempts to implement frequent-item-set computations in SQL DBMS and OR DBshave failed to produce good performance• In-depth investigation by Sarawagi; Thomas; andAgrawal [ACM/SIGMOD 98] established this as the acid test for any SQL extension claimingto do support data mining,*,*,*
Pierre Wolper (Liège); General Chair,Jan Chomicki; David Toman; X Sean Wang; Walid Aref; Claudio Bettini; Bernard Boigelot; Clare Dixon; Curtis Dyreson; Enrico Franconi; Floris Geerts; Dimitrios Gunopulos; Ian Hodkinson; Christian Jensen; Peter Jonsson; George Kollios; Bart Kuijpers; Orna Kupferman; Oded Maler; Angelo Montanari; Wojciech Penczek; Jean-Francois Raskin; Peter Revesz; Mark Reynolds; Mark Ryan; Cyrus Shahabi; Vassilis Tsotras; Michael Zakharyaschev; Carlo Zaniolo,Pierre Wolper (Liège); General Chair Jan Chomicki (Buffalo) and David Toman (Waterloo); ProgramCo-Chairs X. Sean Wang (Vermont); Organization Chair … Walid Aref; Purdue University; USAClaudio Bettini; Università di Milano; Italy Bernard Boigelot; Université de Liège; Belgium ClareDixon; University of Liverpool; UK Curtis Dyreson; Washington State University; USA EnricoFranconi; Free University of Bozen-Bolzano; Italy Floris Geerts; University of Edinburgh; UK/Universityof Limburg; Belgium Dimitrios Gunopulos; University of California; Riverside; USA IanHodkinson; Imperial College; UK Christian Jensen; Aalborg University; Denmark PeterJonsson; Linköping University; Sweden George Kollios; Boston University; USA BartKuijpers; University of Limburg; Belgium Orna Kupferman; Hebrew University; Israel GerardLigozat; LIMSI; France Oded Maler; Verimag; France Angelo Montanari; Università di …,*,*,*
