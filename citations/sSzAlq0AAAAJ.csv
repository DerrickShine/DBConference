The open provenance model core specification (v1. 1),Luc Moreau; Ben Clifford; Juliana Freire; Joe Futrelle; Yolanda Gil; Paul Groth; Natalia Kwasnikowska; Simon Miles; Paolo Missier; Jim Myers; Beth Plale; Yogesh Simmhan; Eric Stephan; Jan Van den Bussche,Abstract The Open Provenance Model is a model of provenance that is designed to meet thefollowing requirements:(1) Allow provenance information to be exchanged betweensystems; by means of a compatibility layer based on a shared provenance model.(2) Allowdevelopers to build and share tools that operate on such a provenance model.(3) Defineprovenance in a precise; technology-agnostic manner.(4) Support a digital representation ofprovenance for any “thing”; whether produced by computer systems or not.(5) Allow multiplelevels of description to coexist.(6) Define a core set of rules that identify the valid inferencesthat can be made on provenance representation. This document contains the specification ofthe Open Provenance Model (v1. 1) resulting from a community effort to achieve inter-operability in the Provenance Challenge series.,Future generation computer systems,2011,863
VisTrails: visualization meets data management,Steven P Callahan; Juliana Freire; Emanuele Santos; Carlos E Scheidegger; Cláudio T Silva; Huy T Vo,Abstract Scientists are now faced with an incredible volume of data to analyze. Tosuccessfully analyze and validate various hypothesis; it is necessary to pose severalqueries; correlate disparate data; and create insightful visualizations of both the simulatedprocesses and observed phenomena. Often; insight comes from comparing the results ofmultiple visualizations. Unfortunately; today this process is far from interactive and containsmany error-prone and time-consuming tasks. As a result; the generation and maintenance ofvisualizations is a major bottleneck in the scientific process; hindering both the ability tomine scientific data and the actual use of the data. The VisTrails system represents our initialattempt to improve the scientific discovery process and reduce the time to insight. InVisTrails; we address the problem of visualization from a data management perspective …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,473
From XML schema to relations: A cost-based approach to XML storage,Philip Bohannon; Juliana Freire; Prasan Roy; Jérôme Siméon,As Web applications manipulate an increasing amount of XML; there is a growing interest instoring XML data in relational databases. Due to the mismatch between the complexity ofXML's tree structure and the simplicity of flat relational tables; there are many ways to storethe same document in an RDBMS; and a number of heuristic techniques have beenproposed. These techniques typically define fixed mappings and do not take applicationcharacteristics into account. However; a fixed mapping is unlikely to work well for allpossible applications. In contrast; LegoDB is a cost-based XML storage mapping engine thatexplores a space of possible XML-to-relational mappings and selects the best mapping for agiven application. LegoDB leverages current XML and relational technologies:(1) it modelsthe target application with an XML Schema; XML data statistics; and an XQuery workload; …,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,469
Provenance and scientific workflows: challenges and opportunities,Susan B Davidson; Juliana Freire,Abstract Provenance in the context of workflows; both for the data they derive and for theirspecification; is an essential component to allow for result reproducibility; sharing; andknowledge re-use in the scientific community. Several workshops have been held on thetopic; and it has been the focus of many research projects and prototype systems. Thistutorial provides an overview of research issues in provenance for scientific workflows; witha focus on recent literature and technology in this area. It is aimed at a general databaseresearch audience and at people who work with scientific data and workflows. We will (1)provide a general overview of scientific workflows;(2) describe research on provenance forscientific workflows and show in detail how provenance is supported in existing systems;(3)discuss emerging applications that are enabled by provenance; and (4) outline open …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,461
Provenance for computational tasks: A survey,Juliana Freire; David Koop; Emanuele Santos; Cláudio T Silva,The problem of systematically capturing and managing provenance for computational taskshas recently received significant attention because of its relevance to a wide range ofdomains and applications. The authors give an overview of important concepts related toprovenance management; so that potential users can make informed decisions whenselecting or designing a provenance solution.,Computing in Science & Engineering,2008,457
Vistrails: Enabling interactive multiple-view visualizations,Louis Bavoil; Steven P Callahan; Patricia J Crossno; Juliana Freire; Carlos E Scheidegger; Cláudio T Silva; Huy T Vo,VisTrails is a new system that enables interactive multiple-view visualizations by simplifyingthe creation and maintenance of visualization pipelines; and by optimizing their execution. Itprovides a general infrastructure that can be combined with existing visualization systemsand libraries. A key component of VisTrails is the visualization trail (vistrail); a formalspecification of a pipeline. Unlike existing dataflow-based systems; in VisTrails there is aclear separation between the specification of a pipeline and its execution instances. Thisseparation enables powerful scripting capabilities and provides a scalable mechanism forgenerating a large number of visualizations. VisTrails also leverages the vistrail specificationto identify and avoid redundant operations. This optimization is especially useful whileexploring multiple visualizations. When variations of the same pipeline need to be …,Visualization; 2005. VIS 05. IEEE,2005,335
The ALPS project release 2.0: open source software for strongly correlated systems,Bela Bauer; LD Carr; Hans G Evertz; Adrian Feiguin; J Freire; S Fuchs; Lukas Gamper; Jan Gukelberger; E Gull; Siegfried Guertler; A Hehn; Ryo Igarashi; Sergei V Isakov; David Koop; Ping Nang Ma; Phillip Mates; H Matsuo; Olivier Parcollet; G Pawłowski; Jean-David Picon; L Pollet; E Santos; VW Scarola; Ulrich Schollwöck; C Silva; Brigitte Surer; S Todo; S Trebst; Matthias Troyer; ML Wall; Philipp Werner; S Wessel,Abstract We present release 2.0 of the ALPS (Algorithms and Libraries for PhysicsSimulations) project; an open source software project to develop libraries and applicationprograms for the simulation of strongly correlated quantum lattice models such as quantummagnets; lattice bosons; and strongly correlated fermion systems. The code development iscentered on common XML and HDF5 data formats; libraries to simplify and speed up codedevelopment; common evaluation and plotting tools; and simulation programs. Theprograms enable non-experts to start carrying out serial or parallel numerical simulations byproviding basic implementations of the important algorithms for quantum lattice models:classical and quantum Monte Carlo (QMC) using non-local updates; extended ensemblesimulations; exact and full diagonalization (ED); the density matrix renormalization group …,Journal of Statistical Mechanics: Theory and Experiment,2011,318
Managing rapidly-evolving scientific workflows,Juliana Freire; Cláudio T Silva; Steven P Callahan; Emanuele Santos; Carlos E Scheidegger; Huy T Vo,Abstract We give an overview of VisTrails; a system that provides an infrastructure forsystematically capturing detailed provenance and streamlining the data exploration process.A key feature that sets VisTrails apart from previous visualization and scientific workflowsystems is a novel action-based mechanism that uniformly captures provenance for dataproducts and workflows used to generate these products. This mechanism not only ensuresreproducibility of results; but it also simplifies data exploration by allowing scientists to easilynavigate through the space of workflows and parameter settings for an exploration task.,International Provenance and Annotation Workshop,2006,267
Visual exploration of big spatio-temporal urban data: A study of new york city taxi trips,Nivan Ferreira; Jorge Poco; Huy T Vo; Juliana Freire; Cláudio T Silva,As increasing volumes of urban data are captured and become available; new opportunitiesarise for data-driven analysis that can lead to improvements in the lives of citizens throughevidence-based decision making and policies. In this paper; we focus on a particularlyimportant urban data set: taxi trips. Taxis are valuable sensors and information associatedwith taxi trips can provide unprecedented insight into many different aspects of city life; fromeconomic activity and human behavior to mobility patterns. But analyzing these datapresents many challenges. The data are complex; containing geographical and temporalcomponents in addition to multiple variables associated with each trip. Consequently; it ishard to specify exploratory queries and to perform comparative analyses (eg; comparedifferent regions over time). This problem is compounded due to the size of the data-there …,IEEE Transactions on Visualization and Computer Graphics,2013,235
Special issue: The first provenance challenge,Luc Moreau; Bertram Ludäscher; Ilkay Altintas; Roger S Barga; Shawn Bowers; Steven Callahan; George Chin; Ben Clifford; Shirley Cohen; Sarah Cohen‐Boulakia; Susan Davidson; Ewa Deelman; Luciano Digiampietri; Ian Foster; Juliana Freire; James Frew; Joe Futrelle; Tara Gibson; Yolanda Gil; Carole Goble; Jennifer Golbeck; Paul Groth; David A Holland; Sheng Jiang; Jihie Kim; David Koop; Ales Krenek; Timothy McPhillips; Gaurang Mehta; Simon Miles; Dominic Metzger; Steve Munroe; Jim Myers; Beth Plale; Norbert Podhorszki; Varun Ratnakar; Emanuele Santos; Carlos Scheidegger; Karen Schuchardt; Margo Seltzer; Yogesh L Simmhan; Claudio Silva; Peter Slaughter; Eric Stephan; Robert Stevens; Daniele Turi; Huy Vo; Mike Wilde; Jun Zhao; Yong Zhao,Abstract The first Provenance Challenge was set up in order to provide a forum for thecommunity to understand the capabilities of different provenance systems and theexpressiveness of their provenance representations. To this end; a functional magneticresonance imaging workflow was defined; which participants had to either simulate or run inorder to produce some provenance representation; from which a set of identified querieshad to be implemented and executed. Sixteen teams responded to the challenge; andsubmitted their inputs. In this paper; we present the challenge workflow and queries; andsummarize the participants' contributions. Copyright© 2007 John Wiley & Sons; Ltd.,Concurrency and computation: practice and experience,2008,232
Method and apparatus for web-site-independent personalization from multiple sites having user-determined extraction functionality,*,A personal Web view is created that includes a plurality of Web clippings. Each Web clippingcontains information from a user-selected Web page; which can be essentially any Webpage that is accessible on any Web server. In creating the page; the user loads apersonalization applet into his browser and retrieves each Web page of interest. The appletthen generates an access script for automatically accessing that Web page and the userspecifies the attributes of the Web clipping derived from that Web page. A specification of theWeb view is then stored in a file. When the Web view specification is later replayed; the Webpage for each Web clipping is retrieved and the specified information is extracted. The pluralWeb clippings are then displayed in a browser in accordance with the specified layout.,*,2005,231
Provenance in scientific workflow systems.,Susan B Davidson; Sarah Cohen Boulakia; Anat Eyal; Bertram Ludäscher; Timothy M McPhillips; Shawn Bowers; Manish Kumar Anand; Juliana Freire,Abstract The automated tracking and storage of provenance information promises to be amajor advantage of scientific workflow systems. We discuss issues related to data andworkflow provenance; and present techniques for focusing user attention on meaningfulprovenance through “user views;” for managing the provenance of nested scientific data;and for using information about the evolution of a workflow specification to understand thedifference in the provenance of similar data products.,IEEE Data Eng. Bull.,2007,223
An adaptive crawler for locating hidden-web entry points,Luciano Barbosa; Juliana Freire,Abstract In this paper we describe new adaptive crawling strategies to efficiently locate theentry points to hidden-Web sources. The fact that hidden-Web sources are very sparselydistributedmakes the problem of locating them especially challenging. We deal with thisproblem by using the contents ofpages to focus the crawl on a topic; by prioritizingpromisinglinks within the topic; and by also following links that may not lead to immediatebenefit. We propose a new frameworkwhereby crawlers automatically learn patterns ofpromisinglinks and adapt their focus as the crawl progresses; thus greatly reducing theamount of required manual setup andtuning. Our experiments over real Web pages in arepresentativeset of domains indicate that online learning leadsto significant gains inharvest rates' the adaptive crawlers retrieve up to three times as many forms as crawlers …,Proceedings of the 16th international conference on World Wide Web,2007,208
The open provenance model: An overview,Luc Moreau; Juliana Freire; Joe Futrelle; Robert E McGrath; Jim Myers; Patrick Paulson,Abstract Provenance is well understood in the context of art or digital libaries; where itrespectively refers to the documented history of an art object; or the documentation ofprocesses in a digital object's life cycle. Interest for provenance in the “e-sciencecommunity”[12] is also growing; since provenance is perceived as a crucial component ofworkflow systems that can help scientists ensure reproducibility of their scientific analysesand processes [2; 4].,International Provenance and Annotation Workshop,2008,207
VeriWeb: Automatically testing dynamic web sites,Michael Benedikt; Juliana Freire; Patrice Godefroid,Abstract Web sites are becoming increasingly complex as more and more services andinformation are made available over the Internet and intranets. At the same time; the correctbehavior of sites has become crucial to the success of businesses and organizations andthus should be tested thoroughly and frequently. Although traditional software testing isalready a notoriously hard; time-consuming and expensive process; testing Web sitespresents even greater challenges: Web interfaces are very dynamic; the environment of Webapplications is more complex than that of typical monolithic or client-server applications;Web applications; most notably e-commerce sites; have a large number of users who haveno training on how to use the application and hence are more likely to exercise it inunpredictable ways. Existing testing tools for automating the process of testing dynamic …,In Proceedings of 11th International World Wide Web Conference (WWW’2002),2002,205
Siphoning hidden-web data through keyword-based interfaces,Luciano Barbosa; Juliana Freire,*,Journal of Information and Data Management,2010,204
XSB: A system for efficiently computing well-founded semantics,Prasad Rao; Konstantinos Sagonas; Terrance Swift; David S Warren; Juliana Freire,Abstract The well-founded model provides a natural and robust semantics for logic programswith negative literals in rule bodies. We implemented the well-founded semantics in the SLG-WAM of XSB [19]. Performance results indicate that the overhead of delay and simplificationto Prolog—or tabled—evaluations is minimal. To compute the well-founded semantics; theSLG-WAM adds to an efficient tabling engine for definite programs three operations—negative loop detection; delay and simplification—which serve to detect; to break and toresolve cycles through negation that might arise in evaluating normal programs. XSB is a fullProlog system that closely approximates the ISO standard; additionally; it supports a tightintegration of tabled predicates with nontabled predicates.,International Conference on Logic Programming and Nonmonotonic Reasoning,1997,181
StatiX: making XML count,Juliana Freire; Jayant R Haritsa; Maya Ramanath; Prasan Roy; Jérôme Siméon,Abstract The availability of summary data for XML documents has many applications; fromproviding users with quick feedback about their queries; to cost-based storage design andquery optimization. StatiX is a novel XML Schema-aware statistics framework that exploitsthe structure derived by regular expressions (which define elements in an XML Schema) topinpoint places in the schema that are likely sources of structural skew. As we discussbelow; this information can be used to build concise; yet accurate; statistical summaries forXML data. StatiX leverages standard XML technology for gathering statistics; notably XMLSchema validators; and it uses histograms to summarize both the structure and values in anXML document. In this paper we describe the StatiX system. We develop algorithms thatdecompose schemas to obtain statistics at different granularities and discuss how …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,179
Searching for Hidden-Web Databases.,Luciano Barbosa; Juliana Freire,ABSTRACT Recently; there has been increased interest in the retrieval and integration ofhidden-Web data with a view to leverage high-quality information available in onlinedatabases. Although previous works have addressed many aspects of the actual integration;including matching form schemata and automatically filling out forms; the problem of locatingrelevant data sources has been largely overlooked. Given the dynamic nature of the Web;where data sources are constantly changing; it is crucial to automatically discover theseresources. However; considering the number of documents on the Web (Google alreadyindexes over 8 billion documents); automatically finding tens; hundreds or even thousandsof forms that are relevant to the integration task is really like looking for a few needles in ahaystack. Besides; since the vocabulary and structure of forms for a given domain are …,WebDB,2005,172
Automating Web navigation with the WebVCR,Vinod Anupam; Juliana Freire; Bharat Kumar; Daniel Lieuwen,Abstract Recent developments in Web technology such as the inclusion of scriptinglanguages; frames; and the growth of dynamic content; have made the process of retrievingWeb content more complicated; and sometimes tedious. For example; Web browsers do notprovide a method for a user to bookmark a frame-based Web site once the user navigateswithin the initial frameset. Also; some sites; such as travel sites and online classifieds;require users to go through a sequence of steps and fill out a sequence of forms in order toaccess their data. Using the bookmark facilities implemented in all popular browsers; often itis not possible to create a shortcut to access such data; and these steps must be manuallyrepeated every time the data is needed. However; hard-to-reach pages are often the bestcandidates for a shortcut; because significantly more effort is required to reach them than …,Computer Networks,2000,166
Method and apparatus for creating and providing personalized access to web content and services from terminals having diverse capabilities,*,A personalized Web view of content in a Web page is created for later access by usersthrough diverse terminals having different types of processing and display capabilities. TheWeb view provides a shortcut to specific content and services; which a user is interested inretrieving through limited bandwidth; high latency “thin” devices such as PDAs and WAPphones. Further; the Web view is customized to the specific type or types of devices that theuser will use to access the Web view. In creating the Web view from a client terminal; a useraccesses the Web page containing the information of interest either directly or by recordinga series of navigation steps used to reach a final Web page from a first Web page. One ormore extraction expressions for extracting the components of interest are generated and aWeb view specification is created and saved at a Web view server that includes the …,*,2002,164
WebViews: accessing personalized web content and services,Juliana Freire; Bharat Kumar; Daniel Lieuwen,ABSTRACT The ability to take information; entertainment and e-commerce on the go hasgreat promise. However; the existing Web infrastructure and content were designed fordesktop computers and are not well-suited for other types of accesses; eg; devices that haveless processing power and memory; small screens; and limited input facilities; or throughwireless data networks with low bandwidth and high latency. Thus; there is a growing needfor techniques that provide alternative means to access Web content and services; be it theability to browse the Web through a wireless PDA or smart phone; or hands-free accessthrough voice interfaces. In this paper; we discuss issues involved in making existing Webcontent and services available for diverse environments; and describe WebViews; a systemthat allows casual Web users to easily create customized views of Web sites that are well …,Proceedings of the 10th international conference on World Wide Web,2001,148
Implementing reproducible research,Victoria Stodden; Friedrich Leisch; Roger D Peng,In computational science; reproducibility requires that researchers make code and dataavailable to others so that the data can be analyzed in a similar manner as in the originalpublication. Code must be available to be distributed; data must be accessible in a readableformat; and a platform must be available for widely distributing the data and code. Inaddition; both data and code need to be licensed permissively enough so that others canreproduce the work without a substantial legal burden. Implementing ReproducibleResearch covers many of the elements necessary for conducting and distributingreproducible research. It explains how to accurately reproduce a scientific result. Dividedinto three parts; the book discusses the tools; practices; and dissemination platforms forensuring reproducibility in computational science. It describes: Computational tools; such …,*,2014,147
Provenance for visualizations: Reproducibility and beyond,Claudio T Silva; Juliana Freire; Steven P Callahan,The demand for the construction of complex visualizations is growing in many disciplines ofscience; as users are faced with ever increasing volumes of data to analyze. In this paper;the authors present VisTrails; an open source provenance-management system thatprovides infrastructure for data exploration and visualization. VisTrails transparently recordsdetailed provenance of exploratory computational tasks and leverages this informationbeyond just the ability to reproduce and share results. In particular; it uses this information tosimplify the process of exploring data through visualization.,Computing in Science & Engineering,2007,144
Managing the evolution of dataflows with vistrails,Steven P Callahan; Juliana Freire; Emanuele Santos; Carlos Eduardo Scheidegger; Claudio T Silva; Huy T Vo,Scientists are now faced with an incredible volume of data to analyze. To successfullyanalyze and validate various hypotheses; it is necessary to pose several queries; correlatedisparate data; and create insightful visualizations of both the simulated processes andobserved phenomena. Data exploration through visualization requires scientists to gothrough several steps. In essence; they need to assemble complex workflows that consist ofdataset selection; specification of series of operations that need to be applied to the data;and the creation of appropriate visual representations; before they can finally view andanalyze the results. Often; insight comes from comparing the results of multiplevisualizations that are created during the data exploration process.,Data Engineering Workshops; 2006. Proceedings. 22nd International Conference on,2006,138
Querying and creating visualizations by analogy,Carlos Scheidegger; Huy Vo; David Koop; Juliana Freire; Claudio Silva,While there have been advances in visualization systems; particularly in multi-viewvisualizations and visual exploration; the process of building visualizations remains a majorbottleneck in data exploration. We show that provenance metadata collected during thecreation of pipelines can be reused to suggest similar content in related visualizations andguide semi-automated changes. We introduce the idea of query-by-example in the context ofan ensemble of visualizations; and the use of analogies as first-class operations in a systemto guide scalable interactions. We describe an implementation of these techniques inVisTrails; a publicly-available; open-source system.,IEEE Transactions on Visualization and Computer Graphics,2007,135
Tackling the provenance challenge one layer at a time,Carlos Scheidegger; David Koop; Emanuele Santos; Huy Vo; Steven Callahan; Juliana Freire; Cláudio Silva,Abstract VisTrails is a new workflow and provenance management system that providessupport for scientific data exploration and visualization. Whereas workflows have beentraditionally used to automate repetitive tasks; for applications that are exploratory in nature;change is the norm. VisTrails uses a new change-based provenance mechanism; whichwas designed to handle rapidly evolving workflows. It uniformly and automatically capturesprovenance information for data products and for the evolution of the workflows used togenerate these products. In this paper; we describe how the VisTrails provenance data areorganized in layers and present a first approach for querying this data that we developed totackle the Provenance Challenge queries. Copyright© 2007 John Wiley & Sons; Ltd.,Concurrency and Computation: Practice and Experience,2008,125
A layered architecture for querying dynamic web content,Hasan Davulcu; Juliana Freire; Michael Kifer; IV Ramakrishnan,Abstract The design of webbases; database systems for supporting Web-based applications;is currently an active area of research. In this paper; we propose a 3-year architecture fordesigning and implementing webbases for querying dynamic Web content (ie; data that canonly be extracted by filling out multiple forms). The lowest layer; virtual physical layer;provides navigation independence by shielding the user from the complexities associatedwith retrieving data from raw Web sources. Next; the traditional logical layer supports siteindependence. The top layer is analogous to the external schema layer in traditionaldatabases. Within this architectural framework we address two problems unique towebbases—retrieving dynamic Web content in the virtual physical layer and querying of theexternal schema by the end user. The layered architecture makes it possible to automate …,ACM SIGMOD Record,1999,112
Querying and re-using workflows with VisTrails,Carlos E Scheidegger; Huy T Vo; David Koop; Juliana Freire; Claudio T Silva,Abstract We show how work flow systems can be augmented to leverage provenanceinformation to enhance usability. In particular; we will demonstrate new mechanisms andintuitive user interfaces designed to allow users to query work flows by example and torefine work flows by analogies. These techniques are implemented in VisTrails; an open-source provenance-enabled scientific work flow system that can be combined with a widerange of tools; libraries; and visualization systems. We will show di erent scenarios wherethese techniques can be used to simplify the notoriously hard tasks of creating and refiningwork flows.,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,110
Scientific process automation and workflow management,Bertram Ludäscher; Ilkay Altintas; Shawn Bowers; Julian Cummings; Terence Critchlow; Ewa Deelman; David D Roure; Juliana Freire; Carole Goble; Matthew Jones; Scott Klasky; Timothy McPhillips; Norbert Podhorszki; Claudio Silva; Ian Taylor; Mladen Vouk,Abstract. We introduce and describe scientific workflows; ie; executable descriptions ofautomatable scientific processes such as computational science simulations and dataanalyses. Scientific workflows are often expressed in terms of tasks and their (dataflow)dependencies. This chapter first provides an overview of the characteristic features ofscientific workflows and outlines their life cycle. A detailed case study highlights workflowchallenges and solutions in simulation management. We then provide a brief overview ofhow some concrete systems support the various phases of the workflow life cycle; ie; design;resource management; execution; and provenance management. We conclude with adiscussion on communitybased workflow sharing.,Scientific Data Management: Challenges; Existing Technology; and Deployment; Computational Science Series,2009,108
Combining classifiers to identify online databases,Luciano Barbosa; Juliana Freire,Abstract We address the problem of identifying the domain of onlinedatabases. Moreprecisely; given a set F of Web forms automaticallygathered by a focused crawler and anonline databasedomain D; our goal is to select from F only the formsthat are entry points todatabases in D. Having a set ofWebforms that serve as entry points to similar onlinedatabasesis a requirement for many applications and techniques thataim to extract andintegrate hidden-Web information; suchas meta-searchers; online database directories;hidden-Webcrawlers; and form-schema matching and merging. We propose a new strategythat automatically and accuratelyclassifies online databases based on features that canbeeasily extracted from Web forms. By judiciously partitioningthe space of form features; thisstrategy allows theuse of simpler classifiers that can be constructed using …,Proceedings of the 16th international conference on World Wide Web,2007,108
Viscomplete: Automating suggestions for visualization pipelines,David Koop; Carlos Scheidegger; Steven P. Callahan; Juliana Freire; Claudio T. Silva,Building visualization and analysis pipelines is a large hurdle in the adoption of visualizationand workflow systems by domain scientists. In this paper; we propose techniques to helpusers construct pipelines by consensus-automatically suggesting completions based on adatabase of previously created pipelines. In particular; we compute correspondencesbetween existing pipeline subgraphs from the database; and use these to predict sets oflikely pipeline additions to a given partial pipeline. By presenting these predictions in acarefully designed interface; users can create visualizations and other data products moreefficiently because they can augment their normal work patterns with the suggestedcompletions. We present an implementation of our technique in a publicly-available; open-source scientific workflow system and demonstrate efficiency gains in real-world …,IEEE Transactions on Visualization and Computer Graphics,2008,101
A fast and robust method for web page template detection and removal,Karane Vieira; Altigran S Da Silva; Nick Pinto; Edleno S De Moura; Joao Cavalcanti; Juliana Freire,Abstract The widespread use of templates on the Web is considered harmful for two mainreasons. Not only do they compromise the relevance judgment of many web IR and webmining methods such as clustering and classification; but they also negatively impact theperformance and resource usage of tools that process web pages. In this paper we presenta new method that efficiently and accurately removes templates found in collections of webpages. Our method works in two steps. First; the costly process of template detection isperformed over a small set of sample pages. Then; the derived template is removed from theremaining pages in the collection. This leads to substantial performance gains whencompared to previous approaches that combine template detection and removal. We show;through an experimental evaluation; that our approach is effective for identifying terms …,Proceedings of the 15th ACM international conference on Information and knowledge management,2006,100
Method for creating and playing back a smart bookmark that automatically retrieves a requested Web page through a plurality of intermediate Web pages,*,Shortcuts to Web pages that require multiple steps to be retrieved are enabled by means ofa smart bookmark. A smart bookmark is a stored sequence of browsing steps performed by auser; that have been recorded in a transparent manner and which can be automaticallyplayed and replayed later when the smart bookmark is accessed. When a user elects tocreate a smart bookmark; a Java recorder-player applet is invoked that starts the recordingprocess. When the recording process is started and an initial URL is inputted by the user; theresponsive Web page at that URL downloaded into the browser is modified to attach eventhandlers to each element in that page that is associated with actions that the user may take.Each user's click; link traversal to another URL; or input of values to those elements on aform submission are automatically recorded as part of the smart bookmark under creation …,*,2003,98
A comprehensive solution to the XML-to-relational mapping problem,Sihem Amer-Yahia; Fang Du; Juliana Freire,Abstract The use of relational database management systems (RDBMSs) to store and queryXML data has attracted considerable interest with a view to leveraging their powerful andreliable data management services. Due to the mismatch between the relational and XMLdata models; it is necessary to first shred and load the XML data into relational tables; andthen btranslate XML queries over the original data into equivalent SQL queries over themapped tables. Although there is a rich literature on XML-relational storage; none of theexisting solutions addresses all the storage problems in a single framework. Works onmapping strategies often have little or no details about query translation; and proposals forquery translation often target a specific mapping strategy. XML-storage solutions providedby RDBMS also have limitations. Notably; they are tied to a specific backend and use …,Proceedings of the 6th annual ACM international workshop on Web information and data management,2004,97
Beyond depth-first: Improving tabled logic programs through alternative scheduling strategies,Juliana Freire; Terrance Swift; David S Warren,Abstract Tabled evaluations ensure termination of logic programs with finite models bykeeping track of which subgoals have been called. Given several variant subgoals in anevaluation; only the first one encountered will use program clause resolution; the rest usesanswer resolution. This use of answer resolution prevents infinite looping which happens inSLD. Given the asynchronicity of answer generation and answer return; tabling systems facean important scheduling choice not present in traditional top-down evaluation: How does theorder of returning answers to consuming subgoals affect program efficiency. This paperinvestigates alternate scheduling strategies for tabling in a WAM implementation; the SLG-WAM. The original SLG-WAM had a simple mechanism of scheduling answers to bereturned to callers which was expensive in terms of trailing and choice point creation. We …,International Symposium on Programming Language Implementation and Logic Programming,1996,96
Learning to extract form labels,Hoa Nguyen; Thanh Nguyen; Juliana Freire,Abstract In this paper we describe a new approach to extract element labels from Web forminterfaces. Having these labels is a requirement for several techniques that attempt toretrieve and integrate information that is hidden behind form interfaces; such as hidden Webcrawlers and metasearchers. However; given the wide variation in form layout; even within awell-defined domain; automatically extracting these labels is a challenging problem.Whereas previous approaches to this problem have relied on heuristics and manuallyspecified extraction rules; our technique makes use of a learning classifier ensemble toidentify element-label mappings; and it applies a reconciliation step which leverages theclassifier-derived mappings to boost extraction accuracy. We present a detailedexperimental evaluation using over three thousand Web forms. Our results show that our …,Proceedings of the VLDB Endowment,2008,78
Organizing hidden-web databases by clustering visible web documents,Luciano Barbosa; Juliana Freire; Altigran Silva,In this paper we address the problem of organizing hidden-Web databases. Given aheterogeneous set of Web forms that serve as entry points to hidden-Web databases; ourgoal is to cluster the forms according to the database domains to which they belong. Wepropose a new clustering approach that models Web forms as a set of hyperlinked objectsand considers visible information in the form context-both within and in the neighborhood offorms-as the basis for similarity comparison. Since the clustering is performed over featuresthat can be automatically extracted; the process is scalable. In addition; because it uses arich set of metadata; our approach is able to handle a wide range of forms; including content-rich forms that contain multiple attributes; as well as simple keyword-based searchinterfaces. An experimental evaluation over real Web data shows that our strategy …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,78
Cost-based storage of extensible markup language (XML) data,*,Extensible Markup Language (XML) data is mapped to be stored in an alternative data basemanagement system (DBMS) by generating a plurality of alternative ones of mappings inresponse to a supplied XML document and corresponding XML schema; evaluating at leasta prescribed attribute of each of the plurality of mappings with respect to an expectedworkload for the storage system; and selecting one of the alternative mappings based on theprescribed attribute which is the most advantageous for the expected system workload. Morespecifically; applicants employ a unique process that utilizes a unique notion of physicalXML Schemas; ie; P-Schemas; a P-Schema costing procedure; a set of P-Schemarewritings; and a search strategy to heuristically determine the P-Schema with the least cost.Specifically; the unique notion of physical XML Schemas; extend XML Schemas to …,*,2004,75
Capturing both types and constraints in data integration,Michael Benedikt; Chee-Yong Chan; Wenfei Fan; Juliana Freire; Rajeev Rastogi,Abstract We propose a framework for integrating data from multiple relational sources intoan XML document that both conforms to a given DTD and satisfies predefined XMLconstraints. The framework is based on a specification language; AIG; that extends a DTD by(1) associating element types with semantic attributes (inherited and synthesized; inspiredby the corresponding notions from Attribute Grammars);(2) computing these attributes viaparameterized SQL queries over multiple data sources; and (3) incorporating XML keys andinclusion constraints. The novelty of AIG consists in semantic attributes and theirdependency relations for controlling context-dependent; DTD-directed construction of XMLdocuments; as well as for checking XML constraints in parallel with document-generation.We also present cost-based optimization techniques for efficiently evaluating AIGs …,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,74
ShreX: Managing XML documents in relational databases,Fang Du; Sihem Amer-Yahia; Juliana Freire,Abstract We describe ShreX; a freely-available system for shredding; loading and queryingXML documents in relational databases. ShreX supports all mapping strategies proposed inthe literature as well as strategies available in commercial RDBMSs. It provides generic(mapping-independent) functions for loading shredded documents into relations and fortranslating XML queries into SQL. ShreX is portable and can be used with any relationaldatabase backend.,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,73
Designing information-preserving mapping schemes for XML,Denilson Barbosa; Juliana Freire; Alberto O Mendelzon,Abstract An XML-to-relational mapping scheme consists of a procedure for shreddingdocuments into relational databases; a procedure for publishing databases back asdocuments; and a set of constraints the databases must satisfy. In previous work; we definedtwo notions of information preservation for mapping schemes: losslessness; whichguarantees that any document can be reconstructed from its corresponding database; andvalidation; which requires every legal database to correspond to a valid document. We alsodescribed one information-preserving mapping scheme; called Edge++; and showed that;under reasonable assumptions; losslessness and validation are both undecidable. Thisleads to the question we study in this paper: how to design mapping schemes that areinformation-preserving. We propose to do it by starting with a scheme known to be …,Proceedings of the 31st international conference on Very large data bases,2005,68
Applications of executable shopping lists,*,An executable shopping list (ESL) enables a user browsing an affiliate Web site's Web pageto order; with a single action; a bundle of multiple items offered for sale by the affiliate'sassociated merchant Web site; wherein the bundle is one that is not offered or available forsale as a bundle to on-line visitors who directly access the merchant's Web site server. Byselecting a link on the affiliate's Web page; the ESL is executed; thereby automaticallyloading the bundle of multiple items on that list into a virtual shopping cart at the merchant'sWeb site on behalf of the user. The virtual shopping cart; filled with the multiple items; is thenreturned to the user's browser for completion of the checkout procedure by the user. ESLscan be implemented on the affiliate site's side using what is referred to as smart bookmarktechnology; which records for later playback; the multiple steps required to load each item …,*,2006,67
ReproZip: Using Provenance to Support Computational Reproducibility.,Fernando Seabra Chirigati; Dennis E Shasha; Juliana Freire,Abstract We describe ReproZip; a tool that makes it easier for authors to publishreproducible results and for reviewers to validate these results. By tracking operating systemcalls; ReproZip systematically captures detailed provenance of existing experiments;including data dependencies; libraries used; and configuration parameters. This informationis combined into a package that can be installed and run on a different environment. Animportant goal that we have for ReproZip is usability. Besides simplifying the creation ofreproducible results; the system also helps reviewers. Because the package isselfcontained; reviewers need not install any additional software to run the experiments. Inaddition; ReproZip generates a workflow specification for the experiment. This not onlyenables reviewers to execute this specification within a workflow system to explore the …,TaPP,2013,65
noWorkflow: Capturing and Analyzing Provenance of Scripts,Leonardo Murta; Vanessa Braganholo; Fernando Chirigati; David Koop; Juliana Freire,Abstract We propose noWorkflow; a tool that transparently captures provenance of scriptsand enables reproducibility. Unlike existing approaches; noWorkflow is non-intrusive anddoes not require users to change the way they work–users need not wrap their experimentsin scientific workflow systems; install version control systems; or instrument their scripts. Thetool leverages Software Engineering techniques; such as abstract syntax tree analysis;reflection; and profiling; to collect different types of provenance; including detailedinformation about the underlying libraries. We describe how noWorkflow captures multiplekinds of provenance and the different classes of analyses it supports: graph-basedvisualization; differencing over provenance trails; and inference queries.,Provenance and Annotation of Data and Processes: 5th International Provenance and Annotation Workshop; IPAW 2014; Cologne; Germany; June 9-13; 2014. Revised Selected Papers,2014,61
The XSB programmer's manual,Kostis Sagonas; Terrance Swift; D Warren,*,*,1993,59
Crowdlabs: Social analysis and visualization for the sciences,Phillip Mates; Emanuele Santos; Juliana Freire; Cláudio T Silva,Abstract Managing and understanding the growing volumes of scientific data is one of themost challenging issues scientists face today. As analyses get more complex and largeinterdisciplinary groups need to work together; knowledge sharing becomes essential tosupport effective scientific data exploration. While science portals and visualization Websites have provided a first step towards this goal; by aggregating data from different sourcesand providing a set of pre-designed analyses and visualizations; they have importantlimitations. Often; these sites are built manually and are not flexible enough to support thevast heterogeneity of data sources; analysis techniques; data products; and the needs ofdifferent user communities. In this paper we describe CrowdLabs; a system that adopts themodel used by social Web sites; allowing users to share not only data but also …,International Conference on Scientific and Statistical Database Management,2011,57
Computational reproducibility: state-of-the-art; challenges; and database research opportunities,Juliana Freire; Philippe Bonnet; Dennis Shasha,Abstract Computational experiments have become an integral part of the scientific method;but reproducing; archiving; and querying them is still a challenge. The first barrier to a wideradoption is the fact that it is hard both for authors to derive a compendium that encapsulatesall the components needed to reproduce a result and for reviewers to verify the results. Inthis tutorial; we will present a series of guidelines and; through hands-on examples; reviewexisting tools to help authors create of reproducible results. We will also outline openproblems and new directions for database-related research having to do with queryingcomputational experiments.,SIGMOD,2012,56
Vismashup: Streamlining the creation of custom visualization applications,Emanuele Santos; Lauro Lins; James Ahrens; Juliana Freire; Claudio Silva,Visualization is essential for understanding the increasing volumes of digital data. However;the process required to create insightful visualizations is involved and time consuming.Although several visualization tools are available; including tools with sophisticated visualinterfaces; they are out of reach for users who have little or no knowledge of visualizationtechniques and/or who do not have programming expertise. In this paper; we proposeVisMashup; a new framework for streamlining the creation of customized visualizationapplications. Because these applications can be customized for very specific tasks; they canhide much of the complexity in a visualization specification and make it easier for users toexplore visualizations by manipulating a small set of parameters. We describe theframework and how it supports the various tasks a designer needs to carry out to develop …,IEEE Transactions on Visualization and Computer Graphics,2009,50
Making computations and publications reproducible with vistrails,Juliana Freire; Claudio T Silva,The VisTrails system supports the creation of reproducible experiments. VisTrails integratesdata acquisition; derivation; analysis; and visualization as executable componentsthroughout the scientific exploration process; and through systematic provenance capture; itmakes it easier to generate and share reproducible results. Using VisTrails; authors can linkresults to their provenance; reviewers can assess the experiment's validity; and readers canrepeat and utilize the computations.,Computing in Science & Engineering,2012,49
Searching for efficient XML-to-relational mappings,Maya Ramanath; Juliana Freire; Jayant R Haritsa; Prasan Roy,Abstract We consider the problem of cost-based strategies to derive efficient relationalconfigurations for XML applications that subscribe to an XML Schema. In particular; wepropose a flexible framework for XML schema transformations and show how it can be usedto design algorithms to search the space of equivalent relational configurations. We studythe impact of the schema transformations and query workload on the search strategies forfinding efficient XML-to-relational mappings. In addition; we propose several optimizations tospeed up the search process. Our experiments indicate that a judicious choice oftransformations and search strategies can lead to relational configurations of substantiallyhigher quality than those recommended by previous approaches.,International XML Database Symposium,2003,49
Parallel visualization on large clusters using MapReduce,Huy T Vo; Jonathan Bronson; Brian Summa; Joao LD Comba; Juliana Freire; Bill Howe; Valerio Pascucci; Cláudio T Silva,Large-scale visualization systems are typically designed to efficiently “push” datasetsthrough the graphics hardware. However; exploratory visualization systems are increasinglyexpected to support scalable data manipulation; restructuring; and querying capabilities inaddition to core visualization algorithms. We posit that new emerging abstractions forparallel data processing; in particular computing clouds; can be leveraged to support large-scale data exploration through visualization. In this paper; we take a first step in evaluatingthe suitability of the MapReduce framework to implement large-scale visualizationtechniques. MapReduce is a lightweight; scalable; general-purpose parallel data processingframework increasingly popular in the context of cloud computing. Specifically; weimplement and evaluate a representative suite of visualization tasks (mesh rendering …,Large Data Analysis and Visualization (LDAV); 2011 IEEE Symposium on,2011,47
LegoDB: Customizing relational storage for XML documents,Philip Bohannon; Juliana Freire; Jayant R Haritsa; Maya Ramanath; Prasan Roy; Jérôme Siméon,eXtensible Markup Language (XML) is becoming the predominant data exchange format ina variety of application domains (supply-chain; scientific data processing;telecommunication infrastructure; etc.). Not only is an increasing amount of XML data nowbeing processed; but XML is also increasingly being used in business-critical applications.Efficient and reliable storage is an important requirement for these applications. By relyingon relational engines for this purpose; XML developers can benefit from a complete set ofdata management services (including concurrency control; crash recovery; and scalability)and from the highly optimized relational query processors. Strategies that automate theprocess of generating XML to relational mappings have been proposed in the literature. Dueto the flexibility of the XML infrastructure; different XML applications exhibit widely …,*,2002,47
A provenance-based infrastructure to support the life cycle of executable papers,David Koop; Emanuele Santos; Phillip Mates; Huy T Vo; Philippe Bonnet; Bela Bauer; Brigitte Surer; Matthias Troyer; Dean N Williams; Joel E Tohline; Juliana Freire; Cláudio T Silva,Abstract As publishers establish a greater online presence as well as infrastructure tosupport the distribution of more varied information; the idea of an executable paper thatenables greater interaction has developed. An executable paper provides more informationfor computational experiments and results than the text; tables; and figures of standardpapers. Executable papers can bundle computational content that allow readers andreviewers to interact; validate; and explore experiments. By including such content; authorsfacilitate future discoveries by lowering the barrier to reproducing and extending results. Wepresent an infrastructure for creating; disseminating; and maintaining executable papers.Our approach is rooted in provenance; the documentation of exactly how data; experiments;and results were generated. We seek to improve the experience for everyone involved in …,Procedia Computer Science,2011,46
YesWorkflow: a user-oriented; language-independent tool for recovering workflow information from scripts,Timothy McPhillips; Tianhong Song; Tyler Kolisnik; Steve Aulenbach; Khalid Belhajjame; Kyle Bocinsky; Yang Cao; Fernando Chirigati; Saumen Dey; Juliana Freire; Deborah Huntzinger; Christopher Jones; David Koop; Paolo Missier; Mark Schildhauer; Christopher Schwalm; Yaxing Wei; James Cheney; Mark Bieda; Bertram Ludaescher,Abstract: Scientific workflow management systems offer features for composing complexcomputational pipelines from modular building blocks; for executing the resulting automatedworkflows; and for recording the provenance of data products resulting from workflow runs.Despite the advantages such features provide; many automated workflows continue to beimplemented and executed outside of scientific workflow systems due to the convenienceand familiarity of scripting languages (such as Perl; Python; R; and MATLAB); and to thehigh productivity many scientists experience when using these languages. YesWorkflow is aset of software tools that aim to provide such users of scripting languages with many of thebenefits of scientific workflow systems. YesWorkflow requires neither the use of a workflowengine nor the overhead of adapting code to run effectively in such a system. Instead …,arXiv preprint arXiv:1502.02403,2015,45
Using topological analysis to support event-guided exploration in urban data,Harish Doraiswamy; Nivan Ferreira; Theodoros Damoulas; Juliana Freire; Cláudio T Silva,The explosion in the volume of data about urban environments has opened up opportunitiesto inform both policy and administration and thereby help governments improve the lives oftheir citizens; increase the efficiency of public services; and reduce the environmental harmsof development. However; cities are complex systems and exploring the data they generateis challenging. The interaction between the various components in a city creates complexdynamics where interesting facts occur at multiple scales; requiring users to inspect a largenumber of data slices over time and space. Manual exploration of these slices is ineffective;time consuming; and in many cases impractical. In this paper; we propose a technique thatsupports event-guided exploration of large; spatio-temporal urban data. We model the dataas time-varying scalar functions and use computational topology to automatically identify …,IEEE transactions on visualization and computer graphics,2014,43
Semantica: Version 1.0 (for NEXTSTEP),Richard K Larson; David S Warren,.".." Semantica" is a graceful tool for visualizing the workings of formal semantics; bothderivations and models. Virtually all students will need it."--Edwin Williams; Professor ofLinguistics; Princeton University" Semantica" is the manual for a new software applicationthat allows the user to explore the semantic structure of language in an engaging; interactiveway. The program; which was produced as part of a National Science Foundation initiativefor improving linguistics instruction; is designed to be used with" Syntactica;" a tool forstudying natural language syntax." Semantica" provides a simple graphical interface forcreating semantic theories; viewing the truth conditions that those theories assign to phrase-markers created in" Syntactica;" and for testing those truth conditions in a pictoriallyrepresented world. Although designed for use in introductory semantics courses;" …,*,1997,43
Scientific exploration in the era of ocean observatories,António Baptista; Bill Howe; Juliana Freire; David Maier; Cláudio T Silva,Copublished by the IEEE CS and the AIP 1521-9615/08/$25.00 ©2008 IEEE 53 … Editors: CláudioSilva; csilva@cs.utah.edu Joel E. Tohline; tohline@rouge.phys.lsu.edu … By AntónioBaptista; Bill Howe; Juliana Freire; David Maier; and Cláudio T. Silva … The authors introducean ocean observatory; offer a vision of observatory-enabled scientific exploration; and discussthe requirements and approaches for generating provenance-aware products in suchenvironments … An Observatory in Evolution CORIE (for Columbia River Estuary) is a coastalmargin observatory op- erated at Oregon Health & Science University since 1996 as anend-to- end; data-to-stakeholder system.3 In 2004; CORIE became a founding contributing nodeto the Northwest Association of Networked Ocean Observing Systems (NANOOS); a regionalassociation of IOOS. Since 2006; CORIE has anchored the devel- opment of an …,Computing in Science & Engineering,2008,41
Supporting exploratory queries in databases,Abhijit Kadlag; Amol V Wanjari; Juliana Freire; Jayant R Haritsa,Abstract Users of database applications; especially in the e-commerce domain; often resortto exploratory “trial-and-error” queries since the underlying data space is huge andunfamiliar; and there are several alternatives for search attributes in this space. For example;scouting for cheap airfares typically involves posing multiple queries; varying flight times;dates; and airport locations. Exploratory queries are problematic from the perspective of boththe user and the server. For the database server; it results in a drastic reduction in effectivethroughput since much of the processing is duplicated in each successive query. For theclient; it results in a marked increase in response times; especially when accessing theservice through wireless channels. In this paper; we investigate the design of automatedtechniques to minimize the need for repetitive exploratory queries. Specifically; we …,International Conference on Database Systems for Advanced Applications,2004,41
Birdvis: Visualizing and understanding bird populations,Nivan Ferreira; Lauro Lins; Daniel Fink; Steve Kelling; Christopher Wood; Juliana Freire; Claudio Silva,Birds are unrivaled windows into biotic processes at all levels and are proven indicators ofecological well-being. Understanding the determinants of species distributions and theirdynamics is an important aspect of ecology and is critical for conservation and management.Through crowdsourcing; since 2002; the eBird project has been collecting bird observationrecords. These observations; together with local-scale environmental covariates such asclimate; habitat; and vegetation phenology have been a valuable resource for a globalcommunity of educators; land managers; ornithologists; and conservation biologists. Byassociating environmental inputs with observed patterns of bird occurrence; predictivemodels have been developed that provide a statistical framework to harness available datafor predicting species distributions and making inferences about species-habitat …,IEEE Transactions on Visualization and Computer Graphics,2011,40
A first study on clustering collections of workflow graphs,Emanuele Santos; Lauro Lins; James P Ahrens; Juliana Freire; Claudio T Silva,Abstract As workflow systems get more widely used; the number of workflows and thevolume of provenance they generate has grown considerably. New tools and infrastructureare needed to allow users to interact with; reason about; and re-use this information. In thispaper; we explore the use of clustering techniques to organize large collections of workflowand provenance graphs. We propose two different representations for these graphs andpresent an experimental evaluation; using a collection of 1;700 workflow graphs; where westudy the trade-offs of these representations and the effectiveness of alternative clusteringtechniques.,International Provenance and Annotation Workshop,2008,40
Taking I/O Seriously: Resolution Reconsidered for Disk.,Juliana Freire; Terrance Swift; David Scott Warren,Abstract Modern compilation techniques can give Prolog programs; in the best cases; aspeed comparable to C. However; Prolog has proven to be unacceptable for data-orientedqueries for two major reasons: its poor termination and complexity properties for Datalog;and its tuple-at-a-time strategy. A number of tabling frameworks and systems haveaddressed the first problem; including the XSB system which has achieved Prolog speedsfor tabled programs. Yet tabling systems such as XSB continue to use the tuple-at-a-timeparadigm. As a result; these systems are not amenable to a tight interconnection with disk-resident data. However; in a tabling framework the difference between tuple-at-a-timebehavior and set-at-a-time can be viewed as one of scheduling. Accordingly; we define abreadth-first set-at-a-time tabling strategy and prove it iteration equivalent to a form of …,ICLP,1997,40
Multilingual schema matching for Wikipedia infoboxes,Thanh Nguyen; Viviane Moreira; Huong Nguyen; Hoa Nguyen; Juliana Freire,Abstract Recent research has taken advantage of Wikipedia's multi-lingualism as a resourcefor cross-language information retrieval and machine translation; as well as proposedtechniques for enriching its cross-language structure. The availability of documents inmultiple languages also opens up new opportunities for querying structured Wikipediacontent; and in particular; to enable answers that straddle different languages. As a steptowards supporting such queries; in this paper; we propose a method for identifyingmappings between attributes from infoboxes that come from pages in different languages.Our approach finds mappings in a completely automated fashion. Because it does notrequire training data; it is scalable: not only can it be used to find mappings between manylanguage pairs; but it is also effective for languages that are under-represented and lack …,Proceedings of the VLDB Endowment,2011,39
Using vistrails and provenance for teaching scientific visualization,Cláudio T Silva; Erik Anderson; Emanuele Santos; Juliana Freire,Abstract Over the last 20 years; visualization courses have been developed and offered atuniversities around the world. Many of these courses use established visualization librariesand tools (eg VTK; ParaView; AVS; VisIt) as a way to provide students a hands-onexperience; allowing them to prototype and explore different visualization techniques. In thispaper; we describe our experiences using VisTrails as a platform to teach scientificvisualization. VisTrails is an open-source system that was designed to support exploratorycomputational tasks such as visualization and data analysis. Unlike previous scientificworkflow and visualization systems; VisTrails provides a comprehensive provenancemanagement infrastructure. We discuss how different features of the system; and inparticular; the provenance information have changed the dynamics of the Scientific …,Computer Graphics Forum,2011,38
Using vistrails and provenance for teaching scientific visualization,Cláudio T Silva; Erik Anderson; Emanuele Santos; Juliana Freire,Abstract Over the last 20 years; visualization courses have been developed and offered atuniversities around the world. Many of these courses use established visualization librariesand tools (eg VTK; ParaView; AVS; VisIt) as a way to provide students a hands-onexperience; allowing them to prototype and explore different visualization techniques. In thispaper; we describe our experiences using VisTrails as a platform to teach scientificvisualization. VisTrails is an open-source system that was designed to support exploratorycomputational tasks such as visualization and data analysis. Unlike previous scientificworkflow and visualization systems; VisTrails provides a comprehensive provenancemanagement infrastructure. We discuss how different features of the system; and inparticular; the provenance information have changed the dynamics of the Scientific …,Computer Graphics Forum,2011,38
Exploiting parallelism in tabled evaluations,Juliana Freire; Rui Hu; Terrance Swift; David S Warren,Abstract This paper addresses general issues involved in parallelizing tabled evaluations byintroducing a model of shared-memory parallelism which we call table-parallelism; and bycomparing it to traditional models of parallelizing SLD. A basic architecture for supportingtable-parallelism in the framework of the SLG-WAM [14] is also presented; along with analgorithm for detecting termination of subcomputations.,International Symposium on Programming Language Implementation and Logic Programming,1995,37
Bridging workflow and data provenance using strong links,David Koop; Emanuele Santos; Bela Bauer; Matthias Troyer; Juliana Freire; Cláudio T Silva,Abstract As scientists continue to migrate their work to computational methods; it is importantto track not only the steps involved in the computation but also the data consumed andproduced. While this provenance information can be captured; in existing approaches; itoften contains only weak references between data and provenance. When data files orprovenance are moved or modified; it can be difficult to find the data associated with theprovenance or to find the provenance associated with the data. We propose a persistentstorage mechanism that manages input; intermediate; and output data files; strengtheningthe links between provenance and data. This mechanism provides better support forreproducibility because it ensures the data referenced in provenance information can bereadily located. Another important benefit of such management is that it allows caching of …,International Conference on Scientific and Statistical Database Management,2010,35
End-to-end escience: Integrating workflow; query; visualization; and provenance at an ocean observatory,Bill Howe; Peter Lawson; Renee Bellinger; Erik Anderson; Emanuele Santos; Juliana Freire; Carlos Scheidegger; António Baptista; Cláudio Silva,Data analysis tasks at an Ocean Observatory require integrative and and domain-specialized use of database; workflow; visualization systems. We describe a platform tosupport these tasks developed as part of the cyberinfrastructure at the NSF Science andTechnology Center for Coastal Margin Observation and Prediction integrating a provenance-aware workflow system; 3D visualization; and a remote query engine for large-scale oceancirculation models. We show how these disparate tools complement each other and giveexamples of real scientific insights delivered by the integrated system. We conclude thatdata management solutions for eScience require this kind of holistic; integrative approach;explain how our approach may be generalized; and recommend a broader; application-oriented research agenda to explore relevant architectures.,eScience; 2008. eScience'08. IEEE Fourth International Conference on,2008,34
Analogy based workflow identification,*,A method of creating an analogous workflow is provided. A first workflow is received at a firstdevice; the first workflow including a plurality of first modules that are connected. A secondworkflow is received at the first device; the second workflow including a plurality of secondmodules that are connected. A third workflow is received at the first device; the third workflowincluding a plurality of third modules that are connected. An analogy workflow is determinedbased on a difference between the received first workflow and the received secondworkflow. The determined analogy workflow is applied to the received third workflow todefine a fourth workflow. A method of identifying a workflow of a plurality of workflows isprovided. A query workflow includes a plurality of modules that are connected. A workflow isidentified that at least partially matches the received query workflow.,*,2014,32
Method and system for clustering identified forms,*,A method is provided for organizing a plurality of documents that include forms. An initial setof clusters is defined for the plurality of documents. The initial set of clusters is reclusteredbased on similarity values calculated in multiple feature spaces. For example; a first featurespace may be associated with a content of a document while a second feature space maybe associated with a content of a form associated with the document. Each cluster has anassociated centroid vector in each feature space that is used to represent the cluster. Thesimilarity between the document and each cluster is calculated in both feature spaces. Eachdocument is assigned to the cluster whose centroid is most similar. The cluster centroidsmay be recalculated and the process repeated until the cluster assignments become stable.,*,2011,30
Integrated Scientific Workflow Management for the Emulab Network Testbed.,Eric Eide; Leigh Stoller; Tim Stack; Juliana Freire; Jay Lepreau,Abstract The main forces that shaped current network testbeds were the needs for realismand scale. Now that several testbeds support large and complex experiments; managementof experimentation processes and results has become more difficult and a barrier to high-quality systems research. The popularity of network testbeds means that new tools formanaging experiment workflows; addressing the ready-made base of testbed users; canhave important and significant impacts.,USENIX Annual Technical Conference; General Track,2006,28
Information preservation in XML-to-relational mappings,Denilson Barbosa; Juliana Freire; Alberto O Mendelzon,Abstract We study the problem of storing XML documents using relational mappings. Wepropose a formalization of classes of mapping schemes based on the languages used fordefining functions that assign relational databases to XML documents and vice-versa. Wealso discuss notions of information preservation for mapping schemes; we define losslessmapping schemes as those that preserve the structure and content of the documents; andvalidating mapping schemes as those in which valid documents can be mapped into legaldatabases; and all legal databases are (equivalent to) mappings of valid documents. Wedefine one natural class of mapping schemes that captures all mappings in the literature;and show negative results for testing whether such mappings are lossless or validating.Finally; we propose a lossless and validating mapping scheme; and show that it performs …,International XML Database Symposium,2004,27
Structured open urban data: understanding the landscape,Luciano Barbosa; Kien Pham; Claudio Silva; Marcos R Vieira; Juliana Freire,Abstract A growing number of cities are now making urban data freely available to thepublic. Besides promoting transparency; these data can have a transformative effect insocial science research as well as in how citizens participate in governance. Theseinitiatives; however; are fairly recent and the landscape of open urban data is not wellknown. In this study; we try to shed some light on this through a detailed study of over 9;000open data sets from 20 cities in North America. We start by presenting general statisticsabout the content; size; nature; and popularity of the different data sets; and then examine inmore detail structured data sets that contain tabular data. Since a key benefit of having alarge number of data sets available is the ability to fuse information; we investigateopportunities for data integration. We also study data quality issues and time-related …,*,2014,26
Synthesizing products for online catalogs,Hoa Nguyen; Ariel Fuxman; Stelios Paparizos; Juliana Freire; Rakesh Agrawal,Abstract A comprehensive product catalog is essential to the success of Product Searchengines and shopping sites such as Yahoo! Shopping; Google Product Search; and BingShopping. Given the large number of products and the speed at which they are released tothe market; keeping catalogs up-to-date becomes a challenging task; calling for the need ofautomated techniques. In this paper; we introduce the problem of product synthesis; a keycomponent of catalog creation and maintenance. Given a set of offers advertised bymerchants; the goal is to identify new products and add them to the catalog; together withtheir (structured) attributes. A fundamental challenge in product synthesis is the scale of theproblem. A Product Search engine receives data from thousands of merchants aboutmillions of products; the product taxonomy contains thousands of categories; where each …,Proceedings of the VLDB Endowment,2011,26
Towards provenance-enabling paraview,Steven P Callahan; Juliana Freire; Carlos E Scheidegger; Cláudio T Silva; Huy T Vo,Abstract Currently; there are no general provenance management systems or tools availablefor existing applications. Our goal is to develop provenance technology that is flexible andadaptable to the wide range of requirements of software applications. By consolidatingprovenance information for a variety of applications; we can provide a uniform environmentfor querying; sharing; and re-using provenance in large-scale; collaborative settings. In thispaper; we describe our framework for provenance-enabling existing applications. Ourapproach is applicable to a variety of software systems that are process driven. As aconcrete example; we describe a working plug-in for an open source application in scientificvisualization.,International Provenance and Annotation Workshop,2008,26
Repeatability and workability evaluation of SIGMOD 2011,Philippe Bonnet; Stefan Manegold; Matias Bjørling; Wei Cao; Javier Gonzalez; Joel Granados; Nancy Hall; Stratos Idreos; Milena Ivanova; Ryan Johnson; David Koop; Tim Kraska; René Müller; Dan Olteanu; Paolo Papotti; Christine Reilly; Dimitris Tsirogiannis; Cong Yu; Juliana Freire; Dennis Shasha,Abstract SIGMOD has offered; since 2008; to verify the experiments published in the papersaccepted at the conference. This year; we have been in charge of reproducing theexperiments provided by the authors (repeatability); and exploring changes to experimentparameters (workability). In this paper; we assess the SIGMOD repeatability process in termsof participation; review process and results. While the participation is stable in terms ofnumber of submissions; we find this year a sharp contrast between the high participationfrom Asian authors and the low participation from American authors. We also find that mostexperiments are distributed as Linux packages accompanied by instructions on how to setupand run the experiments. We are still far from the vision of executable papers.,ACM SIGMOD Record,2011,25
Using provenance to support real-time collaborative design of workflows,Tommy Ellkvist; David Koop; Erik W Anderson; Juliana Freire; Cláudio Silva,Abstract Because designing workflows is a notoriously difficult task; it often requires multipleusers to collaborate. In such scenarios; sharing workflow evolution provenance in a timelymanner is critical. We present an environment where collaborating users can see eachother's changes in real-time. The synchronization of workflow evolution provenance isautomatic; immediate; and unobtrusive; allowing users to see collaborators' changes as theyare made. This enables a richer and fuller method of collaboration. We present the interfaceand algorithm for the synchronization and discuss common scenarios where this mechanismhas been utilized.,International Provenance and Annotation Workshop,2008,25
Transfer function design based on user selected samples for intuitive multivariate volume exploration,Liang Zhou; Charles Hansen,Multivariate volumetric datasets are important to both science and medicine. We propose atransfer function (TF) design approach based on user selected samples in the spatialdomain to make multivariate volumetric data visualization more accessible for domain users.Specifically; the user starts the visualization by probing features of interest on slices and thedata values are instantly queried by user selection. The queried sample values are thenused to automatically and robustly generate high dimensional transfer functions (HDTFs) viakernel density estimation (KDE). Alternatively; 2D Gaussian TFs can be automaticallygenerated in the dimensionality reduced space using these samples. With the extractedfeatures rendered in the volume rendering view; the user can further refine these featuresusing segmentation brushes. Interactivity is achieved in our system and different views …,Visualization Symposium (PacificVis); 2013 IEEE Pacific,2013,24
Personalizing the Web using site descriptions,Vinod Anupam; Yuri Breitbart; Juliana Freire; Bharat Kumar,The information overload on the Web has created a great need for efficient filteringmechanisms. Many sites (eg; CNN and Quicken) address this problem by allowing a user tocreate personalized pages that contain only information that is of interest to the user. Wepropose a new approach for personalization that improves on existing services in threesignificant ways: the user can create personalized pages with information from any site(without being restricted to sites that offer personalization); personalized pages may containinformation from multiple Web sites (eg; a user can create a personalized page that containsnot only news categories from her favorite news sources; but also information about theprices of all stocks whose names appear in the headlines of selected news; and weatherinformation for a particular city); and users have more privacy since they are not required …,Database and Expert Systems Applications; 1999. Proceedings. Tenth International Workshop on,1999,23
Using mediation to achieve provenance interoperability,Tommy Ellqvist; David Koop; Juliana Freire; Cláudio Silva; Lena Strömbäck,Provenance is essential in scienti¿ c experiments. It contains information that is key topreserving the data; to determining their quality and authorship; and to reproduce as well asvalidate the results. In complex experiments and analyses; where multiple tools are used toderive data products; provenance captured by these tools must be combined in order todetermine the complete lineage of the derived products. In this paper we describe amediator-based architecture for integrating provenance information from multiple sources.This architecture contains two key components: a global mediated schema that is generaland capable of representing provenance information represented in different model; anddescribe a new system-independent query API that is general and able to express complexqueries over provenance information from different sources. We also present a case study …,Services-I; 2009 World Conference on,2009,22
Managing XML data: An abridged overview,Juliana Freire; Michael Benedikt,XML's flexibility makes it a natural format for both exchanging and integrating data fromdiverse data sources. In this survey; the authors give an overview of issues in managingXML data; discuss existing solutions; and outline the current technology's open problemsand limitations.,Computing in science & engineering,2004,22
PruSM: a prudent schema matching approach for web forms,Thanh Hoang Nguyen; Hoa Nguyen; Juliana Freire,Abstract There has been a substantial increase in the number of Web data sources whosecontents are hidden and can only be accessed through form interfaces. To leverage thisdata; several applications have emerged that aim to automate and simplify the access tothese data sources; from hidden-Web crawlers and meta-searchers to Web informationintegration systems. A requirement shared by these applications is the ability to understandthese forms; so that they can automatically fill them out. In this paper; we address a keyproblem in form understanding: how to match elements across distinct forms. Although thisproblem has been studied in the literature; existing approaches have important limitations.Notably; they only handle small form collections and assume that form elements are cleanand normalized; often through manual pre-processing. When a large number of forms is …,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,21
Siphon++: a hidden-webcrawler for keyword-based interfaces,Karane Vieira; Luciano Barbosa; Juliana Freire; Altigran Silva,Abstract The hidden Web consists of data that is generally hidden behind form interfaces;and as such; it is out of reach for traditional search engines. With the goal of leveraging thehigh-quality information in this largely unexplored portion of the Web; in this paper; wepropose a new strategy for automatically retrieving data hidden behind keyword-based forminterfaces. Unlike previous approaches to this problem; our strategy adapts the querygeneration and selection by detecting features of the index. We describe an extensiveexperimental evaluation which shows that: our strategy is able to derive appropriate queriesto obtain high coverage while; at the same time; avoiding the retrieval of redundant data;and it obtains higher coverage and is more efficient approaches that use a fixed strategy forquery generation.,Proceedings of the 17th ACM conference on Information and knowledge management,2008,21
Scalable wavelength-selective crossconnect switch based on MEMS and planar waveguides,R Ryf; P Bernasconi; P Kolodner; J Kim; JP Hickey; D Carr; F Pardo; C Bolle; R Frahm; N Basavanhally; C Yoh; D Ramsey; R George; J Kraus; C Lichtenwalner; R Papazian; J Gates; HR Shea; A Gasparyan; V Muratov; JE Griffith; JA Prybyla; S Goyal; CD White; MT Lin; R Ruel; C Nijander; S Amey; DT Neilson; DJ Bishop; S Pau; C Nuzman; A Weis; B Kumar; D Lieuwen; V Aksyuk; DS Greywall; TC Lee; HT Soh; WM Mansfield; S Jin; WY Lai; HA Huggins; DL Barr; RA Cirelli; GR Bogart; K Teffeau; R Vella; H Mavoori; A Ramirez; NA Ciampa; FP Klemens; MD Morris; T Boone; JQ Liu; JM Rosamilia; CR Giies,A 72/spl times/72 wavelength-selective crossconnect switch; that is scalable to 1296/spltimes/1296 with current technology; is presented. Silica-on-silicon wavelength multiplexerswith integrated monitoring taps and a MEMS micromirror array were assembled in a hybrid3D beam steering crossconnect having 20 dB insertion loss; 100 GHz channel spacing and30 GHz passbands.,Optical Communication; 2001. ECOC'01. 27th European Conference on,2001,21
Creating and exploring web form repositories,Luciano Barbosa; Hoa Nguyen; Thanh Nguyen; Ramesh Pinnamaneni; Juliana Freire,Abstract We present DeepPeep (http://www. deeppeep. org); a new system for discovering;organizing and analyzing Web forms. DeepPeep allows users to explore the entry points tohidden-Web sites whose contents are out of reach for traditional search engines. Besidesdemonstrating important features of DeepPeep and describing the infrastructure we used tobuild the system; we will show how this infrastructure can be used to create form collectionsand form search engines for different domains. We also present the analysis component ofDeepPeep which allows users to explore and visualize information in form repositories;helping them not only to better search and understand forms in different domains; but also torefine the form gathering process.,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,19
On finding templates on web collections,Karane Vieira; André Luiz da Costa Carvalho; Klessius Berlt; Edleno S de Moura; Altigran S da Silva; Juliana Freire,Abstract Templates are pieces of HTML code common to a set of web pages usuallyadopted by content providers to enhance the uniformity of layout and navigation of theirsWeb sites. They are usually generated using authoring/publishing tools or by programs thatbuild HTML pages to publish content from a database. In spite of their usefulness; thecontent of templates can negatively affect the quality of results produced by systems thatautomatically process information available in web sites; such as search engines; clusteringand automatic categorization programs. Further; the information available in templates isredundant and thus processing and storing such information just once for a set of pages maysave computational resources. In this paper; we present and evaluate methods for detectingtemplates considering a scenario where multiple templates can be found in a collection of …,World Wide Web,2009,19
Managing provenance for an evolutionary workflow process in a collaborative environment,*,A method of and a device for supporting a collaborative workflow process that includes aplurality of workflows are provided. A first modified workflow process is received from a firstdevice at a second device. The first modified workflow process is created by modifying anevolutionary workflow process. The first modified workflow process is compared with theevolutionary workflow process to identify a first identifier associated with an action includedin the first modified workflow process and not included in the evolutionary workflow process.If the identified first identifier is included in the evolutionary workflow process is determined.If the identified first identifier is included in the evolutionary workflow process; a secondidentifier is defined. The defined second identifier is associated with the action. The secondaction is added with the associated second identifier to the evolutionary workflow process …,*,2008,19
Bridging the XML–Relational Divide with LegoDB: A Demonstration,Philip Bohannon; Juliana Freire; Jayant R Haritsa; Maya Ramanath; Prasan Roy; Jérôme Siméon,XML is becoming the predominant data exchange for- mat in a variety of application domains(supply-chain; sci- entific data processing; telecommunication infrastructure; etc.). By relyingon relational engines for storage purposes; XML developers can benefit from a complete setof data management services (including concurrency control; crash recovery; and scalability)and from the highly optimized re- lational query processors. However; due to the mismatch betweenthe XML and the relational models and the many different ways to map an XML document intorelations; it is very hard to tune a relational engine and ensure that XML queries will be evaluatedefficiently. In fact; many current products (eg; [5]) require developers to go through an often lengthyand complex process of manually defining a map- ping from XML into relations. In thisdemonstration; we present our LegoDB system; which is aimed at automati- cally …,IEEE International Conference on Data Engineering (ICDE),2003,19
Efficient Acquisition of Web Data through Restricted Query Interfaces.,Simon Byers; Juliana Freire; Cláudio T Silva,ABSTRACT A wealth of information is available on the Web. But often; such data are hiddenbehind form interfaces which allow only a restrictive set of queries over the underlyingdatabases; greatly hindering data exploration. The ability to materialize these databases hasendless applications; from allowing the data to be effectively mined to providing betterresponse times in Web information integration systems. However; reconstructing databaseimages through restricted interfaces can be a daunting task; and sometimes infeasible dueto network traffic and high latencies from Web servers. In this paper we introduce theproblem of generating efficient query covers; ie; given a restricted query interface; how toefficiently reconstruct a complete image of the underlying database. We propose a solutionto the problem of finding covers for spatial queries over databases accessible through …,WWW Posters,2001,19
Visual Summaries for Graph Collections,David Koop; Juliana Freire; Cláudio T Silva,Graphs can be used to represent a variety of information; from molecular structures tobiological pathways to computational workflows. With a growing volume of data representedas graphs; the problem of understanding and analyzing the variations in a collection ofgraphs is of increasing importance. We present an algorithm to compute a single summarygraph that efficiently encodes an entire collection of graphs by finding and merging similarnodes and edges. Instead of only merging nodes and edges that are exactly the same; weuse domain-specific comparison functions to collapse similar nodes and edges which allowsus to generate more compact representations of the collection. In addition; we havedeveloped methods that allow users to interactively control the display of these summarygraphs. These interactions include the ability to highlight individual graphs in the …,IEEE Pacific Vis 2013,2013,17
Looking at both the present and the past to efficiently update replicas of web content,Luciano Barbosa; Ana Carolina Salgado; Francisco De Carvalho; Jacques Robin; Juliana Freire,Abstract Since Web sites are autonomous and independently updated; applications thatkeep replicas of Web data; such as Web warehouses and search engines; must periodicallypoll the sites and check for changes. Since this is a resource-intensive task; in order to keepthe copies up-to-date; it is important to devise efficient update schedules that adapt to thechange rate of the pages and avoid visiting pages not modified since the last visit. In thispaper; we propose a new approach that learns to predict the change behavior of Web pagesbased both on the static features and change history of pages; and refreshes the copiesaccordingly. Experiments using real-world data show that our technique leads to substantialperformance improvements compared to previously proposed approaches.,Proceedings of the 7th annual ACM international workshop on Web information and data management,2005,17
Active database trigger processing using a trigger gateway,*,A database system includes a trigger gateway for implementing trigger functionality. Thetrigger gateway is located at a communication point between a user and a database system.The trigger gateway receives database commands destined for the database and processestriggers associated with the database commands. Where appropriate; the trigger gatewayforwards the database command to the database. A trigger action server; which is locatedremote from the trigger gateway; may execute trigger actions in response to triggerexecution requests sent from the trigger gateway. Alternatively; trigger actions may beexecuted within the trigger gateway. Trigger processing includes the processing of triggersafter failed database commands. Security features are implemented to prevent unauthorizeddatabase access through the use of triggers.,*,2003,17
Syntactica,Richard Larson; David S Warren; Juliana Freire; Konstantinos Sagonas,In Syntactica; a grammar consists of a set of context-free phrase structure rules and(typically) a lexicon. Phrase structure rules are created in a rule window. Lexicons arecreated in a lexicon window. Rules and lexicons are loaded into TreeViewer window wherethey are used to generate phrase-markers (or tree diagrams). The user enters a sentence (orother expression) and Syntactica tries to generate a phrase-marker for it using the grammarthat has been loaded. When more than one structure is available; Syntactica displays therange. Multiple rule and lexicon windows can be open at any one time; making it easy toload alternate grammars; and to test and compare their results. Phrase-markers can besaved for later viewing; printing or inclusion in homework assignments emailed to a centrallocation. Sentence and Tree windows allow you to conveniently collect sentences and …,*,1996,17
Data polygamy: the many-many relationships among urban spatio-temporal data sets,Fernando Chirigati; Harish Doraiswamy; Theodoros Damoulas; Juliana Freire,Abstract The increasing ability to collect data from urban environments; coupled with a pushtowards openness by governments; has resulted in the availability of numerous spatio-temporal data sets covering diverse aspects of a city. Discovering relationships betweenthese data sets can produce new insights by enabling domain experts to not only test butalso generate hypotheses. However; discovering these relationships is difficult. First; arelationship between two data sets may occur only at certain locations and/or time periods.Second; the sheer number and size of the data sets; coupled with the diverse spatial andtemporal scales at which the data is available; presents computational challenges on allfronts; from indexing and querying to analyzing them. Finally; it is non-trivial to differentiatebetween meaningful and spurious relationships. To address these challenges; we …,Proceedings of the 2016 International Conference on Management of Data,2016,16
Examining statistics of workflow evolution provenance: A first study,Lauro Lins; David Koop; Erik W Anderson; Steven P Callahan; Emanuele Santos; Carlos E Scheidegger; Juliana Freire; Cláudio T Silva,Abstract Provenance (also referred to as audit trail; lineage; and pedigree) capturesinformation about the steps used to generate a given data product. Such informationprovides documentation that is key to determining data quality and authorship; andnecessary for preserving; reproducing; sharing and publishing the data. Workflow design; inparticular for exploratory tasks (eg; creating a visualization; mining a data set); requires aninvolved; trial-and-error process. To solve a problem; a user has to iteratively refine aworkflow to experiment with different techniques and try different parameter values; as sheformulates and test hypotheses. The maintenance of detailed provenance (or history) of thisprocess has many benefits that go beyond documentation and result reproducibility.Notably; it supports several operations that facilitate exploration; including the ability to …,International Conference on Scientific and Statistical Database Management,2008,16
Towards integrating workflow and database provenance,Fernando Chirigati; Juliana Freire,Abstract While there has been substantial work on both database and workflow provenance;the two problems have only been examined in isolation. It is widely accepted that theexisting models are incompatible. Database provenance is fine-grained and captureschanges to tuples in a database. In contrast; workflow provenance is represented at acoarser level and reflects the functional model of workflow systems; which is stateless—eachcomputational step derives a new artifact. In this paper; we propose a new approach tocombine database and workflow provenance. We address the mismatch between thedifferent kinds of provenance by using a temporal model which explicitly represents thedatabase states as updates are applied. We discuss how; under this model; reproducibility isobtained for workflows that manipulate databases; and how different queries that straddle …,International Provenance and Annotation Workshop,2012,15
Provenance in web applications,Geetika T Lakshmanan; Francisco Curbera; Juliana Freire; Amit Sheth,*,*,2011,15
ReproZip: computational reproducibility with ease,Fernando Chirigati; Rémi Rampin; Dennis Shasha; Juliana Freire,Page 1. REPROZIP Using Provenance to Support Computational Reproducibility FernandoChirigati NYU-Poly Dennis Shasha NYU Juliana Freire NYU-Poly & NYU TaPP'13 Page 2.Reproducibility Good science requires reproducibility Computational experiments requirereproducibility A program P running on computational environment E at time T is said to bereproducible if it yields the same answer on environment E' at time T' > T “If I have seen further;it is by standing on the shoulders of giants.” Isaac Newton Page 3. Computational ReproducibilityFew computational experiments are reproducible Why? We need provenance How toencapsulate my experiment? What should be included? Too many dependencies… Too manyfiles to keep track… Sigh. Author Description of the data Specification of the experimentDescription of the environment Page 4. Computational Reproducibility …,Proceedings of the ACM SIGMOD,*,15
Customer relationship management system with network contact center server configured to control automated web and voice dialogues,*,A customer relationship management system is disclosed for matching a given work itemwith an agent. The system includes a database to hold customer information; agentinformation; and system information; and a network contact center server operative toreceive the given work item from the customer and route it to the agent based at least in parton the information in the database. The network contact center server is configured to controlthe conduction of an automated web dialogue with the customer via a separate web serverand is further configured to control the conduction of an automated voice dialogue with thecustomer via an interactive voice response system. The automated web dialogue and theautomated voice dialogue are thereby both conductable under the control of the networkcontact center server.,*,2011,14
Exploring the coming repositories of reproducible experiments: Challenges and opportunities,Juliana Freire; Philippe Bonnet; Dennis Shasha,ABSTRACT Computational reproducibility efforts in many communities will soon give rise tovalidated software and data repositories of high quality. A scientist in a field may want toquery the components of such repositories to build new software workflows; perhaps afteradding the scientist's own algorithms. This paper explores research challenges necessary toachieving this goal.,Proceedings of the VLDB Endowment,2011,14
The provenance of workflow upgrades,David Koop; Carlos E Scheidegger; Juliana Freire; Cláudio T Silva,Abstract Provenance has become an increasingly important part of documenting; verifying;and reproducing scientific research; but as users seek to extend or share results; it may beimpractical to start from the exact original steps due to system configuration differences;library updates; or new algorithms. Although there have been several approaches forcapturing workflow provenance; the problem of managing upgrades of the underlying toolsand libraries orchestrated by workflows has been largely overlooked. In this paper weconsider the problem of maintaining and re-using the provenance of workflow upgrades. Wepropose different kinds of upgrades that can be applied; including automatic mechanisms;developer-specified; and user-defined. We show how to capture provenance from suchupgrades and suggest how this provenance might be used to influence future upgrades …,International Provenance and Annotation Workshop,2010,14
Towards enabling social analysis of scientific data,Juliana Freire; Cláudio Silva,Abstract Computing has been an enormous accelerator to science and it has led to aninformation explosion in many different fields. Future advances in science depend on theability to comprehend these vast amounts of data. In this paper; we discuss challenges andopportunities for social data analysis in the scientific domain.,CHI Social Data Analysis Workshop,2008,14
Reproducibility of data-oriented experiments in e-Science (Dagstuhl Seminar 16041),Juliana Freire; Norbert Fuhr; Andreas Rauber,Abstract This report documents the program and the outcomes of Dagstuhl Seminar 16041"Reproducibility of Data-Oriented Experiments in e-Science". In many subfields of computerscience; experiments play an important role. Besides theoretic properties of algorithms ormethods; their effectiveness and performance often can only be validated viaexperimentation. In most of these cases; the experimental results depend on the input data;settings for input parameters; and potentially on characteristics of the computationalenvironment where the experiments were designed and run. Unfortunately; mostcomputational experiments are specified only informally in papers; where experimentalresults are briefly described in figure captions; the code that produced the results is seldomavailable. This has serious implications. Scientific discoveries do not happen in isolation …,Dagstuhl Reports,2016,13
A scalable approach for data-driven taxi ride-sharing simulation,Masayo Ota; Huy Vo; Claudio Silva; Juliana Freire,As urban population grows; cities face many challenges related to transportation; resourceconsumption; and the environment. Ride sharing has been proposed as an effectiveapproach to reduce traffic congestion; gasoline consumption; and pollution. Despite greatpromise; researchers and policy makers lack adequate tools to assess tradeoffs and benefitsof various ride-sharing strategies. Existing approaches either make unrealistic modelingassumptions or do not scale to the sizes of existing data sets. In this paper; we propose areal-time; data-driven simulation framework that supports the efficient analysis of taxi ridesharing. By modeling taxis and trips as distinct entities; our framework is able to simulate arich set of realistic scenarios. At the same time; by providing a comprehensive set ofparameters; we are able to study the taxi ride-sharing problem from different angles …,Big Data (Big Data); 2015 IEEE International Conference on,2015,13
Using provenance to streamline data exploration through visualization,Steven P Callahan; Juliana Freire; Emanuele Santos; Carlos E Scheidegger; Claudio T Silva; Huy T Vo,Abstract Scientists are faced with increasingly larger volumes of data to analyze. To analyzeand validate various hypotheses; they need to create insightful visual representations ofboth observed data and simulated processes. Often; insight comes from comparing multiplevisualizations. But data exploration through visualization requires scientists to assemblecomplex workflows—pipelines consisting of sequences of operations that transform the datainto appropriate visual representations—and today; this process contains many error-proneand time-consuming tasks. We show how a new action-based model for capturing andmaintaining detailed provenance of the visualization process can be used to streamline thedata exploration process and reduce the time to insight. This model enables the flexible re-use of workflows; a scalable mechanism for creating a large number of visualizations; and …,*,2006,13
Packing experiments for sharing and publication,Fernando Chirigati; Dennis Shasha; Juliana Freire,Abstract Reproducibility is a core component of the scientific process. Revisiting and reusingpast results allow science to move forward-" standing on the shoulders of giants"; as Newtononce said. An impediment to the adoption of computational reproducibility is that authors findit difficult to generate a compendium that encompasses all the required components tocorrectly reproduce their experiments. Even when a compendium is available; reviewersand readers may have difficulties in verifying the results on platforms different from the oneswhere the experiments were originally run. As a step towards simplifying the process ofcreating reproducible experiments; we have developed ReproZip; a tool that automaticallycaptures the provenance of experiments and packs all the necessary files; librarydependencies and variables to reproduce the results. Reviewers can then unpack and …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,12
Provenance and Annotation of Data and Processes: Second International Provenance and Annotation Workshop (IPAW) 2008,Juliana Freire; David Koop; Luc Moreau,This book constitutes the thoroughly refereed post-conference proceedings of the SecondInternational Provenance and Annotation Workshop; IPAW 2008; held in Salt Lake City; UT;USA; in June 2007. The 14 revised full papers and 15 revised short and demo paperspresented together with 2 keynote lectures were carefully reviewed and selected from 40submissions. The paper are organized in topical sections on provenance: models andquerying; provenance: visualization; failures; identity; provenance and workflows;provenance for streams and collaboration; and applications.,*,2008,12
Managing provenance of the evolutionary development of workflows,*,A method of and a system for presenting a plurality of workflows that describe anevolutionary workflow process associated with creating a result are provided. A first workflowis received at a first device. The first workflow comprises a first module which applies a firstinstruction to form a first result. A modification of the first workflow is received at the firstdevice. The received modification includes a second workflow which includes a secondmodule that applies a second instruction to form a second result. The evolutionary workflowprocess is presented to a user at a second device. The evolutionary workflow processincludes the first workflow and the second workflow and indicates a parent-child relationshipbetween the first workflow and the second workflow.,*,2008,12
Adaptive XML shredding: Architecture; implementation; and challenges,Juliana Freire; Jérôme Siméon,Abstract As XML data becomes central to business-critical applications; there is a growingneed for efficient and reliable XML storage. Two main approaches have been proposed forstoring XML data: native and colonial systems. Native systems (eg;[19];[20]) are designedfrom the ground up specifically for XML and XML query languages. Colonial systems(eg;[5];[7];[19]); on the other hand; attempt to reuse existing commercial database systems(DBMS) by mapping XML into the underlying model used by the DBMS. Colonial systemscan thus leverage features; such as concurrency control; crash recovery; scalability; andhighly optimized query processors available in the DMBS; making them an attractivealternative for managing XML data. However; several technical challenges need to beaddressed in terms of architecture; algorithms; and implementation of these systems. In …,*,2003,12
Practical problems in coupling deductive engines with relational databases,Juliana Freire,Abstract There has been a considerable demand for applications that extend the capabilitiesof databases; such as data warehouses; decision support systems and knowledgediscovery; to name a few. Unfortunately; adding new functionality by coupling separatecomponents with commercial relational databases is usually a non-trivial task. One of themain reasons is the fact that the application programming interfaces of commercial relationaldatabases do not provide adequate mechanisms that support efficient communicationbetween client applications and the database servers. In this paper we illustrate this problemby focusing on the more specific issue of coupling deductive database query engines andrelational databases.,In Proceedings of the 5th KRDB Workshop,1998,12
Visually exploring transportation schedules,Cesar Palomo; Zhan Guo; Cláudio T Silva; Juliana Freire,Public transportation schedules are designed by agencies to optimize service quality undermultiple constraints. However; real service usually deviates from the plan. Therefore;transportation analysts need to identify; compare and explain both eventual and systemicperformance issues that must be addressed so that better timetables can be created. Thepurely statistical tools commonly used by analysts pose many difficulties due to the largenumber of attributes at tripand station-level for planned and real service. Also challenging isthe need for models at multiple scales to search for patterns at different times and stations;since analysts do not know exactly where or when relevant patterns might emerge and needto compute statistical summaries for multiple attributes at different granularities. To aid in thisanalysis; we worked in close collaboration with a transportation expert to design TR-EX; a …,IEEE transactions on visualization and computer graphics,2016,11
Exploring Traffic Dynamics in Urban Environments Using Vector‐Valued Functions,Jorge Poco; Harish Doraiswamy; Huy Vo; João LD Comba; Juliana Freire; Cláudio Silva,Abstract The traffic infrastructure greatly impacts the quality of life in urban environments. Tooptimize this infrastructure; engineers and decision makers need to explore traffic data. Indoing so; they face two important challenges: the sparseness of speed sensors that coveronly a limited number of road segments; and the complexity of traffic patterns they need toanalyze. In this paper we take a first step at addressing these challenges. We use New YorkCity (NYC) taxi trips as sensors to capture traffic information. While taxis provide substantialcoverage of the city; the data captured about taxi trips contain neither the location of taxis atfrequent intervals nor their routes. We propose an efficient traffic model to derive speed anddirection information from these data; and show that it provides reliable estimates. Usingthese estimates; we define a time-varying vector-valued function on a directed graph …,Computer Graphics Forum,2015,11
Designing a provenance-based climate data analysis application,Emanuele Santos; David Koop; Thomas Maxwell; Charles Doutriaux; Tommy Ellqvist; Gerald Potter; Juliana Freire; Dean Williams; Cláudio T Silva,Abstract Climate scientists have made substantial progress in understanding Earth's climatesystem; particularly at global and continental scales. Climate research is now focused onunderstanding climate changes over wider ranges of time and space scales. These effortsare generating ultra-scale data sets at very high spatial resolution. An insightful analysis inclimate science depends on using software tools to discover; access; manipulate; andvisualize the data sets of interest. These data exploration tasks can be complex and time-consuming; and they frequently involve many resources from both the modeling andobservational climate communities. Because of the complexity of the explorations;provenance is critical; allowing scientists to ensure reproducibility; revisit existingcomputational pipelines; and more easily share analyses and results. In addition; as the …,International Provenance and Annotation Workshop,2012,11
Automatically constructing a directory of molecular biology databases,Luciano Barbosa; Sumit Tandon; Juliana Freire,Abstract There has been an explosion in the volume of biology-related information that isavailable in online databases. But finding the right information can be challenging. Not onlyis this information spread over multiple sources; but often; it is hidden behind form interfacesof online databases. There are several ongoing efforts that aim to simplify the process offinding; integrating and exploring these data. However; existing approaches are notscalable; and require substantial manual input. Notable examples include the NCBIdatabases and the NAR database compilation. As an important step towards a scalablesolution to this problem; we describe a new infrastructure that automates; to a large extent;the process of locating and organizing online databases. We show how this infrastructurecan be used to automate the construction and maintenance of a Molecular Biology …,International Conference on Data Integration in the Life Sciences,2007,11
IMAX: Incremental maintenance of schema-based XML statistics,Maya Ramanath; Lingzhi Zhang; Juliana Freire; Jayant R Haritsa,Current approaches for estimating the cardinality of XML queries are applicable to a staticscenario wherein the underlying XML data does not change subsequent to the collection ofstatistics on the repository. However; in practice; many XML-based applications are dynamicand involve frequent updates to the data. In this paper; we investigate efficient strategies forincrementally maintaining statistical summaries as and when updates are applied to thedata. Specifically; we propose algorithms that handle both the addition of new documents aswell as random insertions in the existing document trees. We also show; through a detailedperformance evaluation; that our incremental techniques are significantly faster than thenaive recomputation approach; and that estimation accuracy can be maintained even with afixed memory budget.,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,11
Predicting taxi demand at high spatial resolution: Approaching the limit of predictability,Kai Zhao; Denis Khryashchev; Juliana Freire; Cláudio Silva; Huy Vo,In big cities; taxi service is imbalanced. In some areas; passengers wait too long for a taxi;while in others; many taxis roam without passengers. Knowledge of where a taxi willbecome available can help us solve the taxi demand imbalance problem. In this paper; weemploy a holistic approach to predict taxi demand at high spatial resolution. We showcaseour techniques using two real-world data sets; yellow cabs and Uber trips in New York City;and perform an evaluation over 9;940 building blocks in Manhattan. Our approach consistsof two key steps. First; we use entropy and the temporal correlation of human mobility tomeasure the demand uncertainty at the building block level. Second; to identify whichpredictive algorithm can approach the theoretical maximum predictability; we implement andcompare three predictors: the Markov predictor (a probability-based predictive algorithm) …,Big Data (Big Data); 2016 IEEE International Conference on,2016,10
Collecting and analyzing provenance on interactive notebooks: when IPython meets noWorkflow,João Felipe Nicolaci Pimentel; Vanessa Braganholo; Leonardo Murta; Juliana Freire,Abstract Interactive notebooks help users explore code; run simulations; visualize results;and share them with other people. While these notebooks have been widely adopted inteaching as well as by scientists and data scientists that perform exploratory analyses; theirprovenance support is limited to the visualization of some intermediate results and codesharing. Once a user arrives at a result; it is hard; and sometimes impossible; to retrace thesteps that led to the result; since they do not collect the provenance for intermediate resuls orof the environment. As a result; users must fulfill this gap using external tools such asworkflow management systems. To overcome this limitation; we propose a new approach tocapture provenance from notebooks. We build upon noWorkflow; a system thatsystematically collects provenance for Python scripts. By integrating noWorkflow and …,Workshop on the Theory and Practice of Provenance (TaPP); Edinburgh; Scotland,2015,10
Using workflow medleys to streamline exploratory tasks,Emanuele Santos; David Koop; Huy T Vo; Erik W Anderson; Juliana Freire; Cláudio Silva,Abstract To analyze and understand the growing wealth of scientific data; complexworkflows need to be assembled; often requiring the combination of loosely-coupledresources; specialized libraries; distributed computing infrastructure; and Web services.However; constructing these workflows is a non-trivial task; especially for users who do nothave programming expertise. This problem is compounded for exploratory tasks; where theworkflows need to be iteratively refined. In this paper; we introduce workflow medleys; a newapproach for manipulating collections of workflows. We propose a workflow manipulationlanguage that includes operations that are common in exploratory tasks and present a visualinterface designed for this language. We briefly discuss how medleys have been applied intwo (real) applications.,International Conference on Scientific and Statistical Database Management,2009,10
Information sharing in science 2.0: Challenges and opportunities,Emanuele Santos; Juliana Freire; Cláudio Silva,ABSTRACT Scientists are beginning to utilize wikis; mashups; blogs and other Web 2.0technologies as a means to improve collaboration and information sharing. The termScience 2.0 has been used to refer to emerging scientific applications that make use ofthese Web 2.0 technologies. In this paper we discuss the benefits and opportunities thatcome from Science 2.0 as well as the challenges involved in building Science 2.0applications.,CHI Workshop on The Changing Face of Digital Science: New Practices in Scientific Collaborations,2009,10
Web services and information delivery for diverse environments,Juliana Freire; Bharat Kumar,Abstract There is a growing need for techniques that provide alternative means to accessWeb content and services; be it the ability to browse the Web through a voice interface likethe PhoneBrowser; or through a wireless PDA or smart phone. The Web was designed andworks well for desktop computers; to be viewed in large screens and through good networkconnections. However; using the Web through a phone or a small wireless device poses anumber of challenges. In this paper; we discuss the issues involved in making existing Webcontent and services available for diverse environments; and describe PersonalClipper; asystem that allows casual users to easily create customized (and simplified) views of Websites that are well-suited for different types of terminals. 1 Introduction The ability to takeinformation; entertainment and e-commerce on the go has a lot of promise. The wireless …,In Proceedings of the VLDB Workshop on Technologies for E-Services,2000,10
An urban data profiler,Daniel Castellani Ribeiro; Huy T Vo; Juliana Freire; Cláudio T Silva,Abstract Large volumes of urban data are being made available through a variety of openportals. Besides promoting transparency; these data can bring benefits to government;science; citizens and industry. It is no longer a fantasy to ask" if you could know anythingabout a city; what do you want to know" and to ponder what could be done with thatinformation. However; the great number and variety of datasets creates a new challenge:how to find relevant datasets. While existing portals provide search interfaces; these areoften limited to keyword searches over the limited metadata associated each dataset; forexample; attribute names and textual description. In this paper; we present a new tool;UrbanProfiler; that automatically extracts detailed information from datasets. This informationincludes attribute types; value distributions; and geographical information; which can be …,Proceedings of the 24th International Conference on World Wide Web,2015,9
Enabling reproducible science with VisTrails,David Koop; Juliana Freire; Cláudio T Silva,Abstract: With the increasing amount of data and use of computation in science; softwarehas become an important component in many different domains. Computing is now beingused more often and in more aspects of scientific work including data acquisition; simulation;analysis; and visualization. To ensure reproducibility; it is important to capture the differentcomputational processes used as well as their executions. VisTrails is an open-sourcescientific workflow system for data analysis and visualization that seeks to address theproblem of integrating varied tools as well as automatically documenting the methods andparameters employed. Growing from a specific project need to supporting a wide array ofusers required close collaborations in addition to new research ideas to design a usableand efficient system. The VisTrails project now includes standard software processes like …,arXiv preprint arXiv:1309.1784,2013,9
The XSB System Version 3.7. x Volume 1: Programmer’s Manual,Terrance Swift David S Warren; Konstantinos Sagonas; Juliana Freire; Prasad Rao; Baoqiu Cui; Ernie Johnson; Luis de Castro; Rui F Marques; Diptikalyan Saha; Steve Dawson; Michael Kifer,In Version 3.8; the core engine development of the SLG-WAM has been mainly implementedby Theresa Swift; David Warren; Kostis Sagonas; Prasad Rao; Juliana Freire; ErnieJohnson; Luis Castro and Rui Marques. The breakdown; very roughly; was that Theresa Swiftwrote the initial tabling engine; the SLG-WAM; and its built-ins; and leads the current developmentof the tabling subsystem. Prasad Rao reimplemented the engine's tabling subsystem to use triesfor variant-based table access and Ernie Johnson extended and refactored these routines ina number of ways; including adding call subsumption. Kostis Sagonas implemented most oftabled negation. Juliana Freire revised the table scheduling mechanism starting from Version1.5.0 to create the batched and local scheduling that is currently used. Baoqiu Cui revised thedata structures used to main- tain delay lists; and added attributed variables to the …,*,2013,9
Automated development of data processing results,*,A method of automatically completing a workflow is provided. An indicator of a partialworkflow is received in a computing device. The partial workflow includes a moduleconfigured to process data. A workflow completion is determined for the partial workflowbased on the partial workflow and a plurality of workflows stored in a computer-readablemedium. The workflow completion is configured to further process the data. A workflow ispresented in a display operably coupled to the computing device. The workflow includes thedetermined workflow completion and the partial workflow.,*,2011,9
Making LDAP active with the LTAP gateway: Case study in providing telecom integration and enhanced services,Robert Arlein; Juliana Freire; Narain Gehani; Daniel Lieuwen; Joann Ordille,Abstract LDAP (Lightweight Directory Access Protocol) directories are being rapidlydeployed on the Web. They are currently used to store data like white pages information;user profiles; and network device descriptions. These directories offer a number ofadvantages over current database technology in that they provide better support forheterogeneity and scalability. However; they lack some basic database functionality (eg;triggers; transactions) that is crucial for Directory Enabled Networking (DEN) tasks likeprovisioning network services; allocating resources; reporting; managing end-to-endsecurity; and offering mobile users customized features that follow them. In order to addressthese limitations while keeping the simplicity and performance features of LDAP directories;unbundled and portable solutions are needed.,Proc. Workshop on Databases in Telecommunication,1999,9
Product synthesis from multiple sources,*,Methods and systems for automatically synthesizing product information from multiple datasources into an on-line catalog are disclosed; and in particular; for automaticallysynthesizing the product information based on attribute-value pairs. Information for a productmay be obtained; via entity extraction; feed ingestion; and other mechanisms; from a pluralityof structured and unstructured data sources having different taxonomies and schemas.Product information may additionally or alternatively be obtained or derived based onpopularity data. The product information may be cleansed; segmented and normalized. Theproduct information may be clustered so closest products; attribute names and attributevalues are associated. A representative value for an attribute name may be determined; andthe on-line catalog may be updated so that entries are comprehensive; meaningful and …,*,2013,8
Querying Wikipedia documents and relationships,Huong Nguyen; Thanh Nguyen; Hoa Nguyen; Juliana Freire,Abstract Wikipedia has become an important source of information which is growing veryrapidly. However; the existing infrastructure for querying this information is limited and oftenignores the inherent structure in the information and links across documents. In this paper;we present a new approach for querying Wikipedia content that supports a simple; yetexpressive query interfaces that allow both keyword and structured queries. A uniquefeature of our approach is that; besides returning documents that match the queries; it alsoexploits relationships among documents to return richer; multi-document answers. Wemodel Wikipedia as a graph and cast the problem of finding answers for queries as graphsearch. To guide the answer-search process; we propose a novel weighting scheme toidentify important nodes and edges in the graph. By leveraging the structured information …,Procceedings of the 13th International Workshop on the Web and Databases,2010,8
Automatically extracting form labels,Hoa Nguyen; Eun Yong Kang; Juliana Freire,We describe a machine-learning-based approach for extracting attribute labels from Webform interfaces. Having these labels is a requirement for several techniques that attempt toretrieve and integrate data that reside in online databases and that are hidden behind forminterfaces; including schema matching and clustering; and hidden-Web crawlers. Whereasprevious approaches to this problem have relied on heuristics and manually specifiedextraction rules; our technique makes use of learning classifiers to identify form labels. Ourpreliminary experiments show this approach is promising and has high accuracy.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,8
A generic and flexible framework for mapping XML documents into relations,Sihem Amer-Yahia; Fang Du; Juliana Freire,*,VLDB’04: Proceedings of 30th International Conference on Very Large Data Bases,2004,8
MetaComm: A meta-directory for telecommunications,Juliana Freire; Daniel Lieuwen; Joann Ordille; Lalit Garg; Michael Holder; Hector Urroz; Gavin Michael; Julian Orbach; Luke Tucker; Qian Ye; Robert Arlein,A great deal of corporate data is buried in network devices-such as PBX messaging/emailplatforms; and data networking equipment-where it is difficult to access and modify.Typically; the data is only available to the device itself for its internal purposes and it must beadministered using either a proprietary interface or a standard protocol against a proprietaryschema. This leads to many problems; most notably: the need for data replication anddifficult interoperation with other devices and applications. MetaComm addresses theseproblems by providing a framework to integrate data from multiple devices into a meta-directory. The system allows user information to be modified through a directory using theLDAP protocol as well as directly through two legacy devices: a Definity (R) PBX and a voicemessaging system. In order to prevent data inconsistencies; updates to any system must …,Data Engineering; 2000. Proceedings. 16th International Conference on,2000,8
Should we all be teaching intro to data science instead of intro to databases?,Bill Howe; Michael J Franklin; Juliana Freire; James Frew; Tim Kraska; Raghu Ramakrishnan,Abstract The Database Community has a unique perspective on the challenges andsolutions of long-term management of data and the value of data as a resource. In currentcomputer science curricula; however; these insights are typically locked up in the context ofthe traditional Intro to Databases class that was developed years (or in some cases;decades) before the modern concept of Data Science arose and embedded in thediscussion of legacy data management systems. We consider how to bring these conceptsfront and center into the emerging wave of Data Science courses; degree programs andeven departments.,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,7
Fine-grained provenance collection over scripts through program slicing,João Felipe Pimentel; Juliana Freire; Leonardo Murta; Vanessa Braganholo,Abstract Collecting provenance from scripts is often useful for scientists to explain andreproduce their scientific experiments. However; most existing automatic approachescapture provenance at coarse-grain; for example; the trace of user-defined functions. Theseapproaches lack information of variable dependencies. Without this information; users maystruggle to identify which functions really influenced the results; leading to the creation offalse-positive provenance links. To address this problem; we propose an approach that usesdynamic program slicing for gathering provenance of Python scripts. By capturingdependencies among variables; it is possible to expose execution paths inside functionsand; consequently; to create a provenance graph that accurately represents the functionactivations and the results they affect.,International Provenance and Annotation Workshop,2016,6
A GPU-Based Index to Support Interactive Spatio-Temporal Queries over Historical Data,Harish Doraiswamy; Huy T. Vo; Claudio Silva; Juliana Freire,There are increasing volumes of spatio-temporal data from various sources such as sensors;social networks and urban environments. Analysis of such data requires flexible explorationand visualizations; but queries that span multiple geographical regions over multiple timeslices are expensive to compute; making it challenging to attain interactive speeds for largedata sets. In this paper; we propose a new indexing scheme that makes use of modernGPUs to efficiently support spatio-temporal queries over point data. The index coversmultiple dimensions; thus allowing simultaneous filtering of spatial and temporal attributes. Ituses a block-based storage structure to speed up OLAP-type queries over historical data;and supports query processing over in-memory and disk-resident data. We present differentquery execution algorithms that we designed to allow the index to be used in different …,Proceedings of IEEE International Conference on Data Engineering (ICDE),2016,6
A model project for reproducible papers: critical temperature for the Ising model on a square lattice,Michele Dolfi; Jan Gukelberger; Andreas Hehn; J Imriška; K Pakrouski; TF Rønnow; Matthias Troyer; I Zintchenko; F Chirigati; Juliana Freire; D Shasha,Abstract: In this paper we present a simple; yet typical simulation in statistical physics;consisting of large scale Monte Carlo simulations followed by an involved statistical analysisof the results. The purpose is to provide an example publication to explore tools for writingreproducible papers. The simulation estimates the critical temperature where the Isingmodel on the square lattice becomes magnetic to be Tc/J= 2.26934 (6) using a finite sizescaling analysis of the crossing points of Binder cumulants. We provide a virtual machinewhich can be used to reproduce all figures and results.,arXiv preprint arXiv:1401.2000,2014,6
A Computational Reproducibility Benchmark.,Fernando Seabra Chirigati; Matthias Troyer; Dennis E Shasha; Juliana Freire,Abstract Creating and testing reproducible computational experiments is hard. Researchersmust derive a compendium that encapsulates all the components needed to reproduce aresult. Reviewers must unpack the encapsulated components; run them in an environmentthat could be different from the source environment; and verify the results. Although manytools support some aspect of reproducibility; there is no common benchmark against whichsingle or multiple tools can be tested. This paper describes a benchmark that can be used tocategorize and better understand existing systems. The benchmark will also serve as thebasis for a competition whereby tool builders will demonstrate if and how their systemssupport end-to-end reproducibility.,IEEE Data Eng. Bull.,2013,6
XML management for bioinformatics applications,Lena Stromback; Juliana Freire,Scientific exploration has become a data-intensive process; and increasing amounts of dataneed to be stored; analyzed; and shared. XML can help address these needs. As concreteexamples from systems biology show; native XML storage can be combined with traditionalrelational databases to offer an effective; usable solution for storing scientific data.,Computing in Science & Engineering,2011,6
A first study on strategies for generating workflow snippets,Tommy Ellkvist; Lena Strömbäck; Lauro Didier Lins; Juliana Freire,Abstract Workflows are increasingly being used to specify computational tasks; fromsimulations and data analysis to the creation of Web mashups. Recently; a number of publicworkflow repositories have become available; for example; myExperiment for scientificworkflows; and Yahoo! Pipes. Workflow collections are also commonplace in many scientificprojects. Having such collections opens up new opportunities for knowledge sharing and re-use. But for this to become a reality; mechanisms are needed that help users explore thesecollections and locate useful workflows. Although there has been work on queryingworkflows; not much attention has been given to presenting query results. In this paper; wetake a first look at the requirements for workflow snippets and study alternative techniquesfor deriving concise; yet informative snippets.,Proceedings of the First International Workshop on Keyword Search on Structured Data,2009,6
Software infrastructure for exploratory visualization and data analysis: past; present; and future,Cláudio T Silva; Juliana Freire,Abstract Future advances in science depend on our ability to comprehend the vast amountsof data being produced and acquired; and scientific visualization is a key enablingtechnology in this endeavor. We posit that visualization should be better integrated with thedata exploration process instead of being done after the fact-when all the science is done-simply to generate presentations of the findings. An important barrier to a wider adoption ofvisualization is complexity: the design of effective visualizations is a complex; multistageprocess that requires deep understanding of existing techniques; and how they relate tohuman cognition. We envision visualization software tools evolving into'scientific discovery'environments that support the creative tasks in the discovery pipeline; from data acquisitionand simulation to hypothesis testing and evaluation; and that enable the publication of …,Journal of Physics: Conference Series,2008,6
Biological Resource Discovery,Zoe Lacroix; Cartik R. Kothari; Peter Mork; Rami Rifaieh; Mark D. Wilkinson; Juliana Freire; Sarah Cohen-Boulakia,*,Encyclopedia of Database Systems,2017,5
Interactive exploration for domain discovery on the web,Yamuna Krishnamurthy; Kien Pham; Aécio Santos; Juliana Freire,ABSTRACT As the volume of information on the Web grows; it has become increasinglydifficult to find web pages that are relevant to a specific domain or topic. In this paper; weexplore the general question of how to assist users in the domain discovery process.Domain discovery entails the translation of a user's information needs and conceptual viewof a domain into a computational model that enables the identification and retrieval ofrelevant content from the Web. We discuss the challenges and propose an initial approachbased on exploratory data analysis that combines techniques from information retrieval;machine learning and data mining to streamline domain discovery. We implemented theapproach in an open-source tool and present the results of a preliminary evaluation.,Proc. of KDD IDEA,2016,5
Exploring What not to Clean in Urban Data: A Study Using New York City Taxi Trips,Juliana Freire; Aline Bessa; Fernando Chirigati; Huy Vo; Kai Zhao,*,IEEE Data Engineering Bulletin,2016,5
Visualizing the evolution of module workflows,Marcel Hlawatsch; Michael Burch; Fabian Beck; Juliana Freire; Claudio Silva; Daniel Weiskopf,Module workflows are used to generate custom applications with modular softwareframeworks. They describe data flow between the modular components and their executionunder certain parameter configurations. In many cases; module workflows are modeled in agraphical way by the user. To come up with the final result or to explore multiple solutions;they often undergo many iterations of adaptation. Furthermore; existing workflows may bereused for new applications. We visualize the evolution of module workflows with a focus-and-context approach and visualization techniques for time-dependent data. Our approachprovides insight into user behavior and the characteristics of the underlying systems. As ourexamples show; this can help identify usability issues and indicate options to improve theeffectiveness of the system. We demonstrate our approach for module workflows in Vis …,Information Visualisation (iV); 2015 19th International Conference on,2015,5
Indexing web form constraints,Ronaldo dos Santos Mello; Ramesh Pinnamaneni; Juliana Freire,Abstract Millions of online databases are available today on the Web that cover manydiferent domains. These databases are accessible through forms and provide several usefulservices from searching for rental cars and airfares to used cars and genes. To leverage thisinformation and locate online databases that are relevant for particular information needs;we have created a search engine that is specialized in forms that serve as the entry points tothese databases. This search engine; however; only provides a keyword-based interfacethat greatly limits the kinds of queries that can be posed. In this paper; we study the problemof supporting structured queries over Web form collections. We formalize the problem ofquerying Web forms as satisfying constraints that hold between form attributes and theirvalues; form metadata; as well as dependencies across distinct attributes. We also …,Journal of Information and Data Management,2010,5
Provenance-enabled data exploration and visualization with vistrails,Juliana Freire; Emanuele Santos; Erik Anderson,Scientists are now faced with an incredible volume of data to analyze. To explore andunderstand the data; they need to assemble complex workflows (pipelines) to manipulatethe data and create insightful visual representations. Provenance is essential in this process.The provenance of a digital artifact contains information about the process and data used toderive the artifact. This information is essential to preserve the data; to determining the data'squality and authorship; aswell as for reproducing and validating results--all importantelements of the scientific process. In this survey; we aim to inform computational andvisualization scientists; users and developers about different approaches to provenancemanagement. Using the VisTrails system as a basis and real application scenarios; we willcover different approaches to acquiring and reusing provenance; including techniques …,Graphics; Patterns and Images Tutorials (SIBGRAPI-T); 2010 23rd SIBGRAPI Conference on,2010,5
Indexing relations on the web,Sergio Luis Sardi Mergen; Juliana Freire; Carlos Alberto Heuser,Abstract There has been a substantial increase in the volume of (semi) structured data onthe Web. This opens new opportunities for exploring and querying these data that goesbeyond the keyword-based queries traditionally used on the Web. But supporting queriesover a very large number of apparently disconnected Web sources is challenging. In thispaper we propose index methods that capture both the structure of the sources andconnections between them. The indexes are designed for data that is represented asrelations; such as HTML tables; and support queries with predicates. We show howassociations between overlapping sources are discovered; captured in the indexes; andused to derive query rewritings that join multiple sources. We demonstrate; through anexperimental evaluation; that our approach scales to a large number of sources.,Proceedings of the 13th International Conference on Extending Database Technology,2010,5
Governance of the open provenance model,Luc Moreau; Juliana Freire; Joe Futrelle; Jim Myers; Patrick Paulson,Introduction The Open Provenance Model (OPM) was originally crafted by the five authors ina meeting held in Salt Lake City in August 2007. OPM v1. 00 [1] was released to thecommunity in December 2007. The first OPM workshop in June 2008 involved some twentyparticipants discussing issues related this specification; and led to a revised specification;referred to as OPM v1. 01 [2]. From the outset; the original authors' intent has been to definea data model that is open from an inter-operability viewpoint but also with respect to thecommunity of its contributors; reviewers and users. The early public release of v1. 00; thefirst OPM workshop and the revised specification v1. 01 are testimony of the communityfocus that is intended for OPM. So far; our approach to discuss changes and agree onrevisions has been adhoc. The purpose of this document is to outline a governance …,URL http://twiki. ipaw. info/pub/OPM/WebHome/governance. pdf,2009,5
Provenance and Annotation of Data and Processes,Juliana Freire; David Koop; Luc Moreau,*,*,2008,5
Towards process provenance for existing applications,Steven P Callahan; Juliana Freire; Carlos E Scheidegger; Cláudio T Silva; Huy T Vo; V INC,*,Proceedings of the 2nd International Provenance and Annotation Workshop,2008,5
VisTrails: Using provenance to streamline data exploration,Erik Anderson; S Callahan; D Koop; Emanuele Santos; C Scheidegger; H Vo; Juliana Freire; C Silva,*,Poster Proceedings of the International Workshop on Data Integration in the Life Sciences (DILS) 2007,2007,5
The xsb system version 3.0 volume 1: Programmer’s manual,Konstantinos Sagonas; Terrance Swift; David S Warren; Juliana Freire; Prasad Rao; Baoqiu Cui; Ernie Johnson; Luis de Castro; Rui F Marques; Steve Dawson; Michael Kifer,In Version 3.1; the core engine development of the SLG-WAM has been mainly imple- mentedby Terrance Swift; Kostis Sagonas; Prasad Rao; Juliana Freire; Ernie Johnson; Luis Castro andRui Marques. The breakdown; very roughly; was that Terrance Swift wrote the initial tablingengine; the SLG-WAM; and built-ins. Prasad Rao reimple- mented the engine's tabling subsystemto use tries for variant-based table access and Ernie Johnson extended and refactored theseroutines in a number of ways; includ- ing adding call subsumption. Kostis Sagonas implementedmost of tabled negation. Juliana Freire revised the table scheduling mechanism starting fromVersion 1.5.0 to create the batched and local scheduling that is currently used. Baoqiu Cui revisedthe data structures used to maintain delay lists; and added attributed variables to the engine.Finally; Luis Castro rewrote the emulator to use jump tables and wrote a heap- garbage …,Technical report; XSB consortium,2006,5
Visualization in radiation oncology: Towards replacing the laboratory notebook,Erik W Anderson; Steven P Callahan; George TY Chen; Juliana Freire; Emanuele Santos; Carlos E Scheidegger; Cláudio T Silva; Huy T Vo,Abstract: Data exploration in radiation oncology requires the creation of a large number ofvisualizations. For treatment planning; detailed information about the processes used tomanipulate data collected and to create visualizations is needed for assessing the quality ofthe results. Current visualization systems allow the interactive creation and manipulation ofcomplex visualizations. However; they lack the ability to manage the data involved in thevisualization process; and in particular; they lack mechanisms to capture the provenance ofboth the visualization process and associated data. Consequently; they do not provideadequate support for the creation and exploration of a large number of visualizations.VisTrails is a visualization management system that manages both the process and meta-data associated with visualizations. A novel feature of VisTrails is an actionbased …,*,2006,5
Visualizing uncertainty with uncertainty multiples,Robert B Gilbert; Fulvio Tonon; Juliana Freire; Claudio T Silva; David R Maidment,It is difficult to depict uncertainty graphically. Conventional means; which rely on mappinguncertainty to graphical attributes in an image; have limited effectiveness because theyrequire presenting additional variables and are not generally intuitive. The concept ofuncertainty multiples is proposed herein to address these challenges. Uncertainty multiplesare small-scale images positioned within the eyespan on a single page or screen showingthe range of possible interpretations of the information. This concept is illustrated and itsimplementation in practice; including the role of software technology; is discussed.,*,2006,5
Using wrappers for device independent web access: Opportunities; challenges and limitations,Juliana Freire,Abstract The availability of technologies that enable mobile access to data has brought greatexpectations that users would be able to access information; entertainment and e-commerceany time; anywhere. However; the existing Web infrastructure and content were designed fordesktop computers and are not well-suited for other types of accesses; eg; devices that haveless processing power and memory; small screens; and limited input facilities; or throughwireless data networks with low bandwidth and high latency. Thus; there is a growing needfor techniques that provide alternative means to access Web content and services; be it theability to browse the Web through a wireless PDA or smart phone; or hands-free accessthrough voice interfaces. In this paper; we discuss issues involved in providing ubiquitousaccess to Web data. We present techniques and systems for building wrappers that …,Extended Abstract Presented at Portland State University; Portland Oregon,2003,5
Combining scheduling strategies in tabled evaluation,Juliana Freire; David S Warren,Tabled evaluations ensure termination for Datalog programs by distinguishing calls to tabledsubgoals. Given several variant subgoals in an evaluation; only the rst (the generator) willuse program clause resolution; the rest (consumers) must perform answer resolution usinganswers computed by the original invocation. This use of answer resolution prevents thepossibility of in nite looping for Datalog programs; which sometimes occurs in SLD. Asvariant subgoals can be called at di erent stages of the evaluation; there is an intrinsicasynchronism between the generation and consumption of answers in SLG. Given thisasynchrony; implementations of tabled logic programs face an important scheduling choicenot present in traditional top-down evaluation: When to return answers to consumersubgoals. We have experimented with di erent orders of scheduling the return of answers …,Workshop on Parallelism and Implementation Technology for Logic Programming,1997,5
Anonymizing NYC Taxi Data: Does It Matter?,Marie Douriez; Harish Doraiswamy; Juliana Freire; Cláudio T Silva,The widespread use of location-based services has led to an increasing availability oftrajectory data from urban environments. These data carry rich information that are useful forimproving cities through traffic management and city planning. Yet; it also containsinformation about individuals which can jeopardize their privacy. In this study; we work withthe New York City (NYC) taxi trips data set publicly released by the Taxi and LimousineCommission (TLC). This data set contains information about every taxi cab ride thathappened in NYC. A bad hashing of the medallion numbers (the ID corresponding to a taxi)allowed the recovery of all the medallion numbers and led to a privacy breach for the drivers;whose income could be easily extracted. In this work; we initiate a study to evaluatewhether" perfect" anonymity is possible and if such an identity disclosure can be avoided …,Data Science and Advanced Analytics (DSAA); 2016 IEEE International Conference on,2016,4
Understanding website behavior based on user agent,Kien Pham; Aécio Santos; Juliana Freire,Abstract Web sites have adopted a variety of adversarial techniques to prevent web crawlersfrom retrieving their content. While it is possible to simulate users behavior using a browserto crawl such sites; this approach is not scalable. Therefore; understanding existingadversarial techniques is important to design crawling strategies that can adapt to retrievethe content as efficiently as possible. Ideally; a web crawler should detect the nature of theadversarial policies and select the most cost-effective means to defeat them. In this paper;we discuss the results of a large-scale study of web site behavior based on their responsesto different user-agents. We issued over 9 million HTTP GET requests to 1.3 million uniqueweb sites from DMOZ using six different user-agents and the TOR network as an anonymousproxy. We observed that web sites do change their responses depending on user-agents …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,4
Tracking and analyzing the evolution of provenance from scripts,João Felipe Pimentel; Juliana Freire; Vanessa Braganholo; Leonardo Murta,Abstract Script languages are powerful tools for scientists. Scientists use them to processdata; invoke programs; and link program outputs/inputs. During the life cycle of scientificexperiments; scientists compose scripts; execute them; and perform analysis on the results.Depending on the results; they modify their script to get more data to confirm the originalhypothesis or to test a new hypothesis; evolving the experiment. While some tools captureprovenance from the execution of scripts; most approaches focus on a single execution;leaving out the possibility to analyze the provenance evolution of the experiment as a whole.This work enables tracking and analyzing the provenance evolution gathered from scripts.Tracking the provenance evolution also helps to reconstruct the environment of previousexecutions for reproduction. Provenance evolution analysis allows comparison of …,International Provenance and Annotation Workshop,2016,4
A first study on temporal dynamics of topics on the web,Aécio Santos; Bruno Pasini; Juliana Freire,Abstract While much work has been devoted to understanding Web dynamics and using thisknowledge to efficiently maintain the freshness of the indexes of generic search engines; thesame is not true for domain-specific indexes constructed by focused crawlers. For the latter;the problem is compounded by the fact that it is important not only to maintain already-crawled pages fresh; but also to identify new relevant content and expand the collection. Inthis paper; we discuss the challenges involved in this problem and describe our preliminaryefforts in building a testbed to better understand the dynamics of specific topics andcharacterize how they evolve over time. We propose a data collection methodology and aset of experiments to answer important questions about temporal dynamics and evolution oftopics. We also present the results of the experimental analysis we carried out using data …,Proceedings of the 25th International Conference Companion on World Wide Web,2016,4
Towards understanding real-estate ownership in New York City: Opportunities and challenges,Tuan-Anh Hoang-Vu; Vicki Been; Ingrid Gould Ellen; Max Weselcouch; Juliana Freire,Abstract Understanding who is investing in real estate; and the patterns of their investments;is critical both for assessing the need for; and the effects of; policy interventions bygovernments; lenders; and non-profit community development organizations. If we knewmore about patterns of ownership; for example; we could target buildings that seem to"produce" disproportionate numbers of homeless families seeking housing in the City'sshelter system. Many other policies could also be made more effective; including policiesrelated the city's property tax assessment; water and sewer lien practices; outreach to smallproperty owners for energy upgrades; enforcement of rent regulation rules and policies toencourage investment through tax subsidies and zoning changes. Not only is understandingthese patterns critical for deciding how to target interventions; but they may have …,Proceedings of the International Workshop on Data Science for Macro-Modeling,2014,4
VisTrails provenance traces for benchmarking,Fernando Chirigati; Juliana Freire; David Koop; Cláudio Silva,Abstract The benchmark provenance traces that we have collected come from the VisTrailssystem for exploratory data analysis and visualization and from the VisTrails; Inc.provenance plugin for Autodesk Maya [1]. They contain diþerent kinds of provenanceinformation; including prospective and retrospective provenance as well as provenance ofthe evolution of work ows and models [4]. Our traces are stored using a change-basedrepresentation to compactly save all directions a user explored when developing a result.,Proceedings of the Joint EDBT/ICDT 2013 Workshops,2013,4
Querying structured information sources on the Web,Sergio Mergen; Juliana Freire; Carlos A Heuser,To provide access to heterogeneous data distributed over the Web; we propose a solutionthat merges the expressiveness of information integration systems with the flexibility found indataspace-aware search engines. Our approach requires neither a mediated schema norsource mappings. In the absence of a mediated schema; the user formulates structuredqueries based on what she expects to find. We demonstrate the feasibility of this approachby providing a query interface for integrating hundreds of (real) structured Web informationsources. We also discuss experimental results which indicate that our query rewritingalgorithm is both effective and scalable.,International Journal of Metadata; Semantics and Ontologies,2010,4
Making LDAP Active with the LTAP Gateway,Robert Arlein; Juliana Freire; Narain Gehani; Daniel Lieuwen; Joann Ordille,Abstract LDAP (Lightweight Directory Access Protocol) directories are being rapidlydeployed on the Web. They are currently used to store data like white pages information;user profiles; and network device descriptions. These directories offer a number ofadvantages over current database technology in that they provide better support forheterogeneity and scalability. However; they lack some basic database functionality (eg;triggers; transactions) that is crucial for Directory Enabled Networking (DEN) tasks likeprovisioning network services; allocating resources; reporting; managing end-to-endsecurity; and offering mobile users customized features that follow them. In order to addressthese limitations while keeping the simplicity and performance features of LDAP directories;unbundled and portable solutions are needed. In this paper we discuss LDAP limitations …,International Workshop on Databases in Telecommunications,1999,4
Scheduling Strategies for Evaluation of Recursive Queries over Memory and Disk-Resident Data,J Freire,*,*,1997,4
STaRS: Simulating taxi ride sharing at scale,Masayo Ota; Huy Vo; Cláudio Silva; Juliana Freire,As urban populations grow; cities face many challenges related to transportation; resourceconsumption; and the environment. Ride sharing has been proposed as an effectiveapproach to reduce traffic congestion; gasoline consumption; and pollution. However;despite great promise; researchers and policy makers lack adequate tools to assess thetradeoffs and benefits of various ride-sharing strategies. In this paper; we propose a real-time; data-driven simulation framework that supports the efficient analysis of taxi ridesharing. By modeling taxis and trips as distinct entities; our framework is able to simulate arich set of realistic scenarios. At the same time; by providing a comprehensive set ofparameters; we are able to study the taxi ride-sharing problem from different angles;considering different stakeholders' interests and constraints. To address the …,IEEE Transactions on Big Data,2017,3
A collaborative approach to computational reproducibility,Fernando Chirigati; Rebecca Capone; Dennis Shasha; Remi Rampin; Juliana Freire,Abstract: Although a standard in natural science; reproducibility has been only episodicallyapplied in experimental computer science. Scientific papers often present a large number oftables; plots and pictures that summarize the obtained results; but then loosely describe thesteps taken to derive them. Not only can the methods and the implementation be complex;but also their configuration may require setting many parameters and/or depend onparticular system configurations. While many researchers recognize the importance ofreproducibility; the challenge of making it happen often outweigh the benefits. Fortunately; aplethora of reproducibility solutions have been recently designed and implemented by thecommunity. In particular; packaging tools (eg; ReproZip) and virtualization tools (eg; Docker)are promising solutions towards facilitating reproducibility for both authors and reviewers …,arXiv preprint arXiv:1709.01154,2017,3
Prov viewer: A graph-based visualization tool for interactive exploration of provenance data,Troy Kohwalter; Thiago Oliveira; Juliana Freire; Esteban Clua; Leonardo Murta,Abstract The analysis of provenance data for an experiment is often crucial to understandthe achieved results. For long-running experiments or when provenance is captured at a lowgranularity; this analysis process can be overwhelming to the user due to the large volumeof provenance data. In this paper we introduce; Prov Viewer; a provenance visualization toolthat enables users to interactively explore provenance data. Among the visualization andexploratory features; we can cite zooming; filtering; and coloring. Moreover; we use of otherproperties such as shape and size to distinguish visual elements. These exploratory featuresare linked to the provenance semantics to ease the comprehension process. We alsointroduce collapsing and filtering strategies; allowing different levels of granularityexploration and analysis. We describe case studies that show how Prov Viewer has been …,International Provenance and Annotation Workshop,2016,3
Grammar and method for integrating XML data from multiple sources,*,A grammar for mapping a first grouping of XML data into a second grouping of XML data anda method for accomplishing same to incorporate the first grouping into the second grouping.The grammar includes a first rule for computing a first child element attribute and a secondrule for computing a second parent element attribute. The first rule and second rule varyaccording to a production of an element type of the first grouping. The element types includePCDATA; disjunctive; conjunctive and Kleene star; each having a unique rule set fordefining inherited and synthesized attributes of the parent and child elements. The methodincludes the step of executing a mapping of a first grouping having at least one parentelement and a set of corresponding child elements into a second grouping in accordancewith the grammar rules based on the production of the element type.,*,2015,3
Bridging Vocabularies to Link Tweets and News,Tuan-Ahn Hoang-Vu; Aline Bessa; Luciano Barbosa; Juliana Freire,ABSTRACT Social media has become a popular platform for publishing; sharing andconsuming news. However; it is not a replacement for traditional sources of news—they arecomplementary. While news sites provide in-depth and comprehensive coverage of eventsand topics; social media postings include comments; opinions and rumors about factspublicized on the news. Social media can thus serve as a useful sensor for how popular astory (or topic) is; for how long; and people's sentiments about it. To use social media as asensor; we first need to associate postings to news stories and topics. But doing so ischallenging since postings are short and the vocabularies used in postings and in news canbe very different. In this paper; we take a first step towards addressing this problem. Wepropose a framework that uses news as a proxy to build a topic model and associates …,Proceedings of International Workshop on the Web and Databases (WebDB),2014,3
Riding from Urban Data to Insight Using New York City Taxis,Juliana Freire; Cláudio Silva; Huy Vo; Harish Doraiswamy; Nivan Ferreira; Jorge Poco,Abstract About half of humanity lives in urban environments today and that number will growto 80% by the middle of this century. Cities are thus the loci of resource consumption; ofeconomic activity; and of innovation. Given our increasing ability to collect; transmit; store;and analyze data; there is a great opportunity to better understand cities; and enable them todeliver services efficiently and sustainably while keeping their citizens safe; healthy;prosperous; and well-informed. But making sense of all the data available is hard. Currently;urban data exploration is often limited to confirmatory analyses consisting of batch-orientedqueries and the exploration of well-defined questions over specific regions. The lack ofinteractivity makes this process both time-consuming and cumbersome. This problem iscompounded in the presence of big; multivariate spatio-temporal data; which is …,IEEE Data Engineering Bulletin,2014,3
Enabling provenance management for pre-existing applications,*,A method of providing provenance management for a pre-existing application is provided. Aprovenance data selection is received. The provenance data selection indicates provenancedata to present to a user. The provenance data is presented to the user as a version treecomprising a plurality of connected nodes. A node selection is received. The node selectionindicates a node selected from the version tree. One or more nodes from a root node of theplurality of connected nodes to the node selected from the version tree are identified. One ormore action parameters associated with the identified one or more nodes are identified. Anaction parameter of the one or more action parameters is associated with a previousinteraction with a pre-existing application. Presentation of a state of the pre-existingapplication associated with the node selected from the version tree is triggered.,*,2012,3
Using latent-structure to detect objects on the web,Luciano Barbosa; Juliana Freire,Abstract An important requirement for emerging applications which aim to locate andintegrate content distributed over the Web is to identify pages that are relevant for a givendomain or task. In this paper; we address the problem of identifying pages that containobjects with a latent structure; ie; the structure is implicitly represented in the page. Wepropose an algorithm which; given a set of instances of an object type; derives rules byautomatically extracting statistically significant patterns present inside the objects. Theserules can then be used to detect the presence of these objects in new; unseen pages. Ourapproach has several advantages when compared against learning-based text classifiers.Because it relies only on positive examples; constructing accurate object detectors is simplerthan constructing learning classifiers; which require both positive and negative examples …,Procceedings of the 13th International Workshop on the Web and Databases,2010,3
Prudent schema matching for web forms,Thanh Nguyen; Hoa Nguyen; Juliana Freire,ABSTRACT There is an increasing number of data sources on the Web whose contents arehidden and can only be accessed through form interfaces. Several applications haveemerged that aim to automate and simplify the access to such content; from hidden-Webcrawlers and meta-searchers to Web information integration systems. Since for any givendomain there many different sources; an important requirement for these applications is theability to automatically understand the form interfaces and determine correspondencesbetween elements from different forms. While the problem of form schema matching hasreceived substantial attention recently; existing approaches have important limitations.Notably; they assume that element labels can be reliably extracted from the forms andnormalized—most adopt manually extracted data for experiments; and their effectiveness …,*,2008,3
The exception that improves the rule,Juliana Freire; Boris Glavic; Oliver Kennedy; Heiko Mueller,Abstract The database community has developed numerous tools and techniques for datacuration and exploration; from declarative languages; to specialized techniques for datarepair; and more. Yet; there is currently no consensus on how to best expose these powerfultools to an analyst in a simple; intuitive; and above all; flexible way. Thus; analysts continueto rely on tools such as spreadsheets; imperative languages; and notebook styleprogramming environments like Jupyter for data curation. In this work; we explore theintegration of spreadsheets; notebooks; and relational databases. We focus on a keyadvantage that both spreadsheets and imperative notebook environments have overclassical relational databases: ease of exception. By relying on set-at-a-time operations;relational databases sacrifice the ability to easily define singleton operations; exceptions …,Proceedings of the Workshop on Human-In-the-Loop Data Analytics,2016,2
Virtual lightweight snapshots for consistent analytics in NoSQL stores,Fernando Chirigati; Jérôme Siméon; Martin Hirzel; Juliana Freire,Increasingly; applications that deal with big data need to run analytics concurrently withupdates. But bridging the gap between big and fast data is challenging: most of theseapplications require analytics' results that are fresh and consistent; but without impactingsystem latency and throughput. We propose virtual lightweight snapshots (VLS); amechanism that enables consistent analytics without blocking incoming updates in NoSQLstores. VLS requires neither native support for database versioning nor a transactionmanager. Besides; it is storage-efficient; keeping additional versions of records only whenneeded to guarantee consistency; and sharing versions across multiple concurrentsnapshots. We describe an implementation of VLS in MongoDB and present a detailedexperimental evaluation which shows that it supports consistency for analytics with small …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,2
RioBusData: Outlier Detection in Bus Routes of Rio de Janeiro,Aline Bessa; Fernando de Mesentier Silva; Rodrigo Frassetto Nogueira; Enrico Bertini; Juliana Freire,Abstract: Buses are the primary means of public transportation in the city of Rio de Janeiro;carrying around 100 million passengers every month. Recently; real-time GPS coordinatesof all operating public buses has been made publicly available-roughly 1 million GPSentries each captured each day. In an initial study; we observed that a substantial number ofbuses follow trajectories that do not follow the expected behavior. In this paper; we presentRioBusData; a tool that helps users identify and explore; through different visualizations; thebehavior of outlier trajectories. We describe how the system automatically detects theseoutliers using a Convolutional Neural Network (CNN) and we also discuss a series of casestudies which show how RioBusData helps users better understand not only the flow andservice of outlier buses but also the bus system as a whole.,arXiv preprint arXiv:1601.06128,2016,2
Reorganizing Workflow Evolution Provenance.,David Koop; Juliana Freire,Abstract The provenance of related computations presents the opportunity to betterunderstand and explore the differences and similarities of various approaches. As usersdesign and refine workflows; evolution provenance captures the relationships betweenworkflows as actions that mutate one workflow to another. However; such provenance maynot always be the most compact or intuitive. This paper presents algorithms to update andtransform workflow evolution provenance to achieve a representation that better exposes thecorrespondences between computations. We evaluate these algorithms based on theefficiency of the representation as well as the speed of the transformation.,TAPP,2014,2
Siphoning Hidden-Web Data through Keyword-Based Interfaces: Retrospective,Luciano Barbosa; Juliana Freire,In this paper; we proposed the first; fully-automatic approach to crawling the Hidden Web throughkeyword-based interfaces. Our crawler uses an algorithm for automatically deriving a series ofkeyword-based queries whose goal is to obtain high coverage while minimizing the costs. Inother words; our goal is to retrieve as much of the hidden contents as possible while minimizingthe number of required queries. The intuition behind our algorithm is that; by obtaining samplesof the hidden contents in a online database or document collection; we are able to discover keywordsthat have high frequency. Then; by using these high-frequency keywords we are able to constructqueries that return a large number of answers … Since our paper was published in the Proceedingsof the Brazilian Database Sypomsium in 2004; it has been cited sixty six times1. Otherhidden-Web crawlers were later proposed which make use of our algorithm [Madhavan …,Journal of Information and Data Management,2010,2
Provenance Management: Challenges and Opportunities.,Juliana Freire,Abstract: Computing has been an enormous accelerator to science and industry alike and ithas led to an information explosion in many different fields. The unprecedented volume ofdata acquired from sensors; derived by simulations and data analysis processes;accumulated in warehouses; and often shared on the Web; has given rise to a new field ofresearch: provenance management. Provenance (also referred to as audit trail; lineage; andpedigree) captures information about the steps used to generate a given data product. Suchinformation provides important documentation that is key to preserve data; to determine thedata's quality and authorship; to understand; reproduce; as well as validate results.Provenance solutions are needed in many different domains and applications; fromenvironmental science and physics simulations; to business processes and data …,BTW,2009,2
Automatically constructing collections of online database directories,Luciano Barbosa; Juliana Freire,Due the the explosion in the number of online databases; there has been increased interestin leveraging the highquality information present in these databases [6; 1; 8]. However;finding the right databases can be very challenging. For example; if a biologist needs tolocate databases related to molecular biology and searches on Google for the keywords“molecular biology database” over 27 million documents are returned. Among these; she willfind pages that contain databases; but the results also include a very large number of pagesfrom journals; scientific articles; etc. Recognizing the need for better mechanisms to locateonline databases; people have started to create online database collections such as theMolecular Biology Database Collection [4]; which lists databases of value to biologists. Thiscollection; however; has been manually created and is manually maintained by the …,Proceedings of the 15th ACM international conference on Information and knowledge management,2006,2
XML Storage,Denilson Barbosa; Philip Bohannon; Juliana Freire; Carl-Christian Kanne; Ioana Manolescu; Vasilis Vassalos; Masatoshi Yoshikawa,*,Encyclopedia of Database Systems,2017,1
ReproZip: The Reproducibility Packer,Rémi Rampin; Fernando Chirigati; Dennis Shasha; Juliana Freire; Vicky Steeves,*,The Journal of Open Source Software,2016,1
A unified index for spatio-temporal keyword queries,Tuan-Anh Hoang-Vu; Huy T Vo; Juliana Freire,Abstract From tweets to urban data sets; there has been an explosion in the volume oftextual data that is associated with both temporal and spatial components. Efficientlyevaluating queries over these data is challenging. Previous approaches have focused onthe spatial aspect. Some used separate indices for space and text; thus incurring theoverhead of storing separate indices and joining their results. Others proposed a combinedindex that either inserts terms into a spatial structure or adds a spatial structure to aninverted index. These benefit queries with highly-selective constraints that match the primaryindex structure but have limited effectiveness and pruning power otherwise. We propose anew indexing strategy that uniformly handles text; space and time in a single structure; andis thus able to efficiently evaluate queries that combine keywords with spatial and …,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,2016,1
Maximum common subelement metrics and its applications to graphs,Lauro Lins; Nivan Ferreira; Juliana Freire; Claudio Silva,Abstract: In this paper we characterize a mathematical model called Maximum CommonSubelement (MCS) Model and prove the existence of four different metrics on such model.We generalize metrics on graphs previously proposed in the literature and identify new onesby showing three different examples of MCS Models on graphs based on (1) subgraphs;(2)induced subgraphs and (3) an extended notion of subgraphs. This latter example can beused to model graphs with complex labels (eg; graphs whose labels are other graphs); andhence to derive metrics on them. Furthermore; we also use (3) to show that graph editdistance; when a metric; is related to a maximum common subelement in a correspondingMCS Model. Subjects: Discrete Mathematics (cs. DM); Combinatorics (math. CO) Cite as:arXiv: 1501.06774 [cs. DM](or arXiv: 1501.06774 v1 [cs. DM] for this version) Submission …,arXiv preprint arXiv:1501.06774,2015,1
Introduction to the VisTrails System,Erik W Anderson; Steven P Callahan; Juliana Freire; Emanuele Santos; Carlos E Scheidegger; Cláudio T Silva; Huy T Vo,1 Summary This document is a short introduction and tutorial of the VisTrails prototype. Thisis not meant for widespread public use! We are making this early alpha release of VisTrailsavailable to a select group of potential collaborators to give them a feel of the system and toget early feedback. You should expect broken or missing features and bugs in this releaseas well as major changes between this and future versions of VisTrails.,*,2012,1
DEFOG: A System for Data-Backed Visual Composition,Lauro Lins; David Koop; Juliana Freire; Claudio Silva,Abstract: As users analyze data; visualization is important for both generating insight duringexploration and displaying information for presentation purposes. While visualizationsystems have been very successful at the latter; their full potential as an aid for dataexploration has yet to be realized. Existing systems have often focused on techniques fordisplaying information; requiring data to conform to specific file formats or allowing verylimited manipulation of the data. In addition; most systems that provide a stronger linkbetween data and visualization sacrifice the freedom of arbitrary composition. In this paper;we present DEFOG; a system that aims to more tightly integrate data exploration andvisualization. DEFOG was designed to manipulate objects and as such; it is able to handle awide range of data types. In addition; it allows flexible data manipulation through both …,*,2011,1
Computational repeatability: The wikiquery case study,Huong Nguyen; Juliana Freire,Setting Workflow Parameters. Figures 3 and 4 show two workflows that use the Basic Run:one for the local and the other for the remote scenario. In both scenarios; to reproduceWikiQuery experiments; some of the input parameters for the workflow must be set by thereviewer to reflect the directories where they uncompressed the WikiQuery files. Asillustrated in Figure 2; to set the parameters: first select the appropriate node in the tree; thenclick on the “Pipeline”,*,2011,1
The Provenance of Workﬂow Upgrades,David Koop; Carlos E Scheidegger; Juliana Freire; Claudio T Silva,Abstract. Provenance has become an increasingly important part of documenting; verifying;and reproducing scientiﬁc research; but as users seek to extend or share results; it may beimpractical to start from the exact original steps due to system conﬁguration diﬀerences;library updates; or new algorithms. Although there have been several approaches forcapturing workﬂow provenance; the problem of managing upgrades of the underlying toolsand libraries orchestrated by workﬂows has been largely overlooked. In this paper weconsider the problem of maintaining and re-using the provenance of workﬂow upgrades. Wepropose diﬀerent kinds of upgrades that can be applied; including automatic mechanisms;developer-speciﬁed; and user-deﬁned. We show how to capture provenance from suchupgrades and suggest how this provenance might be used to inﬂuence future upgrades …,Provenance and Annotation of Data and Process: Third International Provenance and Annotation Workshop; Troy; NY; June 15-16; 2010; Revised Selected Papers,2011,1
Towards Supporting Collaborative Data Analysis and Visualization in a Coastal Margin Observatory,Emanuele Santos; Phillip Mates; Erik Anderson; Brad Grimm; Juliana Freire; Cláudio Silva,ABSTRACT Managing and understanding the large volumes of scientific data is one of themost difficult challenges scientists face today. As interdisciplinary groups work together; theability to generate a diversified collection of analyses for a broad audience and in an ad-hocmanner is essential to support effective data exploration. Science portals and Web-basedvisualization tools have been used to simplify this task by aggregating data from differentsources and providing a set of pre-defined analyses and visualizations. These; however; areexpensive to build and lack the flexibility necessary to support the vast heterogeneity of datasources; analysis techniques; and information needs from multiple user communities. In thispaper; we present a system that adopts the model used by social Web sites and; bycombining a set of usable tools and a scalable infrastructure; simplifies the construction of …,CSCW 2010 Workshop on The Changing Dynamics of Scientific Collaboration,2010,1
Query-driven visualization in the cloud with mapreduce,Bill Howe; Huy Vo; Claudio Silva; J Freire,We explore the MapReduce programming model for massivescale query-driven visualanalytics. Massively parallel programming frameworks such as MapReduce are increasinglypopular for simplifying data processing on hundreds and thousands of cores; offering faulttolerance; linear scale-up; and a high-level programming interface. However; these tools arebatch-oriented and are awkward to use directly for visualization. Informed by the successand popularity of MapReduce in the database research community; we evaluate thetradeoffs of using MapReduce to support massivescale query-driven visualization; where“query" implies not just simple subsetting; but database-style algebraic manipulation. Cloudcomputing promises an economy of scale for hardware; power; facilities; management; and;increasingly; software by moving computation and data to large; shared data centers. Two …,Proceedings of the Fourth Annual Workshop on Ultrascale Visualization,2009,1
Whiteboard: a collaborative pen-based annotation tool for e-learning,Juliana Freire; E Silva; Carlos HO Jardim; S Lara; T Kudo; Anselmo Martelini Jr; F Santos; R Fortes; MGC Pimentel,*,II Workshop TIDIA; São Paulo; Brazil,2005,1
A Flexible Infrastructure for Gathering XML Statistics and Estimating Query Cardinality.,Juliana Freire; Maya Ramanath; Lingzhi Zhang,A key component of XML data management systems is the result size estimator; whichestimates the cardinalities of user queries. Estimated cardinalities are needed in a variety oftasks; including query optimization and cost-based storage design; and they can also beused to give users early feedback about the expected outcome of their queries. In [2]; weproposed StatiX. In contrast to previously proposed result estimators; which use specializeddata structures and estimation algorithms; StatiX uses histograms to uniformly capture boththe structural and value skew present in documents. It also leverages schema information toproduce high-quality and concise statistical summaries. In particular; it exploits XML Schematransformations [1] to obtain statistics at different granularities. The original version of StatiXwas built as a proof of concept. With the goal of making the system publicly available; we …,ICDE,2004,1
Scheduling in SLG revisited,Juliana Freire; Terrance Swift; David Scott Warren,*,Tabulation en analyse syntaxique et déduction. Journées,1998,1
Controlling the Search in Tabled Evaluations.,Juliana Freire; David Scott Warren,Abstract SLG has proven to be an efficient and elegant strategy to evaluate normal logicprograms with respect to the well-founded semantics. Given the flexibility tabling provides inthe choice of when to schedule answers; efficiency of evaluation can be further improved bychoosing an appropriate scheduling strategy; that is; how and when answers are returned toconsuming nodes. Several different scheduling strategies for SLG have been investigatedincluding a set-at-a-time strategy well-suited for accessing relations in external databases.This paper explores Local Scheduling; a strategy that by following the dependencies amongsubgoals during evaluation; simplifies the computation of the well-founded model ofprograms with negation; and avoids non-productive computation in the presence of answersubsumption (eg; in aggregate computation and abstract interpretation). Even though …,ILPS,1997,1
Bell Labs Research; 600 Mountain Ave.; Murray Hill; NJ 07974 (Received 31 May 2000; in nal revised form||),Juliana Freire; Daniel Lieuwen; Joann Ordille,*,Information Systems,1994,1
Querying the Web,Daniela Florescu; J Freira,Page 1. D. Florescu ; J. Freire 1 SBBD 2000 - Querying the Web Querying the Web DanielaFlorescu INRIA Juliana Freire Database Systems Research Bell Labs - Lucent TechnologiesPage 2. D. Florescu ; J. Freire 2 SBBD 2000 - Querying the Web The Web - some history x 1989First Web browser x 1993 Mosaic is released; there are 50 Web sites x 1994 First search engines(WWWW; WebCrawler) x 1996 US$1 billion spent in Internet shopping; users in 150 countriesx 1997 1 million Web sites x 1998 300 thousand servers world-wide x 2000 More than 1 billiondistinct Web pages (http://www.inktomi.com/new/press/billion.html) Page 3. D. Florescu ; J. Freire3 SBBD 2000 - Querying the Web Data on the Web x What is the Web today? – HTML documentsintended for human consumption – easy to fetch any Web page; from any server and platformx Everything is on the Web - over 1 billion pages …,*,*,1
A Process-Driven Approach to Provenance-Enabling Existing Applications,Steven P Callahan; Juliana Freire; Carlos E Scheidegger; Cláudio T Silva; Huy T Vo,Abstract. Currently; there are no general provenance management systems or toolsavailable for existing applications. Groups that do not have the resources or expertise tobuild the provenance infrastructure needed resort to the manual creation and maintenanceof this information; greatly hindering their ability to do large-scale and/or complex dataexploration and processing. Even with the resources; applicationdependent solutions arenot general enough and can be hard to re-use in different settings and applications. Thiscauses problems with interoperability due to differences in the provenance models usedacross systems. Our goal is to develop provenance technology and design systems that areflexible and adaptable to the wide range of requirements of software applications. Byconsolidating provenance information for a variety of applications; we can provide a …,Google Scholar,*,1
Data Quality: The Role of Empiricism,Shazia Sadiq; Tamraparni Dasu; Xin Luna Dong; Juliana Freire; Ihab F Ilyas; Sebastian Link; Miller J Miller; Felix Naumann; Xiaofang Zhou; Divesh Srivastava,Abstract We outline a call to action for promoting empiricism in data quality research. Theaction points result from an analysis of the landscape of data quality research. Thelandscape exhibits two dimensions of empiricism in data quality research relating to type ofmetrics and scope of method. Our study indicates the presence of a data continuum rangingfrom real to synthetic data; which has implications for how data quality methods areevaluated. The dimensions of empiricism and their inter-relationships provide a means ofpositioning data quality research; and help expose limitations; gaps and opportunities.,ACM SIGMOD Record,2018,*
Learning to Discover Domain-Specific Web Content,Kien Pham; Aécio Santos; Juliana Freire,Abstract The ability to discover all content relevant to an information domain has manyapplications; from helping in the understanding of humanitarian crises to countering humanand arms trafficking. In such applications; time is of essence: it is crucial to both maximizecoverage and identify new content as soon as it becomes available; so that appropriateactions can be taken. In this paper; we propose new methods for efficient domain-specific re-crawling that maximize the yield for new content. By learning patterns of pages that have ahigh yield; our methods select a small set of pages that can be re-crawled frequently;increasing the coverage and freshness while conserving resources. Unlike previousapproaches to this problem; our methods combine different factors to optimize the re-crawling strategy; do not require full snapshots for the learning step; and dynamically …,Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining,2018,*
Real-time understanding of humanitarian crises via targeted information retrieval,KT Pham; P Sattigeri; A Dhurandhar; AC Jacob; M Vukovic; P Chataigner; J Freire; A Mojsilović; KR Varshney,Humanitarian relief agencies must assess humanitarian crises occurring in the world toprioritize the aid that can be offered. While the rapidly growing availability of relevantinformation enables better decisions to be made; it also creates an important challenge:How to find; collect; and categorize this information in a timely manner. To address theproblem; we propose a targeted retrieval system that automates these tasks. The systemuses historical data collected and labeled by subject matter experts to train a classifier thatidentifies relevant content. Using this classifier; it deploys a focused crawler to locate andretrieve data at scale. The system also incorporates feedback from subject matter experts toadapt to new concepts and information sources. A novel component of the system is analgorithm for re-crawling that improves the crawler efficiency in retrieving recent data. Our …,IBM Journal of Research and Development,2017,*
ARIES: Enabling Visual Exploration and Organization of Art Image Collections,Lhaylla Crissaff; Louisa Wood Ruby; Samantha Deutch; R Luke DuBois; Jean-Daniel Fekete; Juliana Freire; Claudio Silva,Art historians have traditionally used physical light boxes to prepare exhibits or curatecollections. On a light box; they can place slides or printed images; move the images aroundat will; group them as desired; and visually compare them. The transition to digital imageshas rendered this workflow obsolete. Now; art historians lack well-designed; unifiedinteractive software tools that effectively support the operations they perform with physicallight boxes. To address this problem; we designed ARIES (ARt Image Exploration Space);an interactive image manipulation system that enables the exploration and organization offine digital art. The system allows images to be compared in multiple ways; offering dynamicoverlays analogous to a physical light box; and supporting advanced image comparisonsand feature-matching functions; available through computational image processing. We …,IEEE computer graphics and applications,2017,*
noWorkflow: a tool for collecting; analyzing; and managing provenance from python scripts,Joao Felipe Pimentel; Leonardo Murta; Vanessa Braganholo; Juliana Freire,Abstract We present noWorkflow; an open-source tool that systematically and transparentlycollects provenance from Python scripts; including data about the script execution and howthe script evolves over time. During the demo; we will show how noWorkflow collects andmanages provenance; as well as how it supports the analysis of computational experiments.We will also encourage attendees to use noWorkflow for their own scripts.,Proceedings of the VLDB Endowment,2017,*
Spatio-temporal analytics; urban analytics,Juliana Freire,*,1st Europe Summer School: Data Science,2017,*
Querying and Exploring Polygamous Relationships in Urban Spatio-Temporal Data Sets,Yeuk-Yin Chan; Fernando Chirigati; Harish Doraiswamy; Cláudio T Silva; Juliana Freire,Abstract The Data Polygamy framework allows users to uncover interesting patterns andinteractions in the data exhaust from different components of an urban environment. Butanalyzing the plethora of relationships derived by the framework is challenging. In thisdemo; we show how visualization can help in the discovery of relationships that arepotentially interesting by allowing users to query and explore the relationship set in anintuitive way. We will demonstrate the effectiveness of the visual interface through casestudies; and demo visitors will also interact with the polygamous relationships.,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Provenance in Workflows,David Koop; Marta Mattoso; Juliana Freire,*,Encyclopedia of Database Systems,2017,*
Provenance and Reproducibility,Fernando Chirigati; Juliana Freire,A computational experiment composed by a sequence of steps S created at time T; onenvironment (hardware and operating system) E; using data D is reproducible if it can beexecuted with a sequence of steps S0 (modified from or equal to S) at time T 0> T; onenvironment E0 (potentially different than E); using data D0 that is similar to (or the same as)D with consistent results [5]. Replication is a special case of reproducibility where S0 DS andD0 D D. While there is substantial disagreement on how to define reproducibility [1]; inparticular across different domains; in this entry; we focus on computational reproducibility;ie; reproducibility for computational experiments or processes. The information needed toreproduce an experiment can be obtained from its provenance: the details of how theexperiment was carried out and the results it derived. For computational experiments …,Encyclopedia of Database Systems,2017,*
XML Selectivity Estimation,Maya Ramanath; Juliana Freire; Neoklis Polyzotis,*,Encyclopedia of Database Systems,2017,*
GPU Rasterization for Real-Time Spatial Aggregation over Arbitrary Polygons,Eleni Tzirita Zacharatou; Harish Doraiswamy; Anastasia Ailamaki; Claudio Silva; Juliana Freire,ABSTRACT Visual exploration of spatial data relies heavily on spatial aggregation queriesthat slice and summarize the data over different regions. These queries comprisecomputationally-intensive point-inpolygon tests that associate data points to polygonalregions; challenging the responsiveness of visualization tools. This challenge iscompounded by the sheer amounts of data; requiring a large number of such tests to beperformed. Traditional pre-aggregation approaches are unsuitable in this setting since theyfix the query constraints and support only rectangular regions. On the other hand; queryconstraints are defined interactively in visual analytics systems; and polygons can be ofarbitrary shapes. In this paper; we convert a spatial aggregation query into a set of drawingoperations on a canvas and leverage the rendering pipeline of the graphics hardware …,Proceedings of the VLDB Endowment,2017,*
Finding seeds to bootstrap focused crawlers,Karane Vieira; Luciano Barbosa; Altigran Soares Da Silva; Juliana Freire; Edleno Moura,Abstract Focused crawlers are effective tools for applications requiring a high number ofpages belonging to a specific topic. Several strategies for implementing these crawlers havebeen proposed in the literature; which aim to improve crawling efficiency by increasing thenumber of relevant pages retrieved while avoiding non-relevant pages. However; animportant aspect of these crawlers has been largely overlooked: the selection of the seedpages that serve as the starting points for a crawl. In this paper; we show that the seeds cangreatly influence the performance of crawlers; and propose a new framework forautomatically finding seeds. We describe a system that implements this framework andshow; through a detailed experimental evaluation; that by providing crawlers a seed set thatis large and varied; they not only obtain higher harvest rates but also an improved topic …,World Wide Web,2016,*
Shrexquery: an improved query translator for xpath,Ding Zhang; Juliana Freire,*,Undergraduate Research Abstracts Journal,2016,*
VisTrails for interactive multiple-view visualizations,Nathan Smith; Juliana Freire,*,Undergraduate Research Abstracts Journal,2016,*
Interactive Web Content Exploration for Domain Discovery,Aécio Santos and Juliana Freire Yamuna Krishnamurthy; Kien Pham,*,ACM KDD Workshop on Interactive Data Exploration and Analytics (IDEA),2016,*
Method and system for adaptive discovery of content on a network,*,A method is provided for identifying documents that include a searchable form relevant to atopic. A document is received. If the received document comprises a form is determined. Aform includes a field presented to a user requesting information from the user. If the receiveddocument is determined to comprise a form; a determination is made concerning whether ornot the form is a searchable form. A searchable form returns non-trivial information to arequester in response to a submission of the form. If the form is determined to be asearchable form; a determination is made concerning whether or not the form is relevant toan identified topic. If the form is determined to be relevant to the identified topic; thedocument is identified as a searchable form relevant to the identified topic.,*,2015,*
A model project for reproducible papers,Michele Dolfi; Jan Gukelberger; Andreas Hehn; Jakub Imriska; Kiryl Pakrouski; Troels Rønnow; Matthias Troyer; Ilia Zintchenko; F Chirigati; J Freire; D Shasha,In this paper we present a simple; yet typical simulation in statistical physics; consisting oflarge scale Monte Carlo simulations followed by an involved statistical analysis of theresults. The purpose is to provide an example publication to explore tools for writingreproducible papers. The simulation estimates the critical temperature where the Isingmodel on the square lattice becomes magnetic to be Tc/J= 2.26934 (6) using a finite sizescaling analysis of the crossing points of Binder cumulants. We provide a virtual machinewhich can be used to reproduce all figures and results.,*,2014,*
Implementing reproducible research,Alexander A Aarts; Anita Alexander; Peter Attridge; Štěpán Bahník; Michael Barnett-Cowan; Elizabeth Bartmess; Frank A Bosco; Mikio Braun; Benjamin Brown; C Titus Brown; Kristina Brown; Jesse J Chandler; Russ Clay; Hayley Cleary; Michael Cohn; Giulio Costantini; Jan Crusius; Andrew Davison; Jamie DeCoster; Michelle DeGaetano; Ryan Donohue; Elizabeth Dunn; Scott Edmunds; Casey Eggleston; Vivien Estel; Frank J Farach; Susann Fiedler; James G Field; Stanka Fitneva; Ian Foster; Joshua D Foster; Rebecca S Frazier; Juliana Freire; Elisa M Galliani; Roger Giner-Sorolla; Lars Goellner; R Justin Goss; Jesse Graham; James A Grange; Philip Guo; Joshua Hartshorne; Timothy B Hayes; Grace Hicks; Holger Hoefling; Bill Howe; Iain Hrynaszkiewicz; Denise Humphries; Christophe Hurlin; Luis Ibanez; Georg Jahn; Kate Johnson; Jennifer A Joy-Gaba; Heather B Kappes; Calvin K Lai; Daniel Lakens; Kristin A Lane; Etienne P LeBel; Minha Lee; Kristi Lemm; Melissa Lewis; Stephanie C Lin; Peter Li; Sean Mackinnon; Heather Mainard; Tanu Malik; Nathaniel Mann; Michael May; Jarrod Millman; Katherine Moore; Matt Motyl; Stephanie M Müller; Dave Murray-Rust; Peter Murray-Rust; Brian A Nosek; Catherine Olsson; Cheng S Ong; Fernando Perez; Christophe Perignon; Marco Perugini; Quan Pham; Michael Pitts; Kate Ratliff; Frank Renkewitz; Anthony Rossini; Abraham M Rutchick; Gillian Sandstrom; Dylan Selterman; William Simpson; Colin T Smith; Jeffrey R Spies; Victoria Stodden; Thomas Talhelm; Anna Veer; Michelangelo Vianello; Yihui Xie,Abstract In computational science; reproducibility requires that researchers make code anddata available to others so that the data can be analyzed in a similar manner as in theoriginal publication. Code must be available to be distributed; data must be accessible in areadable format; and a platform must be available for widely distributing the data and code.In addition; both data and code need to be licensed permissively enough so that others canreproduce the work without a substantial legal burden. Implementing ReproducibleResearch covers many of the elements necessary for conducting and distributingreproducible research. It explains how to accurately reproduce a scientific result. Dividedinto three parts; the book discusses the tools; practices; and dissemination platforms forensuring reproducibility in computational science. It describes: Computational tools; such …,*,2014,*
ReproZip: Packing experiments for sharing and publications,Fernando Chirigati; Dennis Shasha; Juliana Freire,Skip to main content …,*,2013,*
Clustering Wikipedia infoboxes to discover their types,Thanh Hoang Nguyen; Huong Dieu Nguyen; Viviane Moreira; Juliana Freire,Abstract Wikipedia has emerged as an important source of structured information on theWeb. But while the success of Wikipedia can be attributed in part to the simplicity of addingand modifying content; this has also created challenges when it comes to using; querying;and integrating the information. Even though authors are encouraged to select appropriatecategories and provide infoboxes that follow pre-defined templates; many do not follow theguidelines or follow them loosely. This leads to undesirable effects; such as templateduplication; heterogeneity; and schema drift. As a step towards addressing this problem; wepropose a new unsupervised approach for clustering Wikipedia infoboxes. Instead of relyingon manually assigned categories and template labels; we use the structured informationavailable in infoboxes to group them and infer their entity types. Experiments using over …,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,*
Letter from the associate editors,Gustavo Alonso; Juliana Freire,Skip to main content University of Utah Logo …,Unknown Journal,2012,*
Letter from the Special Issue Editor.,Juliana Freire,The explosion in the volume of digital data and its wide availability is revolutionizing manyscientific domains. At the same time; scientists faced with this data deluge must overcomemany challenges to manage and explore these data. Complex processes are needed toacquire; process; and analyze the data. Even through there are robust and efficientdatabases systems; they fail to meet many of the requirements of emerging scientificapplications which involve diverse data and require operations that go beyond what iscurrently supported. These new users and applications present new research problems indata management as well as a great opportunity for our community to have practical impact.In this issue; we have collected a set of articles that highlight new directions for databaseresearch; relate limitations in current data management technology; and provide …,IEEE Data Eng. Bull.,2012,*
Towards Integrating Workflow and Database Provenance: A Practical Approach,Fernando Chirigati and Juliana Freire,*,International Provenance and Annotation Workshop,2012,*
The Web has completely changed the way in which we share data; rapidly shifting us from a world of paper documents to a world of digital objects that include online...,GT Lakshmanan; F Curbera; J Freire; A Sheth,It seems likely that the Web as we know it will undergo several paradigm shifts in the comingyears: its content will expand from 2D documents to also accommodate 3D virtual worlds;with new technologies like RFID and smartphones; be able to link to a place or thing (in thereal or virtual world) and associate more metadata annotations with things in the real andvirtual worlds to create increasingly...,IEEE Internet Computing,2011,*
Provenance management for data exploration,Juliana Freire,Abstract Computing has been an enormous accelerator to science and industry alike and ithas led to an information explosion in many different fields. The unprecedented volume ofdata acquired by sensors; derived by simulations and analysis processes; and shared onthe Web opens up new opportunities; but it also creates many challenges when it comes tomanaging and analyzing these data.,International Conference on Data Integration in the Life Sciences,2010,*
VISMASHUP: streamlining the creation of custom visualization applications,James P Ahrens; Emanuele Santos; Lauro Lins; Juliana Freire; Cl'audio T Silva,Abstract-Visualization is essential for understanding the increasing volumes of digital data.However; the process required to create insightful visualizations is involved and timeconsuming. Although several visualization tools are available; including tools withsophisticated visual interfaces; they are out of reach for users who have little or noknowledge of visualization techniques and/or who do not have programming expertise. Inthis paper; we propose VISMASHUP; a new framework for streamlining the creation ofcustomized visualization applications. Because these applications can be customized forvery specific tasks; they can hide much of the complexity in a visualization specification andmake it easier for users to explore visualizations by manipulating a small set of parameters.We describe the framework and how it supports the various tasks a designer needs to …,*,2010,*
Provenance and Annotation of Data and Processes,David Hutchison; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum; Juliana Freire; David Koop; Luc Moreau,*,*,2008,*
Provenance and Annotation of Data and Processes: Second International Provenance and Annotation Workshop; IPAW 2008; Salt Lake City; UT; USA; June 17-18;...,David Hutchison; Juliana Freire; Takeo Kanade; Josef Kittler; Jon M Kleinberg; David Koop; Friedemann Mattern; John C Mitchell; Luc Moreau; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum,*,*,2008,*
Tom Crecelius 1480 Carlo A. Curino 761; 882 Emiran Curtmola 1408; 1448 D Harish D. 1124; 1325,Florian Daniel; David DeWitt; Amol Deshpande; AnHai Doan; Marcus Fontoura; Juliana Freire; Venkatesh Ganti; Hong Gao; Hector Garcia-Molina; Minos Garofalakis; Charles Garrod; Tingjian Ge; Lise Getoor; Phillip Gibbons; Lukasz Golab; Wojciech Golab; Yihong Gong; Albert Greenberg; Maxim Grinev; Peter Haas; Wook-Shin Han; Michael Hay; Monika Henzinger; Mauricio Hernandez; Mark Hill; Howard Ho; Allison Holloway; Mingsheng Hong; Chien-Yi Hou; Yanli Hu; Kien Hua; Jiansheng Huang; Ihab Francis Ilyas; Zachary G Ives; Marie Jacob; HV Jagadish; Magesh Jayapandian; David Jensen; Haifeng Jiang; Cheqing Jin; Ryan Johnson; Theodore Johnson; Vanja Josifovski,*,*,2008,*
Q ue ry in gand C re a tin g V isu a liz a tio ns by A na lo gy,Carlos E Scheidegger; Huy T Vo; David Koop; Juliana Freire; Claudio T Silva,*,IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,2007,*
Large-scale analytics,Juliana Freire,*,International Conference on Management of Data: Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,*
VisTrails: enabling interactive multiple-view visualizations.,Carlos E Scheidegger; Huy T Vo; Patricia Joyce Crossno; Steven P Callahan; Louis Bavoil; Juliana Freire; Claudio Silva,*,*,2005,*
Supporting Exploratory Queries in Database Centric Web Applications,Abhijit Kadlag; Amol Wanjari; Juliana Freire; Jayant R Haritsa,Abstract: Users of database-centric Web applications; especially in the e-commerce domain;often resort to exploratory``trial-and-error''queries since the underlying data space is hugeand unfamiliar; and there are several alternatives for search attributes in this space. Forexample; scouting for cheap airfares typically involves posing multiple queries; varying flighttimes; dates; and airport locations. Exploratory queries are problematic from the perspectiveof both the user and the server. For the database server; it results in a drastic reduction ineffective throughput since much of the processing is duplicated in each successive query.For the client; it results in a marked increase in response times; especially when accessingthe service through wireless channels. In this paper; we investigate the design of automatedtechniques to minimize the need for repetitive exploratory queries. Specifically; we …,arXiv preprint cs/0310035,2003,*
Efficiency and Effectiveness of XML Tools and Techniques (EEXTT)-XML Storage-Adaptive XML Shredding: Architecture; Implementation; and Challenges,Juliana Freire; Jerome Simeon,*,Lecture Notes in Computer Science,2003,*
Integrating network devices in a meta-directory: the MetaComm experience,Juliana Freire; Daniel Lieuwen; Joann Ordille,Abstract Recently; meta-directories have begun to be used for unifying and centrallymanaging disparate directories/devices within an enterprise. Meta-directories must addressmany problems that have been extensively studied in the area of data integration; such asdata cleaning and schema integration. However; some new challenges arise. Whereasprevious data integration systems have focused on read-only queries; update support formsa major component of meta-directories. Network devices performing time-critical tasksshould not be interrupted (queried) too often; and thus meta-directories must materialize theintegrated data. At the same time; network devices must sometimes be managedindependently from the meta-directory. Thus; meta-directories must ensure that data remainsconsistent across devices and the integrated view in the presence of updates to both …,Information Systems,2002,*
Aaltonen; M.; see Kaasinen; E. 231,N Abe; T Kamba; R Agrawal; JC Shafer; J Angele; S Staab; V Anupam; J Freire; B Kumar; D Lieuwen; GJ Badros; JJ Barton; AC Huang; A Basso; S Gruber; K Bharat; R Stata; N Bhatti; A Bouch; A Kuchinsky; M Bishop; B Hashii; A Bongio; S Ceri; A Bouch; NO Bouvin; K Grønbæk; BE Brewington; G Cybenko; A Broder; R Kumar; F Maghoul; P Raghavan; S Rajagopalan; R Stata; A Tomkins; J Wiener; J Brustoloni; J Garay; O Buyukkokten; H Garcia-Molina; A Paepcke; A Cawsey; P Fraternali; A Bongio; S Chakrabarti; S Srivastava; M Subramanyam; M Tiwari; SG Chang; V Krishnan; K Chon; J Jung; WW Cohen; W Fan; J Cupitt; K Martinez; E Damiani; S De Capitani di Vimercati; S Paraboschi; P Samarati; L Dannstedt; K Forsberg; G Davenport; P Pan; S De Capitani di Vimercati; S Decker; Y Dong; D Zhang; M Erdmann; O Etzioni; A Sugiura; W Fan; A Feldmann; M Fernández; WC Tan; D Suciu; D Florescu; D Kossmann; I Manolescu; L Dannstedt; SL Fowler; AMJ Novack; MJ Stillings; A Fox; P Fraternali; J Freire; J Garay; H Garcia-Molina; J Hirai; M Girardot; N Sundaresan; S Goose; M Newman; C Schmidt; L Hue; L Sloth; NO Bouvin; J Rexford; A Basso,*,Computer Networks,2000,*
Batched Answer: An Alternative Scheduling for Tabling Systems,Juliana Freire; Terrance Swift; David S Warren,Abstract Tabled logic programs ensure termination for programs with nite models by makingdistinctions among calls to tabled subgoals. Given several variant subgoals in an evaluation;only the rst will use program clause resolution; the rest must use answer resolution. This useof answer resolution prevents the possibility of in nite looping which is a feature of SLD.However; to ensure completeness tabled evaluations must also have the ability to returnanswers to subgoals; even if these answers are derived after the subgoal has been called.Implementations of tabled logic programs thus face an important scheduling choice notpresent in traditional top-down evaluation: How to e ciently return answers to consumingsubgoals. This paper investigates an alternate scheduling strategy for tabling in a WAMimplementation; the SLG-WAM 7]. The original SLG-WAM had a simple mechanism of …,tc,1996,*
Taking I/O Seriously: Resolution Reconsidered for Disk (Expanded Version {Draft),Juliana Freire; Terrance Swift; David S Warren,Abstract Modern compilation techniques can give Prolog programs; in the best cases; aspeed comparable to C. However; Prolog has proven to be unacceptable for data-orientedqueries for two major reasons: its sometimes poor termination and complexity properties forDatalog; and its tuple-at-a-time strategy. A number of tabling frameworks and systems haveaddressed the rst problem; most notably the XSB system which has achieved Prolog speedsfor tabled programs. Yet; tabling systems such as XSB continue to use the tuple-at-a-timeparadigm. As a result; these systems are not amenable to a tight interconnection with disk-resident data. However; in a tabling framework the di erence between tuple-at-a-timebehavior and set-at-a-time can be viewed as one of scheduling. Accordingly; we de ne abreadth-rst set-at-a-time tabling strategy and prove it iteration equivalent to a form of semi …,*,1996,*
Logic Programming and Databases integrated at last,Juliana Freire; Terrance Swift; David Scott Warren,*,GMD STUDIEN,1996,*
Parallelizing Tabled Evaluations,Juliana Freire; Rui Hu; Terrance Swift; David Scott Warren,Abstract SLG is a table-oriented resolution method that extends SLD evaluation in two ways.It computes; with polynomial data complexity; the well-founded model for logic programswith negation and it terminates for programs with the bounded-term-size property.Furthermore SLG has an e cient sequential implementation for left-to-right modularly stratied programs in the SLG-WAM of XSB 13]. This paper addresses general issues involved inparallelizing tabled evaluations by introducing a model of shared-memory parallelism whichwe call tableparallelism and by comparing it to traditional models of parallelizing SLD. Abasic architecture for supporting table-parallelism in the framework of the SLG-WAM is alsopresented; along with an algorithm for detecting termination of subcomputations.,Workshop on Design and Impl. of Parallel Logic Programming Systems,1994,*
Data Engineering,Arnaud Sahuguet; John Krauss; Luis Palacios; David Sangokoya; Charlie Catlett; Tanu Malik; Brett Goldstein; Jonathan Giuffrida; Yetong Shao; Alessandro Panella; Derek Eder; Eric van Zanten; Robert Mitchum; Severin Thaler; Ian Foster; Juliana Freire; Cláudio Silva; Huy Vo; Harish Doraiswamy; Nivan Ferreira; Jorge Poco,Bulletin of the Technical Committee on Data Engineering December 2014 Vol. 37 No. 4 IEEEComputer Society Letters Letter from the Editor-in-Chief...................................................... DavidLomet 1 Letter from the Special Issue Editors............................ David Maier; VM Megler; KristinTufte 2 Special Issue on Urban Informatics Collaborative Sensing for UrbanTransportation.................. Sergio Ilarri; Ouri Wolfson; Thierry Delot 3 Open Civic Data: Of thePeople; For the People; By the People … Editorial Board Editor-in-Chief DavidB. Lomet Microsoft Research One Microsoft Way Redmond; WA 98052; USA lomet@microsoft. com Associate Editors Christopher Jermaine Department of Computer Science RiceUniversity Houston; TX 77005 Bettina Kemme School of Computer Science McGill UniversityMontreal; Canada David Maier Department of Computer Science Portland State …,*,*,*
PROVENANCE FOR VISUALIZATIONS,Claudio T Silva; Juliana Freire; Steven P Callahan,The demand for the construction of complex visualizations is growing in many disciplines ofscience; as users are faced with ever increasing volumes of data to analyze. The authorspresent VisTrails; an open source provenance-management system that providesinfrastructure for data exploration and visualization.,*,*,*
6 Working groups 6.1 PRIMAD–Information gained by different types of reproducibility,Andreas Rauber; Vanessa Braganholo; Jens Dittrich; Nicola Ferro; Juliana Freire; Norbert Fuhr; Daniel Garijo; Carole Goble; Kalervo Järvelin; Bertram Ludäscher; Benno Stein; Rainer Stotzka,What is “reproducibility” anyways? And how is it different from “repeatability”;“replicability”; orany of the other r-words? There are already a number of attempts at defining and sorting outthese different notions. De Roure [1] lists 21 different r-words grouped into 6 categories;stating that reproducibility means reusing a research object with a change to somecircumstances; inputs; resources or components in order to see if the same results areachieved independent of those changes. Often these notions are context-sensitive (eg;validation vs verification have rather precise and very different meanings in differentcommunities. As an alternative approach to sort out terminological confusions; we attemptedto look at a different perspective. When trying to reproduce a study; what are the things thatare kept the same (eg; the overall method or algorithm) and what is changed (eg; the …,Reproducibility of Data-Oriented Experiments in e-Science,*,*
Provenance Capture,Yang Ji; Sangho Lee; Wenke Lee; João Felipe Pimentel; Juliana Freire; Vanessa Braganholo; Leonardo Murta; Manolis Stamatogiannakis; Hasanat Kazmi; Hashim Sharif; Remco Vermeulen; Ashish Gehani; Herbert Bos; Paul Groth; Peng Chen; Tom Evans; Beth Plale; Wellington Oliveira; Paolo Missier; Kary Ocaña; Daniel de Oliveira; Troy Kohwalter; Thiago Oliveira; Esteban Clua; Danius T Michaelides; Richard Parker; Chris Charlton; William J Browne; Luc Moreau; Darren P Richardson; David Koop,Page 1. Contents Provenance Capture RecProv: Towards Provenance-Aware User SpaceRecord and Replay. . . . . 3 Yang Ji; Sangho Lee; and Wenke Lee Tracking and Analyzing theEvolution of Provenance from Scripts . . . . . 16 João Felipe Pimentel; Juliana Freire; VanessaBraganholo; and Leonardo Murta Trade-Offs in Automatic Provenance Capture . . . . . 29 ManolisStamatogiannakis; Hasanat Kazmi; Hashim Sharif; Remco Vermeulen; Ashish Gehani; HerbertBos; and Paul Groth Analysis of Memory Constrained Live Provenance . . . . . 42 Peng Chen;Tom Evans; and Beth Plale Provenance Analysis and Visualization Analyzing ProvenanceAcross Heterogeneous Provenance Graphs . . . . . 57 Wellington Oliveira; Paolo Missier; KaryOcaña; Daniel de Oliveira; and Vanessa Braganholo …,*,*,*
Data Engineering,Anat Eyal; Bertram Ludascher; Timothy McPhillips; Shawn Bowers; Manish Kumar Anand; Juliana Freire,The Bulletin of the Technical Committee on Data Engineering is published quarterly and isdistributed to all TC members. Its scope includes the design; implementation; modelling;theory and application of database systems and their technology. Letters; conferenceinformation; and news should be sent to the Editor-in-Chief. Papers for each issue aresolicited by and should be sent to the Associate Editor responsible for the issue. Opinionsexpressed in contributions are those of the authors and do not necessarily reflect thepositions of the TC on Data Engineering; the IEEE Computer Society; or the authors'organizations. Membership in the TC on Data Engineering is open to all current members ofthe IEEE Computer Society who are interested in database systems. There are two DataEngineering Bulletin web sites: http://www. research. microsoft. com/research/db/debull …,*,*,*
Exploring Traffic Dynamics in Urban Environments Using Vector-Valued Functions,H Carr; KL Ma; G Santucci; Jorge Poco; Harish Doraiswamy; Huy T Vo; João LD Comba; Juliana Freire; Cláudio T Silva,Yen's ranking loopless shortest paths algorithm [Yen71] computes the k-shortest pathsbetween a source and destination in a graph for a given k. It inductively computes the ithshortest path between two nodes using the common subpaths of the (i− 1)-shortest paths.The algorithm uses the result of the shortest path as starting point; followed by a relaxationprocedure until the distance constraint is met. This algorithm can be used to compute theclosest path as follows: identify a set of k shortest paths; for a large enough k such that the kth shortest path is the closest path. Unfortunately; due to a grid-like structure of the roadnetwork in most part of NYC; the above algorithm doesn't work well. This is because it has tocompute and discard a lot of suboptimal paths. For example; Fig. 1 (b) illustrates the class ofpaths that Yen's algorithm produced for a taxi route shown in Fig. 1 (a). Note that the …,*,*,*
Understanding the Landscape,Luciano Barbosa; Kien Pham; Claudio Silva; Marcos R Vieira; Juliana Freire,Abstract A growing number of cities are now making urban data freely available to thepublic. Besides promoting transparency; these data can have a transformative effect insocial science research as well as in how citizens participate in governance. Theseinitiatives; however; are fairly recent and the landscape of open urban data is not wellknown. In this study; we try to shed some light on this through a detailed study of over 9;000open data sets from 20 cities in North America. We start by presenting general statisticsabout the content; size; nature; and popularity of the different data sets; and then examine inmore detail structured data sets that contain tabular data. Since a key benefit of having alarge number of data sets available is the ability to fuse information; we investigateopportunities for data integration. We also study data quality issues and time-related …,*,*,*
Workflows and Provenance,Juliana Freire,*,*,*,*
NOWORKFLOW,João Felipe Pimentel; Leonardo Murta; Vanessa Braganholo; Fernando Chirigab; David Koop; Juliana Freire,*,*,*,*
The Architecture of an Emergency Plan Deployment System,Juliana Freire; Marco Antonio Casanova,Abstract. This paper outlines the architecture and early implementation of an emergencyplan deployment system that helps teams of human agents develop and executecomprehensive emergency plans; hyperlinked to conventional as well as geographicaldocuments. The architecture features six components: a document database; a planmanagement module; a resource management module; a geographical documentmanagement module; a conventional document management module; and the planmonitoring module. Central to the architecture is a simple plan modeling language thathelps overcome some of the limitations of conventional emergency plan descriptionschemes.,*,*,*
Workshop Organization,Alberto HF Laender; Juliana Freire; Dan Suciu; Mirella M Moro; Vanessa Braganholo; Clodoveu Davis Jr; Marcos André Gonçalves; Francesco Bonchi; Angela Bonifati; Andrea Calì; Sara Cohen; Isabel Cruz; Wolfgang Gatterbauer; Boris Glavic; Claudio Gutierrez; Solmaz Kolahi; Dongwon Lee; Domenico Lembo; Marta Mattoso; Regina Motz; Frank Neven; Rachel Pottinger; Vibhor Rastogi; Altigran S da Silva; Cristina Sirangelo; Divesh Srivastava; Julia Stoyanovich; David Toman; Alejandro Vaisman; Stijn Vansummeren; Ke Yi; Daniel Oliveira,The Alberto Mendelzon International Workshop on Foundations of Data Management (AMW2012) held in Ouro Preto; Brazil; on June 27-30; 2012; is the sixth workshop of a serieswhich started in 2006; as part on an initiative of the Latin American community ofresearchers in data management to honor the memory of our friend; colleague and mentorAlberto Mendelzon. The AMW series has been a venue for high-quality research onfoundational aspects of data management and it has helped foster and solidify the researchin this area throughout Latin America. This event; as the previous ones; has encouraged theparticipation of Latin American graduate students and includes activities specially designedfor them. In addition; with sponsorship from the VLDB Endowment; travel grants have beenprovided for students to attend the event. The proceedings of the workshop consist of 14 …,*,*,*
Managing the Evolution of Dataflows with VisTrails,Juliana Freire,Page 1. Managing the Evolution of Dataflows with VisTrails Juliana Freire http://www.cs.utah.edu/~juliana University of Utah Joint work with: Steven P. Callahan; Emanuele Santos; Carlos E.Scheidegger; Claudio T. Silva and Huy T. Vo Page 2. Juliana Freire 2 SciFlow 2006 DataExploration through Visualization ◆ Hard to make sense out of large volumes of raw data; eg;sensor feeds; simulations; MRI scans ◆ Insightful visualizations help analyze and validate varioushypothesis ◆ But creating a visualization is a complex process Data Image SpecificationKnowledge Visualization Perception & Cognition Exploration Data Visualization User Page 3.Juliana Freire 3 SciFlow 2006 Visualization Systems: State of the Art ◆ Systems: SCIRun;ParaView ◆ Visual programming for creating visualization pipelines—dataflows of visualizationoperations – Simplify and automate and the creation of visualizations …,*,*,*
page 1,Juliana Freire; Rui Hu; Terrance Swift,Abstract: SLG is a table-oriented resolution method that extends SLD evaluation in twoways. It computes the well-founded model for logic programs with negation with polynomialdata complexity; and it terminates for programs with the bounded-term-size property.Furthermore SLG has an efficient sequential implementation for modularly stratifiedprograms in the SLG-WAM of XSB. This paper addresses general issues involved inparallelizing tabled evaluations by introducing a model of shared-memory parallelism whichwe call table parallelism and by comparing it to traditional models of parallelizing SLD. Abasic architecture for supporting table parallelism in the framework of the SLG-WAM is alsopresented; along with an algorithm for detecting termination of subcomputations.,*,*,*
2014 Reviewer Thanks,Andreas Adelmann; Yuri Alekseev; Ann Almgren; Rommie Amaro; Ramesh Balakrishnan; Wolfgang Bangerth; Blaise Barney; Steven Barrett; Klaus Bartschat; Andrew Benson; Sarah Benziane; David Bernholdt; Martin Berzins; Allen Brookes; Tim Campbell; John Cazes; Yung-Kuan Chan; Sotirios Chatzis; Steve Chiu; Bruce Cohen; Adrian Victor Crisciu; Romaric David; Carleton De Tar; Thomas Deboni; Stephen Elbert; Thomas Epperly; Johannes Feist; Hal Finkel; Juliana Freire,Page 1. REVIEWER THANKS 68 Computing in Science & Engineering 1521-9615/15/$31.00 ©2015 IEEE Copublished by the IEEE CS and the AIP January/February 2015 A AndreasAdelmann Yuri Alekseev Ann Almgren Rommie Amaro B Ramesh Balakrishnan WolfgangBangerth Blaise Barney Steven Barrett Klaus Bartschat Andrew Benson Sarah BenzianeDavid Bernholdt Martin Berzins Allen Brookes C Tim Campbell John Cazes Yung-Kuan ChanSotirios Chatzis Steve Chiu Bruce Cohen Adrian Victor Crisciu D Romaric David CarletonDe Tar Thomas Deboni E Stephen Elbert Thomas Epperly F Johannes Feist Hal Finkel JulianaFreire 2014 Reviewer Thanks REviEwER ThAnks The articles appearing in CiSE are theresult of hard work by many people. We deeply appreciate the efforts of everyone whoreviewed the many articles submitted to CiSE last year …,*,*,*
V isualization C orner,Claudio T Silva; Juliana Freire; Steven P Callahan,*,*,*,*
Data Engineering,Thomas Heinis; Farhan Tauheed; Mirjana Pavlovic; Anastasia Ailamaki; Jacob Vanderplas; Emad Soroush; Simon Krughoff; Magdalena Balazinska; Michael Stonebraker; Jennie Duggan; Leilani Battle; Olga Papaemmanouil; Colin Talbert; Marian Talbert; Jeff Morisette; David Koop; Fernando Chirigati; Matthias Troyer; Dennis Shasha; Juliana Freire,Abstract Researchers in several scientific disciplines are struggling to cope with the massesof data resulting from either increasingly precise instruments or from simulation runs on evermore powerful supercomputers. Efficiently managing this deluge of data has become key tounderstand the phenomena they are studying. Scientists in the simulation sciences; forexample; build increasingly big and detailed models; as detailed as the hardware allows;but they lack the efficient technology to update and analyze them. In this paper we discusshow innovative data management techniques we have developed; enable scientists to buildand analyze bigger and more detailed spatial models and how these techniques ultimatelyaccelerate discovery in the simulation sciences. These include spatial join methods (inmemory and on disk); techniques for the efficient navigation in detailed meshes; an index …,*,*,*
DeepPeep: A Form Search Engine,Luciano Barbosa; Hoa Nguyen; Thanh Nguyen; Ramesh Pinnamaneni; Juliana Freire,We present DeepPeep (http://www. deeppeep. org); a new search engine specialized inWeb forms. DeepPeep uses a scalable infrastructure for discovering; organizing andanalyzing Web forms which serve as entry points to hidden-Web sites. DeepPeep providesan intuitive interface that allows users to explore and visualize large form collections.,*,*,*
Department of Computer Science State University of New York at Stony Brook Stony Brook; NY 11794-4400,Juliana Freire; Rui Hu J Terrance Swift; David S Warren,*,*,*,*
Banakar; A. 165 Bose; T. 1 Buchtala; O. 85,YQ Chen; MY Chow; FN Chowdhury; F Cupertino; UB Desai; R Dinoff; H Dong; B Eames; TP Fries; XZ Gao; M Garg; CS Gargour; C Giraud-Carrier; E Grant; C Gruber; JH Gunther; RL Haupt; SE Haupt; R Hecht; BT Hemmelman; S Hettiarachchi; TK Ho; J Hofer; C Hook; B Kleinjohann; S Kubisch; B Kumar; K Lavangnananda; DH Lee; WD Lee; D Lieuwen; J Maeda; J Martikainen; T Martinez; H Marzi; MJ Mendenhall; SN Merchant; E Merenyi; E Mininno; M Mnif; TK Moon; KL Moore; C Müller-Schloer; A Murmann; D Naso; AL Nelson; MS Nokleby; I Ohyama; SJ Ovaska; RP Ramachandran,Page 1. XIII A Ahn; H.-S. 72 Akhbardeh; A. 42; 207 Allen; CT 243 Askildsen; B. 30 Azeem;MF 165 B Banakar; A. 165 Bose; T. 1 Buchtala; O. 85 C Chen; YQ 13; 72 Chow; M.-Y. 149Chowdhury; FN 160 Cupertino; F. 66 D Desai; UB 116 Dinoff; R. 91 Dong; H. 104 E Eames;B. 127 F Fries; TP 177 G Gao; XZ 133 Garg; M. 116 Gargour; CS 19 Giraud-Carrier; C. 213Grant; E. 201 Gruber; C. 36; 110 Gunther; JH 1; 122 H Haupt; RL 249 Haupt; SE 243 Hecht;R. 98 Hemmelman; BT 30 Hettiarachchi; S. 195 Ho; TK 91 Hofer; J. 36 Hook; C. 110 AuthorIndex Hsiang; SM 104; 149 Hull; R. 91 J Junnila; S. 42 K Kannan; G. 116 Karakostas; T. 104Kempf; J. 110 Kim; D.-H. 154 Kleinjohann; B. 171 Kubisch; S. 98 Kumar; B. 91 LLavangnananda; K. 237 Lee; D.-H. 154 Lee; WD 154 Lieuwen; D. 91 …,*,*,*
Special Issue: Database Theory 2005 Guest Editor: Dan Suciu,Dan Suciu; Dirk Leinders; Jan Van den Bussche; Wim Martens; Joachim Niehren; Wolfgang Faber; Gianluigi Greco; Nicola Leone; Ariel Fuxman; Renée J Miller; Solmaz Kolahi; Sara Cohen; Yehoshua Sagiv; J Nathan Foster; Michael B Greenwald; Christian Kirkegaard; Benjamin C Pierce; Alan Schmitt,*,*,*,*
Data Engineering,Ronald Fagin; Ariel Fuxman; Laura M Haas; Mauricio A Hernández; Howard Ho; Anastasios Kementsietsidis; Renée J Miller; Felix Nauman; Lucian Popa; Yannis Velegrakis; Charlotte Vilarem; Ling-Ling Yan; Andrea Calı; Diego Calvanese; Giuseppe De Giacomo; Maurizio Lenzerini,Bulletin of the Technical Committee on Data Engineering September 2002 Vol. 25 No. 3 IEEEComputer Society Letters Letter from the Editor-in-Chief...................................................... DavidLomet 1 Letter from the Special Issue Editor.................................................. Renée J. Miller 2 SpecialIssue on Integration Management Data Integration: Where Does the Time Go?...... LenSeligman; Arnon Rosenthal; Paul Lehner; Angela Smith 3 Integration Through a Practitioner'sEye.... Srinivasa Narayanan; Subbu N. Subramanian and the Tavant Team 11 Data … EditorialBoard Editor-in-Chief David B. Lomet Microsoft Research One Microsoft Way; Bldg. 9 RedmondWA 98052-6399 lomet@ microsoft. com Associate Editors Umeshwar Dayal Hewlett-PackardLaboratories 1501 Page Mill Road; MS 1142 Palo Alto; CA 94304 Johannes Gehrke Departmentof Computer Science Cornell University Ithaca; NY 14853 Christian S. Jensen …,Urbana,*,*
Statement of Research Interests,Ariel Fuxman,When we integrate data from multiple sources; the resulting database may contain errorsand inconsistencies. Ideally; all errors should be cleaned automatically. In reality; though;data cleaning necessarily requires human intervention. To facilitate this; I envision a dataintegration process where it is possible to obtain meaningful and consistent answers from anintegrated database even if it is partially dirty. The challenge of query answering over dirtydatabases is that we cannot naively reuse existing database technology; which is designedto work on clean databases. For this reason; I designed; implemented and evaluatedConQuer [SIGMOD05; VLDB05]; a scalable system for query answering over dirtydatabases. 1 This system helps users take advantage of the query results in order tointeractively clean the integrated database.,*,*,*
Aberson; S.; see Zhang; X.; Jan./Feb. pp. 13-21.,J Ahrens; M Aivazis; FJ Alexander; C Amerault; JS Andrade Jr; S Andreyev; S Bagchi; SF Barrett; S Behnel; I Beichl; J Beuken; A Bhattacharyya; J Bielak; P Bienstman; AB Bjornsson; W Bogaerts; BJ Bos; R Bradshaw; RE Bryant; G Caldwell; A Catlin; S Cavallo; A Censi; C Chen; D Chen; C Citro; JLD Comba; J Comba; A Conci; C Correa; D Correa; L Dalcin; I Das; C Davis; D Davison; C Day; D Dennis; T Deutsch; M Di Pierro; J Done; J Dongarra; J Doyle; J Dudhia; R Eigenmann; A Elgamal; T Engel; J Enos; T Ertl; F Esquembre; E Fehr; R Feitosa; M Fiers; S Fredrick; J Freire; MD Galloy; D Gayen; L Gendrisch; A George; B Geveci; R Glassbrook; S Goldenberg,*,*,*,*
Aberson; S.; see Zhang; X.; Jan./Feb. pp. 13-21. Ahrens; J.; B. Hendrickson; G. Long; S. Miller; R. Ross; and D. Williams;" Data-Intensive Science in the US DOE: Cas...,M Aivazis; FJ Alexander; C Amerault; JS Andrade Jr; S Andreyev; S Bagchi; SF Barrett; S Behnel; I Beichl; J Beuken; A Bhattacharyya; J Bielak; P Bienstman; AB Bjornsson; W Bogaerts; BJ Bos; R Bradshaw; RE Bryant; G Caldwell; A Catlin; S Cavallo; A Censi; C Chen; D Chen; C Citro; JLD Comba; J Comba; A Conci; C Correa; D Correa; L Dalcin; I Das; C Davis; D Davison; C Day; D Dennis; T Deutsch; M Di Pierro; J Done; J Dongarra; J Doyle; J Dudhia; R Eigenmann; A Elgamal; T Engel; J Enos; T Ertl; F Esquembre; E Fehr; R Feitosa; M Fiers; S Fredrick; J Freire; MD Galloy; D Gayen; L Gendrisch; A George; B Geveci; R Glassbrook; S Goldenberg,Page 1. AUTHOR INDEX A Aberson; S.; see Zhang; X.; Jan./Feb. pp. 13-21. Ahrens; J.; B.Hendrickson; G. Long; S. Miller; R. Ross; and D. Williams; "Data-Intensive Science in the USDOE: Case Studies and Future Challenges;" Nov./Dec.; pp. 14-24. Aivazis; M.; see Millman;K..J.; Mar./Apr. pp. 9-12. Alexander; FJ; A. Hoisie; and A. Szalay; "Big Data;" Nov./Dec.; pp.10-13. Amerault; C.; see Doyle; J.; Jan./Feb. pp. 31-39. Andrade; Jr.; JS; SDS Reis; EA Oliveira;E. Fehr; and HJ Herrmann; "Ubiquitous Fractal Dimension of Optimal Paths;" Jan./Feb.; pp.74-81. Andreyev; S.; "Into the World of Movable Objects;" July/Aug.; pp. 79-84. B Bagchi; S.; seeHacker; TJ; July/Aug. pp. 67-78. Barrett; SF; "Mentoring and Making a Difference: What Can OnePerson Do?;" Jan./Feb.; pp. 70-73. Behnel; S.; R. Bradshaw; C. Citro; L. Dalcin; DS Seljebotn;and K. Smith; "Cython: The Best of Both Worlds;" Mar./Apr.; pp. 31-39 …,*,*,*
Exploring the Coming Repositories of Repeatable Experiments: Challenges and Opportunities,Juliana Freire; Philippe Bonnet; Dennis Shasha,ABSTRACT Computational repeatability efforts in many communities will soon give rise tovalidated software and data repositories of high quality. A scientist in a field may want toquery the components of such repositories to build new software workflows; perhaps afteradding the scientist's own algorithms. This paper explores research challenges necessary toachieving this goal.,*,*,*
Department of Computer Science State University of New York at Stony Brook,Juliana Freire; Terrance Swift; David S Warren,*,*,*,*
Program Vice-Chairs,Elisa Bertino; Divesh Srivastava; Surajit Chaudhuri; Jiawei Han; Bernhard Mitschang; David Lomet; Keith Jeffery; Raghu Ramakrishnan; Alberto Mendelzon; Hongjun Lu; Amr El Abbadi; Yannis Ioannidis; Karl Aberer; Charu C Aggarwal; Divy Agrawal; Demet Aksoy; Sihem Amer-Yahia; Paolo Atzeni; Daniel Barbara; Roger Barga; Alfonso Cardenas; Barbara Catania; Sang K Cha; Soumen Chakrabarti; Sharma Chakravarthy; Edward Chang; Kevin Chang; Qiming Chen; David W Cheung; Stavros Christodoulakis; Lois Delcambre; Stefan Dessloch; Max Egenhofer; Ahmed Elmagarmid; Martin Ester; Georgios Evangelidis; Mary Fernandez; Daniela Florescu; Juliana Freire; Christoph Freytag,*,*,*,*
PruSM: A Prudent Schema Matching Strategy for Web Forms,Thanh Nguyen; Juliana Freire,ABSTRACT There is an increasing number of data sources on the Web whose contents arehidden and can only be accessed through form interfaces. Several applications haveemerged that aim to automate and simplify the access to such content; from hidden-Webcrawlers and meta-searchers to Web information integration systems. In this paper; we focuson the problem of matching elements in form schemata. Although this problem has beenstudied in the literature; existing approaches have important limitations. Notably; theyassume that the number of forms are small and the form elements are clean and normalized;often through manual pre-processing. Matching form schemata at a large scale presentsnew challenges: data heterogeneity is compounded with the Web-scale. As a result; whenmatching is applied over large and heterogeneous data; the accuracy is greatly reduced …,*,*,*
Using Structure to Discover Types and Relationships for Wikipedia Infoboxes,Thanh Nguyen; Huong Nguyen; Viviane Moreira; Juliana Freire,*,*,*,*
Combining Scheduling Strategies in an SLG Evaluation Extended Abstract,Juliana Freire; David S Warren,Abstract SLG has proven to be an e cient and elegant strategy to evaluate normal logicprograms with respect to the well-founded semantics. Given the exibility tabling provides inthe choice of when to schedule answers; e ciency of evaluation can be further improved bychoosing an appropriate scheduling strategy; that is; how and when answers are returned toconsuming nodes. Several di erent scheduling strategies for SLG have been investigatedincluding Local Scheduling; a strategy that avoids non-productive computation in thepresence of answer subsumption. Local Scheduling can also bene t the evaluation ofprograms with negation; and in this paper we explore an extension of Local Scheduling thatcomputes the well-founded model of normal programs. Even though Local Schedulingperforms well in general; there are cases where it leads to unacceptable performance| the …,*,*,*
Provenance-Enabled Data Exploration and Visualization IEEE VisWeek 2009 Tutorial,Cláudio Silva; Juliana Freire; Emanuele Santos; Erik Anderson,*,*,*,*
Appeared in Proceedings of the 2006 USENIX Annual Technical Conference; Boston; MA; May–Jun. 2006. Integrated Scientific Workflow Management for the Emula...,Eric Eide; Leigh Stoller; Tim Stack; Juliana Freire; Jay Lepreau,Abstract The main forces that shaped current network testbeds were the needs for realismand scale. Now that several testbeds support large and complex experiments; managementof experimentation processes and results has become more difficult and a barrier to high-quality systems research. The popularity of network testbeds means that new tools formanaging experiment workflows; addressing the ready-made base of testbed users; canhave important and significant impacts.,*,*,*
SWF07 Organization,Shiyong Lu; Farshad Fotouhi; Ilkay Altintas; Roger Barga; Adam Barker; Shawn Bowers; Rajkuma Buyya; Hasan Davulcu; Youping Deng; Ian Foster; Juliana Freire; Carole Goble; Jing Hua; Xiaolin Li; Ling Liu; Weisong Shi; Yogesh Simmhan; Ian Taylor; Liqiang Wang; Guizhen Yang; Ping Yang; Zijiang Yang; Yong Zhao; Zhiming Zhao; Hui Zhang; Jing Tie,Shiyong Lu; Wayne State University; USA Farshad Fotouhi; Wayne State University; USA …Ilkay Altintas; San Diego Supercomputer Center; USA Roger Barga; Microsoft Research; USAAdam Barker; University of Edinburgh; UK Shawn Bowers; UC Davis Genome Center; USA RajkumaBuyya; University of Melbourne; Australia Hasan Davulcu; Arizona State University; USA YoupingDeng; University of Southern Mississippi; USA Ian Foster; Argonne National Laboratory & Universityof Chicago; USA Juliana Freire; University of Utah; USA Carole Goble; University ofManchester; UK Jing Hua; Wayne State University; USA Xiaolin Li; Oklahoma StateUniversity; USA Ling Liu; Georgia Institute of Technology; USA Weisong Shi; Wayne StateUniversity; USA Yogesh Simmhan; Indiana University; USA Ian Taylor; Cardiff University; UKLiqiang Wang; University of Wyoming; USA Guizhen Yang; SRI international; USA Ping …,*,*,*
Provenance-Enabled Data Exploration and Visualization with VisTrails Sibgrapi 2010—Tutorial Submission,Cláudio Silva; Juliana Freire; Emanuele Santos; Erik Anderson,Abstract—Scientists are now faced with an incredible volume of data to analyze. To exploreand understand the data; they need to assemble complex workflows (pipelines) tomanipulate the data and create insightful visual representations. Provenance is essential inthis process. The provenance of a digital artifact contains information about the process anddata used to derive the artifact. This information is essential for preserving the data; fordetermining the data's quality and authorship; for both reproducing and validating results–allimportant elements of the scientific process. Provenance has shown to be particularly usefulfor enabling comparative visualization and data analysis. This tutorial will informcomputational and visualization scientists; users and developers about different approachesto provenance and the trade-offs among them. Using the VisTrails project as a basis; we …,*,*,*
North East DB/IR Day,Juliana Freire,Computing has been an enormous accelerator to science and it has led to an informationexplosion in many different fields. The unprecedented volume of data acquired by sensors;derived by simulations and analysis processes; and shared on the Web opens up newopportunities; but it also creates many challenges when it comes to managing and makingsense out of these data. In this talk; I discuss the importance of maintaining detailedprovenance (also referred to as lineage and pedigree) for digital data. Provenance providesimportant documentation that is key to preserve data; to determine the data's quality andauthorship; to understand; reproduce; as well as validate results. I will review some of thestate-of-the-art techniques; as well as research challenges and open problems involved inmanaging provenance throughout the data life cycle. I will also discuss benefits of …,*,*,*
Calls for Papers ls for Papers alls for Papers Calls for Papers alls for Papers lls for Papers ls for Papers Calls for Papers lls for Papers Calls for Papers alls for Papers...,Francisco Curbera; Juliana Freire,All submissions must be original manuscripts of fewer than 5;000 words; focused on Internettechnologies and implementations. All manuscripts are subject to peer review on both technicalmerit and relevance to IC's international readership — primarily system and software designengineers. We do not accept white papers; and we discourage strictly theoretical or mathematicalpapers. To submit a manuscript; please log on to Manuscript Central (https://mc.manuscriptcentral.com:443/ic-cs) to create or access an account; which you can use to logon to IC's Author Center and upload your submission … Calls for Papers ls for Papers alls forPapers Calls for Papers alls for Papers lls for Papers ls for Papers Calls for Papers lls for PapersCalls for Papers alls for Papers Calls for Papers alls for Papers ls for Papers … Submit a manuscripton Manuscript Central at https://mc.manuscriptcentral.com:443/ic-cs …,*,*,*
North East DB/IR Day,Renee Miller; Doug Oard; Juliana Freire,North East DB/IR Day. October 22; 2010 AT&T Shannon Labs Building 103; 180 Park AvenueFlorham Park; NJ. Organizers: Graham Cormode; AT & T Research; graham atresearch.att.com Srinivas Bangalore; AT&T Research; srini at research.att.com Sponsoredby DIMACS and AT&T Workshop Program: 10:30 - 10:55 Welcome and Introductions 10:55 -12:00 On Schema Discovery Renee Miller; University of Toronto 12:00 - 1:45 Lunch; postersession 1:45 - 2:50 Who 'Dat? Identity resolution in large email collections Doug Oard;University of Maryland 2:50 - 3:10 Break 3:10 - 4:15 Provenance-Rich Science Juliana Freire;University of Utah 4:15 Close Friday; October 22; 2010 10:30am - 4:00pm Poster Session --details TBA Previous: Participation Next: Registration Workshop Index DIMACS HomepageContacting the Center Document last modified on Ocotber 22; 2010.,*,*,*
SIBGRAPI-T 2010,Luciano Silva; Manuel Menezes de Oliveira Neto,It includes different tutorials types (regular or hands-on lab) and levels (elementary andintermediate) that cover fundamental and emerging topics in Computer Vision; ImageProcessing; Computer Graphics; and Pattern Recognition areas. Within the context and scopeof the tutorials included in this proceedings one can find: the development of applications in computergraphics and image processing for smart- phones; manipulation of data to create visualrepresentations; design and management of immersive multi-projection systems; and developmentof sketch-based visual interfaces. We would like to thank Universidade Federal do Rio Grandedo Sul (UFRGS) for providing financial support; classrooms; and labs for this event. We alsowould like to acknowledge the support from all sponsors; including the Brazilian Computer Society(SBC); CNPq; and CAPES. Finally; we thank all tutorial authors; reviewers; and the …,*,*,*
Steering and Program Committees,Ricardo Baeza-Yates; Luis Olsina; Alberto Mendelzon; Alfredo Sanchez; Alvaro Arenas; Andrea Zisman; Arno Scharl; Athena Vakali; Bebo White; Cesar Collazos; Claudia Pons; Claudio Gutierrez; Daniel Schwabe; David Lowe; Emilia Mendes; Enrico Motta; Ethan Munson; Gastón Mousques; Geert-Jan Houben; Gustavo Rossi; Horacio Leone; Jean Vanderdonckt; Jesus Favela; João Falcão e Cunha; Juliana Freire; Luisa Mich; Maria da Graça Pimentel; Mario Piattini; Maristella Matera; Martin Gaedke; Monica Scannapieco; Nora Koch; Olga De Troyer,Oscar Pastor; Valencia University of Technology (Spain) Peter Dolog; L3S Research Center(Germany) Piero Fraternali; Politecnico di Milano (Italy) Ray Welland; University of Glasgow(UK) Ricardo Baeza-Yates; University of Chile (Chile) Ricardo Falbo; Federal University of EspiritoSanto (Brazil) San Murugesan; Southern Cross University (Australia) Sandro Morasca; Universityof Insubria (Italy) Silvia Abrahão; Valencia University of Technology (Spain) Stefan Decker;DERI; Galway (Ireland) Steven Furnell; University of Plymouth (UK) Symeon Retalis; Universityof Piraeus (Greece) Virgilio Almeida; Federal University of Minas Gerais (Brazil) YogeshDeshpande; UWS (Australia) Wieland Schwinger; SCC Hagenberg (Austria),*,*,*
Vincent Breton; NCAR John Brooke; University of Manchester; United Kingdom Kris Bubendorfe; Victoria University of Wellington; New Zealand Kevin Burrage; Oxfo...,Sasha Buzko; Annamaria Carusi; Donatella Castelli; Kum Won Cho; Keith Cole; Simon Coles; Katie Cooper; Simon Cox; Patricia Cruse; Vasa Curcin; Arun Datta; Stephen Downie; Gillian Elliott; David Emerson; Tom Finholt; Geoffrey Fox; Juliana Freire; Alberto Garcia; Wolfgang Gentzsch; Anna Gerber; Madhusudhan Govindaraju; Lutz Gross; Ken Hawick; Mark Hedges; Margaret Henty; Frederic Herman; Marty Humphrey,Ilkay Altintas; San Diego Supercomputing Centre; USA Rommie Amaro; University ofCalifornia; Irvine; USA Sheila Anderson; Kings College London; United Kingdom AndreasAschenbrenner; University of Goettingen; Germany Kevin Ashley; University of Pittsburgh; USAMalcolm Atkinson; National UK e-Science Centre; United Kingdom David Bainbridge; Universityof Waikato; New Zealand Henri Bal; University of Vienna; Austria Adam Barker; University ofMelbourne; Australia Chaitan Baru; San Diego Supercomputing Centre; USA ConradBessant; University of Cranfield; United Kingdom Karan Bhatia; San Diego SupercomputingCentre; USA John Blower; University of Reading; United Kingdom Paul Bonnington; MonashUniversity; Australia Shawn Bowers; University of California; Davis; USA Vincent Breton; NCARJohn Brooke; University of Manchester; United Kingdom Kris Bubendorfe; Victoria …,*,*,*
Abiteboul; S. 41 Aggarwal; CC 261;593 Agrawal; D 93; 274;496;639 Agrawal; S. 5,M Akinde; S AI-Khalifa; G Alonso; M Areal; WG Aref; V Atluri; I Atmosukarto; D Baker; R Barga; K Barker; B Benatallah; G Bhalotia; HE Blok; M Bohlen; A Bonifati; D Braga; S Bressan; N Bruno; F Buccafurri; A Campi; F Casati; AC Catlin; S Ceri; S Chakrabarti; NH Chan; S Chaudhuri; B Chen; CM Chen; J Chen; MS Chen; F Chiu; J Cho; HD Chon; L Cohen; B Cooper; R Cordova; G Cormode; G Das; S Davey; U Daya; S Decker; A Descour; A Deshpande; J Desmarais; DJ DeWitt; A Doan; M Dumas; J Dunn; MG Elfeky; CJ El1mann; AK Elmagarmid; R Elmasri; C Fa1outsos; J Fan; A Faradjian; P Felber; J Feng; S Flesca; I Foudos; J Freire; AW Fu; F Furfaro; A Gal; H Garcia-Molina; M Garofalakis; J Gehrke; D Georgakopoulos; M Gertz; A Goel,333 369 264 331 129 567; 685 ; 673 ; 490 ; 266 ; 271 263 29 393 490 212;276 716 498 492271 498 431 176 490 605 29 141 278;463 265; 663 335 488 262 ; 494 ; 266 ; 166 129 309 309333 494 275 673 309 141;567;605 309 262 453 706 329 … 267 269 176 583 269 617 268; 268 543 269;327 155;331;335 155 697 555 507 279 333 267 583 41;369 490 271 29 117 331685 333 212;498 685 685 ; 245 605 333 270 431 529 345;697 271 ;498 273 583 272 297 ;485685 274 275 333 265;663 327 … Ounopulos; D PUO; J. ..; Gtirel; A. Haas; L. ãas; p .J.Haclgtimti; H. I … ¥alevy; A. ãmmad; M. ¥aritsa; JR ¥ellerstein; J .M … Lee; D. Lee; MLLehner; W Leung; CK-S Ling; T. W ' … Ling; Y. Liu; B Liu; J Lomet; D. Low; WL Lu; H. ; Lu; JXLuo; G. Madden; S. Madhyastha; T. Maier; D. Major; G. Mani; M. Mannila; H Marian; A.,*,*,*
Using Logic Programming to E ciently Evaluate Recursive Queries,Juliana Freire,Much of the success of the relational database model can be attributed to thedeclarativeness of its query language SQL. Unfortunately; SQL is not expressive enough.There are many useful queries (eg; queries that involve recursion) which cannot beexpressed in this language. Usually; when one wants to reason about the contents of adatabase; it is necessary to leave the relational model by embedding SQL into a lower-levellanguage such as C; which results in an impedance mismatch and consequent loss ofdeclarativeness. Deductive databases 4; 5] address this problem by adopting logicprogramming or a restriction such as Datalog 11] as the query language. Prolog engineshave been used to evaluate Datalog queries; but they have proven to be unacceptable fordata-oriented queries for two major reasons: their poor termination and complexity …,*,*,*
Mesa: A Search Engine for Querying Web Tables,Sergio Mergen; Juliana Freire; Carlos Heuser,Abstract. The volume of structured data on the Web has grown considerably in the recentpast. In contrast to unstructured (textual) documents; which can be searched through simplekeyword-based interfaces; the presence of structure enables rich queries to be posedagainst Web data. In this paper we present a search engine designed for queryingstructured information sources on the Web and show how our system can support on-the-fly;complex queries over content published in hundreds HTML tables.,*,*,*
