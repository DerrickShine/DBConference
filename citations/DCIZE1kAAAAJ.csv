MonetDB/X100: Hyper-Pipelining Query Execution.,Peter A Boncz; Marcin Zukowski; Niels Nes,Abstract Database systems tend to achieve only low IPC (instructions-per-cycle) efficiencyon modern CPUs in compute-intensive application areas like decision support; OLAP andmultimedia retrieval. This paper starts with an in-depth investigation to the reason why thishappens; focusing on the TPC-H benchmark. Our analysis of various relational systems andMonetDB leads us to a new set of guidelines for designing a query processor. The secondpart of the paper describes the architecture of our new X100 query engine for the MonetDBsystem that follows these guidelines. On the surface; it resembles a classical Volcano-styleengine; but the crucial difference to base all execution on the concept of vector processingmakes it highly CPU efficient. We evaluate the power of MonetDB/X100 on the 100GBversion of TPC-H; showing its raw execution power to be between one and two orders of …,Cidr,2005,523
Database architecture optimized for the new bottleneck: Memory access,Peter A Boncz; Stefan Manegold; Martin L Kersten,Abstract In the past decade; advances in speed of commodity CPUs have far out-pacedadvances in memory latency. Main-memory access is therefore increasingly a performancebottleneck for many computer applications; including database systems. In this article; weuse a simple scan test to show the severe impact of this bottleneck. The insights gained aretranslated into guidelines for database architecture; in terms of both data structures andalgorithms. We discuss how vertically fragmented data structures optimize cacheperformance on sequential data access. We then focus on equi-join; typically a random-access operation; and introduce radix algorithms for partitioned hash-join. The performanceof these algorithms is quantified using a detailed analytical model that incorporates memoryaccess cost. Experiments that validate this model were performed on the Monet database …,VLDB,1999,410
Super-scalar RAM-CPU cache compression,Marcin Zukowski; Sandor Heman; Niels Nes; Peter Boncz,High-performance data-intensive query processing tasks like OLAP; data mining or scientificdata analysis can be severely I/O bound; even when high-end RAID storage systems areused. Compression can alleviate this bottleneck only if encoding and decoding speedssignificantly exceed RAID I/O bandwidth. For this purpose; we propose three new versatilecompression schemes (PDICT; PFOR; and PFOR-DELTA) that are specifically designed toextract maximum IPC from modern CPUs. We compare these algorithms with compressiontechniques used in (commercial) database and information retrieval systems. Ourexperiments on the MonetDB/X100 database system; using both DSM and PAX diskstorage; show that these techniques strongly accelerate TPC-H performance to the point thatthe I/O bottleneck is eliminated.,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,406
MonetDB/XQuery: a fast XQuery processor powered by a relational engine,Peter Boncz; Torsten Grust; Maurice Van Keulen; Stefan Manegold; Jan Rittinger; Jens Teubner,Abstract Relational XQuery systems try to re-use mature relational data managementinfrastructures to create fast and scalable XML database technology. This paper describesthe main features; key contributions; and lessons learned while implementing such asystem. Its architecture consists of (i) a range-based encoding of XML documents intorelational tables;(ii) a compilation technique that translates XQuery into a basic relationalalgebra;(iii) a restricted (order) property-aware peephole relational query optimizationstrategy; and (iv) a mapping from XML update statements into relational updates. Thus; thissystem implements all essential XML database functionalities (rather than a single feature)such that we can learn from the full consequences of our architectural decisions. Whileimplementing this system; we had to extend the state-of-the-art with a number of new …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,360
Column-oriented database systems,Daniel J Abadi; Peter A Boncz; Stavros Harizopoulos,Abstract Column-oriented database systems (column-stores) have attracted a lot of attentionin the past few years. Column-stores; in a nutshell; store each database table columnseparately; with attribute values belonging to the same column stored contiguously;compressed; and densely packed; as opposed to traditional database systems that storeentire records (rows) one after the other. Reading a subset of a table's columns becomesfaster; at the potential expense of excessive disk-head seeking from column to column forscattered reads or updates. After several dozens of research papers and at least a dozen ofnew column-store start-ups; several questions remain. Are these a new breed of systems orsimply old wine in new bottles? How easily can a major row-based system achieve column-store performance? Are column-stores the answer to effortlessly support large-scale data …,Proceedings of the VLDB Endowment,2009,242
Breaking the memory wall in MonetDB,Peter A Boncz; Martin L Kersten; Stefan Manegold,Abstract In the past decades; advances in speed of commodity CPUs have far outpacedadvances in RAM latency. Main-memory access has therefore become a performancebottleneck for many computer applications; a phenomenon that is widely known as the"memory wall." In this paper; we report how research around the MonetDB database systemhas led to a redesign of database architecture in order to take advantage of modernhardware; and in particular to avoid hitting the memory wall. This encompasses (i) aredesign of the query execution model to better exploit pipelined CPU architectures andCPU instruction caches;(ii) the use of columnar rather than row-wise data storage to betterexploit CPU data caches;(iii) the design of new cache-conscious query processingalgorithms; and (iv) the design and automatic calibration of memory cost models to …,Communications of the ACM,2008,217
Monet: A next-generation DBMS kernel for query-intensive applications,Peter Alexander Boncz,3.11 Monet Goals 21 3.22 Relational Performance Problems 22 3.2. 11 Problem 1: Column-Access to Row-Storage 23 3.2. 22 Problem 2: Commodity Hardware Has Changed 24 3.33Monet Architecture 27 3.3. 11 idea 1: provide DBMS back-end functionality 27 3.3. 22 idea 2:you can do everything with just binary tables 29 3.3. 33 idea 3: do not re-invent the OS 333.3. 44 idea 4: optimize main-memory query execution 34 3.44 Conclusion 38,*,2002,200
The meaningful use of big data: four perspectives--four challenges,Christian Bizer; Peter Boncz; Michael L Brodie; Orri Erling,Abstract Twenty-five Semantic Web and Database researchers met at the 2011 STISemantic Summit in Riga; Latvia July 6-8; 2011 [1] to discuss the opportunities andchallenges posed by Big Data for the Semantic Web; Semantic Technologies; and Databasecommunities. The unanimous conclusion was that the greatest shared challenge was notonly engineering Big Data; but also doing so meaningfully. The following are fourexpressions of that challenge from different perspectives.,Acm Sigmod Record,2012,197
Optimizing main-memory join on modern hardware,Stefan Manegold; Peter Boncz; Martin Kersten,In the past decade; the exponential growth in commodity CPU's speed has far outpacedadvances in memory latency. A second trend is that CPU performance advances are notonly brought by increased clock rates; but also by increasing parallelism inside the CPU.Current database systems have not yet adapted to these trends and show poor utilization ofboth CPU and memory resources on current hardware. In this paper; we show how theseresources can be optimized for large joins and translate these insights into guidelines forfuture database architectures; encompassing data structures; algorithms; cost modeling andimplementation. In particular; we discuss how vertically fragmented data structures optimizecache performance on sequential data access. On the algorithmic side; we refine thepartitioned hash-join with a new partitioning algorithm called" radix-cluster"; which is …,IEEE Transactions on Knowledge and Data Engineering,2002,167
MIL primitives for querying a fragmented world,Peter A Boncz; Martin L Kersten,Abstract. In query-intensive database application areas; like decision support and datamining; systems that use vertical fragmentation have a significant performance advantage. Inorder to support relational or object oriented applications on top of such a fragmented datamodel; a flexible yet powerful intermediate language is needed. This problem has beensuccessfully tackled in Monet; a modern extensible database kernel developed by ourgroup. We focus on the design choices made in the Monet interpreter language (MIL); itsalgebraic query language; and outline how its concept of tactical optimization enhances andsimplifies the optimization of complex queries. Finally; we summarize the experience gainedin Monet by creating a highly efficient implementation of MIL.,The VLDB Journal,1999,167
Database architecture evolution: Mammals flourished long before dinosaurs became extinct,Stefan Manegold; Martin L Kersten; Peter Boncz,Abstract The holy grail for database architecture research is to find a solution that is Scalable& Speedy; to run on anything from small ARM processors up to globally distributed computeclusters; Stable & Secure; to service a broad user community; Small & Simple; to becomprehensible to a small team of programmers; Self-managing; to let it run out-of-the-boxwithout hassle.,Proceedings of the VLDB Endowment,2009,152
Optimizing database architecture for the new bottleneck: memory access,Stefan Manegold; Peter A Boncz; Martin L Kersten,Abstract In the past decade; advances in the speed of commodity CPUs have far out-pacedadvances in memory latency. Main-memory access is therefore increasingly a performancebottleneck for many computer applications; including database systems. In this article; weuse a simple scan test to show the severe impact of this bottleneck. The insights gained aretranslated into guidelines for database architecture; in terms of both data structures andalgorithms. We discuss how vertically fragmented data structures optimize cacheperformance on sequential data access. We then focus on equi-join; typically a random-access operation; and introduce radix algorithms for partitioned hash-join. The performanceof these algorithms is quantified using a detailed analytical model that incorporates memoryaccess cost. Experiments that validate this model were performed on the Monet database …,The VLDB Journal—The International Journal on Very Large Data Bases,2000,146
Generic database cost models for hierarchical memory systems,Stefan Manegold; Peter Boncz; Martin L Kersten,This chapter proposes a generic technique to create accurate cost functions for databaseoperations. Accurate prediction of operator execution time is a prerequisite for databasequery optimization. Although extensively studied for conventional disk-based DBMSs; costmodeling in main memory DBMSs is still an open issue. Recent database research hasdemonstrated that memory access is more and more becoming a significant—if not themajor—cost component of database operations. If used properly; fast but small cachememories—usually organized in cascading hierarchy between CPU and main memory—can help to reduce memory access costs. However; they make the cost estimation problemmore complex. Database cost models provide the foundation for query optimizers to derivean efficient execution plan. Such models consist of two parts; a logical and a physical …,*,2002,140
MonetDB/X100-A DBMS In The CPU Cache.,Marcin Zukowski; Peter A Boncz; Niels Nes; Sándor Héman,Abstract X100 is a new execution engine for the MonetDB system; that improves executionspeed and overcomes its main memory limitation. It introduces the concept of in-cachevectorized processing that strikes a balance between the existing column-at-a-time MILexecution primitives of MonetDB and the tuple-at-a-time Volcano pipelining model; avoidingtheir drawbacks: intermediate result materialization and large interpretation overhead;respectively. We explain how the new query engine makes better use of cache memories aswell as parallel computation resources of modern super-scalar CPUs. MonetDB/X100 canbe one to two orders of magnitude faster than commercial DBMSs and close to hand-codedC programs for computationally intensive queries on in-memory datasets. To address largerdisk-based datasets with the same efficiency; a new ColumnBM storage layer is …,IEEE Data Eng. Bull.,2005,135
Cooperative scans: dynamic bandwidth sharing in a DBMS,Marcin Zukowski; Sándor Héman; Niels Nes; Peter Boncz,Abstract This paper analyzes the performance of concurrent (index) scan operations in bothrecord (NSM/PAX) and column (DSM) disk storage models and shows that existingscheduling policies do not fully exploit data-sharing opportunities and therefore result inpoor disk bandwidth utilization. We propose the Cooperative Scans framework thatenhances performance in such scenarios by improving data-sharing between concurrentscans. It performs dynamic scheduling of queries and their data requests; taking into accountthe current system situation. We first present results on top of an NSM/PAX storage layout;showing that it achieves significant performance improvements over traditional policies interms of both the number of I/Os and overall execution time; as well as latency of individualqueries. We provide benchmarks with varying system parameters; data sizes and query …,Proceedings of the 33rd international conference on Very large data bases,2007,128
The design and implementation of modern column-oriented database systems,Daniel Abadi; Peter Boncz; Stavros Harizopoulos; Stratos Idreos; Samuel Madden,Abstract In this article; we survey recent research on column-oriented database systems; orcolumn-stores; where each attribute of a table is stored in a separate file or region onstorage. Such databases have seen a resurgence in recent years with a rise in interest inanalytic queries that perform scans and aggregates over large portions of a few columns of atable. The main advantage of a column-store is that it can access just the columns needed toanswer such queries. We specifically focus on three influential research prototypes;MonetDB [46]; MonetDB/X100 [18]; and C-Store [86]. These systems have formed the basisfor several well-known commercial column-store implementations. We describe theirsimilarities and differences and discuss their specific architectural features for compression;late materialization; join processing; vectorization and adaptive indexing (database …,Foundations and Trends® in Databases,2013,124
Monet. An impressionist sketch of an advanced database system,Peter A Boncz; Martin L Kersten,Abstract Monet is a customizable database system developed at CWI and University ofAmsterdam; intended to be used as the database backend for widely varying applicationdomains. It is designed to get maximum database performance out of today's workstationsand multiprocessor systems. It has already achieved considerable success in supporting aData Mining application [12; 13]; and work is well under way in a project where it is used in ahigh-end GIS application. Monet is a type-and algebra-extensible database system andemploys shared memory parallelism. In this paper; we give the goals and motivation ofMonet; and outline its architectural features; including its use of the Decomposed StorageModel (DSM); emphasis on bulk operations; use of main virtual-memory and servercustomization. As a case example; we discuss some issues on how to build a GIS on top …,In Proc. IEEE BIWIT workshop,1994,121
Flattening an object algebra to provide performance,Peter Boncz; AN Wilshut; Martin L Kersten,Algebraic transformation and optimization techniques have been the method of choice inrelational query execution; but applying them in object-oriented (OO) DBMSs is difficult dueto the complexity of OO query languages. This paper demonstrates that the problem can besimplified by mapping an OO data model to the binary relational model implemented byMonet; a state-of-the-art database kernel. We present a generic mapping scheme to flattendata models and study the case of straightforward OO model. We show how flatteningenabled us to implement a query algebra; using only a very limited set of simple operations.The required primitives and query execution strategies are discussed; and their performanceis evaluated on the 1-GByte TPC-D (Transaction-processing Performance Council'sBenchmark D); showing that our divide-and-conquer approach yields excellent results.,Data Engineering; 1998. Proceedings.; 14th International Conference on,1998,100
Linked stream data processing engines: Facts and figures,Danh Le-Phuoc; Minh Dao-Tran; Minh-Duc Pham; Peter Boncz; Thomas Eiter; Michael Fink,Abstract Linked Stream Data; ie; the RDF data model extended for representing stream datagenerated from sensors social network applications; is gaining popularity. This hasmotivated considerable work on developing corresponding data models associated withprocessing engines. However; current implemented engines have not been thoroughlyevaluated to assess their capabilities. For reasonable systematic evaluations; in this workwe propose a novel; customizable evaluation framework and a corresponding methodologyfor realistic data generation; system testing; and result analysis. Based on this evaluationenvironment; extensive experiments have been conducted in order to compare the state-of-the-art LSD engines wrt. qualitative and quantitative properties; taking into account theunderlying principles of stream processing. Consequently; we provide a detailed analysis …,International Semantic Web Conference,2012,96
Morsel-driven parallelism: a NUMA-aware query evaluation framework for the many-core age,Viktor Leis; Peter Boncz; Alfons Kemper; Thomas Neumann,Abstract With modern computer architecture evolving; two problems conspire against thestate-of-the-art approaches in parallel query execution:(i) to take advantage of many-cores;all query work must be distributed evenly among (soon) hundreds of threads in order toachieve good speedup; yet (ii) dividing the work evenly is difficult even with accurate datastatistics due to the complexity of modern out-of-order cores. As a result; the existingapproaches for plan-driven parallelism run into load balancing and context-switchingbottlenecks; and therefore no longer scale. A third problem faced by many-core architecturesis the decentralization of memory controllers; which leads to Non-Uniform Memory Access(NUMA). In response; we present the morsel-driven query execution framework; wherescheduling becomes a fine-grained run-time task that is NUMA-aware. Morsel-driven …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,93
The LDBC social network benchmark: Interactive workload,Orri Erling; Alex Averbuch; Josep Larriba-Pey; Hassan Chafi; Andrey Gubichev; Arnau Prat; Minh-Duc Pham; Peter Boncz,Abstract The Linked Data Benchmark Council (LDBC) is now two years underway and hasgathered strong industrial participation for its mission to establish benchmarks; andbenchmarking practices for evaluating graph data management systems. The LDBCintroduced a new choke-point driven methodology for developing benchmark workloads;which combines user input with input from expert systems architects; which we outline. Thispaper describes the LDBC Social Network Benchmark (SNB); and presents databasebenchmarking innovation in terms of graph query functionality tested; correlated graphgeneration techniques; as well as a scalable benchmark driver on a workload with complexgraph dependencies. SNB has three query workloads under development: Interactive;Business Intelligence; and Graph Algorithms. We describe the SNB Interactive Workload …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,90
What happens during a join? Dissecting CPU and memory optimization effects,Stefan Manegold; Peter A Boncz; Martin L Kersten,*,Proceedings of the 26th international conference on very large data bases,2000,78
Heuristics-based query optimisation for SPARQL,Petros Tsialiamanis; Lefteris Sidirourgos; Irini Fundulaki; Vassilis Christophides; Peter Boncz,Abstract Query optimization in RDF Stores is a challenging problem as SPARQL queriestypically contain many more joins than equivalent relational plans; and hence lead to a largejoin order search space. In such cases; cost-based query optimization often is not possible.One practical reason for this is that statistics typically are missing in web scale setting suchas the Linked Open Datasets (LOD). The more profound reason is that due to the absence ofschematic structure in RDF; join-hit ratio estimation requires complicated forms of correlatedjoin statistics; and currently there are no methods to identify the relevant correlationsbeforehand. For this reason; the use of good heuristics is essential in SPARQL queryoptimization; even in the case that are partially used with cost-based statistics (ie; hybridquery optimization). In this paper we describe a set of useful heuristics for SPARQL query …,Proceedings of the 15th International Conference on Extending Database Technology,2012,77
Positional update handling in column stores,Sándor Héman; Marcin Zukowski; Niels J Nes; Lefteris Sidirourgos; Peter Boncz,Abstract In this paper we investigate techniques that allow for on-line updates to columnardatabases; leaving intact their high read-only performance. Rather than keeping differentialstructures organized by the table key values; the core proposition of this paper is that thiscan better be done by keeping track of the tuple position of the modifications. Not only doesthis minimize the computational overhead of merging in differences into read-only queries;but this makes the differential structure oblivious of the value of the order keys; allowing it toavoid disk I/O for retrieving the order keys in read-only queries that otherwise do not needthem-a crucial advantage for a column-store. We describe a new data structure formaintaining such positional updates; called the Positional Delta Tree (PDT); and describedetailed algorithms for PDT/column merging; updating PDTs; and for using PDTs in …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,70
AmbientDB: relational query processing in a P2P network,Peter Boncz; Caspar Treijtel,Abstract A new generation of applications running on a network of nodes; that share data onan ad-hoc basis; will benefit from data management services including powerful queryingfacilities. In this paper; we introduce the goals; assumptions and architecture of AmbientDB;a new peer-to-peer (P2P) DBMS prototype developed at CWI. Our focus is on the queryprocessing facilities of AmbientDB; that are based on a three-level translation of a globalquery algebra into multi-wave stream processing plans; distributed over an ad-hoc P2Pnetwork. We illustrate the usefulness of our system by outlining how it eases construction ofa music player that generates intelligent playlists with collaborative filtering over distributedmusic logs. Finally; we show how the use of Distributed Hash Tables (DHT) at the basis ofAmbientDB allows applications like the P2P music player to scale to large amounts of …,International Workshop on Databases; Information Systems; and Peer-to-Peer Computing,2003,65
XIRAF–XML-based indexing and querying for digital forensics,Wouter Alink; RAF Bhoedjang; Peter A Boncz; Arjen P de Vries,Abstract This paper describes a novel; XML-based approach towards managing andquerying forensic traces extracted from digital evidence. This approach has beenimplemented in XIRAF; a prototype system for forensic analysis. XIRAF systematicallyapplies forensic analysis tools to evidence files (eg; hard disk images). Each tool producesstructured XML annotations that can refer to regions (byte ranges) in an evidence file. XIRAFstores such annotations in an XML database; which allows us to query the annotations usinga single; powerful query language (XQuery). XIRAF provides the forensic investigator with arich query environment in which browsing; searching; and predefined query templates areall expressed in terms of XML database queries.,digital investigation,2006,62
Vectorization vs. compilation in query execution,Juliusz Sompolski; Marcin Zukowski; Peter Boncz,Abstract Compiling database queries into executable (sub-) programs provides substantialbenefits comparing to traditional interpreted execution. Many of these benefits; such asreduced interpretation overhead; better instruction code locality; and providing opportunitiesto use SIMD instructions; have previously been provided by redesigning query processors touse a vectorized execution model. In this paper; we try to shed light on the question of howstate-of-the-art compilation strategies relate to vectorized execution for analytical databaseworkloads on modern CPUs. For this purpose; we carefully investigate the behavior ofvectorized and compiled strategies inside the Ingres VectorWise database system in threeuse cases: Project; Select and Hash Join. One of the findings is that compilation shouldalways be combined with block-wise query execution. Another contribution is identifying …,Proceedings of the Seventh International Workshop on Data Management on New Hardware,2011,59
Sciborq: Scientific data management with bounds on runtime and quality,Lefteris Sidirourgos; PA Boncz; ML Kersten,ABSTRACT Data warehouses underlying virtual observatories stress the capabilities ofdatabase management systems in many ways. They are filled; on a daily basis; with largeamounts of factual information derived from intensive data scrubbing and computationalfeature extraction pipelines. The predominant data processing techniques focus on parallelloads and map-reduce feature extraction algorithms. Querying these huge databasesrequire a sizable computing cluster; while ideally the initial investigation should runinteractively; using as few resources as possible. In this paper; we explore a different route;one based on the observation that at any given time only a fraction of the data is of primaryvalue for a specific task. This fraction becomes the focus of scientific reflection through aniterative process of ad-hoc query refinement. Steering through data to facilitate scientific …,*,2011,59
Architecture-conscious hashing,Marcin Zukowski; Sándor Héman; Peter Boncz,Abstract Hashing is one of the fundamental techniques used to implement query processingoperators such as grouping; aggregation and join. This paper studies the interactionbetween modern computer architecture and hash-based query processing techniques. First;we focus on extracting maximum hashing performance from super-scalar CPUs. Inparticular; we discuss fast hash functions; ways to efficiently handle multi-column keys andpropose the use of a recently introduced hashing scheme called Cuckoo Hashing over thecommonly used bucket-chained hashing. In the second part of the paper; we focus on theCPU cache usage; by dynamically partitioning data streams such that the partial hash tablesfit in the CPU cache. Conventional partitioning works as a separate preparatory phase;forcing materialization; which may require I/O if the stream does not fit in RAM. We …,Proceedings of the 2nd international workshop on Data management on new hardware,2006,59
How good are query optimizers; really?,Viktor Leis; Andrey Gubichev; Atanas Mirchev; Peter Boncz; Alfons Kemper; Thomas Neumann,Abstract Finding a good join order is crucial for query performance. In this paper; weintroduce the Join Order Benchmark (JOB) and experimentally revisit the main componentsin the classic query optimizer architecture using a complex; real-world data set and realisticmulti-join queries. We investigate the quality of industrial-strength cardinality estimators andfind that all estimators routinely produce large errors. We further show that while estimatesare essential for finding a good join order; query performance is unsatisfactory if the queryengine relies too heavily on these estimates. Using another set of experiments that measurethe impact of the cost model; we find that it has much less influence on query performancethan the cardinality estimates. Finally; we investigate plan enumeration techniquescomparing exhaustive dynamic programming with heuristic algorithms and find that …,Proceedings of the VLDB Endowment,2015,58
Pathfinder: XQuery---the relational way,Peter Boncz; Torsten Grust; Maurice van Keulen; Stefan Manegold; Jan Rittinger; Jens Teubner,Abstract Relational query processors are probably the best understood (as well as the bestengineered) query engines available today. Although carefully tuned to process instances ofthe relational model (tables of tuples); these processors can also provide a foundation forthe evaluation of" alien"(non-relational) query languages: if a relational encoding of the aliendata model and its associated query language is given; the RDBMS may act like a special-purpose processor for the new language.,Proceedings of the 31st international conference on Very large data bases,2005,57
DSM vs. NSM: CPU performance tradeoffs in block-oriented query processing,Marcin Zukowski; Niels Nes; Peter Boncz,Abstract Comparisons between the merits of row-wise storage (NSM) and columnar storage(DSM) are typically made with respect to the persistent storage layer of database systems. Inthis paper; however; we focus on the CPU efficiency tradeoffs of tuple representations insidethe query execution engine; while tuples flow through a processing pipeline. We analyze theperformance in the context of query engines using so-called" block-oriented" processing--arecently popularized technique that can strongly improve the CPU efficiency. With this highefficiency; the performance trade-offs between NSM and DSM can have a decisive impacton the query execution performance; as we demonstrate using both microbenchmarks andTPC-H query 1. This means that NSM-based database systems can sometimes benefit fromconverting tuples into DSM on-the-fly; and vice versa.,Proceedings of the 4th international workshop on Data management on new hardware,2008,56
S3g2: A scalable structure-correlated social graph generator,Minh-Duc Pham; Peter Boncz; Orri Erling,Abstract Benchmarking graph-oriented database workloads and graph-oriented databasesystems is increasingly becoming relevant in analytical Big Data tasks; such as socialnetwork analysis. In graph data; structure is not mainly found inside the nodes; butespecially in the way nodes happen to be connected; ie structural correlations. Becausesuch structural correlations determine join fan-outs experienced by graph analysisalgorithms and graph query executors; they are an essential; yet typically neglected;ingredient of synthetic graph generators. To address this; we present S3G2: a ScalableStructure-correlated Social Graph Generator. This graph generator creates a synthetic socialgraph; containing non-uniform value distributions and structural correlations; which isintended as test data for scalable graph analysis algorithms and graph database systems …,Technology Conference on Performance Evaluation and Benchmarking,2012,55
Vectorwise: Beyond column stores,Marcin Zukowski; Peter A Boncz,Abstract This paper tells the story of Vectorwise; a high-performance analytical databasesystem; from multiple perspectives: its history from academic project to commercial product;the evolution of its technical architecture; customer reactions to the product and its futureresearch and development roadmap. One take-away from this story is that the novelty inVectorwise is much more than just column-storage: it boasts many query processinginnovations in its vectorized execution model; and an adaptive mixed row/column datastorage model with indexing support tailored to analytical workloads. Another one is thatthere is a long road from research prototype to commercial product; though databaseresearch continues to achieve a strong innovative influence on product development.,*,2012,52
Flexible and efficient IR using array databases,Roberto Cornacchia; Sándor Héman; Marcin Zukowski; Arjen P Vries; Peter Boncz,Abstract The Matrix Framework is a recent proposal by Information Retrieval (IR)researchers to flexibly represent information retrieval models and concepts in a single multi-dimensional array framework. We provide computational support for exactly this frameworkwith the array database system SRAM (Sparse Relational Array Mapping); that works on topof a DBMS. Information retrieval models can be specified in its comprehension-based arrayquery language; in a way that directly corresponds to the underlying mathematical formulas.SRAM efficiently stores sparse arrays in (compressed) relational tables and translates andoptimizes array queries into relational queries. In this work; we describe a number of arrayquery optimization rules. To demonstrate their effect on text retrieval; we apply them in theTREC TeraByte track (TREC-TB) efficiency task; using the Okapi BM25 model as our …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,52
Vectorized data processing on the cell broadband engine,Sándor Héman; Niels Nes; Marcin Zukowski; Peter Boncz,Abstract In this work; we research the suitability of the Cell Broadband Engine for databaseprocessing. We start by outlining the main architectural features of Cell and use micro-benchmarks to characterize the latency and throughput of its memory infrastructure. Then;we discuss the challenges of porting RDBMS software to Cell:(i) all computations need toSIMD-ized;(ii) all performance-critical branches need to be eliminated;(iii) a very small andhard limit on program code size should be respected. While we argue that conventionaldatabase implementations; ie row-stores with Volcano-style tuple pipelining; are a hard fit toCell; it turns out that the three challenges are quite easily met in databases that use column-wise processing. We managed to implement a proof-of-concept port of the vectorized queryprocessing model of MonetDB/X100 on Cell by running the operator pipeline on the …,Proceedings of the 3rd international workshop on Data management on new hardware,2007,48
XRPC: interoperable and efficient distributed XQuery,Ying Zhang; Peter Boncz,Abstract We propose XRPC; a minimal XQuery extension that enables distributed yetefficient querying of heterogeneous XQuery data sources. XRPC enhances the existingconcept of XQuery functions with the Remote Procedure Call (RPC) paradigm. By calling outof an XQuery for-loop to multiple destinations; and by calling functions that themselvesperform XRPC calls; complex P2P communication patterns can be achieved. The XRPCextension is orthogonal to all XQuery features; including the XQuery Update Facility (XQUF).We provide formal semantics for XRPC that encompasses execution of both read-only andupdate queries. XRPC is also a network SOAP sub-protocol; that integrates seamlessly withweb services and Service Oriented Architectures (SOA); and AJAX-based GUIs. A crucialfeature of the protocol is bulk RPC; that allows remote execution of many different calls to …,Proceedings of the 33rd international conference on Very large data bases,2007,47
Cache-conscious radix-decluster projections,Stefan Manegold; Peter Boncz; Niels Nes; Martin Kersten,Abstract As CPUs become more powerful with Moore's law and memory latencies stayconstant; the impact of the memory access performance bottleneck continues to grow onrelational operators like join; which can exhibit random access on a memory region largerthan the hardware caches. While cache-conscious variants for various relational algorithmshave been described; previous work has mostly ignored (the cost of) projection columns.However; real-life joins almost always come with projections; such that proper projectioncolumn manipulation should be an integral part of any generic join algorithm. In this paper;we analyze cache-conscious hash-join algorithms including projections on two storageschemes: N-ary Storage Model (NSM) and Decomposition Storage Model (DSM). It turnsout; that the strategy of first executing the join and only afterwards dealing with the …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,47
Monet and its geographical extensions: A novel approach to high performance GIS processing,Peter A Boncz; Wilko Quak; Martin L Kersten,Abstract We describe Monet; a novel database system; designed to get maximumperformance out of today's workstations and symmetric multiprocessors. Monet is a type-andalgebra-extensible database system using the Decomposed Storage Model (DSM) andemploying shared memory parallelism. It applies purely main-memory algorithms forprocessing and uses OS virtual memory primitives for handling large data. Monet providesmany options in memory management and virtual-memory clustering strategies to optimizeaccess to its tables. We discuss how these unusual features impacted the design;implementation and performance of a set of GIS extension modules; that can be loaded atruntime in Monet; to obtain a functional complete GIS server. The validity of our approach isshown by excellent performance figures on both the Regional and National Sequoia …,International Conference on Extending Database Technology,1996,46
TPC-H analyzed: Hidden messages and lessons learned from an influential benchmark,Peter Boncz; Thomas Neumann; Orri Erling,Abstract The TPC-D benchmark was developed almost 20 years ago; and even though itscurrent existence as TPC-H could be considered superseded by TPC-DS; one can still learnfrom it. We focus on the technical level; summarizing the challenges posed by the TPC-Hworkload as we now understand them; which we call “choke points”. We identify 28 differentsuch choke points; grouped into six categories: Aggregation Performance; JoinPerformance; Data Access Locality; Expression Calculation; Correlated Subqueries andParallel Execution. On the meta-level; we make the point that the rich set of choke-pointsfound in TPC-H sets an example on how to design future DBMS benchmarks.,Technology Conference on Performance Evaluation and Benchmarking,2013,44
Column-store database architecture utilizing positional delta tree update system and methods,*,A column-store database computer system responsive to database requests for the updateand retrieval of data from within a stable data table providing for the storage of databasetuples within a column-store organized database structure. A positional delta tree datastructure is implemented in the memory space of the computer system and is operativelycoupled in an update data transfer path between a database engine interface and the stabledata table. The positional delta tree data structure includes a differential data storage layeroperative to store differential update data values in positionally defined relative reference todatabase tuples stored by the stable data table.,*,2010,43
AmbientDB: P2P data management middleware for ambient intelligence,Willem Fontijn; Peter Boncz,The future generation of consumer electronics devices is envisioned to provide automaticcooperation between devices and run applications that are sensitive to people's likings;personalized to their requirements; anticipatory of their behavior and responsive to theirpresence. We see this' ambient intelligence'as a key feature of future pervasive computing.We focus here on one of the challenges in realizing this vision: information management.This entails integrating; querying; synchronizing and evolving structured data; on aheterogeneous and ad-hoc collection of (mobile) devices. Rather than hard-coding datamanagement functionality in each individual application; we argue for adding high-leveldata management functionalities to the distributed middleware layer. Our ambientDB P2Pdatabase management system addresses this by providing a global database abstraction …,Pervasive Computing and Communications Workshops; 2004. Proceedings of the Second IEEE Annual Conference on,2004,43
The linked data benchmark council: a graph and RDF industry benchmarking effort,Renzo Angles; Peter Boncz; Josep Larriba-Pey; Irini Fundulaki; Thomas Neumann; Orri Erling; Peter Neubauer; Norbert Martinez-Bazan; Venelin Kotsev; Ioan Toma,Abstract The Linked Data Benchmark Council (LDBC) is an EU project that aims to developindustry-strength benchmarks for graph and RDF data management systems. It includes thecreation of a non-profit LDBC organization; where industry players and academia cometogether for managing the development of benchmarks as well as auditing and publishingofficial results. We present an overview of the project including its goals and organization;and describe its process and design methodology for benchmark development. Weintroduce so-called" choke-point" based benchmark development through which expertsidentify key technical challenges; and introduce them in the benchmark workload. Finally;we present the status of two benchmarks currently in development; one targeting graph datamanagement systems using a social network data case; and the other targeting RDF …,ACM SIGMOD Record,2014,41
Vectorwise: A vectorized analytical DBMS,Marcin Zukowski; Mark van de Wiel; Peter Boncz,Vector wise is a new entrant in the analytical database marketplace whose technologycomes straight from innovations in the database research community in the past years. Theproduct has since made waves due to its excellent performance in analytical customerworkloads as well as benchmarks. We describe the history of Vectorwise; as well as its basicarchitecture and the experiences in turning a technology developed in an academic contextinto a commercial-grade product. Finally; we turn our attention to recent performance results;most notably on the TPC-H benchmark at various sizes.,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,40
Updating the Pre/Post Plane in MonetDB/XQuery.,Peter A Boncz; Stefan Manegold; Jan Rittinger,ABSTRACT We outline an efficient ACID-compliant mechanism for structural inserts anddeletes in relational XML document storage that uses a region based pre/size/levelencoding (equivalent to the pre/post encoding). Updates to such node-numbering schemesare considered prohibitive (ie physical cost linear to document size); because structuralupdates cause shifts in all pre-numbers after the update point; and require updates of thesize of all ancestors; such that the root of the tree becomes a locking bottleneck. We showhow such locking can be avoided by updating the size of ancestors using deltaincrements;which are transaction-commutative operations. We also reduce the physical cost to theminimum (ie linear to update volume) by carefully exploiting the virtual column feature ofMonetDB to store pre numbers (virtual columns are never materialized; and thus need not …,*,2005,37
Graphalytics: A big data benchmark for graph-processing platforms,Mihai Capotă; Tim Hegeman; Alexandru Iosup; Arnau Prat-Pérez; Orri Erling; Peter Boncz,Abstract Graphs are increasingly used in industry; governance; and science. This hasstimulated the appearance of many and diverse graph-processing platforms. Althoughplatform diversity is beneficial; it also makes it very challenging to select the best platform foran application domain or one of its important applications; and to design new and tuneexisting platforms. Continuing a long tradition of using benchmarking to address suchchallenges; in this work we present our vision for Graphalytics; a big data benchmark forgraph-processing platforms. We have already benchmarked with Graphalytics a variety ofpopular platforms; such as Giraph; GraphX; and Neo4j.,Proceedings of the GRADES'15,2015,35
Micro adaptivity in vectorwise,Bogdan Răducanu; Peter Boncz; Marcin Zukowski,Abstract Performance of query processing functions in a DBMS can be affected by manyfactors; including the hardware platform; data distributions; predicate parameters;compilation method; algorithmic variations and the interactions between these. Given thatthere are often different function implementations possible; there is a latent performancediversity which represents both a threat to performance robustness if ignored (as is usualnow) and an opportunity to increase the performance if one would be able to use the bestperforming implementation in each situation. Micro Adaptivity; proposed here; is aframework that keeps many alternative function implementations (flavors) in a system. It usesa learning algorithm to choose the most promising flavor potentially at each function call;guided by the actual costs observed so far. We argue that Micro Adaptivity both increases …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,35
ROX: run-time optimization of XQueries,Riham Abdel Kader; Peter Boncz; Stefan Manegold; Maurice Van Keulen,Abstract Optimization of complex XQueries combining many XPath steps and joins iscurrently hindered by the absence of good cardinality estimation and cost models forXQuery. Additionally; the state-of-the-art of even relational query optimization still strugglesto cope with cost model estimation errors that increase with plan size; as well as with theeffect of correlated joins and selections. In this research; we propose to radically depart fromthe traditional path of separating the query compilation and query execution phases; byhaving the optimizer execute; materialize partial results; and use sampling based estimationtechniques to observe the characteristics of intermediates. The proposed technique takes asinput a Join Graph where the edges are either equi-joins or XPath steps; and the executionenvironment provides value-and structural-join algorithms; as well as structural and value …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,35
Materialized view selection in XML databases,Nan Tang; Jeffrey Xu Yu; Hao Tang; M Tamer Özsu; Peter Boncz,Abstract Materialized views; a rdbms silver bullet; demonstrate its efficacy in manyapplications; especially as a data warehousing/decison support system tool. The pivot ofplaying materialized views efficiently is view selection. Though studied for over thirty years inrdbms; the selection is hard to make in the context of xml databases; where both the semi-structured data and the expressiveness of xml query languages add challenges to the viewselection problem. We start our discussion on producing minimal xml views (in terms of size)as candidates for a given workload (a query set). To facilitate intuitionistic view selection; wepresent a view graph (called vcube) to structurally maintain all generated views. By basingour selection on vcube for materialization; we propose two view selection strategies;targeting at space-optimized and space-time tradeoff; respectively. We built our …,International Conference on Database Systems for Advanced Applications,2009,30
Robust runtime optimization and skew-resistant execution of analytical SPARQL queries on PIG,Spyros Kotoulas; Jacopo Urbani; Peter Boncz; Peter Mika,Abstract We describe a system that incrementally translates SPARQL queries to Pig Latinand executes them on a Hadoop cluster. This system is designed to work efficiently oncomplex queries with many self-joins over huge datasets; avoiding job failures even in thecase of joins with unexpected high-value skew. To be robust against cost estimation errors;our system interleaves query optimization with query execution; determining the next stepsto take based on data samples and statistics gathered during the previous step.Furthermore; we have developed a novel skew-resistant join algorithm that replicates tuplescorresponding to popular keys. We evaluate the effectiveness of our approach both on asynthetic benchmark known to generate complex queries (BSBM-BI) as well as on a Yahoo!case of data analysis using RDF data crawled from the web. Our results indicate that our …,International Semantic Web Conference,2012,25
Pathfinder: Relational xquery over multi-gigabyte XML inputs in interactive time,Peter Alexander Boncz; Torsten Grust; Stefan Manegold; Jan Rittinger; Jens Teubner,textabstractUsing a relational DBMS as back-end engine for an XQuery processing systemleverages relational query optimization and scalable query processing strategies providedby mature DBMS engines in the XML domain. Though a lot of theoretical work has beendone in this area and various solutions have been proposed; no complete systems havebeen made available so far to give the practical evidence that this is a viable approach. Inthis paper; we describe the ourely relational XQuery processor Pathfinder that has been builton top of the extensible RDBMS MonetDB. Performance results indicate that the system iscapable of evaluating XQuery queries efficiently; even if the input XML documents becomehuge. We additionally present further contributions such as loop-lifted staircase join;techniques to derive order properties and to reduce sorting effort in the generated …,Information Systems [INS],2005,25
Integration of vectorwise with ingres,Doug Inkster; Marcin Zukowski; Peter Boncz,Abstract Actian Corporation recently entered into a cooperative relationship with VectorWiseBV to integrate its Vector-Wise technology into the Ingres RDBMS server. The resultingcommercial product has already achieved phenomenal performance results with the TPC-Hindustry standard benchmark; and has been well received in the analytical RDBMS market.This paper describes the integration of the VectorWise technology with Ingres; some of thedesign decisions made as part of the integration project; and the problems that had to besolved in the process.,ACM SIGMOD Record,2011,24
Efficient distribution of full-fledged XQuery,Ying Zhang; Nan Tang; Peter Boncz,We investigate techniques to automatically decompose any XQuery query into subqueries;that can be executed near their data sources; ie; function-shipping. In this scenario; thesubqueries being executed remotely may have XML node-valued parameters or results; thatmust be shipped in some way. The main challenge addressed here is to ensure that thedecomposed queries properly respect XML node identity and preserve structural properties;when (parts of) XML nodes are sent over the network; effectively copying them. We start byprecisely characterizing the conditions; under which pass-by-value parameter passingcauses semantic differences between remote execution of an XQuery expression and itslocal execution. We then formulate a conservative strategy that effectively avoidsdecomposition in such cases. To broaden the possibilities of query distribution; we extend …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,23
Ldbc graphalytics: A benchmark for large-scale graph analysis on parallel and distributed platforms,Alexandru Iosup; Tim Hegeman; Wing Lung Ngai; Stijn Heldens; Arnau Prat-Pérez; Thomas Manhardto; Hassan Chafio; Mihai Capotă; Narayanan Sundaram; Michael Anderson; Ilie Gabriel Tănase; Yinglong Xia; Lifeng Nai; Peter Boncz,Abstract In this paper we introduce LDBC Graphalytics; a new industrial-grade benchmarkfor graph analysis platforms. It consists of six deterministic algorithms; standard datasets;synthetic dataset generators; and reference output; that enable the objective comparison ofgraph analysis platforms. Its test harness produces deep metrics that quantify multiple kindsof system scalability; such as horizontal/vertical and weak/strong; and of robustness; such asfailures and performance variability. The benchmark comes with open-source software forgenerating data and monitoring performance. We describe and analyze six implementationsof the benchmark (three from the community; three from the industry); providing insights intothe strengths and weaknesses of the platforms. Key to our contribution; vendors perform thetuning and benchmarking of their platforms.,Proceedings of the VLDB Endowment,2016,22
The drill down benchmark,Peter A Boncz; Tim Rühl; Fred Kwakkel,Abstract Data Mining places specific requirements on DBMS query performance that cannotbe evaluated satisfactorily using existing OLAP benchmarks. The DD Benchmark-definedhere-provides a practical case and yardstick to explore how well a DBMS is able to supportData Mining applications. It was derived from real-life data mining tasks performed by ourData SurveyorTM tool running on a variety of DBMS backends. We describe initial resultsobtained using both the Monet system and a relational DBMS product as backend.,VLDB,1998,22
Data blocks: Hybrid OLTP and OLAP on compressed storage using both vectorization and compilation,Harald Lang; Tobias Mühlbauer; Florian Funke; Peter A Boncz; Thomas Neumann; Alfons Kemper,Abstract This work aims at reducing the main-memory footprint in high performance hybridOLTP & OLAP databases; while retaining high query performance and transactionalthroughput. For this purpose; an innovative compressed columnar storage format for colddata; called Data Blocks is introduced. Data Blocks further incorporate a new light-weightindex structure called Positional SMA that narrows scan ranges within Data Blocks even ifthe entire block cannot be ruled out. To achieve highest OLTP performance; thecompression schemes of Data Blocks are very light-weight; such that OLTP transactions canstill quickly access individual tuples. This sets our storage scheme apart from those used inspecialized analytical databases where data must usually be bit-unpacked. Up to now; high-performance analytical systems use either vectorized query execution or just-in-time (JIT) …,Proceedings of the 2016 International Conference on Management of Data,2016,21
Efficient xquery support for stand-off annotation,Wouter Alink; Raoul Bhoedjang,Abstract XML annotations are a widely occurring phenomenon in many application fields;and XML databases should be used to store and query such data. To provide intuitive andfast querying of annotations; we make a case for extending XPath with four new axis steps;that correspond with socalled StandOff joins; introduced here. The new steps can beefficiently implemented using a region index and fast looplifted StandOff MergeJoinalgorithms. These techniques were added to the open-source XML DBMS MonetDB/X-Query; and we show in our evaluation it thus becomes capable of interactively querying> GBannotation databases.,*,2006,20
High performance OO traversals in Monet,Peter A Boncz; Fred Kwakkel; Martin L Kersten,Abstract In this paper we discuss how Monet; a novel multimodel database system; can beused to efficiently support OODB applications. We show how Monet's offbeat view on keyissues in database architecture provided both challenges and opportunities in building ahigh-performance ODMG-93 compliant Runtime System on top of it. We describe how an OOdata-model can be mapped onto Monet's decomposed storage scheme while maintainingphysical data independence; and how OO queries are translated into an algebraiclanguage. A generic model for specifying OO class-attribute traversals is presented; thatpermits the OODB to algebraicly optimize and parallelize their execution. To demonstratethe success of our approach; we give OO7 benchmark results of our Runtime System forboth the standard pointer-based object navigation; and our declarative model based on a …,British National Conference on Databases,1996,20
The calibrator (v0. 9e); a cache-memory and TLB calibration tool,Stefan Manegold; Peter Boncz,*,*,2004,19
Monet: a next-generation database kernel for query-intensive applications,PA Boncz,htmlabstractMonet is a database kernel targeted at query-intensive; heavy analysisapplications (the opposite of transaction processing); which include OLAP and data mining;but also go beyond the business domain in GIS processing; multi-media retrieval and XML.The clean sheet approach of Monet tries to depart from the traditional RDBMS design andimplementation patterns in an attempt to obtain best performance on modern hardware;which has changed a lot since the currently dominant relational database systems weredesigned and developed. While most hardware components have experienced exponentialgrowth in power over the years (aka Moore's law); I/O and especially memory latency havebeen lagging; creating an exponentially growing bottleneck. Additionally; modernhyperpipelined CPUs increasingly require code that is fully predictable (as to avoid …,*,2002,19
Recycling in pipelined query evaluation,Fabian Nagel; Peter Boncz; Stratis D Viglas,Database systems typically execute queries in isolation. Sharing recurring intermediate andfinal results between successive query invocations is ignored or only exploited by cachingfinal query results. The DBA is kept in the loop to make explicit sharing decisions byidentifying and/or defining materialized views. Thus decisions are made only after a longtime and sharing opportunities may be missed. Recycling intermediate results has beenproposed as a method to make database query engines profit from opportunities to reusefine-grained partial query results; that is fully autonomous and is able to continuously adaptto changes in the workload. The technique was recently revisited in the context of MonetDB;a system that by default materializes all intermediate results. Materializing intermediateresults can consume significant system resources; therefore most other database systems …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,18
The linked data benchmark council project,Peter Boncz; Irini Fundulaki; Andrey Gubichev; Josep Larriba-Pey; Thomas Neumann,Abstract Despite the fast growth and increasing popularity; the broad field of RDF and Graphdatabase systems lacks an independent authority for developing benchmarks; and forneutrally assessing benchmark results through industry-strength auditing which would allowto quantify and compare the performance of existing and emerging systems. Inspired by theimpact of the Transaction Processing Performance Council (TPC) Benchmarks on relationaldatabases; the LDBC consortium formed by University and Industry researchers andpractitioners has recently launched a European Commision sponsored project that will offerthe first comprehensive set of open and vendor-independent benchmarks for RDF andGraph technologies. The consortium will incorporate the Linked Data Benchmark Council(LDBC) which will survive the project and will supervise the process of obtaining and …,Datenbank-Spektrum,2013,17
Efficient and Flexible Information Retrieval using MonetDB/X100.,Sándor Héman; Marcin Zukowski; Arjen P de Vries; Peter A Boncz,ABSTRACT Today's large-scale IR systems are not implemented using general-purposedatabase systems; as the latter tend to be significantly less efficient than custom-built IRengines. This paper demonstrates how recent developments in hardwareconsciousdatabase architecture may however satisfy IR needs. The advantage is flexibility ofexperimentation; as implementing a retrieval system on top of a DBMS boils down torelational query formulation; rather than system programming. We demonstrate in thecontext of the TeraByte TREC efficiency task that our experimental MonetDB/X100 databasesystem provides highly competitive results both regarding precision and speed. We analyzethe two innovations in MonetDB/X100 that most contributed to this successful application ofDB technology in IR; namely vectorized incache processing and the use of two new light …,CIDR,2007,17
Representing and querying multi-dimensional markup for question answering,Wouter Alink; Valentin Jijkoun; David Ahn; Maarten De Rijke; Peter Boncz; Arjen de Vries,Abstract This paper describes our approach to representing and querying multi-dimensional;possibly overlapping text annotations; as used in our question answering (QA) system. Weuse a system extending XQuery; the W3C-standard XML query language; with new axes thatallow one to jump easily between different annotations of the same data. The new axes areformulated in terms of (partial) overlap and containment. All annotations are made usingstand-off XML in a single document; which can be efficiently queried using the XQueryextension. The system is scalable to gigabytes of XML annotations. We show examples ofthe system in QA scenarios.,Proceedings of the 5th Workshop on NLP and XML: Multi-Dimensional Markup in Natural Language Processing,2006,17
Method and apparatus for using data access time prediction for improving data buffering policies,*,A system and method for buffer management in a database are provided in which apredictive buffer manager may be used. The predictive buffer manager and process maypredict when each block in a buffer is going to be used and then manages the buffer basedon the prediction.,*,2014,15
Deriving an emergent relational schema from RDF data,Minh-Duc Pham; Linnea Passing; Orri Erling; Peter Boncz,Abstract We motivate and describe techniques that allow to detect an" emergent" relationalschema from RDF data. We show that on a wide variety of datasets; the found structureexplains well over 90% of the RDF triples. Further; we also describe technical solutions tothe semantic challenge to give short names that humans find logical to these emergenttables; columns and relationships between tables. Our techniques can be exploited in manyways; eg; to improve the efficiency of SPARQL systems; or to use existing SQL-basedapplications on top of any RDF dataset using a RDBMS.,Proceedings of the 24th International Conference on World Wide Web,2015,13
LDBC: benchmarks for graph and RDF data management,Peter Boncz,Abstract The Linked Data Benchmark Council (LDBC) is an EU project that aims to developindustry-strength benchmarks for graph and RDF data management systems. LDBCintroduces a so-called" choke-point" based benchmark development; through which expertsidentify key technical challenges; and introduce them in the benchmark workload; which wedescribe in some detail. We also present the status of two LDBC benchmarks currently indevelopment; one targeting graph data management systems using a social network datacase; and the other targeting RDF systems using a data publishing case.,Proceedings of the 17th International Database Engineering & Applications Symposium,2013,13
MonetDB/X100 at the 2006 TREC Terabyte Track.,Sándor Héman; Marcin Zukowski; Arjen P de Vries; Peter A Boncz,Requirements of database management (DB) and information retrieval (IR) systems overlapmore and more. Database systems are being applied to scenarios where features such astext search and similarity scoring on multiple attributes become crucial. Many informationretrieval systems are being extended beyond plain text; to rank semi-structured documentsmarked up in XML; or maintain ontologies or thesauri. In both areas; these new features areusually implemented using specialized solutions limited in their features and performance.Full integration of DB and IR has been considered highly desirable; see eg [5; 1] for somerecent advocates. Yet; none of the attempts into this direction has been very successful. Theexplanation can be sought in what has been termed the 'structure chasm'[8]: databaseresearch builds upon the idea that all data should satisfy a pre-defined schema; and the …,TREC,2006,12
Cooperative scans,Marcin Zukowski; Peter A Boncz; Martin L Kersten,textabstractData mining; information retrieval and other application areas exhibit a queryload with multiple concurrent queries touching a large fraction of a relation. This leads toindividual query plans based on a table scan or large index scan. The implementation of thisaccess path in most database systems is straightforward. The Scan operator issues nextpage requests to the buffer manager without concern for the system state. Conversely; thebuffer manager is not aware of the work ahead and it focuses on keeping the most-recently-used pages in the buffer pool. This paper introduces cooperative scans--a new algorithm;based on a better sharing of knowledge and responsibility between the Scan operator andthe buffer manager; which significantly improves performance of concurrent scan queries. Inthis approach; queries share the buffer content; and progress of the scans is optimized by …,Information Systems [INS],2004,12
Monet and its geographic extensions,Peter A Boncz; CW Quak; Martin L Kersten,Abstract We describe Monet; a novel database system; designed to get maximumperformance out of today's workstations and symmetric multiprocessors. Monet is a type-andalgebra-extensible database system using the Decomposed Storage Model (DSM) andemploying shared memory parallelism. It applies purely main-memory algorithms forprocessing and uses OS virtual memory primitives for handling large data. Monet providesmany options in memory management and virtual-memory clustering strategies to optimizeaccess to its tables. We discuss how these unusual features impacted the design;implementation and performance of a set of GIS extension modules; that can be loaded atruntime in Monet; to obtain a functional complete GIS server. The validity of our approach isshown by excellent performance gures on both the Regional and National Sequoia …,Proceedings of the 1996 EDBT conference,1996,12
XRPC: distributed XQuery and update processing with heterogeneous XQuery engines,Ying Zhang; Peter Boncz,Abstract We demonstrate XRPC; a minimal XQuery extension that enables distributedquerying between heterogeneous XQuery engines. The XRPC language extensionenhances the existing concept of XQuery functions with the Remote Procedure Call (RPC)paradigm. XRPC is orthogonal to all XQuery features; including the XQuery Update Facility(XQUF). Note that executing xquf updating functions over XRPC leads to the phenomenon ofdistributed transactions. XRPC achieves heterogeneity by an open SOAP-based networkprotocol; that can be implemented by any engine; and an XRPC Wrapper that allows evenXRPC-oblivious XQuery engines to handle XRPC requests efficiently. XRPC is fullyimplemented in the open-source MonetDB/XQuery engine; and is demonstrated here to co-operate with Saxon; Galax and X-Hive through the XRPC wrapper. This demonstration …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,10
MonetDB/XQuery—consistent and efficient updates on the pre/Post plane,Peter Boncz; Jan Flokstra; Torsten Grust; Maurice van Keulen; Stefan Manegold; Sjoerd Mullender; Jan Rittinger; Jens Teubner,Abstract Relational XQuery processors aim at leveraging mature relational DBMS queryprocessing technology to provide scalability and efficiency. To achieve this goal; variousstorage schemes have been proposed to encode the tree structure of XML documents in flatrelational tables. Basically; two classes can be identified:(1) encodings using fixed-lengthsurrogates; like the preorder ranks in the pre/post encoding [5] or the equivalentpre/size/level encoding [8]; and (2) encodings using variable-length surrogates; like; eg;ORDPATH [9] or P-PBiTree [12]. Recent research [1] showed a clear advantage of theformer for efficient evaluation of XPath location steps; exploiting techniques like cheap nodeorder tests; positional lookup; and node skipping in staircase join [7]. However; onceupdates are involved; variable-length surrogates are often considered the better choice …,International Conference on Extending Database Technology,2006,10
Exploiting emergent schemas to make RDF systems more efficient,Minh-Duc Pham; Peter Boncz,Abstract We build on our earlier finding that more than 95% of the triples in actual RDF triplegraphs have a remarkably tabular structure; whose schema does not necessarily follow fromexplicit metadata such as ontologies; but for which an RDF store can automatically derive bylooking at the data using so-called “emergent schema” detection techniques. In this paperwe investigate how computers and in particular RDF stores can take advantage from thisemergent schema to more compactly store RDF data and more efficiently optimize andexecute SPARQL queries. To this end; we contribute techniques for efficient emergentschema aware RDF storage and new query operator algorithms for emergent schema awarescans and joins. In all; these techniques allow RDF schema processors fully catch up withrelational database techniques in terms of rich physical database design options and …,International Semantic Web Conference,2016,9
Parameter curation for benchmark queries,Andrey Gubichev; Peter Boncz,Abstract In this paper we consider the problem of generating parameters for benchmarkqueries so these have stable behavior despite being executed on datasets (real-world orsynthetic) with skewed data distributions and value correlations. We show that uniformrandom sampling of the substitution parameters is not well suited for such benchmarks;since it results in unpredictable runtime behavior of queries. We present our approach ofParameter Curation with the goal of selecting parameter bindings that have consistently low-variance intermediate query result sizes throughout the query plan. Our solution is illustratedwith IMDB data and the recently proposed LDBC Social Network Benchmark (SNB).,Technology Conference on Performance Evaluation and Benchmarking,2014,8
Query processing of pre-partitioned data using sandwich operators,Stephan Baumann; Peter Boncz; Kai-Uwe Sattler,Abstract In this paper we present the “Sandwich Operators”; an elegant approach to exploitpre-sorting or pre-grouping from clustered storage schemes in operators such asAggregation/Grouping; HashJoin; and Sort of a database management system. Thereby;each of these operator types is “sandwiched” by two new operators; namely PartitionSplitand PartitionRestart. PartitionSplit splits the input relation into its smaller independentgroups on which the sandwiched operator is executed. After a group is processed;PartitionRestart is used to trigger the execution on the following group. Executing each ofthese operator types with the help of the Sandwich Operators introduces minimal overheadand does not penalize performance of the sandwiched operator; as its implementationremains unchanged. On the contrary; we show that sandwiched execution of each …,International Workshop on Business Intelligence for the Real-Time Enterprise,2012,8
From cooperative scans to predictive buffer management,Michał Świtakowski; Peter Boncz; Marcin Zukowski,Abstract In analytical applications; database systems often need to sustain workloads withmultiple concurrent scans hitting the same table. The Cooperative Scans (CScans)framework; which introduces an Active Buffer Manager (ABM) component into the databasearchitecture; has been the most effective and elaborate response to this problem; and wasinitially developed in the X100 research prototype. We now report on the the experiences ofintegrating Cooperative Scans into its industrial-strength successor; the Vectorwisedatabase product. During this implementation we invented a simpler optimization ofconcurrent scan buffer management; called Predictive Buffer Management (PBM). PBM isbased on the observation that in a workload with long-running scans; the buffer managerhas quite a bit of information on the workload in the immediate future; such that an …,Proceedings of the VLDB Endowment,2012,8
From X100 to Vectorwise: Opportunities; challenges and things most researchers do not think about,Marcin Zukowski; Peter Boncz,Abstract In 2008 a group of researchers behind the X100 database kernel createdVectorwise: a spin-off which together with the Actian corporation (previously Ingres) workedon bringing this technology to the market. Today; Vectorwise is a popular product and one ofthe examples of conversion of a research prototype into successful commercial software. Wedescribe here some of the interesting aspects of the work performed by the Vectorwisedevelopment team in the process; and discuss the opportunities and challenges resultingfrom the decision of integrating a prototype-quality kernel with Ingres; an establishedcommercial product. We also discuss how requirements coming from reallife scenariossometimes clashed with design choices and simplifications often found in research projects;and how Vectorwise team addressed some of of them.,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,8
Space-economical partial gram indices for exact substring matching,Nan Tang; Lefteris Sidirourgos; Peter Boncz,Abstract Exact substring matching queries on large data collections can be answered usingq-gram indices; that store for each occurring q-byte pattern an (ordered) posting list with thepositions of all occurrences. Such gram indices are known to provide fast query responsetime and to allow the index to be created quickly even on huge disk-based datasets. Theirmain drawback is relatively large storage space; that is a constant multiple (typically> 2) ofthe original data size; even when compression is used. In this work; we study methods toconserve the scalable creation time and efficient exact substring query properties of gramindices; while reducing storage space. To this end; we first propose a partial gram indexbased on a reduction from the problem of omitting indexed q-grams to the set cover problem.While this method is successful in reducing the size of the index; it generates false …,Proceedings of the 18th ACM conference on Information and knowledge management,2009,8
Methods of operating a column-store database engine utilizing a positional delta tree update system,*,Abstract A column-store database engine operates in response to database requests for theupdate and retrieval of data from within a stable data table and provides for the storage ofdatabase tuples within a column-store organized database structure. A positional delta treedata structure is implemented in the memory space of the database engine and isoperatively coupled in an update data transfer path between a database engine interfaceand the stable data table. The positional delta tree data structure includes a differential datastorage layer operative to store differential update data values in positionally defined relativereference to database tuples stored by the stable data table.,*,2018,7
Scientific discovery through weighted sampling,Lefteris Sidirourgos; Martin Kersten; Peter Boncz,Scientific discovery has shifted from being an exercise of theory and computation; to becomethe exploration of an ocean of observational data. Scientists explore data originated frommodern scientific instruments in order to discover interesting aspects of it and formulate theirhypothesis. Such workloads press for new database functionality. We aim at samplingscientific databases to create many different impressions of the data; on which the scientistscan quickly evaluate exploratory queries. However; scientific databases introduce differentchallenges for sample construction compared to classical business analytical applications.We propose adaptive weighted sampling as an alternative to uniform sampling. Withweighted sampling only the most informative data is being sampled; thus more relevant datato the scientific discovery is available to examine a hypothesis. Relevant data is …,Big Data; 2013 IEEE International Conference on,2013,7
Distributed data streams,Minos Garofalakis,realized using such logical structures. For example; in tree based data acquisition protocols;a collection tree is built that is rooted at the data collection center such as the sink node [8].The dissemination of the data requests from the participating nodes and collection of datafrom the sensor nodes are accomplished using this tree. A cluster based data acquisitionmechanism has been proposed in [3]. As shown in Fig. 1; nodes are organized into a fixednumber of clusters; and nodes within each cluster dynamically elect a cluster head. The dataacquisition is carried out in two phases. In the first phase; cluster heads collect data fromtheir cluster nodes. In the second phase; cluster heads send collected data to the nodes thathave subscribed to the data. The cluster heads are re-elected to balance energyconsumption among the nodes in the cluster. Zhang et al.[13] have proposed an adaptive …,*,2009,7
Positional Delta Trees to reconcile updates with read-optimized data storage,Sándor Héman; Niels J Nes; Marcin Żukowski; Peter Alexander Boncz,Abstract—We investigate techniques that marry the high read-only analytical queryperformance of compressed; replicated column storage (“read-optimized” databases) withthe ability to handle a high-throughput update workload. Today's large RAM sizes and thegrowing gap between sequential vs. random IO disk throughput; bring this once elusive goalin reach; as it has become possible to buffer enough updates in memory to allowbackground migration of these updates to disk; where efficient sequential IO is amortizedamong many updates. Our key goal is that read-only queries always see the latest databasestate; yet are not (significantly) slowed down by the update processing. To this end; wepropose the Positional Delta Tree (PDT); that is designed to minimize the overhead of on-the-fly merging of differential updates into (index) scans on stale disk-based data. We …,*,2008,7
Integrating XQuery and P2P in MonetDB/XQuery*.,Ying Zhang; Peter A Boncz,Abstract. MonetDB/XQuery* is a fully functional publicly available XML DBMS that has beenextended with distributed and P2P data management functionality. Our (minimal) XQuerylanguage extension XRPC adds the concept of RPC to XQuery; and exploits the set-at-a-time database processing model to optimize the networking cost through a technique calledBulk RPC. We describe our approach to include the services offered by diverse P2P networkstructures (such as DHTs); in a way that avoids any further intrusion in the XQuery languageand semantics; and show how this; similarly to Bulk RPC; will lead to further queryoptimization opportunities where the XDBMS interacts with the underlying P2P network. Wealso discuss some P2P data management applications were MonetDB/XQuery* is beingused (an in-home small scenario and a wide-area collaborative application). As this …,EROW,2007,7
Loop-lifted XQuery RPC with deterministic updates,Ying Zhang; Peter Alexander Boncz,<filmDB> <film> <filmName>The Rock</filmName> <actorName>SeanConnery</actorName> </film> <film> <filmName>Goldfinger</filmName> <actorName>SeanConnery</actorName> </film> <film> <filmName>Green Card</filmName> <actorName>GerardDepardieu</actorName> </film> </filmDB> … ($actor as xs:string) as node()* {doc(“filmDB.xml”)//filmName [../actorName=$actor] }; … <films> <filmName>TheRock</filmName> <filmName>Goldfinger</filmName> </films> … A natural choice as the underlyingprotocol of XRPC … Easy to be processed by an XQuery engine … Passing values of all XDMdata type … <?xml version=”1.0” encoding=”utf8”?> <env:Envelop xmlns:xrpc=”http://monetdb.cwi.nl/XQuery” xmlns:env=”.../2003/05/soapenvelope” xmlns:xs=”.../2001/XMLSchema”xmlns:xsi=”.../2001/XMLSchemainstance” xsi:schemaLocation= ”http://monetdb.cwi.nl …,*,2006,7
VectorH: taking SQL-on-Hadoop to the next level,Andrei Costea; Adrian Ionescu; Bogdan Răducanu; Michał Switakowski; Cristian Bârca; Juliusz Sompolski; Alicja Łuszczak; Michał Szafrański; Giel De Nijs; Peter Boncz,Abstract Actian Vector in Hadoop (VectorH for short) is a new SQL-on-Hadoop system builton top of the fast Vectorwise analytical database system. VectorH achieves fault toleranceand storage scalability by relying on HDFS; and extends the state-of-the-art in SQL-on-Hadoop systems by instrumenting the HDFS replication policy to optimize read locality.VectorH integrates with YARN for workload management; achieving a high degree ofelasticity. Even though HDFS is an append-only filesystem; and VectorH supports (update-averse) ordered tables; trickle updates are possible thanks to Positional Delta Trees (PDTs);a differential update structure that can be queried efficiently. We describe the changes madeto single-server Vectorwise to turn it into a Hadoop-based MPP system; encompassingworkload management; parallel query optimization and execution; HDFS storage …,Proceedings of the 2016 International Conference on Management of Data,2016,6
The tears of Donald Knuth,Thomas Haigh,Has the history of computing taken a tragic turn? the much smaller community of historiansof computing but; even by Google Scholar's generous definitions; the paper that saddenedKnuth has been cited only nine times. Knuth then enumerated his motivations; as acomputer scientist; to read the history of science. First; reading history helped him tounderstand the process of discovery. Second; understanding the difficulty and false startsexperienced by brilliant historical scientists in making discoveries that specialists now findobvious helped him to,Communications of the ACM,2015,6
Experiences with virtuoso cluster RDF column store,Peter A Boncz; Orri Erling; P Minh Duc,KNAW Narcis. Back to search results. Publication Experiences With VirtuosoCluster RDF Column Store (2013). Pagina-navigatie: Main …,Linked Data Management: Principles and Techniques,2013,6
Rox: The robustness of a run-time xquery optimizer against correlated data,Riham Abdel Kader; Peter Boncz; Stefan Manegold; Maurice Van Keulen,We demonstrate ROX; a run-time optimizer of XQueries; that focuses on finding the bestexecution order of XPath steps and relational joins in an XQuery. The problem of joinordering has been extensively researched; but the proposed techniques are stillunsatisfying. These either rely on a cost model which might result in inaccurate estimations;or explore only a restrictive number of plans from the search space. ROX is developed totackle these problems. ROX does not need any cost model; and defers query optimization torun-time intertwining optimization and execution steps. In every optimization step; samplingtechniques are used to estimate the cardinality of unexecuted steps and joins to make adecision which sequence of operators to process next. Consequently; each execution stepwill provide updated and accurate knowledge about intermediate results; which will be …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,6
Loop-lifted staircase join: from XPath to XQuery,Peter Alexander Boncz; Torsten Grust; Maurice vanKeulen; Stefan Manegold; Jan Rittinger; Jens Teubner,textabstractVarious techniques have been proposed for efficient evaluation of XPathexpressions; where the XPath location steps are rooted in a single sequence of contextnodes. Among these techniques; the staircase join allows to evaluate XPath location stepsalong arbitrary axes in at most one scan over the XML document; exploiting the XPathaccelerator encoding (aka. pre/post encoding). In XQuery; however; embedded XPath sub-expressions occur in arbitrarily nested for-loops. Thus; they are rooted in multiple sequencesof context nodes (one per iteration). Consequently; the previously proposed algorithms needto be applied repeatedly; requiring multiple scans over the XML document encoding. In thiswork; we present loop-lifted staircase join; an extension of the staircase join that allows toefficiently evaluate XPath sub-expressions in arbitrarily nested XQuery iteration scopes …,Information Systems [INS],2005,6
Advances in large-scale RDF data management,Peter Boncz; Orri Erling; Minh-Duc Pham,Abstract One of the prime goals of the LOD2 project is improving the performance andscalability of RDF storage solutions so that the increasing amount of Linked Open Data(LOD) can be efficiently managed. Virtuoso has been chosen as the basic RDF store for theLOD2 project; and during the project it has been significantly improved by incorporatingadvanced relational database techniques from MonetDB and Vectorwise; turning it into acompressed column store with vectored execution. This has reduced the performance gap(“RDF tax”) between Virtuoso's SQL and SPARQL query performance in a way that stillrespects the “schema-last” nature of RDF. However; by lacking schema information; RDFdatabase systems such as Virtuoso still cannot use advanced relational storageoptimizations such as table partitioning or clustered indexes and have to execute …,*,2014,5
Benchmarking RDF storage engines,Ying Zhang; M Pham; FE Groffen; Erietta Liarou; PA Boncz; ML Kersten; Jean-Paul Calbimonte; Oscar Corcho,htmlabstractIn this deliverable; we present version V1. 0 of SRBench; the first benchmark forStreaming RDF engines; designed in the context of Task 1.4 of PlanetData; completelybased on real-world datasets. With the increasing problem of too much streaming data butnot enough knowledge; researchers have set out for solutions in which Semantic Webtechnologies are adapted and extended for the publishing; sharing; analysing andunderstanding of such data. Various approaches are emerging. To help researchers andusers to compare streaming RDF engines in a standardised application scenario; wepropose SRBench; with which one can assess the abilities of a streaming RDF engine tocope with a broad range of use cases typically encountered in real-world scenarios. We offera set of queries that cover the major aspects of streaming RDF engines; ranging from …,PlanetData Deliverables,2012,5
Projective Distribution of XQuery with Updates,Ying Zhang; Nan Tang; Peter Boncz,We investigate techniques to automatically decompose any XQuery query-includingupdating queries specified by the XQuery Update Facility (XQUF)-into subqueries; that canbe executed near their data sources; ie; function-shipping. The main challenge addressedhere is to ensure that the decomposed queries properly respect XML node identity andpreserve structural properties; when (parts of) XML nodes are sent over the network;effectively copying them. We start by precisely characterizing the conditions; under whichpass-by-value parameter passing causes semantic differences between remote execution ofan XQuery expression and its local execution. We then formulate a conservative strategythat effectively avoids decomposition in such cases. To broaden the possibilities of querydistribution; we extend the pass-by-value semantics to a pass-by-fragment semantics …,IEEE Transactions on Knowledge and Data Engineering,2010,5
P2P; Ad Hoc and Sensor Networks–All the Different or All the Same,Peter A Boncz; Angela Bonifati; J Böse; S Böttcher; P Kypros; CL Gruenwald; B König-Ries; W May; A Mondal; S Obermeier; A Ouksel; G Samaras; G Sapkota; R Steinmetz; SD Viglas,*,Proc. of Scalable Data Management in Evolving Networks,2007,5
AmbientDB: P2P Database Technology for Ambient Intelligent Multimedia Applications,PA Boncz,KNAW Narcis. Back to search results. Publication AmbientDB: P2P Database Technology forAmbient Intelligent Multimedia Applications (2003). Pagina-navigatie: Main …,ERCIM News,2003,5
Optimizing main-memory join on modern hardware,Stefan Manegold; Peter A Boncz; Martin L Kersten,Abstract In the past decade; the exponential growth in commodity CPUs speed has faroutpaced advances in memory latency. A second trend is that CPU performance advancesare not only brought by increased clock rate; but also by increasing parallelism inside theCPU. Current database systems have not yet adapted to these trends; and show poorutilization of both CPU and memory resources on current hardware. In this article; we showhow these resources can be optimized for large joins and translate these insights intoguidelines for future database architectures; encompassing data structures; algorithms; costmodeling; and implementation. In particular; we discuss how vertically fragmented datastructures optimize cache performance on sequential data access. On the algorithmic side;we refine the partitioned hash-join with a new partitioning algorithm called radix-cluster …,*,1999,5
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,David Roman,Resolving the problem of y2k compliance is a serious issue for the distributed enterprise. Asorganizations rely on distributed desktops for decision making and productivity; the risksassociated with noncompliant desktops are receiving increased attention from the media;industry analysts; government officials; and corporate leaders. Although most organizationshave been aggressively correcting the Y2K problem on their central mainframe applications;many are only beginning to address the significant risks posed by errant desktop software;hardware; and firmware. Since these distributed assets are critical to corporate productivity;organizations are vulnerable to significant risks if any of the distributed informationtechnology assets cannot properly process four-digit dates.Consider; for example;investment bankers who issue their first bond trades of the new millennium using …,Communications of the ACM,2000,4
Bitwise dimensional co-clustering for analytical workloads,Stephan Baumann; Peter Boncz; Kai-Uwe Sattler,Abstract Analytical workloads in data warehouses often include heavy joins where queriesinvolve multiple fact tables in addition to the typical star-patterns; dimensional grouping andselections. In this paper we propose a new processing and storage framework called bitwisedimensional co-clustering (BDCC) that avoids replication and thus keeps updates fast; yet isable to accelerate all these foreign key joins; efficiently support grouping and pushes downmost dimensional selections. The core idea of BDCC is to cluster each table on a mix ofdimensions; each possibly derived from attributes imported over an incoming foreign keyand this way creating foreign key connected tables with partially shared clusterings. Theseare later used to accelerate any join between two tables that have some dimension incommon and additionally permit to push down and propagate selections (reduce I/O) and …,The VLDB Journal,2016,3
System and method using partial just-in-time complation to resolve memory access pattern problems in hash table probing,*,A system and method for just in time compilation for hash table probing are disclosed. In oneimplementation; the method of using just-in-time compilation is used to combine the stagesof hash value computation and initial lookup into a single compound operation. In anotherimplementation; the method of using just-in-time compilation is used to combine the stagesof a hash table record and input record checking to detect if these records are equal; and ifnot; fetching the next possible record in the linked list of the per-bucket records.,*,2013,3
High-performance database engine implementing a positional delta tree update system,*,A computer system hosting a column-store database engine is responsive to databaserequests for the update and retrieval of data from within a stable data table and providing forthe storage of database tuples within a column-store organized database structure. Apositional delta tree data structure is implemented in the memory space of the databaseengine and is operatively coupled in an update data transfer path between a databaseengine interface and the stable data table. The positional delta tree data structure includes adifferential data storage layer operative to store differential update data values in positionallydefined relative reference to database tuples stored by the stable data table.,*,2013,3
Run-time Optimization for Pipelined Systems,R Abdel Kader; Maurice Van Keulen; Peter Boncz; Stefan Manegold,Abstract Traditional optimizers fail to pick good execution plans; when faced withincreasingly complex queries and large data sets. This failure is even more acute in thecontext of XQuery; due to the structured nature of the XML language. To overcome thevulnerabilities of traditional optimizers; we have previously proposed ROX; a Run-timeOptimizer for XQueries; which interleaves optimization and execution of full tables. ROX hasproved to be robust; even in the presence of strong correlations; but it has one limitation: ituses full materialization of intermediate results making it unsuitable for pipelined systems.Therefore; this paper proposes ROX-sampled; a variant of ROX; which executes small datasamples; thus generating smaller intermediates. We conduct extensive experiments whichproved that ROX-sampled is comparable to ROX in performance; and that it is still robust …,*,2010,3
Emerging database systems in support of scientific data,Per Svensson; Peter Boncz; Milena Ivanova; Martin Kersten; Niels Nes; Doron Rotem,Abstract. This chapter surveys and discusses the evolution of a certain class of databasearchitectures; more recently referred to as “vertical databases” and “array databases”. Thetopics discussed in this chapter include the evolution of storage structures from the 1970‟ still now; data compression techniques; and query processing techniques for single-and multi-variable queries in vertical databases. Next; the chapter covers in detail the architecture anddesign considerations of a particular (open source) vertical database system; calledMonetDB. This is followed by an example of using MonetDB for the SkyServer data; and thequery processing improvements it offers. The chapter concludes with a discussion of therequirements and planned implementation of a new science data base system; calledSciDB.,Scientific Data Management: Challenges Technology and Deployment,2010,3
StreetTiVo: Using a P2P XML Database System to Manage Multimedia Data in Your Living Room,Ying Zhang; Arjen De Vries; Peter Boncz; Djoerd Hiemstra; Roeland Ordelman,Abstract StreetTiVo is a project that aims at bringing research results into the living room; inparticular; a mix of current results in the areas of Peer-to-Peer XML Database ManagementSystem (P2P XDBMS); advanced multimedia analysis techniques; and advancedinformation retrieval techniques. The project develops a plug-in application for the so-calledHome Theatre PCs; such as set-top boxes with MythTV or Windows Media Center Editioninstalled; that can be considered as programmable digital video recorders. StreetTiVodistributes compute-intensive multimedia analysis tasks over multiple peers (ie; StreetTiVousers) that have recorded the same TV program; such that a user can search in the contentof a recorded TV program shortly after its broadcasting; ie; it enables near real-timeavailability of the meta-data (eg; speech recognition) required for searching the recorded …,*,2009,3
Distributed XQuery and updates processing with heterogeneous XQuery engines,Y Zhang; PA Boncz,textabstractWe demonstrate XRPC; a minimal XQuery extension that enables distributedquerying between heterogeneous XQuery engines. The XRPC language extensionenhances the existing concept of XQuery functions with the Remote Procedure Call (RPC)paradigm. XRPC is orthogonal to all XQuery features; including the XQuery Update Facility(XQUF). Note that executing xquf updating functions over XRPC leads to the phenomenon ofdistributed transactions. XRPC achieves heterogeneity by an open SOAP-based networkprotocol; that can be implemented by any engine; and an XRPC Wrapper that allows evenXRPC-oblivious XQuery engines to handle XRPC requests efficiently. XRPC is fullyimplemented in the open-source MonetDB/XQuery engine; and is demonstrated here to co-operate with Saxon; Galax and X-Hive through the XRPC wrapper. This demonstration …,*,2008,3
PathFinder/MonetDB: XQuery-The Relational Way,Peter Boncz; Torsten Grust; M Keulen,Relational query processors are probably the best understood (as well as the bestengineered) query engines available today. Although carefully tuned to process instances ofthe relational model (tables of tuples); these processors can also provide a foundation forthe evaluation of “alien”(non-relational) query languages: if a relational encoding of the aliendata model and its associated query language is given; the RDBMS may act like a special-purpose processor for the new language. This demonstration features our XQuery compilerPathfinder; the continuation of our earlier work on a purely relational XPath and XQueryprocessing stack [4; 5; 6] in which we developed relational encodings and processingstrategies for the tree-shaped XML data model. The Pathfinder project is an exploration ofhow far we can push the idea of using mature RDBMS technology to design and build a …,Proc. of the 31st VLDB Conference,2005,3
Mx documentation tool,Martin L Kersten; F Schippers; Carel A van den Berg; Peter A Boncz,*,*,1996,3
Parallelizing the crossword generation game in Orca,Peter Boncz,Abstract The Crossword Generation Game (kece) is a problem from the Cowichan SetWil94]; a programming benchmark designed to compare the usability of parallel systems.This paper describes the implementation of-search on kece games in the parallel languageOrca BKT92]; running on the Amoeba distributed operating system MvRT+90]. A benchmarkserial kece implementation was rst written in ANSI C. Some computational analysis wasperformed on the problem. Kece game trees turned out to have a quite variable branchingfactor; but showed very little variation in node values. Serial and parallel versions were thendeveloped in Orca. This took little e ort; but the serial Orca program proved 20 times slowerthan the serial C version. Pro ling was used to determine which Orca constructs werebottlenecks. Performance improved sharply to a factor of 1.2 slower; but only at the …,Student Project Report; Vrije Universiteit Amsterdam,1994,3
Extending the Lighthouse graph engine for shortest path queries,Peter Rutgers; Peter Boncz; Spyros Voulgaris; Claudio Martella,Graph databases are increasingly used to store and analyze datasets that contain a network-like structure. Modelling such a structure in a relational database can be inefficient; becauseanalyzing paths or patterns in the network then requires a large number of join operations.For many applications; such as routing problems; social networks and user behavioranalysis; 1 graph databases allow for very efficient querying. Another advantage is that thedata model can be extended in a natural way: new types of vertices and relations can easilybe added without modifying the existing data structure. Considering that databases need tostore an increasing amount of information; scalability is a very important aspect in designingsuch databases. A way to keep the system scalable is to distribute the graph over a numberof processors. This has been implemented by the Lighthouse graph pattern matching …,*,2015,2
Query Processing and Optimization in Graph Databases.,Andrey Gubichev; Peter Boncz; Sihem Amer-Yahia,A data management problem becomes a graph problem when it concerns not only analysisof the values; but also discovering and exploiting connections between them. The lastdecade has seen the rise of interest in graph data management problems; both in academiaand in industry. The interest was caused by the rapid spread of graph-shaped data comingfrom multiple domains with the two main examples being (i) the Linked Data initiative; whichexploits cross-domain techniques from data mining; natural language processing andmachine learning to construct web-scale knowledge bases; and (ii) Social Media; where theusers and their generated content form a quickly growing graph; the availability of large-scale social networks has motivated numerous Social Network Analysis projects. Theinformation needs in the two domains are best expressed in terms of graph problems …,*,2015,2
Benchmarking Linked Open Data Management Systems,R Angles Rojas; M Pham; PA Boncz,htmlabstractWith inherent support for storing and analysing highly interconnected data;graph and RDF databases appear as natural solutions for developing Linked Open Dataapplications. However; current benchmarks for these database technologies do not fullyattain the desirable characteristics in industrial-strength benchmarks [1](eg relevance;verifiability; etc.) and typically do not model scenarios characterized by complex queriesover skewed and highly correlated data [2]. The Linked Data Benchmark Council (LDBC) isan EU FP7 ICT project that brings together a community of academic researchers andindustry; whose main objective is the development of industrial-strength benchmarks forgraph and RDF databases.,ERCIM News,2014,2
S3g2: A Scalable Structure-Correlated Social Graph Generator,P Minh Duc; PA Boncz; O Erling,Abstract. Benchmarking graph-oriented database workloads and graph-oriented databasesystems is increasingly becoming relevant in analytical Big Data tasks; such as socialnetwork analysis. In graph data; structure is not mainly found inside the nodes; butespecially in the way nodes happen to be connected; ie structural correlations. Becausesuch structural correlations determine join fan-outs experienced by graph analysisalgorithms and graph query executors; they are an essential; yet typically neglected;ingredient of synthetic graph generators. To address this; we present S3G2: a ScalableStructure-correlated Social Graph Generator. This graph generator creates a synthetic socialgraph; containing non-uniform value distributions and structural correlations; which isintended as test data for scalable graph analysis algorithms and graph database systems …,*,2012,2
Column-Oriented Database Systems (Tutorial),D Abadi; Peter A Boncz; Stavros Harizopoulos,textabstractColumn-oriented database systems (column-stores) have attracted a lot ofattention in the past few years. Column-stores; in a nutshell; store each database tablecolumn separately; with attribute values belonging to the same column stored contiguously;compressed; and densely packed; as opposed to traditional database systems that storeentire records (rows) one after the other. Reading a subset of a table's columns becomesfaster; at the potential expense of excessive disk-head seeking from column to column forscattered reads or updates. After several dozens of research papers and at least a dozen ofnew column-store start-ups; several questions remain. Are these a new breed of systems orsimply old wine in new bottles? How easily can a major row-based system achieve column-store performance? Are column-stores the answer to effortlessly support large-scale data …,*,2009,2
Generic and updatable XML value indices covering equality and range lookups,Lefteris Sidirourgos; Peter Boncz,Abstract We describe a collection of indices for XML text; element; and attribute node valuesthat (i) consume little storage;(ii) have low maintenance overhead;(iii) permit fast equi-lookup on string values; and (iv) support range-lookup on any XML typed value (eg; double;dateTime). The equi-lookup string value index depends on an elaborate hash function andon an associative combination function to facilitate updates on both mixed-content andelement nodes. We also present techniques for creating range-lookup indices supportingany ordered XML typed value. These indices rely on a finite state machine that accepts thetype specific language; and on a state combination table for combining states to speed-upupdates. We evaluate the stability of the hash function; the storage overhead; and theindices creation and maintenance time in the context of the open-source XML database …,Proceedings of the 2009 EDBT/ICDT Workshops,2009,2
06431 Working Group Summary: P2P; Ad Hoc and Sensor Networks-All the Different or All the Same?.,Peter A Boncz; Angela Bonifati; Joos-Hendrik Böse; Stefan Böttcher; Panos K Chrysanthis; Le Gruenwald; Arantza Illarramendi; Peter Janacik; Birgitta König-Ries; Wolfgang May; Anirban Mondal; Sebastian Obermeier; Aris M Ouksel; George Samaras,*,Scalable Data Management in Evolving Networks,2006,2
Flexible and efficient IR using Array Databases,R Cornacchia; S Héman; M Zukowski; AP de Vries; PA Boncz; M Zukowski; S Heman; N Nes; PA Boncz; S Heman; N Nes; M Zukowski; PA Boncz; S Heman; M Zukowski; AP de Vries; PA Boncz; S Heman; M Zukowski; AP de Vries; PA Boncz; M Zukowski; S Heman; PA Boncz; M Zukowski; S Heman; N Nes; PA Boncz; M Zukowski; M Zukowski; M Zukowski; PA Boncz; N Nes; S Heman; PA Boncz; M Zukowski; N Nes,*,Submitted for publication,2006,2
Macro-and micro-parallelism in a dbms,Martin Kersten; Stefan Manegold; Peter Boncz; Niels Nes,Abstract Large memories have become an affordable storage medium for databasesinvolving hundreds of Gigabytes on multi-processor systems. In this short note; we reviewour research on building relational engines to exploit this major shift in hardwareperspective. It illustrates that key design issues related to parallelism poses architecturalproblems at all levels of a system architecture and whose impact is not easily predictable.The sheer size/complexity of a relational DBMS and the sliding requirements of frontierapplications are indicative that a substantial research agenda remains wide open.,European Conference on Parallel Processing,2001,2
Query optimization through the looking glass; and what we found running the Join Order Benchmark,Viktor Leis; Bernhard Radke; Andrey Gubichev; Atanas Mirchev; Peter Boncz; Alfons Kemper; Thomas Neumann,Abstract Finding a good join order is crucial for query performance. In this paper; weintroduce the Join Order Benchmark that works on real-life data riddled with correlations andintroduces 113 complex join queries. We experimentally revisit the main components in theclassic query optimizer architecture using a complex; real-world data set and realistic multi-join queries. For this purpose; we describe cardinality-estimate injection and extractiontechniques that allow us to compare the cardinality estimators of multiple industrial SQLimplementations on equal footing; and to characterize the value of having perfect cardinalityestimates. Our investigation shows that all industrial-strength cardinality estimators routinelyproduce large errors: though cardinality estimation using table samples solves the problemfor single-table queries; there are still no techniques in industrial systems that can deal …,The VLDB Journal,2017,1
Special issue: Modern hardware,Peter Boncz; Wolfgang Lehner; Thomas Neumann,While database systems have long enjoyed a “free ride” with ever-increasing clock cycles ofthe CPU; in the last decade this increase stalled. On the computational side; we have seenan ever-increasing number of cores as well as the advent of specialized computing unitsranging from GPUs via FPGA to chips with specific extensions. On the memory side; we notonly observe a significant growth of the capacity of main memory; but a continued largeperformance impact of RAM latency on data access cost; recently aggravated by increasingNUMA effects. Storage-wise we have witnessed the introduction of NAND devices (eg;SSDs) impacting the established role of magnetic disk drive. These advances taken togetherimpact current database architectures and ask for adjustments; extensions or even acomplete re-write in order to establish a scalable; affordable; and flexible foundation for …,The VLDB Journal,2016,1
Ldbc graphalytics: A benchmark for large-scale graph analysis on parallel and distributed platforms; a technical report,Alexandru Iosup; Tim Hegeman; Wing Lung Ngai; Stijn Heldens; Arnau Prat Pérez; Thomas Manhardt; Hassan Chafi; Mihai Capota; Narayanan Sundaram; Michael Anderson; Ilie Gabriel Tanase; Yinglong Xia; Lifeng Nai; Peter Boncz,Abstract In this paper we introduce LDBC Graphalytics; a new industrial-grade benchmarkfor graph analysis platforms. It consists of six deterministic algorithms; standard datasets;synthetic dataset generators; and reference output; that enable the objective comparison ofgraph analysis platforms. Its test harness produces deep metrics that quantify multiple kindsof system scalability; such as horizontal/vertical and weak/strong; and of robustness; such asfailures and performance variability. The benchmark comes with opensource software forgenerating data and monitoring performance. We describe and analyze six implementationsof the benchmark (three from the community; three from the industry); providing insights intothe strengths and weaknesses of the platforms. Key to our contribution; vendors perform thetuning and benchmarking of their platforms. Date Version Changes 2016-Mar-18 1.0-First …,Delft University of Technology; Tech. Rep,2016,1
Automatic schema design for co-clustered tables,Stephan Baumann; Peter Boncz; Kai-Uwe Sattler,Schema design of analytical workloads provides opportunities to index; cluster; partitionand/or materialize. With these opportunities also the complexity of finding the right setuprises. In this paper we present an automatic schema design approach for a table co-clustering scheme called Bitwise Dimensional Co-Clustering; aimed at schemas with amoderate amount dimensions; but not limited to typical star and snowflake schemas. Thegoal is to design one primary schema and keep the knobs to turn to a minimum whileproviding a robust schema for a wide range of queries. In our approach a clustered schemais derived by trying to apply dimensions throughout the whole schema and co-cluster asmany tables as possible according to at least one common dimension. Our approach isbased on the assumption that initially foreign key relationships and a set of dimensions …,Data Engineering Workshops (ICDEW); 2013 IEEE 29th International Conference on,2013,1
Letter from the Special Issue Editor,Peter A Boncz,KNAW Narcis. Back to search results. Publication Letter from theSpecial Issue Editor (2012). Pagina-navigatie: Main …,IEEE Data Engineering Bulletin,2012,1
The Linked Data Benchmark Council (LDBC),Irini Fundulaki; Josep Larriba Pey; David Dominguez-Sal; Ioan Toma; Dieter Fensel; Barry Bishop; Thomas Neumann; Orri Erling; Peter Neubauer; Paul Groth; Frank Van Harmelen; Peter Boncz,In the last years we have seen an explosion of massive amounts of graph shaped datacoming from a variery of applications that are related to social networks (Facebook; Twitter;blogs and other on-line media) and telecommunication networks. Furthermore; the W3CLinking Open Data Initiative [8] has boosted the publication and interlinkage of a largenumber of datasets on the Semantic Web [2] resulting to the Linked Open Data Cloud. Thesedatasets with billions of RDF triples such as Wikipedia [5]; US Census bureau [4]; CIA WorldFactbook [1]; DBPedia [5]; and government sites1 have been created and published online.Moreover; numerous datasets and vocabularies from e-science are published nowadays asRDF graphs most notably in life and earth sciences; astronomy [6][7][3] in order to facilitatecommunity annotation and interlinkage of both scientific and scholarly data of interest …,Proc. First Eur. Data Forum,2012,1
Main Memory DBMS,Peter Boncz,Advances in high throughput sequencing and ''omics''technologies and the resultingexponential growth in the amount of macromolecular sequence; structure; gene expressionmeasurements; have unleashed a transformation of biology from a data-poor science into anincreasingly data-rich science. Despite these advances; biology today; much like physicswas before Newton and Leibnitz; has remained a largely descriptive science. Machinelearning [6] currently offers some of the most cost-effective tools for building predictivemodels from biological data; eg; for annotating new genomic sequences; for predictingmacromolecular function; for identifying functionally important sites in proteins; for identifyinggenetic markers of diseases; and for discovering the networks of genetic interactions thatorchestrate important biological processes [3]. Advances in machine learning eg …,*,2009,1
06472 Abstracts Collection--XQuery Implementation Paradigms,Peter A Boncz; Torsten Grust; Jérôme Siméon; Maurice van Keulen,Abstract From 19.11. 2006 to 22.11. 2006; the Dagstuhl Seminar 06472``XQueryImplementation Paradigms''was held in the International Conference and Research Center(IBFI); Schloss Dagstuhl. During the seminar; several participants presented their currentresearch; and ongoing work and open problems were discussed. Abstracts of thepresentations given during the seminar as well as abstracts of seminar results and ideas areput together in this paper. The first section describes the seminar topics and goals ingeneral. Links to extended abstracts or full papers are provided; if available.,Dagstuhl Seminar Proceedings,2007,1
Working Group Report on Managing and Integrating Data in P2P Databases,Peter A Boncz; Angela Bonifati; Peter Janacik; Birgitta Konig-Ries; Arantza Illarramendi; Wolfgang Lehner; Wolfgang May; Aris Ouksel; Kay Romer; Brahmananda Sapkota; Kai-Uwe Sattler; Heinz Schweppe; Rita Steinmetz; Can Turker,Page 1. Working Group Report on Managing and Integrating Data in P2P Databases PeterA. Boncz CWI; The Netherlands Angela Bonifati ∗ National Research Council; Italy PeterJanacik University of Padeborn; Germany Birgitta K¨onig-Ries Jena University; GermanyArantza Illarramendi Basque Country University; Spain Wolfgang Lehner TU Dresden;Germany Pedro J. Marr ˙on University of Stuttgart; Germany Wolfgang May G¨ottingenUniversity; Germany Aris Ouksel University of Illinois at Chicago; USA Kay R¨omer ETH Zurich;Switzerland Brahmananda Sapkota DERI Research Center; Ireland Kai-Uwe Sattler TUIlmenau; Germany Heinz Schweppe Freie Universitaet Berlin; Germany Rita SteinmetzUniversity of Padeborn; Germany Can T¨urker ETH Zurich; Switzerland …,Proc. of the conference on Scalable Data Management in Evolving Networks,2007,1
Efficient and Flexible Information Retrieval Using a Relational Database Engine,Marcin Zukowski; Sándor Heman; Arjen P de Vries; Peter Boncz,*,*,2006,1
Organic database systems to support an ambient world,Martin Kersten; Peter Boncz,Ambient Intelligence has been identified by the research councils of the EuropeanCommunity as an area of great social and economic potential. The 6-th Framework program(2003-2007) has been crafted along this vision; calling for targeted research anddevelopment to make it come true. For the database community it is one of the rareoccasions where our proven technology could make a difference; provided we are able toidentify its requirements and we are willing to relax our drive for incremental researchactivities. The play-ground for Ambient Intelligence is a physical area with manyautonomous digital devices; which are sensitive; adaptive; and responsive to the presenceof people. Within a home setting it is geared at improving the quality of life by creating adesired atmosphere and functionality via intelligent; personalised interconnected systems …,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,1
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,Sandra A Vannoy; Prashant Palvia,The evolution of programming languages is the stepwise introduction of abstractions hiding theunderlying computer hardware and the details of program execution. Assembly languages introducemnemonic instructions and symbolic labels for hiding machine codes and addresses. Fortranintroduces arrays and expressions in standard mathematical notation for hiding registers.Algol-like languages introduce structured statements for hiding gotos and jump labels.Object-oriented languages introduce visibility levels and encapsulation for hiding the representationof data and the management of memory. Along these lines; declarative languages—the mostprominent representatives of which are functional and logic languages—hide the order of evaluationby removing assignment and other control statements. A declarative program is a set of logicalstatements describing properties of the application domain. The execution of a …,Communications of the ACM,*,1
Adaptive Geospatial Joins for Modern Hardware,Andreas Kipf; Harald Lang; Varun Pandey; Raul Alexandru Persa; Peter Boncz; Thomas Neumann; Alfons Kemper,Abstract: Geospatial joins are a core building block of connected mobility applications. Anespecially challenging problem are joins between streaming points and static polygons.Since points are not known beforehand; they cannot be indexed. Nevertheless; points needto be mapped to polygons with low latencies to enable real-time feedback. We present anadaptive geospatial join that uses true hit filtering to avoid expensive geometriccomputations in most cases. Our technique uses a quadtree-based hierarchical grid toapproximate polygons and stores these approximations in a specialized radix tree. Weemphasize on an approximate version of our algorithm that guarantees a user-definedprecision. The exact version of our algorithm can adapt to the expected point distribution byrefining the index. We optimized our implementation for modern hardware architectures …,arXiv preprint arXiv:1802.09488,2018,*
Exploring Query Execution Strategies for JIT; Vectorization and SIMD,Tim Gubner; Peter Boncz,ABSTRACT This paper partially explores the design space for efficient query processors onfuture hardware that is rich in SIMD capabilities. It departs from two well-knownapproaches:(1) interpreted block-at-a-time execution (aka “vectorization”) and (2)“data-centric” JIT compilation; as in the HyPer system. We argue that in between these two designpoints in terms of granularity of execution and unit of compilation; there is a whole designspace to be explored; in particular when considering exploiting SIMD. We focus on TPC-HQ1; providing implementation alternatives (“flavors”) and benchmarking these on variousarchitectures. In doing so; we explain in detail considerations regarding operating on SQLdata in compact types; and the system features that could help using as compact data aspossible. We also discuss various implementations of aggregation; and propose a new …,*,2017,*
JCC-H: Adding Join Crossing Correlations with Skew to TPC-H,Peter Boncz; Angelos-Christos Anatiotis; Steffen Kläbe,Abstract We introduce JCC-H; a drop-in replacement for the data and query generator ofTPC-H; that introduces Join-Crossing-Correlations (JCC) and skew into its dataset andquery workload. These correlations are carefully designed such that the filter predicates ontable columns in the existing TPC-H queries now suddenly can have effects on the value-;frequency-and join-fan-out-distributions; experienced by operators in the query plan. Thequery generator of JCC-H is able to generate parameter bindings for the 22 query templatesin two different equivalence classes: query templates that receive “normal” parameters donot experience skew and behave very similar to default TPC-H queries. Query templatesexpanded with the “skewed” parameters; though; experience strong join-crossing-correlations and skew in filter; aggregation and join operations. In this paper we discuss …,Technology Conference on Performance Evaluation and Benchmarking,2017,*
Multi-Hypothesis CSV Parsing,Till Döhmen; Hannes Mühleisen; Peter Boncz,Abstract Comma Separated Value (CSV) files are commonly used to represent data. CSV isa very simple format; yet we show that it gives rise to a surprisingly large amount ofambiguities in its parsing and interpretation. We summarize the state-of-the-art in CSVparsers; which typically make a linear series of parsing and interpretation decisions; suchthat any wrong decision at an earlier stage can negatively affect all downstream decisions.Since computation time is much less scarce than human time; we propose to turn CSVparsing into a ranking problem. Our quality-oriented multi-hypothesis CSV parsing approachgenerates several concurrent hypotheses about dialect; table structure; etc. and ranks thesehypotheses based on quality features of the resulting table. This approach makes it possibleto create an advanced CSV parser that makes many different decisions; yet keeps the …,Proceedings of the 29th International Conference on Scientific and Statistical Database Management,2017,*
Extending SQL for Computing Shortest Paths,Dean De Leo; Peter Boncz,Abstract Reachability and shortest paths are among two of the most common queriesrealized on graphs. While graph frameworks and property graph databases provide anextensive and convenient built-in support for these operations; it is still both clunky andinefficient to perform on standard SQL DBMSs. In this paper; we present an extension to thestandard SQL language to compute both reachability predicates and many-to-many shortestpath queries. We first describe a methodology to represent a directed graph starting fromvirtual table expressions. Second; we introduce a new type of operator to compute shortestpaths on the given graph. Our semantic abides by the rules of operating with tableexpressions; ensuring that the property of the closure from the relational algebra is retained.Finally; we developed a prototype implementation of our extension on top of MonetDB; an …,Proceedings of the Fifth International Workshop on Graph Data-management Experiences & Systems,2017,*
Faster across the PCIe bus: a GPU library for lightweight decompression: including support for patched compression schemes,Eyal Rozenberg; Peter Boncz,Abstract This short paper present a collection of GPU lightweight decompression algorithmsimplementations within a FOSS library; Giddy-the first to be published to offer suchfunctionality. As the use of compression is important in ameliorating PCIe data transferbottlenecks; we believe this library and its constituent implementations can serve as usefulbuilding blocks in GPU-accelerated DBMSes---as well as other data-intensive systems.,Proceedings of the 13th International Workshop on Data Management on New Hardware,2017,*
Constant-vector computation system and method that exploits constant-value sequences during data processing,*,Data compression; commonly used in database systems provides multiple benefits; includingreduced disk and memory requirements and reduced data transfer bandwidth. It also highlightsproperties of data that can be used to execute queries more efficiently by processing compresseddata without de-compressing it. Different solutions for such compressed execution have beenproposed but most focused on different forms of data encoding; reducing processing time forindividual records. As such; they did not exploit opportunities provided by databases that workon multiple records in one processing stage by processing data on a vector or block granularityas discussed in Marcin Zukowski. Balancing Vectorized Query Execution with Bandwidth-OptimizedStorage. PhD thesis; Universiteit van Amsterdam; 2009 (hereinafter “Zuk09”.) The only well-knownmethod that operates on multiple compressed records with identical values at the same …,*,2016,*
Powerful and efficient bulk shortest-path queries: cypher language extension & Giraph implementation,Peter Rutgers; Claudio Martella; Spyros Voulgaris; Peter Boncz,Abstract Shortest-path computation is central to many graph queries. However; currentgraph-processing platforms tend to offer limited solutions; typically supporting only single-source and all-pairs shortest path algorithms; with poor filtering options. In this paper weaddress the shortest-path computation problem in two complementary directions. First; weintroduce a restrictable; top-N" bulk" shortest-weighted-paths operator in the Cypher graphquery language; that subsumes all previously known shortest path variants. In addition toease of use; both in terms of short notation and more robust performance thanks toguaranteed amenability to pruning; this operator supports calculated path weights; as wellas filtering on the path edges and vertices. Second; we provide a scalable algorithm for theparallel implementation of this top-N operator on Giraph; a graph-processing system …,Proceedings of the Fourth International Workshop on Graph Data Management Experiences and Systems,2016,*
MonetDB Jun2016 feature release,ML Kersten; PA Boncz; NJ Nes; S Manegold; KS Mullender; JA deRijke; Y Zhang; M Pham; LHA Scheers; E Petraki; THJ Sellam; Y Kargin; MM Gawade; HF Mühleisen; K Kyzirakos; DG Nedev; CP Cijvat; F Alvanaki; M vanDinther; E Sidirourgos; RA Koopmanschap; M Raasveldt; P Koutsourakis; TR Döhmen; BB Kruit; A Wits; PE Ferreira; SG Stalinov,KNAW Narcis. Back to search results. Publication MonetDB Jun2016 feature release (2016).Pagina-navigatie: Main. Save publication: Save as MODS; Export to Mendeley; Save as EndNote;Export to RefWorks. Title; MonetDB Jun2016 feature release. Author; ML Kersten (Martin); PA Boncz(Peter); NJ Nes (Niels); S. Manegold (Stefan); KS Mullender (Sjoerd); JA de Rijke (Arjen); Y. Zhang(Ying); M.-D. Pham (Minh-Duc); LHA Scheers (Bart); E. Petraki (Eleni); THJ Sellam (Thibault); Y.Kargin (Yagiz); MM Gawade (Mrunal); HF Mühleisen (Hannes); K. Kyzirakos (Konstantinos); DGNedev (Dimitar); CP Cijvat (Robin); F. Alvanaki (Foteini); M. van Dinther (Martin); E. Sidirourgos(Eleftherios); RA Koopmanschap (Richard); M. Raasveldt (Mark); P. Koutsourakis (Panagiotis);TR Döhmen (Till); BB Kruit (Benno); A. Wits (Abe); PE Silva Ferreira (Pedro); SG Stalinov (Svetlin).Supporting host; Database Architectures. Date; 2016 …,*,2016,*
VectorH: taking SQL-on-Hadoop to the next level,M Switakowski; A Costea; A Ionescu; B Raducanu; C Bârca; J Sompolski; A Łuszczak; M Szafranski; G De Nijs; PA Boncz,htmlabstractIn this paper we describe VectorH: a new SQL-on-Hadoop system built on top ofthe fast Vectorwise analytical database system. VectorH achieves fault tolerance andscalable data storage by relying on HDFS; extending the state-of-the-art in SQL-on-Hadoopsystems by instrumenting the HDFS block replication policy to ensure local reads undermost circumstances. VectorH integrates with YARN for workload management; achieving ahigh degree of elasticity. Even though HDFS is an append-only filesystem; and it supportsordered table storage; VectorH can accommodate trickle updates through Positional DeltaTrees (PDTs); a differential update structure that can be queried efficiently. We describe themain technical extensions to single-server Vectorwise that turned it into a Hadoop-basedMPP system; in terms of workload management; parallel query optimization and …,*,2016,*
MonetDB Oct2014-SP1 bugfix release,ML Kersten; PA Boncz; NJ Nes; S Manegold; KS Mullender; JA deRijke; Y Zhang; H Pirk; M Pham; LHA Scheers; E Petraki; THJ Sellam; Y Kargin; MM Gawade; HF Mühleisen; S Héman; K Kyzirakos; DG Nedev; CP Cijvat; F Alvanaki; M vanDinther; E Sidirourgos; MG Ivanova,KNAW Narcis. Back to search results. Publication MonetDB Oct2014-SP1 bugfix release (2014).Pagina-navigatie: Main. Save publication: Save as MODS; Export to Mendeley; Save as EndNote;Export to RefWorks. Title; MonetDB Oct2014-SP1 bugfix release. Author; ML Kersten (Martin);PA Boncz (Peter); NJ Nes (Niels); S. Manegold (Stefan); KS Mullender (Sjoerd); JA de Rijke (Arjen);Y. Zhang (Ying); H. Pirk (Holger); M.-D. Pham (Minh-Duc); LHA Scheers (Bart); E. Petraki (Eleni);THJ Sellam (Thibault); Y. Kargin (Yagiz); MM Gawade (Mrunal); HF Mühleisen (Hannes); S. Héman(Sándor); K. Kyzirakos (Konstantinos); DG Nedev (Dimitar); CP Cijvat (Robin); F. Alvanaki (Foteini);M. van Dinther (Martin); E. Sidirourgos (Eleftherios); MG Ivanova (Milena). Supporting host;Database Architectures; Database Architectures. Date; 2014-11-01. Language; English. Type;Other. Publication; https://ir.cwi.nl/pub/22666 …,*,2014,*
MonetDB Oct2014 feature release,ML Kersten; PA Boncz; NJ Nes; S Manegold; KS Mullender; JA deRijke; Y Zhang; H Pirk; M Pham; LHA Scheers; E Petraki; THJ Sellam; Y Kargin; MM Gawade; HF Mühleisen; S Héman; K Kyzirakos; DG Nedev; CP Cijvat; F Alvanaki; M vanDinther; E Sidirourgos; MG Ivanova,KNAW Narcis. Back to search results. Publication MonetDB Oct2014 feature release (2014).Pagina-navigatie: Main. Save publication: Save as MODS; Export to Mendeley; Save as EndNote;Export to RefWorks. Title; MonetDB Oct2014 feature release. Author; ML Kersten (Martin); PABoncz (Peter); NJ Nes (Niels); S. Manegold (Stefan); KS Mullender (Sjoerd); JA de Rijke (Arjen);Y. Zhang (Ying); H. Pirk (Holger); M.-D. Pham (Minh-Duc); LHA Scheers (Bart); E. Petraki (Eleni);THJ Sellam (Thibault); Y. Kargin (Yagiz); MM Gawade (Mrunal); HF Mühleisen (Hannes); S.Héman (Sándor); K. Kyzirakos (Konstantinos); DG Nedev (Dimitar); CP Cijvat (Robin); F. Alvanaki(Foteini); M. van Dinther (Martin); E. Sidirourgos (Eleftherios); MG Ivanova (Milena). Supportinghost; Database Architectures; Database Architectures. Date; 2014-10-01. Language; English.Type; Other. Publication; https://ir.cwi.nl/pub/22647 …,*,2014,*
Achieving many-core scalability in Vectorwise,Peter Boncz,Abstract This thesis has two major purposes:(1) to analyze the issues hindering (many-core)scalability in Vectorwise and (2) to modernize the still dominant approach for intra-queryparallelization: Volcano-model parallelism. The analysis was conducted over the TPC-Hqueries which showed that hash-based (re-) distribution from producer threads to consumerthreads is one of factors limiting scalability. Other issues limiting scalability were thesequential building of HashJoin's hash table and the Reuse operator which runs sequentialand forces parallel stream to be joined before and forked afterwards. Further the surveyshowed that locking in the I/O layer can become a scalability issue. A general issue inparallel query evaluation is skew which may dynamically appear in query plans. TheVolcano-model parallelism provides no way handling dynamic effects due to its static …,*,2014,*
Accelerating Big Data Analytics with Vector Processing: A Q&A with Peter Boncz of Actian-BeyeNETWORK,PA Boncz,KNAW Narcis. Back to search results. Publication Accelerating Big Data Analyticswith Vector Processing: A Q&A... (2014). . Pagina-navigatie: Main …,*,2014,*
How to generate query parameters in RDF benchmarks?,Andrey Gubichev; Renzo Angles; Peter Boncz,In this paper we consider the problem of generating parameters for queries in RDFbenchmarks. We show that uniform random sampling of the substitution parameters is notwell suited for RDF benchmarks; since it results in unpredictable runtime behavior ofqueries. We formulate a formal problem of parameter generation to ensure stable andstatistically significant benchmark results.,Data Engineering Workshops (ICDEW); 2014 IEEE 30th International Conference on,2014,*
Benchmarking Graph Data Management Systems,Peter Boncz,Page 1. Benchmarking Graph Data Management Systems EDBT Summer School 2015 PeterBoncz boncz@cwi.nl 1. LDBC Social Network Benchmark Tuesday: LDBC & SNB introductionFriday: SNB in depth 2. SNB Programming Challenge www.cwi.nl/~boncz/snb-challengeTuesday: what it is about & hardware properties & tips Friday: the solution space & winnersPage 2. www.cwi.nl/~boncz/snb-challenge • make competing products comparable • accelerateprogress; make technology viable Why Benchmarking? © Jim Gray; 2005 Page 3.www.cwi.nl/~boncz/snb-challenge What is the LDBC? Linked Data Benchmark Council = LDBC •Industry entity similar to TPC (www.tpc.org) • Focusing on graph and RDF store benchmarkingPage 4. www.cwi.nl/~boncz/snb-challenge LDBC Organization (non-profit) “sponsors” +non-profit members (FORTH; STI2) & personal members …,10442/13867,2014,*
Database Innovator Peter Boncz to Present on Expanded Actian Analytics Platform SQL-in-Hadoop Capabilities at Hadoop-Fort Mill Times Summit,PA Boncz,KNAW Narcis. Back to search results. Publication Database Innovator Peter Bonczto Present on Expanded Actian... (2014). . Pagina-navigatie: Main …,*,2014,*
Benchmarking Linked Open Data Management Systems,Renzo Angles; Minh-Duc Pham; Peter Boncz,With inherent support for storing and analysing highly interconnected data; graph and RDFdatabases appear as natural solutions for developing Linked Open Data applications.However; current benchmarks for these database technologies do not fully attain thedesirable characteristics in industrial-strength benchmarks [1](eg relevance; verifiability; etc.)and typically do not model scenarios characterized by complex queries over skewed andhighly correlated data [2]. The Linked Data Benchmark Council (LDBC) is an EU FP7 ICTproject that brings together a community of academic researchers and industry; whose mainobjective is the development of industrial-strength benchmarks for graph and RDFdatabases.,ERCIM News,2014,*
MonetDB/RDF: Discovering and Exploiting the Emergent Schema of RDF Data,M Pham; Peter A Boncz,htmlabstractThe Resource Description Framework (RDF) has been used as the main datamodel for the semantic web and Linked Open Data; providing great flexibility for users torepresent and evolve data without need for a prior schema. This flexibility; however; poseschallenges in implementing efficient RDF stores. It i) leads to query plan with many self-joinsin triple tables; ii) blocks the use of advanced relational physical storage optimization suchas clustered indexes and data partitioning; and iii) the lack of a schema sometimes makes itproblematic for users to comprehend the data and formulate queries [1]. In the DatabaseArchitecture group at CWI; Amsterdam; we tackle these RDF data management problems byautomatically recovering the structure present in RDF data; leveraging this structure bothinternally inside the database systems (in storage; optimization; and execution); and …,ERCIM News,2014,*
Designing engines for data analysis,PA Boncz,The age of Big Data. As the world turns increasingly digital; and all individuals andorganizations interact more and more via the internet; for instance in social networks such asFacebook and Twitter; as they look for information online with Google; and watch movies orTV on YouTube and Netflix; the amount of information flying around is huge. Slide 1 depictswhat happens on the internet during one minute; 60 seconds. The numbers are staggering;and are expected to keep growing. For instance; the amount of information being sentcorresponds to a thousand full large disk drives; per minute. This is a hard disk drive stack of20 meters high. All stored digital information together is estimated currently at 3 billion fulldisk drives; that is 4 zettabytes (when you start with a megabyte; a 1000 times that is agigabyte; by multiplying again you get a petabyte; and then a zettabyte),*,2014,*
Foundations and Trends® in Databases,Daniel Abadi; Peter Boncz; Stavros Harizopoulos; Stratos Idreos; Samuel Madden,*,Foundations and Trends® in Databases,2013,*
MonetDB Feb2013-SP4 bugfix release,ML Kersten; PA Boncz; NJ Nes; S Manegold; KS Mullender; JA deRijke; Y Zhang; S Idreos; E Sidirourgos; H Pirk; M Pham; LHA Scheers; E Petraki; THJ Sellam; Y Kargin; MM Gawade; HF Mühleisen; S Héman; K Kyzirakos,KNAW Narcis. Back to search results. Publication MonetDB Feb2013-SP4 bugfix release (2013).Pagina-navigatie: Main. Save publication: Save as MODS; Export to Mendeley; Save as EndNote;Export to RefWorks. Title; MonetDB Feb2013-SP4 bugfix release. Author; ML Kersten (Martin);PA Boncz (Peter); NJ Nes (Niels); S. Manegold (Stefan); KS Mullender (Sjoerd); JA de Rijke(Arjen); Y. Zhang (Ying); S. Idreos (Stratos); E. Sidirourgos (Eleftherios); H. Pirk (Holger); M.-D.Pham (Minh-Duc); LHA Scheers (Bart); E. Petraki (Eleni); THJ Sellam (Thibault); Y. Kargin (Yagiz);MM Gawade (Mrunal); HF Mühleisen (Hannes); S. Héman (Sándor); K. Kyzirakos (Konstantinos).Supporting host; Database Architectures; Database Architectures. Date; 2013-09-01. Language;English. Type; Other. Publication; https://ir.cwi.nl/pub/21480. Persistent Identifier;urn:NBN:nl:ui:18-21480. Metadata; XML. Source; CWI …,*,2013,*
Humboldt Award voor CWI'er Peter Boncz-Automatiseringgids. nl,PA Boncz,KNAW Narcis. Back to search results. Publication Humboldt Award voor CWI'er PeterBoncz - Automatiseringgids.nl (2013). Pagina-navigatie: Main …,*,2013,*
MonetDB Feb2013-SP1 bugfix release,ML Kersten; PA Boncz; NJ Nes; S Manegold; KS Mullender; FE Groffen; JA deRijke; Y Zhang; S Idreos; RA Goncalves; E Sidirourgos; H Pirk; M Pham; LHA Scheers; E Petraki; THJ Sellam; Y Kargin; MM Gawade; HF Mühleisen,KNAW Narcis. Back to search results. Publication MonetDB Feb2013-SP1 bugfix release (2013).Pagina-navigatie: Main. Save publication: Save as MODS; Export to Mendeley; Save as EndNote;Export to RefWorks. Title; MonetDB Feb2013-SP1 bugfix release. Author; ML Kersten (Martin);PA Boncz (Peter); NJ Nes (Niels); S. Manegold (Stefan); KS Mullender (Sjoerd); FE Groffen(Fabian); JA de Rijke (Arjen); Y. Zhang (Ying); S. Idreos (Stratos); RA Goncalves (Romulo); E.Sidirourgos (Eleftherios); H. Pirk (Holger); M.-D. Pham (Minh-Duc); LHA Scheers (Bart); E. Petraki(Eleni); THJ Sellam (Thibault); Y. Kargin (Yagiz); MM Gawade (Mrunal); HF Mühleisen (Hannes).Supporting host; Database Architectures; Database Architectures. Date; 2013-03-01. Language;English. Type; Other. Publication; https://ir.cwi.nl/pub/21316. Persistent Identifier;urn:NBN:nl:ui:18-21316. Metadata; XML. Source; CWI …,*,2013,*
Optimizing database architecture for machine architecture: is there still hope?,Peter Boncz,In the keynote; I will give some examples of how computer architecture has strongly evolvedin the past decennia and how this influences the performance; and therefore the design; ofalgorithms and data structure for data management. One the one hand; these changes inhardware architecture have caused the (continuing) need for new data managementresearch. ie hardware-conscious database research. Here; I will draw examples fromhardware-conscious research performed on the CWI systems MonetDB and Vectorwise.This diversification trend in computer architectural characteristics of the various solutions inthe market seems to be intensifying. This is seen in quite different architectural options; suchas CPU vs GPU vs FPGA; but also even restricting oneself to just CPUs there seems to beincreasing design variation in architecture and platform behavior. This poses a challenge …,Liebe Teilnehmerinnen und Teilnehmer,2013,*
TR-IEEE Data Engineering Bulleting (DEBULL),PA Boncz,In the past five years; columnar storage technologies have gained momentum in the analyticalmarketplace. This resulted from (i) a number of new successful entrants in the database marketwith products based on columnar technology; as well as (ii) established vendors introducingcolumnar products or even integrating columnar technology deep inside their existingproducts. In this issue; you find a number of papers that describe systems that fit one or evenboth of these catagories: Infobright and MonetDB as pure column stores; and Virtuoso;Vectorwise; IBM's Blink based products; Microsoft SQLserver; and SAP HANA (through itsP*TIME subsystem) are products that combine row- and column-technology; this also holds forthe research systems HYRISE and Hyper. Given that column store systems have been on themarket now for a few years; the commercial system papers also describe customer …,*,2013,*
De rekenmeesters van de NS en de Deltawerken. Elsevier; 14 juli 2012,CWI CWI; JCM Baeten; RD van deMei; GJ Hoekstra; PA Boncz; R Wolf; PD Grünwald,KNAW Narcis. Back to search results. Publication De rekenmeesters van de NS en deDeltawerken. Elsevier; 14 juli 2012 (2012). Pagina-navigatie: Main …,*,2012,*
MonetDB Dec2011 feature release,ML Kersten; PA Boncz; NJ Nes; S Manegold; KS Mullender; FE Groffen; JA deRijke; Y Zhang; MG Ivanova; S Idreos; RA Goncalves; E Sidirourgos; E Liarou; H Pirk; M Pham; LHA Scheers; E Petraki; THJ Sellam; Y Kargin; MM Gawade; VV Meduri,KNAW Narcis. Back to search results. Publication MonetDB Dec2011 feature release (2012).Pagina-navigatie: Main. Save publication: Save as MODS; Export to Mendeley; Save as EndNote;Export to RefWorks. Title; MonetDB Dec2011 feature release. Author; ML Kersten (Martin); PABoncz (Peter); NJ Nes (Niels); S. Manegold (Stefan); KS Mullender (Sjoerd); FE Groffen (Fabian);JA de Rijke (Arjen); Y. Zhang (Ying); MG Ivanova (Milena); S. Idreos (Stratos); RA Goncalves(Romulo); E. Sidirourgos (Eleftherios); E. Liarou (Erietta); H. Pirk (Holger); M.-D. Pham(Minh-Duc); LHA Scheers (Bart); E. Petraki (Eleni); THJ Sellam (Thibault); Y. Kargin (Yagiz); MMGawade (Mrunal); VV Meduri (Vamsi). Supporting host; Database Architectures; name unknown.Date; 2012-01-01. Language; English. Type; Other. Publication; https://ir.cwi.nl/pub/19382.Persistent Identifier; urn:NBN:nl:ui:18-19382. Metadata; XML. Source; CWI …,*,2012,*
Linked Stream Data Processing: Facts and Figures,LP Danh; DT Minh; P Minh Duc; PA Boncz; E Thomas; F Michael,Abstract. Linked Stream Data; ie; the RDF data model extended for representing stream datagenerated from sensors social network applications; is gaining popularity. This hasmotivated considerable work on developing corresponding data models associated withprocessing engines. However; current implemented engines have not been thoroughlyevaluated to assess their capabilities. For reasonable systematic evaluations; in this workwe propose a novel; customizable evaluation framework and a corresponding methodologyfor realistic data generation; system testing; and result analysis. Based on this evaluationenvironment; extensive experiments have been conducted in order to compare the state-of-the-art LSD engines wrt. qualitative and quantitative properties; taking into account theunderlying principles of stream processing. Consequently; we provide a detailed analysis …,*,2012,*
Query processing of pre-partitioned data using Sandwich Operators,PA Boncz; S Baumann; K-U Sattler,Abstract. In this paper we present the “Sandwich Operators”; an elegant approach to exploitpre-sorting or pre-grouping from clustered storage schemes in operators such asAggregation/Grouping; HashJoin; and Sort of a database management system. Thereby;each of these operator types is “sandwiched” by two new operators; namely PartitionSplitand PartitionRestart. PartitionSplit splits the input relation into its smaller independentgroups on which the sandwiched operator is executed. After a group is processedPartitionRestart is used to trigger the execution on the following group. Executing one ofthese operator types with the help of the Sandwich Operators introduces minimal overheadand does not penalty performance of the sandwiched operator as its implementationremains unchanged. On the contrary; we show that sandwiched execution of an operator …,*,2012,*
Vectorwise: a Vectorized Analytical DBMS,PA Boncz; M Zukowski; M Van der Wiel,Abstract—Vectorwise is a new entrant in the analytical database marketplace whosetechnology comes straight from innovations in the database research community in the pastyears. The product has since made waves due to its excellent performance in analyticalcustomer workloads as well as benchmarks. We describe the history of Vectorwise; as wellas its basic architecture and the experiences in turning a technology developed in anacademic context into a commercial-grade product. Finally; we turn our attention to recentperformance results; most notably on the TPC-H benchmark at various sizes.,*,2012,*
MonetDB Aug2011-SP1 bugfix release,ML Kersten; PA Boncz; NJ Nes; S Manegold; KS Mullender; FE Groffen; JA deRijke; Y Zhang; MG Ivanova; S Idreos; RA Goncalves; E Sidirourgos; E Liarou; H Pirk; M Pham; LHA Scheers; E Petraki,KNAW Narcis. Back to search results. Publication MonetDB Aug2011-SP1 bugfix release (2011).Pagina-navigatie: Main. Save publication: Save as MODS; Export to Mendeley; Save as EndNote;Export to RefWorks. Title; MonetDB Aug2011-SP1 bugfix release. Author; ML Kersten (Martin);PA Boncz (Peter); NJ Nes (Niels); S. Manegold (Stefan); KS Mullender (Sjoerd); FE Groffen(Fabian); JA de Rijke (Arjen); Y. Zhang (Ying); MG Ivanova (Milena); S. Idreos (Stratos); RAGoncalves (Romulo); E. Sidirourgos (Eleftherios); E. Liarou (Erietta); H. Pirk (Holger); M.-D. Pham(Minh-Duc); LHA Scheers (Bart); E. Petraki (Eleni). Supporting host; Database Architectures;name unknown. Date; 2011-09-01. Language; English. Type; Other. Publication;https://ir.cwi.nl/pub/19379. Persistent Identifier; urn:NBN:nl:ui:18-19379. Metadata; XML. Source;CWI. Go to Website Navigation: Home; about narcis; login; Nederlands. contact …,*,2011,*
Integrating Cooperative Scans in a column-oriented DBMS,Peter A Boncz; Frank J Seinstra; Marcin Żukowski; BV VectorWise,Abstract Data warehousing requires huge amounts of data to be stored on persistentstorage. The usual way to access this data is to perform a sequential scan that loads andprocesses all the data in order it is stored. In many application areas including businessintelligence; data mining and decision support systems there are multiple concurrent queriestouching a considerable fraction of a relation. In most database management systems thedata is accessed using a „Scan” operator that sequentially issues page request to theunderlying buffer manager that caches most recently used data. In a concurrentenvironment; this approach may lead to suboptimal utilization of the available diskbandwidth; as the buffer manager may evict pages that could soon be reused by an anotherrunning scan. Cooperative Scans is a new algorithm that improves performance of …,*,2011,*
Just-in-time Compilation in Vectorized Query Execution,Marcin Zukowski; BV VectorWise; Peter Boncz; Henri Bal,Abstract Database management systems face high interpretation overhead when supportingflexible declarative query languages such as SQL. One approach to solving this problem isto on-thefly generate and compile specialized programs for evaluating each query. Another;used by the VectorWise DBMS; is to process data in bigger blocks at once; which allows toamortize interpretation overhead over many database records and enables modern CPUs toeg use SIMD instructions. Both of these approaches provide an order of magnitudeperformance improvement over traditional DBMSes. In this master thesis we ask thequestion whether combining these two techniques can yield yet significantly better resultsthan using each of them alone. Using a proof of concept system we perform microbenchmarks and identify cases where compilation should be combined with block-wise …,*,2011,*
Simple Solutions for Compressed Execution in Vectorized Database System,Marcin Zukowski; BV VectorWise; Peter Boncz; Jacopo Urbani,Abstract Compressed execution is a method of operating directly on compressed data toimprove database management system performance. Unfortunately; previously proposedtechniques for compressed execution required major modifications of database engine andwere difficult to introduce into a DBMS. In this master thesis we look for solutions that areeasier to implement; unintrusive and well suited to the architecture of VectorWise. Weintroduce an optimization based on RLE-compressed execution that consists of constantvectors and primitive swapping mechanism. We also investigate possible benefits ofoperating on dictionary-encoded data and propose on-the-fly dictionaries—a solution forproviding uniform domain-wide encoding while avoiding concurrency issues. The conductedexperiments show a considerable potential of proposed techniques.,*,2011,*
MonetDB Apr2011-SP1 bugfix release,ML Kersten; PA Boncz; NJ Nes; S Manegold; KS Mullender; FE Groffen; JA deRijke; Y Zhang; MG Ivanova; S Idreos; RA Goncalves; E Sidirourgos; E Liarou; H Pirk; M Pham; LHA Scheers; E Petraki,KNAW Narcis. Back to search results. Publication MonetDB Apr2011-SP1 bugfix release (2011).Pagina-navigatie: Main. Save publication: Save as MODS; Export to Mendeley; Save as EndNote;Export to RefWorks. Title; MonetDB Apr2011-SP1 bugfix release. Author; ML Kersten (Martin);PA Boncz (Peter); NJ Nes (Niels); S. Manegold (Stefan); KS Mullender (Sjoerd); FE Groffen(Fabian); JA de Rijke (Arjen); Y. Zhang (Ying); MG Ivanova (Milena); S. Idreos (Stratos); RAGoncalves (Romulo); E. Sidirourgos (Eleftherios); E. Liarou (Erietta); H. Pirk (Holger); M.-D. Pham(Minh-Duc); LHA Scheers (Bart); E. Petraki (Eleni). Supporting host; Database Architectures;name unknown. Date; 2011-05-01. Language; English. Type; Other. Publication;https://ir.cwi.nl/pub/19376. Persistent Identifier; urn:NBN:nl:ui:18-19376. Metadata; XML. Source;CWI. Go to Website Navigation: Home; about narcis; login; Nederlands. contact …,*,2011,*
The Story of Vectorwise (BDA Keynote),PA Boncz,KNAW Narcis. Back to search results. Publication The Story of Vectorwise(BDA Keynote) (2011). Pagina-navigatie: Main …,*,2011,*
Letter from the Special Issue Editor,Peter A Boncz,KNAW Narcis. Back to search results. Publication Letter from theSpecial Issue Editor (2010). . Pagina-navigatie: Main …,IEEE Data Engineering Bulletin,2010,*
2010 Index IEEE Transactions on Knowledge and Data Engineering Vol. 22,Osman Abul; Grigoris Antoniou; Kiyoshi Asai; Andrey Balmin; Robert M Balzer; Zhifeng Bao; Andrzej Bargiela; Payam Barnaghi; Nick Bassiliades; Dizza Beimel; David Bell; Thomas Bernecker; James Bezdek; Zeungnam Bien; Antonis Bikakis; Roland Billen; Harold Boley; PA Bonatti; Francesco Bonchi; Peter Boncz; Mihaela A Bornea; Athman Bouguettaya; Ramadhana Bramandia; Nicholas J Bryan; Di Cai; K Selcuk Candan; Bin Cao; Longbing Cao; Yong Cao; Chia-Hui Chang; Michael Chau; Muhammad Aamir Cheema; Bo Chen; Degang Chen; Honghui Chen; Hsinchun Chen; Huanhuan Chen; Jie Chen; Jinlin Chen; Lei Chen; Ming-Syan Chen; Xue-wen Chen; Yanhua Chen; Haibin Cheng; Hong Cheng; Reynold Cheng; David W-l Cheung; Byron Choi; Chung-Hua Chu; Yi-Hong Chu; Kun-Ta Chuang; Yon Dohn Chung; Eliseo Clementini; Christopher Clifton; Sergio Consoli,This index covers all technical items-papers; correspondence; reviews; etc.-that appeared inthis periodical during the year; and items from previous years that were commented upon orcorrected in this year. Departments and other items may also be covered if they have beenjudged to have archival value. The Author Index contains the primary entry for each item;listed under the first author's name. The primary entry includes the coauthors' names; the titleof the paper or other item; and its location; specified by the publication abbreviation; year;month; and inclusive pagination. The Subject Index contains entries describing the itemunder all appropriate subject headings; plus the first author's name; the publicationabbreviation; month; and year; and inclusive pages. Note that the item title is found onlyunder the primary entry in the Author Index.,IEEE Transactions on Knowledge and Data Engineering,2010,*
Run-time Optimization for Pipelined Systems,R Abdel Kader; Maurice vanKeulen; PA Boncz; Stefan Manegold,textabstractTraditional optimizers fail to pick good execution plans; when faced withincreasingly complex queries and large data sets. This failure is even more acute in thecontext of XQuery; due to the structured nature of the XML language. To overcome thevulnerabilities of traditional optimizers; we have previously proposed ROX; a Run-timeOptimizer for XQueries; which interleaves optimization and execution of full tables. ROX hasproved to be robust; even in the presence of strong correlations; but it has one limitation: ituses full materialization of intermediate results making it unsuitable for pipelined systems.Therefore; this paper proposes ROX-sampled; a variant of ROX; which executes small datasamples; thus generating smaller intermediates. We conduct extensive experiments whichproved that ROX-sampled is comparable to ROX in performance; and that it is still robust …,*,2010,*
Multi-core parallelization of vectorized query execution,Henri Bal; Peter Boncz; Marcin Żukowski,Abstract Due to the recent trends in computer hardware; especially processors; parallelquery execution is becoming more and more important in modern database managementsystems (DBMS). In this master thesis; we discuss the implementation of a parallel queryexecution system based on a new family of operators that was firstly presented in theVolcano DBMS. We combine this approach with a highly efficient vectorized in-cacheexecution model used in the VectorWise DBMS. We present different strategies ofincorporating new operators into an execution tree. Finally; we propose possibleoptimizations and measure the performance of implemented solutions.,*,2010,*
Oogstjaar voor CWI-onderzoekers ‘We investeren bewust in technologie’,PA Boncz; ML Kersten; S Manegold; D Riksen,textabstractIn augustus 2009 ontvingen Peter Boncz; Stefan Manegold en Martin Kerstenvan het Centrum Wiskunde & Informatica de prestigieuze VLDB 10-year Best Paper Award.Hun winnende artikel beschreef tien jaar geleden de eerste ideeën over 'hardware-aware'ontwerpen van databasesystemen. Sindsdien maken zowel databaseleveranciers alsspin-offs gebruik van de ideeën en de open source code van het CWI.,*,2010,*
ROX: The Robustness of a Run-time XQuery Optimizer Against Correlated Data (Demo Paper),R Abdel Kader; PA Boncz; S Manegold; M vanKeulen,textabstractWe demonstrate ROX; a run-time optimizer of XQueries; that focuses on findingthe best execution order of XPath steps and relational joins in an XQuery. The problem ofjoin ordering has been extensively researched; but the proposed techniques are stillunsatisfying. These either rely on a cost model which might result in inaccurate estimations;or explore only a restrictive number of plans from the search space. ROX is developed totackle these problems. ROX does not need any cost model; and defers query optimization torun-time intertwining optimization and execution steps. In every optimization step; samplingtechniques are used to estimate the cardinality of unexecuted steps and joins to make adecision which sequence of operators to process next. Consequently; each execution stepwill provide updated and accurate knowledge about intermediate results; which will be …,*,2010,*
Database architecture (R) evolution: New hardware vs. new software,Stavros Harizopoulos; Tassos Argyros; Peter A Boncz; Dan Dietterich; Samuel Madden; Florian M Waas,The last few years have been exciting for data management system designers. Theexplosion in user and enterprise data coupled with the availability of newer; cheaper; andmore capable hardware have lead system designers and researchers to rethink and; insome cases; reinvent the traditional DBMS architecture. In the space of data warehousingand analytics alone; more than a dozen new database product offerings have recentlyappeared; and dozens of research system papers are routinely published each year. Amongthese efforts; one school of thought promotes research on exploiting and anticipating newhardware (many-core CPUs [4; 7; 8]; GPUs [3]; FPGAs [5; 11]; flash SSDs [6]; other non-volatile storage technologies). Another school of thought focuses on software andalgorithmic issues (column and hybrid stores [1; 10; 13]; scale out architectures using …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,*
Letter from the Special Issue Editor.,Mohamed F Mokbel,Spatial and spatio-temporal databases provide backbone support for a set of widely usedapplications including geographic information systems; location-based services; movingobjects databases; transportation; and emergency services. This special issue includes tenarticles geared towards new frontiers of spatial and spatiotemporal databases. The firstarticle by Sankaranarayanan and Samet presents a paradigm shift in querying roadnetworks where they strongly advocate for storing road networks in relational databases; asopposed to the widely used graph data structure. The article introduces a new data structure;called road network oracle; that resides in a database and enables the processing of manyoperations on road networks with just the aid of relational operators. Doing so also takesadvantage of the power of SQL queries along with the database query optimizers. The …,IEEE Data Eng. Bull.,2010,*
Processor Cache,Peter Boncz,P/FDM [5–7] integrated a functional data model with the logic programming language Prologfor general-purpose computation. The data model can be seen as an Entity-Relationshipdiagram with sub-types; much like a UML Class Diagram. The idea was for the user to beable to define a computation over objects in the diagram; instead of just using it as a schemadesign aid. Later versions of P/FDM included a graphic interface [2; 4] to build queries inDAPLEX syntax by clicking on the diagram and filling in values from menus.,*,2009,*
Disk,Peter Boncz,realized using such logical structures. For example; in tree based data acquisition protocols;a collection tree is built that is rooted at the data collection center such as the sink node [8].The dissemination of the data requests from the participating nodes and collection of datafrom the sensor nodes are accomplished using this tree. A cluster based data acquisitionmechanism has been proposed in [3]. As shown in Fig. 1; nodes are organized into a fixednumber of clusters; and nodes within each cluster dynamically elect a cluster head. The dataacquisition is carried out in two phases. In the first phase; cluster heads collect data fromtheir cluster nodes. In the second phase; cluster heads send collected data to the nodes thathave subscribed to the data. The cluster heads are re-elected to balance energyconsumption among the nodes in the cluster. Zhang et al.[13] have proposed an adaptive …,*,2009,*
Main Memory,Peter Boncz,Advances in high throughput sequencing and ''omics''technologies and the resultingexponential growth in the amount of macromolecular sequence; structure; gene expressionmeasurements; have unleashed a transformation of biology from a data-poor science into anincreasingly data-rich science. Despite these advances; biology today; much like physicswas before Newton and Leibnitz; has remained a largely descriptive science. Machinelearning [6] currently offers some of the most cost-effective tools for building predictivemodels from biological data; eg; for annotating new genomic sequences; for predictingmacromolecular function; for identifying functionally important sites in proteins; for identifyinggenetic markers of diseases; and for discovering the networks of genetic interactions thatorchestrate important biological processes [3]. Advances in machine learning eg …,*,2009,*
Address to the special edition of the infolab seminar (stellar cast),Peter Boncz,*,*,2008,*
REPORT INS-E0802 DECEMBER 2008,E Sidirourgos; PA Boncz,ABSTRACT We describe a collection of indices for XML text; element; and attribute nodevalues that (i) consume little storage;(ii) have low maintenance overhead;(iii) permit fast equi-lookup on string values; and (iv) support range-lookup on any XML typed value (eg; double;dateTime). The equi-lookup string value index depends on an elaborate hash function andon an associative combination function to facilitate updates on both mixed-content andelement nodes. We also present techniques for creating range-lookup indices supportingany ordered XML typed value. These indices rely on a finite state machine that accepts thetype specific language; and on a state combination table for combining state to speed-upupdates. We evaluate the stability of the hash function; the storage overhead; and theindices creation and maintenance time in the context of the open-source XML database …,*,2008,*
REPORT INS-E0801 AUGUST 2008,S Héman; N Nes; M Zukowski; PA Boncz,ABSTRACT We investigate techniques that marry the high readonly analytical queryperformance of compressed; replicated column storage (“read optimized” databases) withthe ability to handle a high-throughput update workload. Today's large RAM sizes and thegrowing gap between sequential vs. random IO disk throughput; bring this once elusive goalin reach; as it has become possible to buffer enough updates in memory to allowbackground migration of these updates to disk; where efficient sequential IO is amortizedamong many updates. Our key goal is that read-only queries always see the latest databasestate; yet are not (significantly) slowed down by the update processing. To this end; wepropose the Positional Delta Tree (PDT); that is designed to minimize the overhead of on-the-fly merging of differential updates into (index) scans on stale disk-based data. We …,*,2008,*
W18-PDMST'07 & GRep'07: 4th international workshop on P2P Data Management; Security; and Trust: 3rd international workshop on Data management in Global d...,Sanjay Kumar Madria; Anirban Mondal; Mukesh K Mohania; Bharat Bhargava; Stephane Bressan; Mizuho Iwaihara; Abdelkader Hameurlain; Takahiro Hara; Leszek Lilien; SK Gupta; Sourav S Bhowmick; Somchai Chatvichienchai; Cui Bin; Ladjel Bellatreche; Ismail Khalil Ibrahim; Kenji Saito; Gao Cong; Jialie Shen; Debojyoti Dutta; Pankaj Garg; Peter Boncz; Claudiu Duma; Mouna Kacimi; Gruenwald Le; Yugyung Lee; Murali Mani; Esther Palomar; Evi Pitoura; Kian Lee Tan; Orazio Tomarchio; Goce Trajcevski; Sabrina De Capitani Di Vimercati; Pinar Yolum,In: Proceedings - International Workshop on Database and Expert Systems Applications;DEXA; 2007; p. 775-776 … Powered by Pure; Scopus & Elsevier Fingerprint Engine™ © 2017Elsevier BV.,Proceedings-International Workshop on Database and Expert Systems Applications; DEXA,2007,*
Generic Database Cost Models for Hierarchical Memory Systems.,PA BONCZ; ML KERSTEN; S MANEGOLD,*,*,2007,*
REPORT INS-E0701 JANUARY 2007,R Cornacchia; S Héman; M Zukowski; AP de Vries; PA Boncz,ABSTRACT The Matrix Framework is a recent proposal by IR researchers to flexiblyrepresent all important information retrieval models in a single multi-dimensional arrayframework. Computational support for exactly this framework is provided by the arraydatabase system SRAM (Sparse Relational Array Mapping) that works on top of a DBMS.Information retrieval models can be specified in its comprehension-based array querylanguage; in a way that directly corresponds to the underlying mathematical formulas. SRAMefficiently stores sparse arrays in (compressed) relational tables and translates andoptimizes array queries into relational queries. In this work; we describe a number of arrayquery optimization rules and demonstrate their effect on text retrieval in the TREC TeraBytetrack (TREC-TB) efficiency task; using the Okapi BM25 model as our example. It turns out …,*,2007,*
Cooperative Scans: Bandwidth Sharing in a Compressed Column Store,Marcin Zukowski; Sándor Héman; Niels Nes; Peter Boncz,*,*,2007,*
06431 Working Group Report on Managing and Integrating Data in P2P Databases,Peter A Boncz; Angela Bonifati; Arantza Illarramendi; Peter Janacik; Birgitta König-Ries; Wolfgang Lehner; Pedro Jose Marrón; Wolfgang May; Aris Ouksel; Kay Römer; Brahmananda Sapkota; Kai-Uwe Sattler; Heinz Schweppe; Rita Steinmetz; Can Türker,Abstract In this report; to our best recollection; we provide a summary of the working groupĆ¢ ā ‚¬ Ė Managing and Integrating Data in P2P DatabasesĆ¢ ā ‚¬ ā „¢ of the DagstuhlSeminar nr. 6431 on Ć¢ ā ‚¬ Ė Scalable Data Management in Evolving NeworksĆ¢ ā ‚¬ ā „¢;held on October 23-27 in Dagstuhl (Germany).,Dagstuhl Seminar Proceedings,2007,*
P2P XQuery and the StreetTiVo application,Peter A Boncz; Yi Zhang,Abstract MonetDB/XQuery* is a fully functional publicly available XML DBMS that has beenextended with distributed and P2P data management functionality. Our (minimal) XQuerylanguage extension XRPC adds the concept of RPC to XQuery; and we outlined ourapproach to include the services offered by diverse P2P network structures (such as DHTs);in a way that avoids any further intrusion in the XQuery language and semantics. We alsodiscussed the StreetTiVo application were mxq is being used for data management in alarge P2P environment. new construct called XRPC.,Dagstuhl Seminar Proceedings,2007,*
06472 Executive Summary--XQuery Implementation Paradigms,Peter A Boncz; Torsten Grust; Jérôme Siméon; Maurice van Keulen,Abstract Only a couple of weeks after the participants of seminar No. 06472 met in Dagstuhl;the W3C published the Final Recommendation documents that fix the XQuery 1.0 syntax;data model; formal semantics; built-in function library and the interaction with the XMLSchema Recommendations (see W3C's XQuery web site at http://www. w3. org/XML/Query/).With the language's standardization nearing its end and now finally in place; the manyefforts to construct correct; complete; and efficient implementations of XQuery finally got ridof the hindering" moving target''syndrome. This Dagstuhl seminar on the different XQueryimplementation paradigms that have emerged in the recent past; thus was as timely as itcould have possibly been.,Dagstuhl Seminar Proceedings,2007,*
Report on the Second International Workshop on Data Management on Modern Hardware (DaMoN'06),Anastassia Ailamaki; Peter Boncz; Stefan Manegold,Abstract: This report summarizes the presentations and discussions that occurred during theSecond International Workshop on Data Management on Modern Hardware (DaMoN).DaMoN was held in Chicago on June 25th; 2006; and was collocated with ACM SIGMOD2006. The aim of this one-day workshop is to bring together researchers interested inoptimizing database performance on modern computing infrastructure by designing newdata management techniques and tools.,ACM SIGMOD Record,2006,*
Second International ACM/SIGMOD Workshop on Data Management on New Hardware (DaMoN 2006),A Ailamaki; PA Boncz; S Manegold,KNAW Narcis. Back to search results. Publication Second International ACM/SIGMODWorkshop on Data Management on New... (2006). Pagina-navigatie: Main …,*,2006,*
Current tools to analyze memory dumps of systems running Microsoft Windows usually build on the concept of enumerating lists maintained by the kernel to keep tra...,Jesse Kornblum; W Alink; RAF Bhoedjang; PA Boncz; AP de Vries,Homologous files share identical sets of bits in the same order. Because such files are notcompletely identical; traditional techniques such as cryptographic hashing cannot be used toidentify them. This paper introduces a new technique for constructing hash signatures bycombining a number of traditional hashes whose boundaries are determined by the contextof the input. These signatures can be...,Digital Investigation,2006,*
Representing and querying multi-dimensional markup for question answering,DD Ahn; W Alink; V Jijkoun; M de Rijke; PA Boncz; A Vries,If you believe that digital publication of certain material infringes any of your rights or(privacy) interests; please let the Library know; stating your reasons. In case of a legitimatecomplaint; the Library will make the material inaccessible and/or remove it from the website.Please send a message to: UBAcoach; or a letter to: Library of the University of Amsterdam;Secretariat; Singel 425; 1012 WP Amsterdam; The Netherlands. You will be contacted as soonas possible.,*,2006,*
XML Databases-a look inside the kitchen,Peter Boncz,*,*,2006,*
XIRAF Ultimate Forensic Querying,Wouter Alink; Raoul Bhoedjang; Peter Boncz; Arjen de Vries,A typical digital forensic investigation involves these four phases: 1. media capture (eg;forensic disk duplication); 2. feature extraction (eg; parsing file systems; mailboxes; chatlogs; etc.); 3. analysis (browsing; querying; correlating); 4. reporting (writing down findingsfor court). This paper addresses two key problems that occur in the feature extraction andanalysis phases of a computer system investigation. First; the amount of data to process in atypical investigation is huge. Modern computer systems are routinely equipped withhundreds of gigabytes of storage and a large investigation will often involve multiplesystems; so the amount of data to process can run into terabytes. The amount of timeavailable for processing this data is often limited (eg; because of legal limitations). Also; theprobability that a forensic investigator will miss important traces increases,digital investigation,2006,*
REPORT INS-E0607 NOVEMBER 2006,Y Zhang; PA Boncz,ABSTRACT XRPC is a minimal XQuery extension that enables distributed query execution;combining the Remote Procedure Call (RPC) paradigm with the existing concept of XQueryfunctions. By calling out of a for-loop to multiple destinations; and by calling functions thatthemselves perform XRPC calls; complex P2P communication patterns can be achieved. Wefurther propose the use of SOAP as the protocol for XRPC; which allows seamlessintegration with web services and Service Oriented Architectures (SOA). XRPC isimplemented in the open source MonetDB/XQuery system. We show that the technique ofloop-lifting; that executes all expressions inside a for-loop in a single bulk operator--pervasively applied in MonetDB/XQuery to obtain efficient relational query plans--alsobenefits XRPC. Loop-lifting enables us to send bulk RPC requests; dramatically reducing …,*,2006,*
MonetDB/X100 at the 2006 TREC Terabyte Track-Notebook paper,S Héman; M Zukowski; AP deVries; PA Boncz,textabstractThis paper describes our participation to the TREC HARD track (High AccuracyRetrieval of Documents) and the the TREC Enterprise track. The main goal of our HARDparticipation is the development and evaluation of so-called query profiles: Short summariesof the retrieved results that enable the user to perform more focused search; for instance byzooming in on a particular time period. The main goal of our Enterprise track participation isto investigate the potential of the structural information for this type of retrieval task. Inparticular; we study the use of the thread information and the subject and header fields of theemail documents. As a secondary and long standing research goal; we aim at developingan information retrieval framework that supports many diverse retrieval applications bymeans of one simple yet powerful query language (similar to SQL or relational algebra) …,*,2006,*
MonetDB/XQuery-Consistent & Efficient Updates on the Pre/Post Plane (Demo Poster),PA Boncz; S Manegold; KS Mullender; J Flokstra; M vanKeulen; T Grust; J Rittinger; J Teubner,KNAW Narcis. Back to search results. Publication MonetDB/XQuery-Consistent & Efficient Updateson the Pre/Post Plane (Demo Poster) (2006). Pagina-navigatie: Main …,*,2006,*
Third Workshop on Ambient Databases,P Boncz; Maurice van Keulen; AH van Bunningen,Skip to main content University of Twente Research Information Logo …,*,2005,*
Organisatie Dutch-Belgian Database Day,P Boncz; Maurice van Keulen; AH van Bunningen,KNAW Narcis. Back to search results. Publication OrganisatieDutch-Belgian Database Day (2005). Pagina-navigatie: Main …,*,2005,*
MonetDB/XQuery; Technology preview open source release 1,P Boncz; S Manegold; Sjoerd Mullender; Maurice van Keulen; Jan Flokstra; T Grust; J Teubner; J Rittinger,Boncz; P; Manegold; S; Mullender; S; van Keulen; M; Flokstra; J; Grust; T; Teubner; J &Rittinger; J MonetDB/XQuery; Technology preview open source release 1 …MonetDB/XQuery; Technology preview open source release 1. / Boncz; P.; Manegold; S.;Mullender; Sjoerd; van Keulen; Maurice; Flokstra; Jan; Grust; T.; Teubner; J.; Rittinger; J … BonczP; Manegold S; Mullender S; van Keulen M; Flokstra J; Grust T et al. MonetDB/XQuery; Technologypreview open source release 1. 2005 … Powered by Pure; Scopus & Elsevier FingerprintEngine™ © 2017 Elsevier BV.,*,2005,*
First International ACM/SIGMOD Workshop on Data Management on New Hardware,A Ailamaki; PA Boncz; S Manegold,KNAW Narcis. Back to search results. Publication First International ACM/SIGMOD Workshopon Data Management on New Hardware (2005). Pagina-navigatie: Main …,*,2005,*
MonetDB,ML Kersten; PA Boncz; NJ Nes; S Manegold; KS Mullender; FE Groffen; MG Ivanova; Y Zhang; RA Goncalves; E Sidirourgos; E Liarou; S Idreos; JA deRijke; AP deVries; W Alink; R Cornacchia; JFP van deAkker; AR vanBallegooij; CA van deBerg; JR Castelo; J Flokstra; CA Galindo-Legaria; T Grust; S Héman; D Hiemstra; T Ianeva; JS Karlsson; M vanKeulen; S deKonink; JA List; N Mamoulis; GJ Molenaar; G Modena; S Göldner; AJ Pellenkoft; HGP Bosch; W Quak; G Ramirez Camps; J Rittinger; H Rode; W Scherphof; AR Schmidt; N Tang; J Teubner; C Treijtel; T Tsikrika; F Waas; THW Westerveld; MA Windhouwer; M Zukowski; A Gafriller; A Singh; A Scherpenisse; B Brodbeck; G deNijs; M Mayr; M Antonelli; M vanDinther; R Aly; R vanOs; S Mayer; S Kerschbaumer; T Ressel; T Schreiber,KNAW Narcis. Back to search results. Publication MonetDB (2005). Pagina-navigatie: Main. Savepublication: Save as MODS; Export to Mendeley; Save as EndNote; Export to RefWorks. Title;MonetDB. Author; ML Kersten (Martin); PA Boncz (Peter); NJ Nes (Niels); S. Manegold (Stefan);KS Mullender (Sjoerd); FE Groffen (Fabian); MG Ivanova (Milena); Y. Zhang (Ying); RA Goncalves(Romulo); E. Sidirourgos (Eleftherios); E. Liarou (Erietta); S. Idreos (Stratos); JA de Rijke (Arjen);AP de Vries (Arjen); W. Alink (Wouter); R. Cornacchia (Roberto); JFP van den Akker; AR vanBallegooij; CA van den Berg; JR Castelo; J. Flokstra; CA Galindo-Legaria; T. Grust; S. Héman(Sándor); D. Hiemstra; T. Ianeva; JS Karlsson; M. van Keulen; S. de Konink (Stefan …,*,2005,*
REPORT INS-E0511 JULY 2005,M Zukowski; S Héman; N Nes; PA Boncz,ABSTRACT High-performance data-intensive query processing tasks like OLAP; datamining or scientific data analysis can be severely I/O bound; even when high-end RAIDstorage systems are used. Compression can alleviate this bottleneck only if encoding anddecoding speeds significantly exceed RAID I/O bandwidth. For this purpose; we proposethree new versatile compression schemes (PDICT; PFOR; and PFOR-DELTA) that arespecifically designed to extract maximum IPC from modern CPUs. We compare thesealgorithms with compression techniques used in (commercial) database and informationretrieval systems. Our experiments on the MonetDB/X100 database system; using both DSMand PAX disk storage; show that these techniques strongly accelerate TPC-H performanceto the point that the I/O bottleneck is eliminated.,*,2005,*
Report INS-E0510 May 2005,PA Boncz; T Grust; M van Keulen; S Manegold; J Rittinger; J Teubner,ABSTRACT Various techniques have been proposed for efficient evaluation of XPathexpressions; where the XPath location steps are rooted in a single sequence of contextnodes. Among these techniques; the staircase join allows to evaluate XPath location stepsalong arbitrary axes in at most one scan over the XML document; exploiting the XPathaccelerator encoding (aka. pre/post encoding).,*,2005,*
REPORT INS-E0506 APRIL 2005,PA Boncz; S Manegold; J Rittinger,ABSTRACT We outline an efficient ACID-compliant mechanism for structural inserts anddeletes in relational XML document storage that uses a region based pre/size/levelencoding (equivalent to the pre/post encoding). Updates to such node-numbering schemesare considered prohibitive (ie physical cost linear to document size); because structuralupdates cause shifts in all pre-numbers after the update point; and require updates of thesize of all ancestors; such that the root of the tree becomes a locking bottleneck. We showhow such locking can be avoided by updating the size of ancestors using delta-increments;which are transaction-commutative operations. We also reduce the physical cost to theminimum (ie linear to update volume) by carefully exploiting the virtual column feature ofMonetDB to store pre numbers (virtual columns are never materialized; and thus need not …,*,2005,*
REPORT INS-E0503 MARCH 2005,PA Boncz; T Grust; S Manegold; J Rittinger; J Teubner,ABSTRACT Using a relational DBMS as back-end engine for an XQuery processing systemleverages relational query optimization and scalable query processing strategies providedby mature DBMS engines in the XML domain. Though a lot of theoretical work has beendone in this area and various solutions have been proposed; no complete systems havebeen made available so far to give the practical evidence that this is a viable approach. Inthis paper; we describe the ourely relational XQuery processor Pathfinder that has been builton top of the extensible RDBMS MonetDB. Performance results indicate that the system iscapable of evaluating XQuery queries efficiently; even if the input XML documents becomehuge. We additionally present further contributions such as loop-lifted staircase join;techniques to derive order properties and to reduce sorting effort in the generated …,*,2005,*
Reminiscences on influential papers,Kenneth A Ross,This paper abstracts from the particular features of PRISMA/DB; and evaluates and analyzesthe performance trade-offs for a wide range of parallel query processing strategies. Its clearstyle of presentation; along with careful attention to previous work both in its discussion aswell as in the experiments and analysis; make this paper into a concise introductory or“refereshment” text for researchers interested in parallel query execution.,ACM SIGMOD Record,2004,*
REPORT INS-E0411 DECEMBER 2004,M Zukowski; PA Boncz; ML Kersten,ABSTRACT Data mining; information retrieval and other application areas exhibit a queryload with multiple concurrent queries touching a large fraction of a relation. This leads toindividual query plans based on a table scan or large index scan. The implementation of thisaccess path in most database systems is straightforward. The" Scan" operator issues nextpage requests to the buffer manager without concern for the system state. Conversely; thebuffer manager is not aware of the work ahead and it focuses on keeping the most-recently-used pages in the buffer pool. This paper introduces" cooperative scans"--a new algorithm;based on a better sharing of knowledge and responsibility between the" Scan" operator andthe buffer manager; which significantly improves performance of concurrent scan queries. Inthis approach; queries share the buffer content; and progress of the scans is optimized by …,*,2004,*
REPORT INS-E0406 JUNE 2004,Cache-Conscious Radix-Decluster Projections; S Manegold; PA Boncz; NJ Nes; ML Kersten,ABSTRACT As CPUs become more powerful with Moore's law and memory latencies stayconstant; the impact of the memory access performance bottleneck continues to grow onrelational operators like join; which can exhibit random access on a memory region largerthan the hardware caches. While cache-conscious variants for various relational algorithmshave been described; previous work has mostly ignored (the cost of) projection columns.However; real-life joins almost always come with projections; such that proper projectioncolumn manipulation should be an integral part of any generic join algorithm. In this paper;we analyze cache-conscious hash-join algorithms including projections on two storageschemes: N-ary Storage Model (NSM) and Decomposition Storage Model (DSM). It turnsout; that the strategy of first executing the join and only afterwards dealing with the …,*,2004,*
REPORT INS-R0306 JUNE 30; 2003,PA Boncz; C Treijtel,*,*,2003,*
REPORT INS-R0203 MARCH 31; 2002,S Manegold; PA Boncz; ML Kersten,X mtp X pR gjs {8 f 8 H {8 DR 5 j E¥{R j 8¥¤{Rm ECE gjc D {R cm R {¢¡ s 8 R!£ j 8 m¤¥ § ¦ R8 R© 8 R {R ªR R¦¡ H c R 8'5 D!8 jª {R j 8 EE¦ 8 «¬{R c®¤ mE¥ g D {¦{©±²¥ E j¬ UR¥ E¡ ³ (m 3hc¦¦ § D 8 ER scf RD¤ ³ tR j § 8 c D {Rc {RH Ë j § DRU 8 {5 jË UR¥ E¡ 9 R``G (UD h DR¥ UDD``8! R© s© 8 D© D j µ¶ R {R 9¥· E¢ µ¹ E8¤ E 8 {R j U {§ 8 c D {R¤ 8 DR 5 j 8 R j¤ h º R cs {8ER¦¡ XCR! D jf¥ D¦¦ x R RR UR¥ EC µ® R f {D¦¦¡ E© DDE£``¢¥ R cjjm R©¢ DR 5 R RR¡'D fC» pR ¹ ¼d½B E {¢¥ E ²¥ RUD¡ µ¢ D¥{R¦ H¶ m RUR¥ E¡! R````E c¤ ¾# j» p fª {R© R f¡ h¥ D«UR § E vc¥ j 8 h {8 g¦ RB¥ EU8¤ D¦ x¤ º 9 Ë¤{¢ R¦ Dp» p {8 DE c¿ 9© ER {RC h RD E m RRj 3 RR gjE¥ DR f 8 R (5 D v 8 c D {R 9 8 DR 5 8 R¤ Às UDR j¡¤ UC f»¶{R § UR¥ E¡! R``{jRRDR 3 {{ªD DUE c DR f 8 R v {jvc¥ j R j# R``cvE cv 5 DR R3¦ fª {R¦ t {R 9¥ RUD¡ h Ë R 5 RRj¡ R¤ § R (ED {f 8 R!{{{5 E¥ f RC£3 R``8¤ U g 8 (ªE RC 8 {¥{{E» RR {R 5 R f RCGDD {8 …,*,2001,*
Proc.\of the Int'1.\Conf.\on Data,PA Boncz; AN Wilschut; ML Kersten,If you believe that digital publication of certain material infringes any of your rights or(privacy) interests; please let the Library know; stating your reasons. In case of a legitimatecomplaint; the Library will make the material inaccessible and/or remove it from the website.Please Ask the Library; or send a letter to: Library of the University of Amsterdam;Secretariat; Singel 425; 1012 WP Amsterdam; The Netherlands. You will be contacted as soonas possible.,*,1998,*
An Impressionist Sketch of an Advanced Database System,Peter A Boncz; Martin L Kersten,Abstract Monet is a customizable database system developed at CWI and University ofAmsterdam; intended to be used as the database backend for widely varying applicationdomains. It is designed to get maximum database performance out of today's workstationsand multiprocessor systems. It has already achieved considerable success in supporting aData Mining application 12; 13]; and work is well under way in a project where it is used in ahigh-end GIS application. Monet is a type-and algebra-extensible database system andemploys shared memory parallelism. In this paper; we give the goals and motivation ofMonet; and outline its architectural features; including its use of the Decomposed StorageModel (DSM); emphasis on bulk operations; use of main virtual-memory and servercustomization. As a case example; we discuss some issues on how to build a GIS on top …,*,1994,*
An Impressionist Sketch of an Advanced Database System Peter A. Boncz University of Amsterdam,Peter A Boncz,*,*,1994,*
Vortex: taking SQL-on-Hadoop to the next level,Andrei Costea; Adrian Ionescu; Bogdan Raducanu; Michał Switakowski; Cristian Bârca; Juliusz Sompolski; Alicja Łuszczak; Michał Szafranski; Giel de Nijs; Peter Boncz,ABSTRACT Vortex is a new SQL-on-Hadoop system built on top of the fast Vectorwiseanalytical database system. Vortex achieves fault tolerance and scalable data storage byrelying on HDFS; extending the state-of-the-art in SQL-on-Hadoop systems by instrumentingthe HDFS block replication policy to ensure local reads under most circumstances. Vortexintegrates with YARN for workload management; achieving a high degree of elasticity. Eventhough HDFS is an append-only filesystem; and it supports ordered table storage; Vortexcan accommodate trickle updates through Positional Delta Trees (PDTs); a differentialupdate structure that can be queried efficiently. We describe the main technical extensionsto single-server Vectorwise that turned it into a Hadoop-based MPP system; in terms ofworkload management; parallel query optimization and execution; HDFS storage …,*,*,*
Prepared By: Vivek Vishal Shrivastava Trainee; INS1 shrivast@ cwi. nl,Under Guidance Of; Niels Nes; Peter Alexander Boncz,ABSTRACT X100 is the prototype of the database kernel aimed to be 100 times faster thanthe normal RDBMS on the modern hardware. This report is a synopsis of my contributions tothe X100 project. I present here a buffer manager for asynchronous I/O and a scheme forlightweight compression of numerical data.,*,*,*
Cost models for Main-Memory database systems,Peter Boncz; Stefan Manegold,Page 1. Cost models for Main-Memory database systems Peter Boncz Stefan Manegold CWI(Amsterdam) “Generic Database Cost Models for Hierarchical Memory Systems”; Manegold; Boncz;Kersten; VLDB'02 Page 2. VLDB 2009 Summer School Shanghai Architecture-Conscious DatabaseTechniques 2 Contents “Generic Database Cost Models for Hierarchical Memory Systems”;Manegold; Boncz; Kersten; VLDB'02 Page 3. VLDB 2009 Summer School ShanghaiArchitecture-Conscious Database Techniques 3 “Generic Database Cost Models for HierarchicalMemory Systems”; Manegold; Boncz; Kersten; VLDB'02 Page 4. VLDB 2009 Summer SchoolShanghai Architecture-Conscious Database Techniques 4 “Generic Database Cost Models forHierarchical Memory Systems”; Manegold; Boncz; Kersten; VLDB'02 Page 5. VLDB 2009 SummerSchool Shanghai Architecture-Conscious Database Techniques 5 vz …,*,*,*
Vortex,Peter Boncz,Page 1. Confidential © 2014 Actian Corporation 1 Vortex Vectorwise-on-Hadoop Peter BonczTUM workshop; February 11; 2016 Page 2. Confidential © 2014 Actian Corporation 2 NewSIGMOD Paper: ▪ www.cwi.nl/~boncz/vortex-sigmod2016.pdf Page 3. Confidential © 2014 ActianCorporation 3 Vortex origin: Vectorwise ▪ 2005: invented as MonetDB/X100 • Vectorized queryprocessing ▪ Reducing interpretation overhead; exploiting SIMD cidr05 ▪ vectorizeddecompression (formatsPFOR;PDELTA;PDICT) icde06 ▪ Cooperative Scans & Predictive BufferManager vldb07&12 ▪ Mixing NSM and DSM in the query pipeline damon08 ▪ Positional DeltaTrees – for updates sigmod10 ▪ Compilation &&-|| Vectorization damon11 ▪ Run-time adaptation:“micro-adaptivity” sigmod13 ▪ Advanced Table Clustering (→ new BDCC paper) vldbj16 ▪Vectorized Scans in Hyper sigmod16 ▪ 2008: spin-off company …,*,*,*
A Big Data Benchmark for Graph-Processing Platforms,Mihai Capotã; Yong Guo; Ana Lucia Varbanescu; Alexandru Iosup; Jose Larriba Pey; Arnau Prat; Peter Boncz; Hassan Chafi; Tim Hegeman; Wing Lung,Page 1. Graphalytics: Benchmarking Graph-Processing Platforms LDBC TUC Meeting IB TJWatson; NY; November 2015 GRAPHALYTICS A Big Data Benchmark for Graph-ProcessingPlatforms Mihai Capotã; Yong Guo; Ana Lucia Varbanescu; Alexandru Iosup; Jose Larriba Pey;Arnau Prat; Peter Boncz; Hassan Chafi 1 http://bl.ocks.org/mbostock/4062045 GRAPHALYTICSwas made possible by a generous contribution from Oracle. Tim Hegeman; Wing Lung Ngai;https://github.com/tudelft-atlarge/graphalytics/ Page 2. (TU) Delft – the Netherlands – Europepop.: 100;000 pop: 16.5 M founded 13th century pop: 100;000 founded 1842 pop: 15;000Barcelona Delft Page 3. The Parallel and Distributed Systems Group at TU Delft 3 Home page …,*,*,*
P2P XQuery and the StreetTiVo application,Ying Zhang; Peter Boncz,In the AmbientDB [2] project; we are building MonetDB/XQuery*; an open-source XMLDBMS (XDBMS) with support for distributed querying and P2P services. Our work ismotivated by the hypothesis that P2P is a disruptive paradigm that should change the natureof database technology. Most of the existing distributed DBMS technologies were developedto be used in (small-scale) local-area networks (LAN). Those technologies usually assumethat (i) there is a central controller and/or peers have complete knowledge of the wholesystem;(ii) peers are uniform and highly available;(iii) placement of data happens in acontrolled way and is rarely changed and (iv) a global database schema is used. Peer-to-Peer (P2P) networks have led the distributed DBMS research to reconsider existingtechnologies in such a new environment; where (i) systems have decentralized …,Scalable Data Management in Evolving Networks,*,*
Querying XML Data Sources,Peter Boncz,Page 1. MonetDB/XQuery -- re-use permitted when acknowledging the © Peter Boncz (2009) --VLDB Summer School 2009 Shanghai Peter Boncz boncz@cwi.nl (CWI Amsterdam) QueryingXML Data Sources using MonetDB/XQuery Page 2. MonetDB/XQuery -- re-use permitted whenacknowledging the © Peter Boncz (2009) -- VLDB Summer School 2009 Shanghai Overview •Crash course XML and XQuery + XQ Update Facility • XML Databases • MonetDB/XQuery •Open-source XML database by me and my group at CWI • Start downloading now Łmonetdb.cwi.nl • Extensions: • Keyword Search in XML (PF/Tijah) • StandOff Annotation •Distributed XML (XRPC) • More Internals • XML Indexing and why it is hard Page 3.MonetDB/XQuery -- re-use permitted when acknowledging the © Peter Boncz (2009) -- VLDBSummer School 2009 Shanghai Overview • Crash course XML and XQuery + XQ Update Facility …,*,*,*
Research Program Committee,Yanif Ahmad; Aris Anagnostopoulos; Walid Aref; Ismail Ari; Shivnath Babu; Zohra Bellahsene; II Elisa Bertino; Claudio Bettini; Michael Bohlen; Paolo Boldi; Francesco Bonchi; Peter Boncz; CWI Angela Bonifati; Vinayak Borkar; Christof Bornhoevd; SAP Randal Burns; Andrea Cali; Selcuk Candan; Barbara Carminati; Deepayan Chakrabarti; Chee Yong Chan; Shimin Chen; Pittsburgh Su Chen; Yi Chen; Reynold Cheng; Sarah Cohen-Boulakia; LRI Orsay; Gao Cong; Mariano Consens; Isabel Cruz; Bin Cui; Colazzo Dario; Gautam Das; Anish Das Sarma; Khuzaima Daudjee; Antonios Deligiannakis; Stefan Dessloch; Anhai Doan; Eduard Dragut,Yanif Ahmad; Johns Hopkins University Aris Anagnostopoulos; Sapienza University of RomeWalid Aref; Purdue University Ismail Ari; Ozyegin University Soeren Auer; Leipzig School of MediaShivnath Babu; Duke University Roger Barga; Microsoft Zohra Bellahsene; University of MontpellierII Elisa Bertino; Purdue University Claudio Bettini; University of Milan Michael Bohlen; Universityof Zurich Paolo Boldi; University of Milan Francesco Bonchi; Yahoo! Research Peter Boncz; CWIAngela Bonifati; ICAR-CNR; Italy Vinayak Borkar; University of California; Irvine ChristofBornhoevd; SAP Randal Burns; Johns Hopkins University Andrea Cali; University of Oxford SelcukCandan; Arizona State University Barbara Carminati; University of Insubria; Italy DeepayanChakrabarti; Yahoo! Research Chee Yong Chan; National University of Singapore BadrishChandramouli; Microsoft Gang Chen; Zhejing University; China Shimin Chen; Intel Labs …,*,*,*
Dimension Encoding for Bitwise Dimensional Co-Clustering,Stephan Baumann; Peter Boncz; Kai-Uwe Sattler,Abstract—In this technical report we explain how to create skew-resistant balanceddimensions for our clustering scheme Bitwise Dimensional Co-Clustering (short BDCC)based on histograms and Hu-Tucker encoding. This is needed to avoid unreliable precisionin BDCCscan when scanning tables at different granularities.,America,*,*
Conference Program Chairpersons,Roland R Wagner; Norman Revell; A Min Tjoa; Vladimir Marik; Witold Abramowicz; Hamideh Afsarmanesh; Fuat Akal; Toshiyuki Amagasa; Bernd Amann; Vasco Amaral; Stanislaw Ambroszkiewicz; Ira Assent; Ramazan S Aygun; Torben Bach Pedersen; Denilson Barbosa; Leonard Barolli; Kurt Bauknecht; Peter Baumann; Bishwaranjan Bhattacharjee; Sourav S Bhowmick; Stephen Blott; Peter Boncz; Angela Bonifati,Page 1. DEXA 2008 Program Committee General Chair Günther Pernul; University of Regensburg;Germany Conference Program Chairpersons Roland R. Wagner; FAW; University of Linz; AustriaNorman Revell; Middlesex University; UK † Workshop Chairpersons A Min Tjoa; TechnicalUniversity of Vienna; Austria Roland R. Wagner; FAW; University of Linz; Austria PublicationChairperson Vladimir Marik; Czech Technical University; Czech Republic Program CommitteeWitold Abramowicz; The Poznan University of Economics; Poland Hamideh Afsarmanesh;University of Amsterdam; The Netherlands Fuat Akal; ETH Zürich; Switzerland Toshiyuki Amagasa;University of Tsukuba; Japan Bernd Amann; LIP6 - UPMC; France Vasco Amaral …,*,*,*
Micro Adaptivity in a Vectorized Database System,Peter Boncz; Henri Bal,Abstract This thesis investigates the benefits of micro adaptivity in a high performanceDBMS. A micro adaptive DBMS is able to tune itself to the context in which it is running; byhaving multiple implementations of performance critical sections and a mechanism thatchooses the best (fastest) one during runtime. This is different than other adaptive databasesystems; such as those that continuously tune the execution plan at runtime because thesedo so at the coarser operator level. Our goal is to augment the Vectorwise system with microadaptivity which should increase performance and also reduce performance variation due todata characteristics; hardware or system state. We first present the factors that causechanges in performance (eg different hardware features; data selectivity) and show theopportunities that they create. Then; we introduce the micro adaptivity sub-system and …,*,*,*
Query Optimization and Execution in Vectorwise MPP,Peter Boncz; Jacopo Urbani,Most organizations nowadays demand from their online analytical processing (OLAP)capable DBMS a short response time to queries on volumes of data that increase at a fastpace. In order to comply with these requirements; many commercial database vendors havedeveloped massively parallel processing (MPP) solutions. The Vectorwise DBMS is basedon a vectorized query execution engine that takes advantage of the capacities of modernCPUs [Zuk09]. In this thesis; we explore the development challenges encountered in theinitial steps towards an MPP solution for the Vectorwise DBMS. Our work will focus on thetwo most critical layers: query optimization and query execution.,*,*,*
MonetDB/ХЮО at the 2006 TREC TeraByte Track,Sándor Héman; Marcin Zukowski; Arjen de Vries; Peter Boncz,*,*,*,*
Emerging “vertical” database systems in support of scientific data,Per Svensson; Peter Boncz; Milena Ivanova; Martin Kersten; Niels Nes,Abstract. This chapter surveys and discusses the evolution of a certain class of databasearchitectures; more recently referred to as “vertical databases”. The topics discussed in thischapter include the evolution of storage structures from the 1970‟ s till now; datacompression techniques; and query processing techniques for single-and multi-variablequeries in vertical databases. Next; the chapter covers in detail the architecture and designconsiderations of a particular (open source) vertical database system; called MonetDB. Thisis followed by an example of using MonetDB for the SkyServer data; and the queryprocessing improvements it offers.,*,*,*
INS-R9912 October 31; 1999,Optimizing Main-Memory Join On Modern; Hardware S Manegold; P Boncz; ML Kersten,ABSTRACT In the past decade; the exponential growth in commodity CPUs speed has faroutpaced advances in memory latency. A second trend is that CPU performance advancesare not only brought by increased clock rate; but also by increasing parallelism inside theCPU. Current database systems have not yet adapted to these trends; and show poorutilization of both CPU and memory resources on current hardware. In this article; we showhow these resources can be optimized for large joins and translate these insights intoguidelines for future database architectures; encompassing data structures; algorithms; costmodeling; and implementation. In particular; we discuss how vertically fragmented datastructures optimize cache performance on sequential data access. On the algorithmic side;we refine the partitioned hash-join with a new partitioning algorithm called radix-cluster …,*,*,*
Agrawal; D.; see Wen-Syan Li; T-KDE Jul-Aug 02 768-791 Agrawal; R.; see Shim; K.; T-KDE Jan-Feb 02 156-171 Ahamad; M.; and M. Chelliah. Flexible robust progr...,NR Adam; CC Aggarwai; CC Aggarwal; G Biswas; S Bodagala; P Boncz; RR Brooks; KS Candan; U Chadaga; B Chaib-draa; PP Chakrabarti; E Chang; W Chang; Chang Chen Meng; Chang Chin-Chen; Chang-tien Lu; Chan Tak-Wai; CR Chatwin,This index covers all technical items-papers; correspondence; reviews; etc.-that appeared inthis periodical during the year; and items from previous years that were commented upon orcorrected in this year. Departments and other items may also be covered if they have beenjudged to have archival value. The Author Index contains the primary entry for each item;listed under the first author's name. The primary entry includes the coauthors' names; the titleof the paper or other item; and its location; specified by the publication abbreviation; year;month; and inclusive pagination. The Subject Index contains entries describing the itemunder all appropriate subject headings; plus the first author's name; the publicationabbreviation; month; and year; and inclusive pages. Note that the item title is found onlyunder he primary entry in the Author Index.,*,*,*
Program Committees,Daniel Abadi; Ashraf ABOULNAGA; Laurent AMSALEG; Walid AREF; Sourav BHOWMICK; Angela BONIFATI; Peter BONCZ; Philippe BONNET; Luc BOUGANIM; Stéphane BRESSAN; Nicolas BRUNO; Barbara CATANIA; Chee Yong CHAN; Ugur CETINTEMEL; Lei CHEN; Shimin CHEN; Reynold CHENG; Brian COOPER; Bin CUI; Gautam DAS; Amol DESHPANDE; Alin DEUTSCH; Yanlei DIAO; Wenfei FAN; Alan FEKETE; Elena FERRARI; Shel FINKELSTEIN; Peter FISCHER; Minos GAROFALAKIS; Johannes GEHRKE; Gabriel GHINITA; Leo GIAKOUMAKIS,Page 1. VLDB 2010 xi SINGAPORE PROGRAM COMMITTEES Core Database Technology KianLee TAN; Program Chair (National University of Singapore; Singapore) Daniel ABADI (YaleUniversity; USA) Ashraf ABOULNAGA (University of Waterloo; Canada) Laurent AMSALEG(IRISA-CNRS; France) Walid AREF (Purdue University; USA) Sourav BHOWMICK (NanyangTechnological University; Singapore) Angela BONIFATI (Icar-CNR; Italy) Peter BONCZ (CWI; TheNetherlands) Philippe BONNET (University of Copenhagen; Denmark) Luc BOUGANIM (INRIA;France) Stéphane BRESSAN (National University of Singapore; Singapore) Nicolas BRUNO(Microsoft Research; USA) Barbara CATANIA (Universita di Genova; Italy ) Chee Yong CHAN(National University of Singapore; Singapore) Ugur CETINTEMEL (Brown University; USA) LeiCHEN (Hong Kong University of Science and Technology; China) …,*,*,*
CS-R9568 1995,PA Boncz; F Kwakkel; ML Kersten,Page 1. Centrum voor Wiskunde en Informatica REPORTRAPPORT High performance supportfor OO traversals in monet PA Boncz; F. Kwakkel and ML Kersten Computer Science/Departmentof Algorithmics and Architecture CS-R9568 1995 Page 2. Report CS-R9568 ISSN 0169-118XCWI PO Box 94079 1090 GB Amsterdam The Netherlands CWI is the National Research Institutefor Mathematics and Computer Science. CWI is part of the Stichting Mathematisch Centrum (SMC);the Dutch foundation for promotion of mathematics and computer science and their applications.SMC is sponsored by the Netherlands Organization for Scientific Research (NWO). CWI is amember of ERCIM; the European Research Consortium for Informatics and Mathematics …,*,*,*
W hat happens during a# oin',Stefan Manegoldu; Peter Boncz; Martin L Kerstenu,*,*,*,*
INS-R9912 October 31; 1999,S Manegold; P Boncz; ML Kersten,ABSTRACT In the past decade; the exponential growth in commodity CPUs speed has faroutpaced advances in memory latency. A second trend is that CPU performance advancesare not only brought by increased clock rate; but also by increasing parallelism inside theCPU. Current database systems have not yet adapted to these trends; and show poorutilization of both CPU and memory resources on current hardware. In this article; we showhow these resources can be optimized for large joins and translate these insights intoguidelines for future database architectures; encompassing data structures; algorithms; costmodeling; and implementation. In particular; we discuss how vertically fragmented datastructures optimize cache performance on sequential data access. On the algorithmic side;we refine the partitioned hash-join with a new partitioning algorithm called radix-cluster …,*,*,*
A SPARQL front-end for MonetDB,Paolo Atzeni; Peter A Boncz; Marco Antonelli,This thesis starts with a collaboration with the CWI; Center for Mathematics and Informatics(in Dutch: Centrum voor Wiskunde en Informatica); a prestigious Dutch research centerlocated in Amsterdam; one of the most important in Europe in these fields and a member ofthe ERCIM; the European Research Consortium for Informatics and Mathematics.,*,*,*
