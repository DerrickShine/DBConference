Optimal histograms with quality guarantees,Hosagrahar Visvesvaraya Jagadish; Nick Koudas; S Muthukrishnan; Viswanath Poosala; Kenneth C Sevcik; Torsten Suel,Abstract Histograms are commonly used to capture attribute value distribution statistics forquery optimizers. More recently; histograms have also been considered as a way to producequick approximate answers to decision support queries. This widespread interest inhistograms motivates the problem of computing histograms that are good under a givenerror metric. In particular; we are interested in an e® cient algorithm for choosing the bucketboundaries in a way that either minimizes the estimation error for a given amount of space(number of buckets) or; conversely; minimizes the space needed for a given upper bound onthe error. Under the assumption that finding optimal bucket boundaries is computationallyine® cient; previous research has focused on heuristics with no provable bounds on thequality of the solutions. In this paper; we present algorithms for computing optimal bucket …,VLDB,1998,468
BSPlib: The BSP programming library,Jonathan Hill; Bill McColl; Dan C Stefanescu; Mark W Goudreau; Kevin Lang; Satish B Rao; Torsten Suel; Thanasis Tsantilas; Rob H Bisseling,Abstract BSPlib is a small communications library for bulk synchronous parallel (BSP)programming which consists of only 20 basic operations. This paper presents the fulldefinition of BSPlib in C; motivates the design of its basic operations; and gives examples oftheir use. The library enables programming in two distinct styles: direct remote memoryaccess (DRMA) using put or get operations; and bulk synchronous message passing(BSMP). Currently; implementations of BSPlib exist for a variety of modern architectures;including massively parallel computers with distributed memory; shared memorymultiprocessors; and networks of workstations. BSPlib has been used in several scientificand industrial applications; this paper briefly describes applications in benchmarking; FastFourier Transforms (FFTs); sorting; and molecular dynamics.,Parallel Computing,1998,466
Design and implementation of a high-performance distributed web crawler,Vladislav Shkapenyuk; Torsten Suel,Broad Web search engines as well as many more specialized search tools rely on Webcrawlers to acquire large collections of pages for indexing and analysis. Such a Web crawlermay interact with millions of hosts over a period of weeks or months; and thus issues ofrobustness; flexibility; and manageability are of major importance. In addition; I/Operformance; network resources; and OS limits must be taken into account in order toachieve high performance at a reasonable cost. In this paper; we describe the design andimplementation of a distributed Web crawler that runs on a network of workstations. Thecrawler scales to (at least) several hundred pages per second; is resilient against systemcrashes and other events; and can be adapted to various crawling applications. We presentthe software architecture of the system; discuss the; performance bottlenecks; and …,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,460
Efficient query processing in geographic web search engines,Yen-Yu Chen; Torsten Suel; Alexander Markowetz,Abstract Geographic web search engines allow users to constrain and order search resultsin an intuitive manner by focusing a query on a particular geographic region. Geographicsearch technology; also called local search; has recently received significant interest frommajor search engine companies. Academic research in this area has focused primarily ontechniques for extracting geographic knowledge from the web. In this paper; we study theproblem of efficient query processing in scalable geographic search engines. Queryprocessing is a major bottleneck in standard web search engines; and the main reason forthe thousands of machines used by the major engines. Geographic search engine queryprocessing is different in that it requires a combination of text and spatial data processingtechniques. We propose several algorithms for efficient query processing in geographic …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,390
An efficient distributed algorithm for constructing small dominating sets,Lujun Jia; Rajmohan Rajaraman; Torsten Suel,Abstract. The dominating set problem asks for a small subset D of nodes in a graph such thatevery node is either in D or adjacent to a node in D. This problem arises in a number ofdistributed network applications; where it is important to locate a small number of centers inthe network such that every node is nearby at least one center. Finding a dominating set ofminimum size is NP-complete; and the best known approximation is logarithmic in themaximum degree of the graph and is provided by the same simple greedy approach thatgives the well-known logarithmic approximation result for the closely related set coverproblem. We describe and analyze new randomized distributed algorithms for thedominating set problem that run in polylogarithmic time; independent of the diameter of thenetwork; and that return a dominating set of size within a logarithmic factor from optimal …,Distributed Computing,2002,258
Inverted index compression and query processing with optimized document ordering,Hao Yan; Shuai Ding; Torsten Suel,Abstract Web search engines use highly optimized compression schemes to decreaseinverted index size and improve query throughput; and many index compression techniqueshave been studied in the literature. One approach taken by several recent studies firstperforms a renumbering of the document IDs in the collection that groups similar documentstogether; and then applies standard compression techniques. It is known that this cansignificantly improve index compression compared to a random document ordering. Westudy index compression and query processing techniques for such reordered indexes.Previous work has focused on determining the best possible ordering of documents. Incontrast; we assume that such an ordering is already given; and focus on how to optimizecompression methods and query processing for this case. We perform an extensive study …,Proceedings of the 18th international conference on World wide web,2009,240
Scalable sweeping-based spatial join,Lars Arge; Octavian Procopiuc; Sridhar Ramaswamy; Torsten Suel; Jeffrey Scott Vitter,Abstract In this paper; we consider the filter step of the spatial join problem; for the casewhere neither of the inputs are indexed. We present a new algorithm; Scalable Sweeping-Based Spatial Join (SSSJ); that achieves both efficiency on real-life data and robustnessagainst highly skewed and worst-case data sets. The algorithm combines a method withtheoretically optimal bounds on I/O transfers based on the recently proposed distribution-sweeping technique with a highly optimized implementation of internal-memory plane-sweeping. We present experimental results based on an efficient implementation of theSSSJ algorithm; and compare it to the state-ofthe-art Partition-Based Spatial-Merge (PBSM)algorithm of Pate1 and Dewitt.,VLDB,1998,239
Performance of compressed inverted list caching in search engines,Jiangong Zhang; Xiaohui Long; Torsten Suel,Abstract Due to the rapid growth in the size of the web; web search engines are facingenormous performance challenges. The larger engines in particular have to be able toprocess tens of thousands of queries per second on tens of billions of documents; makingquery throughput a critical issue. To satisfy this heavy workload; search engines use avariety of performance optimizations including index compression; caching; and earlytermination. We focus on two techniques; inverted index compression and index caching;which play a crucial rule in web search engines as well as other high-performanceinformation retrieval systems. We perform a comparison and evaluation of several invertedlist compression algorithms; including new variants of existing algorithms that have not beenstudied before. We then evaluate different inverted list caching policies on large query …,Proceedings of the 17th international conference on World Wide Web,2008,228
ODISSEA: A Peer-to-Peer Architecture for Scalable Web Search and Information Retrieval.,Torsten Suel; Chandan Mathur; Jo-Wen Wu; Jiangong Zhang; Alex Delis; Mehdi Kharrazi; Xiaohui Long; Kulesh Shanmugasundaram,ABSTRACT We consider the problem of building a P2P-based search engine for massivedocument collections. We describe a prototype system called ODISSEA (Open DIStributedSearch Engine Architecture) that is currently under development in our group. ODISSEAprovides a highly distributed global indexing and query execution service that can be usedfor content residing inside or outside of a P2P network. ODISSEA is different from manyother approaches to P2P search in that it assumes a two-tier search engine architecture anda global index structure distributed over the network. We give an overview of the proposedsystem and discuss the basic design choices. Our main focus is on efficient query execution;and we discuss how recent work on top-k queries in the database community can be appliedin a highly distributed environment. We also give preliminary simulation results on a real …,WebDB,2003,218
The Perron-Frobenius theorem: some of its applications,SU Pillai; Torsten Suel; Seunghun Cha,The Perron-Frobenius theorem provides a simple characterization of the eigenvectors andeigenvalues of certain types of matrices with nonnegative entries. The importance of thePerron-Frobenius theorem stems from the fact that eigenvalue problems on these types ofmatrices frequently arise in many different fields of science and engineering. In this article;the authors discuss the applications of this theorem in diverse areas such as steady statebehavior of Markov chains; power control in wireless networks; commodity pricing models ineconomics; population growth models; and Web search engines. The article starts with areview and discussion of the mathematical foundations.,Signal Processing Magazine; IEEE,2005,186
On rectangular partitionings in two dimensions: Algorithms; complexity and applications,S Muthukrishnan; Viswanath Poosala; Torsten Suel,Abstract Partitioning a multi-dimensional data set into rectangular partitions subject tocertain constraints is an important problem that arises in many database applications;including histogram-based selectivity estimation; load-balancing; and construction of indexstructures. While provably optimal and efficient algorithms exist for partitioning one-dimensional data; the multi-dimensional problem has received less attention; except for afew special cases. As a result; the heuristic partitioning techniques that are used in practiceare not well understood; and come with no guarantees on the quality of the solution. In thispaper; we present algorithmic and complexity-theoretic results for the fundamental problemof partitioning a two-dimensional array into rectangular tiles of arbitrary size in a way thatminimizes the number of tiles required to satisfy a given constraint. Our main results are …,International Conference on Database Theory,1999,184
Three-level caching for efficient query processing in large web search engines,Xiaohui Long; Torsten Suel,Abstract Large web search engines have to answer thousands of queries per second withinteractive response times. Due to the sizes of the data sets involved; often in the range ofmultiple terabytes; a single query may require the processing of hundreds of megabytes ormore of index data. To keep up with this immense workload; large search engines employclusters of hundreds or thousands of machines; and a number of techniques such ascaching; index compression; and index and query pruning are used to improve scalability. Inparticular; two-level caching techniques cache results of repeated identical queries at thefrontend; while index data for frequently used query terms are cached in each node at alower level. We propose and evaluate a three-level caching scheme that adds anintermediate level of caching for additional performance gains. This intermediate level …,World Wide Web,2006,181
-Optimized Query Execution in Large Search Engines with Global Page Ordering,Xiaohui Long; Torsten Suel,This chapter discusses the optimized query execution in large search engines with globalpage ordering. Large web search engines have to answer thousands of queries per secondwith interactive response times. A major factor in the cost of executing a query is given by thelengths of the inverted lists for the query terms; which increase with the size of the documentcollection and are often in the range of many megabytes. To address this issue; informationretrieval (IR) and database researchers have proposed pruning techniques that compute orapproximate term-based ranking functions without scanning over the full inverted lists. Thischapter focuses on the question of how such techniques can be efficiently integrated intoquery processing. It studies pruning techniques for query execution in large engines in thecase where one has a global ranking of pages; as provided by Pagerank or any other …,*,2003,145
Improved techniques for result caching in web search engines,Qingqing Gan; Torsten Suel,Abstract Query processing is a major cost factor in operating large web search engines. Inthis paper; we study query result caching; one of the main techniques used to optimize queryprocessing performance. Our first contribution is a study of result caching as a weightedcaching problem. Most previous work has focused on optimizing cache hit ratios; but giventhat processing costs of queries can vary very significantly we argue that total cost savingsalso need to be considered. We describe and evaluate several algorithms for weightedresult caching; and study the impact of Zipf-based query distributions on result caching. Oursecond and main contribution is a new set of feature-based cache eviction policies thatachieve significant improvements over all previous methods; substantially narrowing theexisting performance gap to the theoretically optimal (clairvoyant) method. Finally; using …,Proceedings of the 18th international conference on World wide web,2009,144
Compressing the graph structure of the web,Torsten Suel; Jun Yuan,A large amount of research has recently focused on the graph structure (or link structure) ofthe World Wide Web. This structure has proven to be extremely useful for improving theperformance of search engines and other tools for navigating the Web. However; since thegraphs in these scenarios involve hundreds of millions of nodes and even more edges;highly space-efficient data structures are needed to fit the data in memory. A first step in thisdirection was done by the DEC connectivity server; which stores the graph in compressedform. We describe techniques for compressing the graph structure of the Web; and giveexperimental results of a prototype implementation. We attempt to exploit a variety ofdifferent sources of compressibility of these graphs and of the associated set of URLs inorder to obtain good compression performance on a large Web graph.,Data Compression Conference; 2001. Proceedings. DCC 2001.,2001,141
Analysis of geographic queries in a search engine log,Qingqing Gan; Josh Attenberg; Alexander Markowetz; Torsten Suel,Abstract Geography is becoming increasingly important in web search. Search engines canoften return better results to users by analyzing features such as user location or geographicterms in web pages and user queries. This is also of great commercial value as it enableslocation specific advertising and improved search for local businesses. As a result; majorsearch companies have invested significant resources into geographic search technologies;also often called local search. This paper studies geographic search queries; ie; text queriessuch as" hotel new york" that employ geographical terms in an attempt to restrict results to aparticular region or location. Our main motivation is to identify opportunities for improvinggeographical search and related technologies; and we perform an analysis of 36 millionqueries of the recently released AOL query trace. First; we identify typical properties of …,Proceedings of the first international workshop on Location and the web,2008,137
Faster top-k document retrieval using block-max indexes,Shuai Ding; Torsten Suel,Abstract Large search engines process thousands of queries per second over billions ofdocuments; making query processing a major performance bottleneck. An important class ofoptimization techniques called early termination achieves faster query processing byavoiding the scoring of documents that are unlikely to be in the top results. We study newalgorithms for early termination that outperform previous methods. In particular; we focus onsafe techniques for disjunctive queries; which return the same result as an exhaustiveevaluation over the disjunction of the query terms. The current state-of-the-art methods forthis case; the WAND algorithm by Broder et al.[11] and the approach of Strohman and Croft[30]; achieve great benefits but still leave a large performance gap between disjunctive and(even non-early terminated) conjunctive queries. We propose a new set of algorithms by …,Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,2011,131
Optimal peer selection for P2P downloading and streaming,Micah Adler; Rakesh Kumar; Keith Ross; Dan Rubenstein; Torsten Suel; David D Yao,In a P2P system; a client peer may select one or more server peers to download a specificfile. In a P2P resource economy; the server peers charge the client for the downloading. Aserver peer's price would naturally depend on the specific object being downloaded; theduration of the download; and the rate at which the download is to occur. The optimal peerselection problem is to select; from the set of peers that have the desired object; the subsetof peers and download rates that minimizes cost. In this paper we examine a number ofnatural peer selection problems for both P2P downloading and P2P streaming. Fordownloading; we obtain the optimal solution for minimizing the download delay subject to abudget constraint; as well as the corresponding Nash equilibrium. For the streamingproblem; we obtain a solution that minimizes cost subject to continuous playback while …,INFOCOM 2005. 24th Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings IEEE,2005,128
Design and Implementation of a Geographic Search Engine.,Alexander Markowetz; Yen-Yu Chen; Torsten Suel; Xiaohui Long; Bernhard Seeger,ABSTRACT In this paper; we describe the design and initial implementation of a geographicsearch engine prototype for Germany; based on a large crawl of the de domain. Geographicsearch engines provide a flexible interface to the Web that allows users to constrain andorder search results in an intuitive manner; by focusing a query on a particular geographicregion. Geographic search technology has recently received significant commercial interest;but there has been only a limited amount of academic work. Our prototype performs massiveextraction of geographic features from crawled data; which are then mapped to coordinatesand aggregated across link and site structure. This assigns to each web page a set ofrelevant locations; called the geographic footprint of the page. The resulting footprint data isthen integrated into a high-performance query processor on a cluster-based architecture …,WebDB,2005,119
Towards efficiency and portability: Programming with the BSP model,Mark Goudreau; Kevin Lang; Satish Rao; Torsten Suel; Thanasis Tsantilas,Abstract The Bulk-Synchronous Parallel (BSP) model was proposed by Valiant as a modelfor general-purpose parallel computation. The objective of the model is to allow the designof parallel programs that can be executed efficiently on a variety of architectures. Whilemany theoretical arguments in support of the BSP model have been presented; the degreeto which the model can be efficiently utilized on existing parallel machines remains unclear.To explore this question; we implemented s small library of BSP functions; called the GreenBSP library; on several parallel platfotms. We also created a number of parallel applicationsbased on this library. Here; we report on the performance of six of these applications onthree different parallel platforms. Our preliminary results suggest that the BSP model can beused to develop efficient and portable programs for arange of machines and applications.,Proceedings of the eighth annual ACM symposium on Parallel algorithms and architectures,1996,113
I/O-efficient techniques for computing PageRank,Yen-Yu Chen; Qingqing Gan; Torsten Suel,Abstract Over the last few years; most major search engines have integrated link-basedranking techniques in order to provide more accurate search results. One widely knownapproach is the Pagerank technique; which forms the basis of the Google ranking scheme;and which assigns a global importance measure to each page based on the importance ofother pages pointing to it. The main advantage of the Pagerank measure is that it isindependent of the query posed by a user; this means that it can be precomputed and thenused to optimize the layout of the inverted index structure accordingly. However; computingthe Pagerank measure requires implementing an iterative process on a massive graphcorresponding to billions of web pages and hyperlinks. In this paper; we study I/O-efficienttechniques to perform this iterative computation. We derive two algorithms for Pagerank …,Proceedings of the eleventh international conference on Information and knowledge management,2002,96
Interactive wrapper generation with minimal user effort,Utku Irmak; Torsten Suel,Abstract While much of the data on the web is unstructured in nature; there is also asignificant amount of embedded structured data; such as product information on e-commerce sites or stock data on financial sites. A large amount of research has focused onthe problem of generating wrappers; ie; software tools that allow easy and robust extractionof structured data from text and HTML sources. In many applications; such as comparisonshopping; data has to be extracted from many different sources; making manual coding of awrapper for each source impractical. On the other hand; fully automatic approaches areoften not reliable enough; resulting in low quality of the extracted data. We describe acomplete system for semi-automatic wrapper generation that can be trained on different datasources in a simple interactive manner. Our goal is to minimize the amount of user effort …,Proceedings of the 15th international conference on World Wide Web,2006,91
Local methods for estimating pagerank values,Yen-Yu Chen; Qingqing Gan; Torsten Suel,Abstract The Google search engine uses a method called PageRank; together with term-based and other ranking techniques; to order search results returned to the user. PageRankuses link analysis to assign a global importance score to each web page. The PageRankscores of all the pages are usually determined off-line in a large-scale computation on theentire hyperlink graph of the web; and several recent studies have focused on improving theefficiency of this computation; which may require multiple hours on a workstation. However;in some scenarios; such as online analysis of link evolution and mining of large webarchives such as the Internet Archive; it may be desirable to quickly approximate or updatethe PageRanks of individual nodes without performing a large-scale computation on theentire graph. We address this problem by studying several methods for efficiently …,Proceedings of the thirteenth ACM international conference on Information and knowledge management,2004,91
Text vs. space: efficient geo-search query processing,Maria Christoforaki; Jinru He; Constantinos Dimopoulos; Alexander Markowetz; Torsten Suel,Abstract Many web search services allow users to constrain text queries to a geographiclocation (eg; yoga classes near Santa Monica). Important examples include local searchengines such as Google Local and location-based search services for smart phones.Several research groups have studied the efficient execution of queries mixing text andgeography; their approaches usually combine inverted lists with a spatial access methodsuch as an R-tree or space-filling curve. In this paper; we take a fresh look at this problem.We feel that previous work has often focused on the spatial aspect at the expense ofperformance considerations in text processing; such as inverted index access; compression;and caching. We describe new and existing approaches and discuss their differentperspectives. We then compare their performance in extensive experiments on large …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,90
Improved file synchronization techniques for maintaining large replicated collections over slow networks,Torsten Suel; Patrick Noel; Dimitre Trendafilov,We study the problem of maintaining large replicated collections of files or documents in adistributed environment with limited bandwidth. This problem arises in a number of importantapplications; such as synchronization of data between accounts or devices; contentdistribution and Web caching networks; Web site mirroring; storage networks; and largescale Web search and mining. At the core of the problem lies the following challenge; calledthe file synchronization problem: given two versions of a file on different machines; say anoutdated and a current one; how can we update the outdated version with minimumcommunication cost; by exploiting the significant similarity between the versions? While apopular open source tool for this problem called rsync is used in hundreds of thousands ofinstallations; there have been only very few attempts to improve upon this tool in practice …,Data Engineering; 2004. Proceedings. 20th International Conference on,2004,89
Using graphics processors for high performance IR query processing,Shuai Ding; Jinru He; Hao Yan; Torsten Suel,Abstract Web search engines are facing formidable performance challenges due to datasizes and query loads. The major engines have to process tens of thousands of queries persecond over tens of billions of documents. To deal with this heavy workload; such enginesemploy massively parallel systems consisting of thousands of machines. The significant costof operating these systems has motivated a lot of recent research into more efficient queryprocessing mechanisms. We investigate a new way to build such high performance IRsystems using graphical processing units (GPUs). GPUs were originally designed toaccelerate computer graphics applications through massive on-chip parallelism. Recently anumber of researchers have studied how to use GPUs for other problem domains such asdatabases and scientific computing. Our contribution here is to design a basic system …,Proceedings of the 18th international conference on World wide web,2009,87
Algorithms for delta compression and remote file synchronization,Torsten Suel; Nasir Memon,In this chapter; we survey techniques; software tools; and applications for delta compression;remote file synchronization; and closely related problems. We first focus on deltacompression; where the sender knows all the similar files that are held by the receiver. In thesecond part; we survey work on the related; but in many ways quite different; problem ofremote file synchronization; where the sender does not have a copy of the files held by thereceiver.,Lossless Compression Handbook,2002,80
Improving web spam classifiers using link structure,Qingqing Gan; Torsten Suel,Abstract Web spam has been recognized as one of the top challenges in the search engineindustry [14]. A lot of recent work has addressed the problem of detecting or demoting webspam; including both content spam [16; 12] and link spam [22; 13]. However; any time ananti-spam technique is developed; spammers will design new spamming techniques toconfuse search engine ranking methods and spam detection mechanisms. Machinelearning-based classification methods can quickly adapt to newly developed spamtechniques. We describe a two-stage approach to improve the performance of commonclassifiers. We first implement a classifier to catch a large portion of spam in our data. Thenwe design several heuristics to decide if a node should be relabeled based on thepreclassified result and knowledge about the neighborhood. Our experimental results …,Proceedings of the 3rd international workshop on Adversarial information retrieval on the web,2007,79
Derandomizing algorithms for routing and sorting on meshes,Michael Kaufmann; Jop F Sibeyn; Torsten Suel,Abstract We describe a new technique that can be used to derandomize a number ofrandomized algorithms for routing and sorting on meshes. We demonstrate the power of thistechnique by deriving improved deterministic algorithms for a variety of routing and sortingproblems. Our main results are an optimal algorithm for kk routing on multi-dimensionalmeshes; a permutation routing algorithm with running time 2n+o (n) and queue size 5; andan optimal algorithm for 1-1 sorting.,SODA,1994,77
A proposal for the BSP worldwide standard library (preliminary version),Mark W Goudreau; Jonathan MD Hill; Kevin Lang; Bill McColl; Satish B Rao; Dan C Stefanescu; Torsten Suel; Thanasis Tsantilas,A bulk synchronous parallel (BSP) computer 7; 11] consists of a set of processor-memorypairs; a global communications network; and a mechanism for the e cient barriersynchronisation of the processors. There are no specialised broadcasting or combiningfacilities; although these can be e ciently realised in software where required. The modelalso does not deal directly with issues such as input-output or the use of vector units;although it can be easily extended to do so. A BSP computer operates in the following way.A computation consists of a sequence of parallel supersteps. During a superstep; eachprocessor-memory pair can perform a number of computation steps on values held locally atthe start of the superstep; send and receive a number of messages; and handle variousremote get and put requests. The model does not prescribe any particular style of …,Tedmical report; available via BSP Worldwide home page http://www. bsp-worldwide. org,1996,71
Efficient query evaluation on large textual collections in a peer-to-peer environment,Jiangong Zhang; Torsten Suel,The authors studied the problem of evaluating ranked (top-k) queries on textual collectionsranging from multiple gigabytes to terabytes in size. The authors focused on the case of aglobal index organization in a highly distributed environment; and consider a class ofranking functions that includes common variants of the Cosine and Okapi measures. Themain bottleneck in such a scenario is the amount of communication required during queryevaluation. Several efficient query evaluation schemes were proposed and theirperformances were evaluated. The results on real search engine query traces and over 120million Web pages show that after careful optimization such queries can be evaluated at areasonable cost; while challenges remain for even larger collections and more generalclasses of ranking functions.,Peer-to-Peer Computing; 2005. P2P 2005. Fifth IEEE International Conference on,2005,69
zdelta: An efficient delta compression tool,Dimitre Trendafilov; Nasir Memon; Torsten Suel,Abstract In this report we describe a tool for delta compression; ie; the efficient encoding of agiven data set in relation to another one. Its possible applications include archiving multipleversions of data; distribution of software updates; delta compression of backup files; orcompression at the file system level. The compressor; called zdelta; could be viewed as amodification of the zlib compression library [4] with some additional ideas inspired by thevdelta/vcdiff tool of Vo [5]. We also present experimental results comparing zdelta to otherdelta compression tools.,*,2002,67
Cluster-based delta compression of a collection of files,Zan Ouyang; Nasir Memon; Torsten Suel; Dimitre Trendafilov,Delta compression techniques are commonly used to succinctly represent an updatedversion of a file with respect to an earlier one. We study the use of delta compression in asomewhat different scenario; where we wish to compress a large collection of (more or less)related files by performing a sequence of pairwise delta compressions. The problem offinding an optimal delta encoding for a collection of files by taking pairwise deltas can bereduced to the problem of computing a branching of maximum weight in a weighted directedgraph; but this solution is inefficient and thus does not scale to larger file collections. Thismotivates us to propose a framework for cluster-based delta compression that uses textclustering techniques to prune the graph of possible pairwise delta encodings. Todemonstrate the efficacy of our approach; we present experimental results on collections …,Web Information Systems Engineering; 2002. WISE 2002. Proceedings of the Third International Conference on,2002,65
Modeling and predicting user behavior in sponsored search,Josh Attenberg; Sandeep Pandey; Torsten Suel,Abstract Implicit user feedback; including click-through and subsequent browsing behavior;is crucial for evaluating and improving the quality of results returned by search engines.Several recent studies [1; 2; 3; 13; 25] have used post-result browsing behavior including thesites visited; the number of clicks; and the dwell time on site in order to improve the rankingof search results. In this paper; we first study user behavior on sponsored search results (ie;the advertisements displayed by search engines next to the organic results); and comparethis behavior to that of organic results. Second; to exploit post-result user behavior for betterranking of sponsored results; we focus on identifying patterns in user behavior and predictexpected on-site actions in future instances. In particular; we show how post-result behaviordepends on various properties of the queries; advertisement; sites; and users; and build a …,Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,2009,63
Portable and efficient parallel computing using the BSP model,Mark W Goudreau; Kevin Lang; Satish B Rao; Torsten Suel; Thanasis Tsantilas,The Bulk-Synchronous Parallel (BSP) model was proposed by Valiant as a standardinterface between parallel software and hardware. In theory; the BSP model has beenshown to allow the asymptotically optimal execution of architecture independent software ona variety of architectures. Our goal in this work is to experimentally examine the practical useof the BSP model on current parallel architectures. We describe the design andimplementation of the Green BSP Library; a small library of functions that implement the BSPmodel; and of several applications that were written for this library. We then discuss theperformance of the library and application programs on several parallel architectures. Ourresults are positive in that we demonstrate efficiency and portability over a range of parallelarchitectures and show that the BSP cost model is useful for predicting performance …,Computers; IEEE Transactions on,1999,60
Improved single-round protocols for remote file synchronization,Utku Irmak; Svilen Mihaylov; Torsten Suel,Given two versions of a file; a current version located on one machine and an outdatedversion known only to another machine; the remote file synchronization problem is how toupdate the outdated version over a network with a minimal amount of communication. Inparticular; when the versions are very similar; the total data transmitted should besignificantly smaller than the file size. File synchronization problems arise in manyapplication scenarios such as Web site mirroring; file system backup and replication; andWeb access over slow links. An open source tool for this problem; called rsync and includedin many Linux distributions; is widely used in such scenarios; rsync uses a single round ofmessages between the two machines. While recent research has shown that significantadditional savings in bandwidth consumption are possible through the use of optimized …,INFOCOM 2005. 24th Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings IEEE,2005,57
Compressing term positions in web indexes,Hao Yan; Shuai Ding; Torsten Suel,Abstract Large search engines process thousands of queries per second on billions ofpages; making query processing a major factor in their operating costs. This has led to a lotof research on how to improve query throughput; using techniques such as massiveparallelism; caching; early termination; and inverted index compression. We focus ontechniques for compressing term positions in web search engine indexes. Most previouswork has focused on compressing docID and frequency data; or position information in othertypes of text collections. Compression of term positions in web pages is complicated by thefact that term occurrences tend to cluster within documents but not across documentboundaries; making it harder to exploit clustering effects. Also; typical access patterns forposition data are different from those for docID and frequency data. We perform a detailed …,Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,2009,55
AQUA: System and techniques for approximate query answering,Phillip B Gibbons; Viswanath Poosala; Swarup Acharya; Yair Bartal; Yossi Matias; S Muthukrishnan; Sridhar Ramaswamy; Torsten Suel,Abstract In large data recording and warehousing environments; it is often advantageous toprovide fast; approximate answers to queries. The goal is to provide an estimated responsein orders of magnitude less time than the time to compute an exact answer; by avoiding orminimizing the number of accesses to the base data. This paper presents the ApproximateQUery Answering (AQUA) System; for fast; highlyaccurate approximate answers to queries.Aqua provides approximate answers using small; precomputed synopses (samples; counts;etc.) of the underlying base data. An important feature of Aqua is that it provides accuracyguarantees without any a priori assumptions on either the data distribution; the order inwhich the base data is loaded; or the layout of the data on the disks. Currently; the systemprovides fast approximate answers for queries with selects; aggregates; group bys and/or …,Bell Labs TR,1998,50
Optimizing top-k document retrieval strategies for block-max indexes,Constantinos Dimopoulos; Sergey Nepomnyachiy; Torsten Suel,Abstract Large web search engines use significant hardware and energy resources toprocess hundreds of millions of queries each day; and a lot of research has focused on howto improve query processing efficiency. One general class of optimizations called earlytermination techniques is used in all major engines; and essentially involves computing topresults without an exhaustive traversal and scoring of all potentially relevant index entries.Recent work in [9; 7] proposed several early termination algorithms for disjunctive top-kquery processing; based on a new augmented index structure called Block-Max Index thatenables aggressive skipping in the index. In this paper; we build on this work by studyingnew algorithms and optimizations for Block-Max indexes that achieve significantperformance gains over the work in [9; 7]. We start by implementing and comparing Block …,Proceedings of the sixth ACM international conference on Web search and data mining,2013,47
Efficient term proximity search with term-pair indexes,Hao Yan; Shuming Shi; Fan Zhang; Torsten Suel; Ji-Rong Wen,Abstract There has been a large amount of research on early termination techniques in websearch and information retrieval. Such techniques return the top-k documents withoutscanning and evaluating the full inverted lists of the query terms. Thus; they can greatlyimprove query processing efficiency. However; only a limited amount of efficient top-kprocessing work considers the impact of term proximity; ie; the distance between termoccurrences in a document; which has recently been integrated into a number of retrievalmodels to improve effectiveness. In this paper; we propose new early termination techniquesfor efficient query processing for the case where term proximity is integrated into the retrievalmodel. We propose new index structures based on a term-pair index; and study newdocument retrieval strategies on the resulting indexes. We perform a detailed …,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,47
A unified approach for indexed and non-indexed spatial joins,Lars Arge; Octavian Procopiuc; Sridhar Ramaswamy; Torsten Suel; Jan Vahrenhold; Jeffrey Scott Vitter,Abstract Most spatial join algorithms either assume the existence of a spatial index structurethat is traversed during the join process; or solve the problem by sorting; partitioning; or on-the-fly index construction. In this paper; we develop a simple plane-sweeping algorithm thatunifies the index-based and non-index based approaches. This algorithm processesindexed as well as non-indexed inputs; extends naturally to multi-way joins; and can be builteasily from a few standard operations. We present the results of a comparative study of thenew algorithm with several index-based and non-index based spatial join algorithms. Weconsider a number of factors; including the relative performance of CPU and disk; the qualityof the spatial indexes; and the sizes of the input relations. An important conclusion from ourwork is that using an index-based approach whenever indexes are available does not …,International Conference on Extending Database Technology,2000,47
Theory and practice of I/O efficient algorithms for multidimensional batched searching problems,Lars Arge; Octavian Procopiuc; Sridhar Ramaswamy; Torsten Suel; Jeffrey Scott Vitter,We describe a powerful framework for designing efficient batch algorithms for certain large-scale dynamic problems that must be solved using external memory. The class of problemswe consider; which we call colorable external decomposable problems; include rectangleintersection; orthogonal line segment intersection; range searching; and point location. Weare particularly interested in these problems in two and higher dimensions. They havenumerous applications in geographic information systems (GIS); spatial databases; andVLSI and CAD design. We present simplified algorithms for problems previously solved bymore complicated approaches (such as rectangle intersection); and we present efficientalgorithms for problems not previously solved in an efficient way (such as point location andhigher dimensional versions of range searching and rectangle intersection). We give …,*,1998,43
Compact full-text indexing of versioned document collections,Jinru He; Hao Yan; Torsten Suel,Abstract We study the problem of creating highly compressed full-text index structures forversioned document collections; that is; collections that contain multiple versions of eachdocument. Important examples of such collections are Wikipedia or the web page archivemaintained by the Internet Archive. A straightforward indexing approach would simply treateach document version as a separate document; such that index size scales linearly with thenumber of versions. However; several authors have recently studied approaches that exploitthe significant similarities between different versions of the same document to obtain muchsmaller index sizes. In this paper; we propose new techniques for organizing andcompressing inverted index structures for such collections. We also perform a detailedexperimental comparison of new techniques and the existing techniques in the literature …,Proceedings of the 18th ACM conference on Information and knowledge management,2009,41
Efficient search in large textual collections with redundancy,Jiangong Zhang; Torsten Suel,Abstract Current web search engines focus on searching only themost recentsnapshot of theweb. In some cases; however; it would be desirableto search over collections that includemany different crawls andversions of each page. One important example of such acollectionis the Internet Archive; though there are many others. Sincethe data size of such anarchive is multiple times that of a singlesnapshot; this presents us with significantperformance challenges. Current engines use various techniques for index compressionandoptimized query execution; but these techniques do not exploit thesignificant similaritiesbetween different versions of a page; or betweendifferent pages. In this paper; we propose ageneral framework for indexing andquery processing of archival collections and; moregenerally; anycollections with a sufficient amount of redundancy. Our approachresults in …,Proceedings of the 16th international conference on World Wide Web,2007,40
Scalable techniques for document identifier assignment in inverted indexes,Shuai Ding; Josh Attenberg; Torsten Suel,Abstract Web search engines depend on the full-text inverted index data structure. Becausethe query processing performance is so dependent on the size of the inverted index; aplethora of research has focused on fast end effective techniques for compressing thisstructure. Recently; several authors have proposed techniques for improving indexcompression by optimizing the assignment of document identifiers to the documents in thecollection; leading to significant reduction in overall index size. In this paper; we proposeimproved techniques for document identifier assignment. Previous work includes simple andfast heuristics such as sorting by URL; as well as more involved approaches based on theTraveling Salesman Problem or on graph partitioning. These techniques achieve goodcompression but do not scale to larger document collections. We propose a new …,Proceedings of the 19th international conference on World wide web,2010,39
Highly portable and efficient implementations of parallel adaptive n-body methods,David Blackston; Torsten Suel,Abstract We describe the design of several portable and efficient parallel implementations ofadaptive N-body methods; including the adaptive Fast Multipole Method; the adaptiveversion of Anderson's Method; and the Barnes-Hut algorithm. Our codes are based on acommunication and work partitioning scheme that allows an efficient implementation ofadaptive multipole methods even on high-latency systems. Our test runs demonstrate highperformance and speedup on several parallel architectures; including traditional MPPs;shared-memory machines; and networks of workstations connected by Ethernet.,Proceedings of the 1997 ACM/IEEE conference on Supercomputing,1997,39
Algorithms for low-latency remote file synchronization,Hao Yan; Utku Irmak; Torsten Suel,The remote file synchronization problem is how to update an outdated version of a filelocated on one machine to the current version located on another machine with a minimalamount of network communication. It arises in many scenarios including Web site mirroring;file system backup and replication; or web access over slow links. A widely used open-source tool called rsync uses a single round of messages to solve this problem (plus aninitial round for exchanging meta information). While research has shown that significantadditional savings in bandwidth are possible by using multiple rounds; such approaches areoften not desirable due to network latencies; increased protocol complexity; and higher I/Oand CPU overheads at the endpoints. In this paper; we study single-round synchronizationtechniques that achieve savings in bandwidth consumption while preserving many of the …,INFOCOM 2008. The 27th Conference on Computer Communications. IEEE,2008,38
Optimized inverted list assignment in distributed search engine architectures,Jiangong Zhang; Torsten Suel,We study efficient query processing in distributed Web search engines with global indexorganization. The main performance bottleneck in this case is due to the large amount ofindex data that is exchanged between nodes during the processing of a query; and previouswork has proposed several techniques for significantly reducing this cost. We describe anapproach that provides substantial additional improvement over previous techniques. Inparticular; we analyze search engine query traces in order to optimize the assignment ofindex data to the nodes in the system; such that terms frequently occurring together inqueries are also often collocated on the same node. Our experiments show that in return fora modest factor increase in storage space; we can achieve a reduction in communicationcost of an order of magnitude over the previous best techniques.,Parallel and Distributed Processing Symposium; 2007. IPDPS 2007. IEEE International,2007,38
Approximation algorithms for array partitioning problems,S Muthukrishnan; Torsten Suel,Abstract We study the problem of optimally partitioning a two-dimensional array of elementsby cutting each coordinate axis into p (respectively; q) intervals; resulting in p× q rectangularregions. This problem arises in several applications in databases; parallel computation; andimage processing. Our main contribution are new approximation algorithms for these NP-complete problems that improve significantly over previously known bounds. The algorithmsare fast and simple; work for a variety of measures of partitioning quality; generalize todimensions d> 2; and achieve almost optimal approximation ratios. We also extend previousNP-completeness results for this class of problems.,Journal of Algorithms,2005,35
Improved index compression techniques for versioned document collections,Jinru He; Junyuan Zeng; Torsten Suel,Abstract Current Information Retrieval systems use inverted index structures for efficientquery processing. Due to the extremely large size of many data sets; these index structuresare usually kept in compressed form; and many techniques for optimizing compressed sizeand query processing speed have been proposed. In this paper; we focus on versioneddocument collections; that is; collections where each document is modified over time;resulting in multiple versions of the document. Consecutive versions of the same documentare often similar; and several researchers have explored ideas for exploiting this similarity todecrease index size. We propose new index compression techniques for versioneddocument collections that achieve reductions in index size over previous methods. Inparticular; we first propose several bitwise compression techniques that achieve a …,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,29
Hierarchical substring caching for efficient content distribution to low-bandwidth clients,Utku Irmak; Torsten Suel,Abstract While overall bandwidth in the internet has grown rapidly over the last few years;and an increasing number of clients enjoy broadband connectivity; many others still accessthe internet over much slower dialup or wireless links. To address this issue; a number oftechniques for optimized delivery of web and multimedia content over slow links have beenproposed; including protocol optimizations; caching; compression; and multimediatranscoding; and several large ISPs have recently begun to widely promote dialupacceleration services based on such techniques. A recent paper by Rhea; Liang; andBrewer proposed an elegant technique called value-based caching that caches substringsof files; rather than entire files; and thus avoids repeated transmission of substrings commonto several pages or page versions. We propose and study a hierarchical substring …,Proceedings of the 14th international conference on World Wide Web,2005,29
Computer method; apparatus and programmed medium for approximating large databases and improving search efficiency,*,A novel and unique method; apparatus and programmed storage medium for approximatinglarge data distributions of a database in order to allow a user to accurately analyze the entiredata distribution using a limited amount of memory space in a reasonable amount of time.The approximation is based on partitioning the data domain into a number of regions;approximating each region using any well-known technique; and composing the errors ofapproximation to optimize suitable criteria for approximating the entire data distribution.,*,2000,26
Top-k aggregation using intersections of ranked inputs,Ravi Kumar; Kunal Punera; Torsten Suel; Sergei Vassilvitskii,Abstract There has been considerable past work on efficiently computing top k objects byaggregating information from multiple ranked lists of these objects. An important instance ofthis problem is query processing in search engines: One has to combine information fromseveral different posting lists (rankings) of web pages (objects) to obtain the top k web pagesto answer user queries. Two particularly well-studied approaches to achieve efficiency in top-k aggregation include early-termination algorithms (eg; TA and NRA) and preaggregation ofsome of the input lists. However; there has been little work on a rigorous treatment ofcombining these approaches. We generalize the TA and NRA algorithms to the case whenpreaggregated intersection lists are available in addition to the original lists. We show thatour versions of TA and NRA continue to remain" instance optimal;" a very strong …,Proceedings of the Second ACM International Conference on Web Search and Data Mining,2009,25
Efficient communication using total-exchange,Satish Rao; Torsten Suel; Thanasis Tsantilas; Mark Goudreau,A central question in parallel computing is to determine the extent to which one can writeparallel programs using a high-level; general-purpose; and architecture-independentprogramming language and have them executed on a variety of parallel and distributedarchitectures without sacrificing efficiency. A large body of research suggests that; at least intheory; general-purpose parallel computing is indeed possible provided certain conditionsare met: an excess of logical parallelism in the program; and the ability of the targetarchitecture to efficiently realize balanced communication patterns. The canonical exampleof a balanced communication pattern is an h-relation; in which each processor is the originand destination of at most h messages. A plethora of protocols has been designed forrouting h-relations in a variety of networks. The goal has been to minimize the value of h …,Parallel Processing Symposium; 1995. Proceedings.; 9th International,1995,25
Computer method; apparatus and programmed medium for more efficient database management using histograms with a bounded error selectivity estimation,*,A computer method; apparatus and programmed medium for optimizing the number ofbuckets; and thus minimizing the necessary amount of memory space; needed to construct ahistogram of a data distribution contained within a computer database with a cumulativeerror bounded by a specified threshold. The method according to the present inventionallows a user to determine the near-minimal memory space necessary to store anapproximation of a database with a maximum error measure at most three times thatspecified by the user to allow the user to maximize computer resources.,*,2000,24
Cleaning search results using term distance features,Josh Attenberg; Torsten Suel,Abstract The presence of Web spam in query results is one of the critical challenges facingsearch engines today. While search engines try to combat the impact of spam pages on theirresults; the incentive for spammers to use increasingly sophisticated techniques has neverbeen higher; since the commercial success of a Web page is strongly correlated to thenumber of views that page receives. This paper describes a term-based technique for spamdetection based on a simple new summary data structure called Term Distance Histogramsthat tries to capture the topical structure of a page. We apply this technique as a post-filteringstep to a major search engine. Our experiments show that we are able to detect many of theartificially generated spam pages that remained in the results of the engine. Specifically; ourmethod is able to detect many web pages generated by utilizing techniques such as …,Proceedings of the 4th international workshop on Adversarial information retrieval on the web,2008,23
Efficient query subscription processing for prospective search engines,Utku Irmak; Svilen Mihaylov; Torsten Suel; Samrat Ganguly; Rauf Izmailov,Abstract Current web search engines are retrospective in that they limit users to searchesagainst already existing pages. Prospective search engines; on the other hand; allow usersto upload queries that will be applied to newly discovered pages in the future. We study andcompare algorithms for efficiently matching large numbers of simple keyword queriesagainst a stream of newly discovered pages.,Proceedings of the 15th international conference on World Wide Web,2006,23
Compact grid layouts of multi-level networks,Shan Muthukrishnan; Mike Paterson; Süleyman Cenk Sahinalp; Torsten Suel,Abstract We consider the problem of generating layouts of multilevel networks; in particular;switching; sorting; and interconnection networks; as compactly as possible on VLSI grids.Besides traditional interest in these problems motivated by interconnection topologies inparallel computing and switching circuits in telecommunications; there is renewed interest insuch layouts in the context of ATM (Asynchronous Transfer Mode) switches. Our resultsimprove on the existing area bounds for these networks by factors of up to three.,Proceedings of the thirty-first annual ACM symposium on Theory of computing,1999,23
On the scalability of an image transcoding proxy server,Anubhav Savant; Nasir Memon; Torsten Suel,Image transcoding proxies are used to improve Web browsing over low bandwidth networksby adapting content-rich Web images to bandwidth-constrained clients. Such transcodingproxies dynamically analyze; manipulate and transcode images (eg quality reduction; downsampling) on the fly enabling significant reductions in download times over low bandwidthlinks. However; transcoding proxies have scalability problems if the objective policy thatdecides whether to transcode an image does not take the client load (eg number ofconcurrent clients) into consideration. We show that seemingly intuitive policies that makedecisions solely based on whether transcoding yields savings in transmission time tail toscale. Under high load; the average latency perceived by a client can be improved by afactor of about two by taking overall client load into consideration and properly scheduling …,Image Processing; 2003. ICIP 2003. Proceedings. 2003 International Conference on,2003,22
Routing and sorting on meshes with row and column buses,Torsten Suel,Gives improved deterministic algorithms for permutation routing and sorting on meshes withrow and column buses. Among our results; we obtain a fairly simple algorithm forpermutation routing on two-dimensional meshes with buses that achieves a running time ofn+ o (n) and a queue size of 2. We also describe an algorithm for routing on r-dimensionalnetworks with a running time of (2/spl minus/1/r) n+ o (n) and a queue size of 2; and showhow to obtain deterministic algorithms for sorting whose running times match those forpermutation routing. An interesting feature of our algorithms is that they can be implementedon a wide variety of different models of meshes with buses within the same bounds on timeand queue size. Finally; we also study the performance of meshes with buses on dynamicrouting problems; and propose fast routing schemes under several different assumptions …,Parallel Processing Symposium; 1994. Proceedings.; Eighth International,1994,22
Faster temporal range queries over versioned text,Jinru He; Torsten Suel,Abstract Versioned textual collections are collections that retain multiple versions of adocument as it evolves over time. Important large-scale examples are Wikipedia and theweb collection of the Internet Archive. Search queries over such collections often usekeywords as well as temporal constraints; most commonly a time range of interest. In thispaper; we study how to support such temporal range queries over versioned text. Our goal isto process these queries faster than the corresponding keyword-only queries; by exploitingthe additional constraint. A simple approach might partition the index into different timeranges; and then access only the relevant parts. However; specialized inverted indexcompression techniques are crucial for large versioned collections; and a naive partitioningcan negatively affect index size and query throughput. We show how to achieve high …,Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,2011,20
To index or not to index: time-space trade-offs in search engines with positional ranking functions,Diego Arroyuelo; Senén González; Mauricio Marin; Mauricio Oyarzún; Torsten Suel,Abstract Positional ranking functions; widely used in Web search engines; improve resultquality by exploiting the positions of the query terms within documents. However; it is wellknown that positional indexes demand large amounts of extra space; typically about threetimes the space of a basic nonpositional index. Textual data; on the other hand; is needed toproduce text snippets. In this paper; we study time-space trade-offs for search engines withpositional ranking functions and text snippet generation. We consider both index-based andnon-index based alternatives for positional data. We aim to answer the question of whetherone should index positional data or not. We show that there is a wide range of practical time-space trade-offs. Moreover; we show that both position and textual data can be stored usingabout 71% of the space used by traditional positional indexes; with a minor increase in …,Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,2012,18
Batch query processing for web search engines,Shuai Ding; Josh Attenberg; Ricardo Baeza-Yates; Torsten Suel,Abstract Large web search engines are now processing billions of queries per day. Most ofthese queries are interactive in nature; requiring a response in fractions of a second.However; there are also a number of important scenarios where large batches of queries aresubmitted for various web mining and system optimization tasks that do not require animmediate response. Given the significant cost of executing search queries over billions ofweb pages; it is a natural question to ask if such batches of queries can be more efficientlyexecuted than interactive queries. In this paper; we motivate and discuss the problem ofbatch query processing in search engines; identify basic mechanisms for improving theperformance of such queries; and provide a preliminary experimental evaluation of theproposed techniques. Our conclusion is that significant cost reductions are possible by …,Proceedings of the fourth ACM international conference on Web search and data mining,2011,18
On probabilistic networks for selection; merging; and sorting,Tom Leighton; Yuan Ma; Torsten Suel,Abstract We study comparator networks for selection; merging; and sorting that output thecorrect result with high probability; given a random input permutation. We prove tightbounds; up to constant factors; on the size and depth of probabilistic (n; k)-selectionnetworks. In the case of (n; n/2)-selection; our result gives a somewhat surprising bound ofΘ(n\log\logn) on the size of networks of success probability in δ;1-1/poly(n); where δ is anarbitrarily small positive constant; thus comparing favorably with the best previously knownsolutions; which have size Θ(n\logn). We also prove tight bounds; up to lower-order terms;on the size and depth of probabilistic merging networks of success probability in δ;1-1/poly(n); where δ is an arbitrarily small positive constant. Finally; we describe two fairlysimple probabilistic sorting networks of success probability at least 1-1/poly(n) and nearly …,Theory of Computing Systems,1997,17
A robust model for paper reviewer assignment,Xiang Liu; Torsten Suel; Nasir Memon,Abstract Automatic expert assignment is a common problem encountered in both industryand academia. For example; for conference program chairs and journal editors; in order tocollect" good" judgments for a paper; it is necessary for them to assign the paper to the mostappropriate reviewers. Choosing appropriate reviewers of course includes a number ofconsiderations such as expertise and authority; but also diversity and avoiding conflicts. Inthis paper; we explore the expert retrieval problem and implement an automatic paper-reviewer recommendation system that considers aspects of expertise; authority; anddiversity. In particular; a graph is first constructed on the possible reviewers and the querypaper; incorporating expertise and authority information. Then a Random Walk with Restart(RWR)[1] model is employed on the graph with a sparsity constraint; incorporating …,Proceedings of the 8th ACM Conference on Recommender systems,2014,16
A candidate filtering mechanism for fast top-k query processing on modern cpus,Constantinos Dimopoulos; Sergey Nepomnyachiy; Torsten Suel,Abstract A large amount of research has focused on faster methods for finding top-k resultsin large document collections; one of the main scalability challenges for web searchengines. In this paper; we propose a method for accelerating such top-k queries that buildson and generalizes methods recently proposed by several groups of researchers based onBlock-Max Indexes. In particular; we describe a system that uses a new filtering mechanism;based on a combination of block maxima and bitmaps; that radically reduces the number ofdocuments that have to be further evaluated. Our filtering mechanism exploits the SIMDprocessing capabilities of current microprocessors; and it is optimized through cachingpolicies that select and store suitable filter structures based on properties of the query load.Our experimental evaluation shows that the mechanism results in very significant speed …,Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval,2013,16
Guest Editors' Introduction: Social Computing in the Blogosphere,Huan Liu; Philip S Yu; Nitin Agarwal; Torsten Suel,The widespread phenomenon of blogging demonstrates the power of citizen journalism andanytime information sharing. People can exchange personal experiences; voice opinions;offer suggestions; and form groups with genuine social activities. Blogs also act as conduits;propagating data at an unprecedented pace that has led to a gigantic and dynamic opensource data archive as well as a unique opportunity for various research activities studyinginfluence; trust; reputation; privacy; search; spam; and group interaction. An importantchallenge lies in modeling and mining this vast pool of data. Social computing is anemerging interdisciplinary field and offers unique opportunities for developing novelalgorithms and tools; such as text and content mining; and graph and link mining. Anassociated challenge is data collection and objective evaluation: How can we effectively …,Internet Computing; IEEE,2010,16
Improved lower bounds for Shellsort,C Greg Plaxton; Bjorn Poonen; Torsten Suel,The authors give improved lower bounds for Shellsort based on a new and relatively simpleproof idea. The lower bounds obtained are both stronger and more general than thepreviously known bounds. In particular; they hold for nonmonotone increment sequencesand adaptive Shellsort algorithms; as well as for some recently proposed variations ofShellsort.,Foundations of Computer Science; 1992. Proceedings.; 33rd Annual Symposium on,1992,16
Permutation routing and sorting on meshes with row and column buses,Torsten Suel,We study the problems of permutation routing and sorting on several models of meshes withfixed and reconfigurable row and column buses. We describe two fast and fairly simpledeterministic algorithms for permutation routing on two-dimensional networks; and a morecomplicated algorithm for multi-dimensional networks. The algorithms are obtained byconverting two known off-line routing schemes into deterministic routing algorithms; and theycan be implemented on a variety of different models of meshes with buses. We also give adeterministic algorithm for 1–1 sorting whose running time matches that for permutationrouting; and another algorithm that matches the bisection lower bound on reconfigurablenetworks of arbitrary constant dimension.,Parallel Processing Letters,1995,14
Compressing file collections with a TSP-based approach,Dimitre Trendafilov; Nasir Memon; Torsten Suel,Abstract Delta compression techniques solve the problem of encoding a given target file withrespect to one or more reference files. Recent work in [15; 12; 7] has demonstrated thebenefits of using such techniques in the context of file collection compression. In thesescenarios; files are often better compressed by computing deltas with respect to other similarfiles from the same collection; as opposed to compressing each file by itself. It is known thatthe optimal set of such delta encodings; assuming that only a single reference file is used foreach target file; can be found by computing an optimal branching on a directed graph. In thispaper we propose two techniques for improving the compression of file collections. The firstone utilizes deltas computed with respect to more than one file; while the second oneimproves the compressibility of batched file collections; such as tar archives; using …,tech. rep.,2004,13
System and method for performing I/O-efficient join processing,*,I/O-efficient methods and apparatus are provided for the d-dimensional join problem in one;two; and three dimensions; and are also generalized for arbitrary higher dimensions. Let Nbe the total number of rectangles in the two sets to be joined; M the total amount of memoryavailable; B the disk block size; and T the total number of pairs in the output of the join.Define n= N/B; m= M/B; and t= T/B. For one and two dimensions; I/O-optimal join methodsare provided that run in O (nlogmn+ t) I/O operations and have utility to temporal and spatialdatabase systems. For dimensions d≧ 3; methods are provided that run in O (nlogm (d− 1)n+ t) I/O operations; which is within a logm (d− 2) n factor of the currently known lowerbounds.,*,2001,12
Optimizing positional index structures for versioned document collections,Jinru He; Torsten Suel,Abstract Versioned document collections are collections that contain multiple versions ofeach document. Important examples are Web archives; Wikipedia and other wikis; or sourcecode and documents maintained in revision control systems. Versioned documentcollections can become very large; due to the need to retain past versions; but there is also alot of redundancy between versions that can be exploited. Thus; versioned documentcollections are usually stored using special differential (delta) compression techniques; anda number of researchers have recently studied how to exploit this redundancy to obtainmore succinct full-text index structures. In this paper; we study index organization andcompression techniques for such versioned full-text index structures. In particular; we focuson the case of positional index structures; while most previous work has focused on the …,Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,2012,11
Server-friendly delta compression for efficient web access,Anubhav Savant; Torsten Suel,Abstract A number of researchers have studied delta compression techniques for improvingthe efficiency of web page accesses over slow communication links. Most of these schemesexploit the fact that updated web pages often change only very slightly; thus resulting in verysmall sizes for the transmitted deltas. However; these schemes are only applicable to aminority of page accesses; and require web or proxy servers to retain potentially manydifferent outdated versions of pages for use as reference files in the encoding. Anotherapproach; studied by Chan and Woo [4]; encodes a page with respect to similar files locatedon the same web server that are already in the client's browser cache. Based on the latterapproach; we study different delta compression policies for web access. Our emphasis is onweb and proxy server-friendly policies that do not require the maintenance of multiple …,*,2004,11
Geographic web usage estimation by monitoring DNS caches,Hüseyin Akcan; Torsten Suel; Hervé Brönnimann,Abstract DNS is one of the most actively used distributed databases on earth; accessed bymillions of people every day to transparently convert host names into IP addresses and viceversa. In order to improve their performance; DNS servers also keep temporary records of allrequested domain names in their cache. While most of the DNS servers are configured to beused by their local users only; there still exist many DNS servers that respond to publicqueries. Querying these DNS servers reveals the recently visited domains. Exploiting thegeographically distributed nature of DNS; one can gather usage statistics ranging from asingle DNS server to global scale. In particular; this enables collecting statistics aboutgeographic differences in web browsing behavior between different regions of a country orthe world. In this paper; we present methods to identify these public DNS servers; discuss …,Proceedings of the first international workshop on Location and the web,2008,10
System and method for predicting user navigation within sponsored search advertisements,*,An improved system and method for predicting user navigation within sponsored searchadvertisements is provided. A list of sponsored advertisements for display on a web page ofsearch results may be received. A click prediction classifier may be applied to predict a clickprobability of each sponsored advertisement and a dwell time prediction classifier may beapplied to predict a dwell time probability on web pages of a website of each sponsoredadvertisement. A probability of user navigation may be predicted for each sponsoredadvertisement using a probability of a click on each sponsored advertisement and aprobability of a dwell time on web pages of a website of each sponsored advertisement. Thelist of the sponsored advertisements may be ranked in part by the probability of usernavigation and served to a web browser executing on a client device for display on a web …,*,2011,9
System and method for increasing the speed of distributed single and multi-commodity flow using second order methods,*,A system and method for increasing the speed of flow of a quantity of data or of a plurality ofdata types through a communication system. The communication system comprises aplurality of switches including a source and a sink; each of the switches connected to aneighboring switch by a communication link having a capacity; each of the links having apair of queue buffers; and one queue buffer of each pair of queue buffers is located at eachswitch connected by a link. For a single data type; a flow is computed for each link as afunction of a last previous amount of flow across the same link. In the multi-data type case; aweighted queue difference is computed as a function of a last previous amount of flowbetween the same pair of queues and an amount of flow fi of said data type i is routed acrosseach said link such that Σ1≦ i≦ Kfi (Δ′ i (e)− fi) is maximized. In other embodiments of …,*,2002,9
Routing and sorting of fixed topologies,Torsten Suel,Abstract This thesis studies the problems of packet routing and sorting on parallel models ofcomputation that are based on a fixed; bounded-degree topology. It establishes lowerbounds for several classes of sorting networks and algorithms; and describes techniquesand algorithms for packet routing and sorting on mesh-connected and related networks. Alower bound of $\Omega $(lg n lg lg n/lg lg lg n) is established for the depth of shuffle-unshuffle sorting networks; a class of sorting networks that maps efficiently to the hypercubeand its bounded-degree variants. A stronger lower bound of $\Omega $(lg $\sp2 $ n/(lg lg n)is shown for a subclass of the shuffle-unshuffle sorting networks whose structurecorresponds to the class of ascend and descend algorithms on the hypercube. These lowerbounds also extend to restricted classes of non-oblivious sorting algorithms on …,*,1994,9
Approximate maximum weight branchings,Amitabha Bagchi; Ankur Bhargava; Torsten Suel,Abstract We consider a special subgraph of a weighted directed graph: one comprising onlythe k heaviest edges incoming to each vertex. We show that the maximum weight branchingin this subgraph closely approximates the maximum weight branching in the original graph.Specifically; it is within a factor of k/(k+ 1). Our interest in finding branchings in this subgraphis motivated by a data compression application in which calculating edge weights isexpensive but estimating which are the heaviest k incoming edges is easy. An additionalbenefit is that since algorithms for finding branchings run in time linear in the number ofedges our results imply faster algorithms although we sacrifice optimality by a small factor.We also extend our results to the case of edge-disjoint branchings of maximum weight andto maximum weight spanning forests.,Information processing letters,2006,8
Second-order methods for distributed approximate single-and multicommodity flow,S Muthukrishnan; Torsten Suel,Abstract We study local-control algorithms for maximum flow and multicommodity flowproblems in distributed networks. We propose a second-order method for accelerating theconvergence of the “first-order” distributed algorithms recently proposed by Awerbuch andLeighton. Our experimental study shows that second-order methods are significantly fasterthan the first-order methods for approximate single-and multicommodity flow problems.Furthermore; our experimental study gives valuable insights into the diffusive processes thatunderly these local-control algorithms; this leads us to identify many open technicalproblems for theoretical study.,International Workshop on Randomization and Approximation Techniques in Computer Science,1998,8
Lower bounds for Shellsort,C Greg Plaxton; Torsten Suel,Abstract We show lower bounds on the worst-case complexity of Shellsort. In particular; wegive a fairly simple proof of an Ω (n (lg 2 n)/(lg lg n) 2) lower bound for the size of Shellsortsorting networks for arbitrary increment sequences. We also show an identical lower boundfor the running time of Shellsort algorithms; again for arbitrary increment sequences. Ourlower bounds establish an almost tight trade-off between the running time of a Shellsortalgorithm and the length of the underlying increment sequence.,Journal of Algorithms,1997,8
Lower bounds for sorting networks,Nabil Kahale; Tom Leighton; Yuan Ma; C Greg Plaxton; Torsten Suel; Endre Szemerédi,Abstract We establish a lower bound of (1.12–o (l)) n log n on the size of any n-input sortingnetwork; this is the first lower bound that improves upon the trivial information-theoreticbound by more than a lower order term. We then extend the lower bound to comparatornetworks that approximately sort a certain fraction of all input permutations. We also prove alower bound of (c–o (l)) log n; where c N 3.27; on the depth of any sorting network; the bestprevious result of approximately(2.41–O (1)) log n was established by Yao in 1980. Ourresult for size is based on a new technique that lower bounds the number of “O-1 collisions”in the network; we provide strong evidence that the technique will lead to even better lowerbounds. 1Part of this work was done while the author was at DIMACS. 2 XEROX Palo AltoResearch Center; 3333 Coyote Hill Road; Palo Alto; CA 943o4. Partially supported by the …,Proceedings of the twenty-seventh annual ACM symposium on Theory of computing,1995,8
Method for partitioning multi-dimensional data sets into rectangular partitions,*,The speed and efficiency at which a computer having a fixed processing capability accomplishesa delineated task is directly proportional to the quantity of data of being processed. To accomplishtasks more quickly; some conventional processing methods partition a data set in a databaseinto a plurality of smaller data sets which can be processed together more quickly than thenon-partitioned data set from which they are derived; thereby increasing the speed at which thedata is processed. One widely used method for processing data in this manner is to constructa histogram approximation of a data set comprised of a plurality of numbers by partitioning thedata set into a plurality of subsets; ie; tiles; and then calculating the average value of the numbersin each tile; which average values are used for processing purposes … The three most widelyused types of partitions constructed for two-dimensional data sets are: an arbitrary …,*,2001,7
O-Efficient join algorithms for temporal; spatial; and constraint databases,S Ramaswamy; T Suel,Abstract We examine I/O-efficient algorithms for join problems arising in spatial; temporal;and constraint databases. Along with retrieval (implemented by hashing or indexing); thejoin is one of the most I/O-intensive operations in database systems. The join problem inmany data models can be defined as the intersection between two sets of orthogonalrectangles in d dimensions. In this paper; we present new I/O-efficient algorithms for the d-dimensional join problem in one; two; and three...,*,1996,7
Beyond the worst-case bisection bound: Fast sorting and ranking on meshes,Michael Kaufmann; Jop Sibeyn; Torsten Suel,Abstract Sorting is an important subroutine in many parallel algorithms and has beenstudied extensively on meshes and related networks. If every processor of an n× n mesh isthe source and destination of at most k elements; then sorting requires at least k· n/2 steps inthe worst-case; and simple algorithms have recently been proposed that nearly match thisbound. However; this lower bound does not extend to non-worst-case inputs; or weakerdefinitions of sorting that are sufficient in many applications. In this paper; we give algorithmsand lower bounds for several such problems. We first present a very simple scheme for kkrouting that performs optimally under both average-case and worst-case inputs. As anapplication of this scheme; we describe a simple kk sorting algorithm based on sample sortthat nearly matches this bound. The main part of the paper considers several 'sorting-like' …,Algorithms—ESA'95,1995,7
A lower bound for sorting networks based on the shuffle permutation,C. Greg  Plaxton; Torsten Suel,Abstract We prove an Ω (lg 2 n/lg lg n) lower bound for the depth of n-input sorting networksbased on the shuffle permutation. The best previously known lower bound was the trivial Ω(lg n) bound; while the best upper bound is given by Batcher's Θ (lg 2 n)-depth bitonicsorting network. The proof technique employed in the lower bound argument may be ofindependent interest.,Mathematical Systems Theory,1994,6
System and method for aggregating a list of top ranked objects from ranked combination attribute lists using an early termination algorithm,*,An improved system and method for aggregating a list of top ranked objects from rankedcombination lists using an early termination algorithm is provided. Ranked lists of individualobject attributes may be aggregated into ranked lists of combination object attributes. Theranked lists of object attributes; including ranked lists of individual object attributes as well asranked lists of combination object attributes; may be scanned in parallel. A fixed number oftop scoring objects may be stored in a results list of top ranked objects. An upper bound ofbest possible aggregation scores of unseen object in the ranked lists of object attributes maybe computed to incorporate the extra information given by the combination lists of attributes.If the upper bound computed is less than the score of top scoring objects in the results list;then the top scoring objects in the results list may be output.,*,2010,5
Improved bounds for routing and sorting on multi-dimensional meshes,Torsten Suel,Abstract We show improved bounds for 1–1 routing and sorting on multi-dimensionalmeshes and tori. In particular; we give a fairly simple deterministic algorithm for sorting onthe d-dimensional mesh of side length n that achieves a running time of 3dn/2+ o (n) for thed-dimensional mesh and torus; respectively; that make one copy of each element. We alsoshow lower bounds for sorting with respect to a large class of indexing schemes; under amodel of the mesh where each processor can hold an arbitrary number of packets. Finally;we describe algorithms for permutation routing whose running times come very close to thediameter lower bound.,Proceedings of the sixth annual ACM symposium on Parallel algorithms and architectures,1994,5
Automated decision support for human tasks in a collaborative system: the case of deletion in Wikipedia,Bluma S Gelley; Torsten Suel,Abstract Wikipedia's low barriers to participation have the unintended effect of attracting alarge number of articles whose topics do not meet Wikipedia's inclusion standards. Many arequickly deleted; often causing their creators to stop contributing to the site. We collect andmake available several datasets of deleted articles; heretofore inaccessible; and use them tocreate a model that can predict with high precision whether or not an article will be deleted.We report precision of 98.6% and recall of 97.5% in the best case and high precision withlower; but still useful; recall; in the most difficult case. We propose to deploy a systemutilizing this model on Wikipedia as a set of decision-support tools to help article creatorsevaluate and improve their articles before posting; and new article patrollers make moreinformed decisions about which articles to delete and which to improve.,Proceedings of the 9th International Symposium on Open Collaboration,2013,4
Inferring tree topologies using flow tests,S Muthukrishnan; Muthukrishnan Torsten Suel; Radek Vingralek,Abstract Introduction We consider the problem of discovering the structure of an unknownhierarchical network by means of measuring the maximum flow between the root andselected subsets of leaf nodes. More precisely; we are given a root node and a set of n leafnodes; each identified by a unique label. The leaf nodes are the leaves of a capaciratedhierarchical network. We do not have any additional information about the structure of thenetwork; including the degrees of any internal nodes; the edge capacities; or which leafnodes are in the same subtree. Our goal is to infer the structure of the network (up to certainequivalences discussed below) by using only a simple test operation; in which we" switchon" a selected subset of the leaf nodes; causing these nodes to transmit data to the root atmaximum speed; and then measure the total rate of data arriving at the root. We will refer …,Proc. 14th ACM-SIAM Symposium on Discrete Algorithms,2003,4
Fast first-phase candidate generation for cascading rankers,Qi Wang; Constantinos Dimopoulos; Torsten Suel,Abstract Current search engines use very complex ranking functions based on hundreds offeatures. While such functions return high-quality results; they create efficiency challengesas it is too costly to fully evaluate them on all documents in the union; or even intersection; ofthe query terms. To address this issue; search engines use a series of cascading rankers;starting with a very simple ranking function and then applying increasingly complex andexpensive ranking functions on smaller and smaller sets of candidate results. Researchershave recently started studying several problems within this framework of query processingby cascading rankers; see; eg;[5; 13; 17; 51]. We focus on one such problem; the design ofthe initial cascade. Thus; the goal is to very quickly identify a set of good candidatedocuments that should be passed to the second and further cascades. Previous work by …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,3
Estimating pairwise distances in large graphs,Maria Christoforaki; Torsten Suel,Point-to-point distance estimation in large scale graphs is a fundamental and well studiedproblem with applications in many areas such as Social Search. Previous work has focusedon selecting an appropriate subset of vertices as landmarks; aiming to derive distance upperor lower bounds that are as tight as possible. In order to compute a distance bound betweentwo vertices; the proposed methods apply triangle inequalities on top of the precomputeddistances between each of these vertices and the landmarks; and then use the tightest one.In this work we take a fresh look at this setting and approach it as a learning problem. Asfeatures; we use structural attributes of the vertices involved as well as the bounds describedabove; and we learn a function that predicts the distance between a source and adestination vertex. We conduct an extensive experimental evaluation on a variety of real …,Big Data (Big Data); 2014 IEEE International Conference on,2014,3
Scalable manipulation of archival web graphs,Yasemin Avcular; Torsten Suel,Abstract In this paper; we study efficient ways to construct; represent and analyze large-scale archival web graphs. We first discuss details of the distributed graph constructionalgorithm implemented in MapReduce and the design of a space-efficient layered graphrepresentation. While designing this representation; we consider both offline and onlinealgorithms for the graph analysis. The offline algorithms; such as PageRank; can useMapReduce and similar large-scale; distributed frameworks for computation. On the otherside; online algorithms can be implemented by tapping into a scalable repository (similar toDEC's Connectivity Server or Scalable Hyperlink Store by Najork); in order to perform thecomputations. Moreover; we also consider updating the graph representation with the mostrecent information available and propose an efficient way to perform updates using …,Proceedings of the 9th workshop on Large-scale and distributed informational retrieval,2011,3
Geo-targeted Web search,Torsten Suel,A simple definition is that gazetteers are dictionaries of placenames. The digital gazetteer asa component of georeferenced information systems; however; is more formally modeled. Agazetteer is defined as a collection of gazetteer entries; each of which contains; at aminimum; the tuple N; F; T where N is a place name; F is a formal expression of geographiclocation–a footprint; and T is a place type expressed with a term (or code) from a typingscheme. Applications often require; in addition; relationships between gazetteer entries;documentation of time frames; and additional information (as described below). Thegazetteer model is a type of knowledge organization system (KOS)–or ontology–which canbe modified to represent other classes of spatial-temporal information; such as named timeperiods and named events [3].,*,2009,3
Web Information Systems Engineering-WISE 2010,Lei Chen; Peter Triantafillou; Torsten Suel,*,Lecture Notes in Computer Science,2010,2
Delta encoding of related web pages,Zan Ouyang; Nasir Memon; Torsten Suel,Abstract The compression and transmission of related web pages were examined using asimple delta compression algorithm called wdelta. Results show that wdelta outperforms apreviously published delta compression algorithm named vdelta when applied to web pagesthat change in time.,Data Compression Conference,2001,2
A super-logarithmic lower bound for hypercubic sorting networks,C Greg Plaxton; Torsten Suel,Abstract Hypercubic sorting networks are a class of comparator networks whose structuremaps efficiently to the hypercube and any of its bounded degree variants. Recently; n-inputhypercubic sorting networks with depth 2 O (√ lg lg n) lg n have been discovered. Thesenetworks are the only known sorting networks of depth o (lg 2 n) that are not based onexpanders; and their existence raises the question of whether a depth of O (lg n) can beachieved by any hypercubic sorting network. In this paper; we resolve this question byestablishing an Ω (lg n lg lg n/lg lg lg n) lower bound on the depth of any n-input hypercubicsorting network. Our lower bound can be extended to certain restricted classes of non-oblivious sorting algorithms on hypercubic machines.,International Colloquium on Automata; Languages; and Programming,1994,2
Optimal Deterministic Routing and Sorting on Mesh-Connected Arrays ofProcessors,Torsten Suel,Abstract In this paper we introduce a new derandomization technique for mesh-connectedarrays of processors that allows us to convert several recently proposed randomizedalgorithms for routing and sorting into deterministic algorithms that achieve the samerunning time; within a lower order additive term. By applying this technique; we obtain anumber of optimal or improved deterministic algorithms for meshes and related networks.Among our main results are the first optimal deterministic algorithms for sorting on the two-dimensional mesh and for routing on the two-dimensional torus and the three-dimensionalmesh; as well as an optimal deterministic routing algorithm for the two-dimensional meshthat achieves a queue size of 5. The new technique is very general and seems to apply tomost of the randomized algorithms for routing and sorting on meshes and related …,*,1993,2
Structural Sentence Similarity Estimation for Short Texts.,Weicheng Ma; Torsten Suel,Abstract Sentence similarity is the basis of most text-related tasks. In this paper; we define anew task of sentence similarity estimation specifically for short while informal; socialnetworkstyled sentences. The new type of sentence similarity; which we call Structural Similarity;eliminates syntactic or grammatical features such as dependency paths and Partof-Speech(POS) tagging which do not have enough representativeness on short sentences. StructuralSimilarity does not consider actual meanings of the sentences either but puts moreemphasis on the similarities of sentence structures; so as to discover purpose-or emotion-level similarities. The idea is based on the observation that people tend to use sentenceswith similar structures to express similar feelings. Besides the definition; we present a newfeature set and a mechanism to calculate the scores; and; for the needs of disambiguating …,FLAIRS Conference,2016,1
Search engine architectures from conventional to P2P,Torsten Suel,Abstract A significant amount of research has focused on the problem of implementing large-scale search engines in peer-to-peer environments. However; gaps in both performanceand perspective remain between conventional approaches and those explored in the peer-to-peer community. In this talk; we give a brief overview of search engine architectures andperformance issues from the perspective of a researcher focusing on conventional engines.We then discuss the main differences between the two approaches; suggest areas for fruitfulinteraction; and list open problems.,Proceeding of the 2008 ACM workshop on Large-Scale distributed systems for information retrieval,2008,1
The Perron-Frobenius theorem,S Unnikrishna Pillai; Torsten Suel; Seunghun Cha,*,IEEE Signal Processing Magazine,2005,1
Optimal histograms with quality guarantees,Torsten Suel; H Jagadish; N Koudas; S Muthukrishnan; V Poosala; K Sevcik,Suel; T; Jagadish; H; Koudas; N; Muthukrishnan; S; Poosala; V & Sevcik; K 1998; Optimal histogramswith quality guarantees. in 24th International Conference on Very Large Data Bases (VLDB'98). pp. 275-286 … Suel T; Jagadish H; Koudas N; Muthukrishnan S; Poosala V; Sevcik K.Optimal histograms with quality guarantees. In 24th International Conference on Very LargeData Bases (VLDB '98). 1998. p. 275-286 … Powered by Pure; Scopus & Elsevier FingerprintEngine™ © 2018 Elsevier BV.,*,1998,1
Efficient index updates for mixed update and query loads,Sergey Nepomnyachiy; Torsten Suel,Inverted index files are commonly used to support keyword search in document collections.While the offline construction of an index can be done efficiently; its incremental updateremains a hard problem; especially when the index does not completely fit in memory. Wepropose a novel approach for maintaining up-to-date index files on a system that constantlyserves document updates and user queries. Unlike previous updating policies; we useknowledge of both the update term distribution and the query term distribution to partition theterms into functional groups. We implement two schemes for selective enforcement ofcontiguous layout of the data on disk; while mandating that the cost of the consolidation isless than its estimated benefit. The first is the “greedy merge” inspired by the ski-rentalproblem as studied in the context of competitive analysis. The second is the “opportunistic …,Big Data (Big Data); 2016 IEEE International Conference on,2016,*
Three-hop distance estimation in social graphs,Pascal Welke; Alexander Markowetz; Torsten Suel; Maria Christoforaki,In this paper; we study a 3-hop approach to distance estimation that uses two intermediatelandmarks; where each landmark only stores distances to vertices in its local neighborhoodand to the other landmarks. We show how to suitably represent and compress the distancedata stored for each landmark; for the 2-hop and 3-hop case. Overall; we find that 3-hopmethods achieve modest but promising improvement in some cases; while beingcomparable or slightly worse than 2-hop methods in others. Furthermore; our lightcompression schemes improve the practical applicability of both the 2-hop and 3-hopmethods.,Big Data (Big Data); 2016 IEEE International Conference on,2016,*
Improved methods for static index pruning,Wei Jiang; Juan Rodriguez; Torsten Suel,Static Index Pruning is a performance optimization technique for search engines thatattempts to identify and remove index postings that are unlikely to lead to top results fortypical user queries. The goal is to obtain a much smaller inverted index that can quicklyreturn results that are (almost) as good as those for the unpruned index. We make twocontributions: First; we improve on previous results for pruned index size through a carefulanalysis of both document and query distribution characteristics. We derive an initial modelbased on unigram probabilities that obtains gains over previous work in some cases; and abigram-based approach that achieves some additional improvements. We also devise asimple method for generating query logs in the absence of real-life queries; useful inmodeling top results. Our second contribution is to explore; and compare to previously …,Big Data (Big Data); 2016 IEEE International Conference on,2016,*
What makes a group fail: Modeling social group behavior in event-based social networks,Xiang Liu; Torsten Suel,Event-based online social networks; which are used to maintain interest-based groups andto distribute and organize offline events; have recently gained increasing popularity. In event-based social networks; some groups survive and thrive; while other groups fail. How to buildsuccessful groups and what factors make a “healthy” group are important open problems.We address the problem of modeling social group behavior and present detailed studies ongroup failure prediction by analyzing a large online event-based social network. Weinvestigate both the statistical properties and the structural features of the social groups; andfind that event features play an important role in distinguishing social groups with differenttopics and categories. We also observe that tightly knit communities have less averageevent participation; and both low level diversity and high level diversity in members' event …,Big Data (Big Data); 2016 IEEE International Conference on,2016,*
Program committee chairs' welcome,Minos Garofalakis; Ian Soboroff; Torsten Suel; Min Wang,Garofalakis; M.; Soboroff; I.; Suel; T.; & Wang; M. (2014). Program committee chairs'welcome. Unknown Journal; iv … Program committee chairs' welcome. / Garofalakis;Minos; Soboroff; Ian; Suel; Torsten; Wang; Min … Garofalakis; M; Soboroff; I; Suel; T &Wang; M 2014; 'Program committee chairs' welcome' Unknown Journal; pp. iv … GarofalakisM; Soboroff I; Suel T; Wang M. Program committee chairs' welcome. Unknown Journal. 2014Nov 3;iv … Powered by Pure; Scopus & Elsevier Fingerprint Engine™ © 2017 Elsevier BV.,Unknown Journal,2014,*
WWW’14 Program Chairs’ Welcome,Andrei Broder; Kyuseok Shim; Torsten Suel,We are very happy to present the technical program of the 23rd International World WideWeb Conference (WWW 2014); held from April 7 to 11; 2014 in Seoul; Korea. Since itsinception in 1994; the International World Wide Web Conference has been the premiervenue for researchers; academics; businesses; and standards organizations to convene anddiscuss the latest Web research and technologies. For the research program; we received atotal of 645 submissions distributed over 11 technical areas. Submissions were reviewed byour 422 program committee members and more than 200 external reviewers: these expertsin the field collectively produced almost 2000 reviews. With this input; after a two-day areachair meeting in Mountain View; CA; a total of 84 papers (13.02%) were accepted as fullresearch papers and 91 papers were referred to the Poster Track. We believe that these …,Unknown Journal,2014,*
Optimizing Top-k Document Retrieval Strategies for Block-Max Indexes,Sergey Nepomnyachiy; Torsten Suel,ABSTRACT Large web search engines use significant hardware and energy resources toprocess hundreds of millions of queries each day; and a lot of research has focused on howto improve query processing efficiency. One general class of optimizations called earlytermination techniques is used in all major engines; and essentially involves computing topresults without an exhaustive traversal and scoring of all potentially relevant index entries.Recent work in [9; 7] proposed several early termination algorithms for disjunctive top-kquery processing; based on a new augmented index structure called Block-Max Index thatenables aggressive skipping in the index. In this paper; we build on this work by studyingnew algorithms and optimizations for Block-Max indexes that achieve significantperformance gains over the work in [9; 7]. We start by implementing and comparing Block …,In Proc. of the Sixth ACM International Conference on Web Search and Data Mining,2013,*
The 3 rd Temporal Web Analytics Workshop (TempWeb),Ricardo Baeza-Yates; Julien Masanès; Marc Spaniol; Omar Alonso; Ralitsa Angelova; Srikanta Bedathur; Andras A Benczur; Klaus Berberich; Roi Blanco; Philipp Cimiano; Renata Galante; Adam Jatowt; Scott Kirkpatrick; Frank McCown; Michael Nelson; Kjetil Nørvåg; Nikos Ntarmos; Rodrygo Luis Teodoro Santos; Philippe Rigaux; Thomas Risse; Torsten Suel; Masashi Toyoda; Gerhard Weikum,Supporters: European Union 7th Framework IST programme STREP (contract no. 258105),*,2013,*
Web Information Systems Engineering-WISE 2010: 11th International Conference; Hong Kong; China; December 12-14; 2010; Proceedings,Lei Chen; Peter Triantafillou; Torsten Suel,th Welcome to the Proceedings of WISE 2010—the 11 International Conference on WebInformation Systems Engineering. This year; WISE returned to the place where the inauguralconference was held in 2000; Hong Kong. WISE has also been held in: 2001 Kyoto (Japan);2002 Singapore; 2003 Rome (Italy); 2004 Brisbane (Australia); 2005 New York (USA); 2006Wuhan (China); 2007 Nancy (France); 2008 Auckland (New Zealand); and 2009 Poznan(Poland). Continuing its trend; this year's WISE provided a forum for engineers and scientiststo present their latest findings in Web-related technologies and solutions. The submittedcontributions address challenging issues in Web services; search; modeling;recommendation and data mining; as well as keyword search; social network analysis;query languages; and information retrieval and extraction. This year; WISE received 170 …,*,2010,*
Welcome from the Conference Chairs,Brian D Davison; Torsten Suel,Welcome to WSDM 2010—the ACM international conference on Web Search and DataMining. These proceedings represent only the third meeting of this conference; and yet theseries has attracted significant attention; right from the beginning; in 2008 in Stanford; CA;and growing in Barcelona last year. As the PC Chairs' message will detail; the number ofsubmissions to the conference has grown substantially over last year; and we have adjustedthe conference format to accommodate more accepted papers. This year; the mainconference runs a full three days; but we have retained the more intimate; single-track formatby reducing presentation time. We hope and expect that attendees will take advantage of thelunches and breaks (and of course the conference banquet) to mix informally with thespeakers to learn more; to provide feedback; and perhaps plan a future collaboration.,Unknown Journal,2010,*
Design and Implementation of a Geographic Search Engine,Torsten Suel; Alexander Markowetz; Yen Yu Chen; Xiaohui Long; B Seeger,Suel; T; Markowetz; A; Chen; YY; Long; X & Seeger; B 2005; Design and Implementation of aGeographic Search Engine. in 8th International Workshop on the Web and Databases(WebDB) … Suel T; Markowetz A; Chen YY; Long X; Seeger B. Design and Implementationof a Geographic Search Engine. In 8th International Workshop on the Web and Databases(WebDB). 2005 … Suel; Torsten ; Markowetz; Alexander ; Chen; Yen Yu ; Long; Xiaohui ;Seeger; B. / Design and Implementation of a Geographic Search Engine. 8th International Workshopon the Web and Databases (WebDB). 2005 … Powered by Pure; Scopus & Elsevier FingerprintEngine™ © 2018 Elsevier BV.,*,2005,*
Aspnes; James; 127,Richard Beigel; Daniel J Bernstein; John M Boyer; Yookun Cho; David Eppstein; Faith Fich; Ben Gum; Dean Hoffman; Peter Johnson; Yoonjeong Kim; Martin Kochol; Andrea LaPaugh; Mun-Kyu Lee; Richard J Lipton; Luiz Felipe Martins; S Muthukrishnan; Kunsoo Park; Robin Pemantle; Rainer Schuler; Torsten Suel; Vilmar Trevisan; Orli Waarts; Biing-Feng Wang; Kenneth Weber; Nadine Wilson,*,Journal of Algorithms,2005,*
Compressing File Collections with a TSP-Approach,Torsten Suel; Dimitre Trendafilov; Nasir Memon,Suel; T.; Trendafilov; D.; & Memon; N. (2004). Compressing File Collections with aTSP-Approach. (Technical Report TR-CIS-2004-02). Polytechnic University … CompressingFile Collections with a TSP-Approach. / Suel; Torsten; Trendafilov; Dimitre; Memon; Nasir …Suel; T; Trendafilov; D & Memon; N 2004; Compressing File Collections with aTSP-Approach. Technical Report TR-CIS-2004-02; Polytechnic University … Suel T; TrendafilovD; Memon N. Compressing File Collections with a TSP-Approach. Polytechnic University;2004. (Technical Report TR-CIS-2004-02) … Suel; Torsten ; Trendafilov; Dimitre ; Memon;Nasir. / Compressing File Collections with a TSP-Approach. Polytechnic University; 2004. (TechnicalReport TR-CIS-2004-02) … Powered by Pure; Scopus & Elsevier Fingerprint Engine™ © 2017Elsevier BV.,*,2004,*
Yossi Azar; Leah Epstein; Yossi Richter; and Gerhard J. Woeginger. All-norm,J Ellis; H Fan; M Fellows; Joan Boyar; Susan Krarup; Morten N Nielsen; Tak-Wah Lam; Tsuen-Wan Johnny Ngan; Kar-Keung To; John M Boyer; Biing-Feng Wang; Robin Pemantle; S Muthukrishnan; Torsten Suel; Abraham Flaxman; Alan Frieze; Eli Upfal; Graham Cormode; S Muthukrishnan; Reuven Bar-Yehuda; Guy Even; Shimon Moni Shahar,*,Journal of Algorithms,2004,*
Rasmus Pagh and Flemming Friche Rodler. Cuckoo hashing,Amotz Bar-Noy; Grzegorz Malewicz; Vincenzo Liberatore; Tak-Wah Lam; Tsuen-Wan Johnny Ngan; Kar-Keung To; Robin Pemantle; S Muthukrishnan; Torsten Suel; Reuven Bar-Yehuda; Guy Even; Shimon Moni Shahar,*,Journal of Algorithms,2004,*
Ben Gum; Richard J. Lipton; Andrea LaPaugh; and Faith Fich. Estimating the maxi,John M Boyer; Rainer Schuler; Biing-Feng Wang; Robin Pemantle; S Muthukrishnan; Torsten Suel,*,Journal of Algorithms,2004,*
Approximate Single-and Multicommodity Flow,S Muthukrishnan; Torsten Suel,Abstract. We study local-control algorithms for maximum flow and multicommodity flowproblems in distributed networks. We propose a second-order method for accelerating theconvergence of the “first-order” distributed algorithms recently proposed by Awerbuch andLeighton. Our experimental study shows that second-order methods are significantly fasterthan the first-order methods for approximate single-and multicommodity flow problems.Furthermore; our experimental study gives valuable insights into the diffusive processes thatunderly these local-control algorithms; this leads us to identify many open technicalproblems for theoretical study.,Randomization and Approximation Techniques in Computer Science: Second International Workshop; RANDOM’98; Barcelona; Spain; October 8–10; 1998 Proceedings,2003,*
zdelta: An Efficient Delta Compression Tool,Dimitre Trendafilov Nasir Memon Torsten Suel,Abstract In this report we describe a tool for delta compression; ie; the efficient encoding of agiven data set in relation to another one. Its possible applications include archiving multipleversions of data; distribution of software updates; delta compression of backup files; orcompression at the file system level. The compressor; called zdelta; could be viewed as amodification of the zlib compression library [4] with some additional ideas inspired by thevdelta/vcdiff tool of Vo [5]. We also present experimental results comparing zdelta to otherdelta compression tools. Additional information about zdelta; including source code andupdates; is available at http://cis. poly. edu/zdelta/.,*,2002,*
Using Delta Encoding for Compressing Related Web,Zan Ouyang; NaSir Memon; TorSten Suel,Abstract This paper examines the compression benefits that can be obtained by exploitingthe possible similarities of pages on the World Wide Web. Two of the elements of ourapproach are innovative. First; we provide a new differential compression schemes forsimilar web pages; which we call wdelta; it is competitive with the current delta algorithmvdelta; and quite outperform diff+ gzip. Secondly; we present an efficient algorithm forcomputing the optimal page ordering for related pages. This algorithm has time complexity O(T2) for a n-page group. In order to accelerate download speed over narrowband links (like;dial-up modems wireless communication); a heuristic method; is also presented here toreduce the time complexity and simplify the decoding chain; that is; for every clusteringgroup; an anchor page is chosen to act as an base page for all the other pages to have …,*,2000,*
A superlogarithmic lower bound for shuffle-unshuffle sorting networks,C Greg Plaxton; Torsten Suel,Abstract. Shuffle-unshuffle sorting networks are a class of comparator networks whosestructure maps efficiently to the hypercube and any of its bounded degree variants. Recently;n-input shuffle-unshuffle sorting networks with depth 2^O(lglgn)lgn have been discovered.These networks are the only known sorting networks of depth o (lg 2 n) that are not based onexpanders; and their existence raises the question of whether a depth of O (lg n) can beachieved by any shuffle-unshuffle sorting network. In this paper we resolve this question byestablishing an Ω (lg n lg lg n/lg lg lg n) lower bound on the depth of any n-input shuffle-unshuffle sorting network. Our lower bound can be extended to certain restricted classes ofnonoblivious sorting algorithms on hypercubic machines.,Theory of Computing Systems,2000,*
A unified approach for indexed and non-indexed spacial joins,Torsten Suel; Lars Arge; Octavian Procopiuc; Sridhar Ramaswamy; J Vahrenhold; Jeffrey Scott Vitter,Suel; T; Arge; L; Procopiuc; O; Ramaswamy; S; Vahrenhold; J & Vitter; JS 2000; A unified approachfor indexed and non-indexed spacial joins. in 7th International Conference on Extending DatabaseTechnology (EDBT 2000). pp. 412-429 … Suel T; Arge L; Procopiuc O; Ramaswamy S; VahrenholdJ; Vitter JS. A unified approach for indexed and non-indexed spacial joins. In 7th InternationalConference on Extending Database Technology (EDBT 2000). 2000. p. 412-429 … Poweredby Pure; Scopus & Elsevier Fingerprint Engine™ © 2018 Elsevier BV.,*,2000,*
On rectangular partitionings of two-dimensional data: Algorithms; complexity and applications,Torsten Suel; S Muthukrishnan; V Poosala,Suel; T; Muthukrishnan; S & Poosala; V 1999; On rectangular partitionings of two-dimensionaldata: Algorithms; complexity and applications. in 7th International Conference on Database Theory(ICDT '99). pp. 236-256 … Suel T; Muthukrishnan S; Poosala V. On rectangular partitioningsof two-dimensional data: Algorithms; complexity and applications. In 7th International Conferenceon Database Theory (ICDT '99). 1999. p. 236-256 … Powered by Pure; Scopus & Elsevier FingerprintEngine™ © 2018 Elsevier BV.,*,1999,*
Scalable sweeping-based spatial join,Torsten Suel; Lars Arge; Octavian Procopiuc; Sridhar Ramaswamy; Jeffrey Scott Vitter,Suel; T; Arge; L; Procopiuc; O; Ramaswamy; S & Vitter; JS 1998; Scalable sweeping-based spatialjoin. in 24th International Conference on Very Large Data Bases (VLDB '98). pp. 570-581 …Suel T; Arge L; Procopiuc O; Ramaswamy S; Vitter JS. Scalable sweeping-based spatialjoin. In 24th International Conference on Very Large Data Bases (VLDB '98). 1998. p.570-581 … Powered by Pure; Scopus & Elsevier Fingerprint Engine™ © 2018 Elsevier BV.,*,1998,*
Portable and E cient Parallel Computing Using the BSP Model,Mark W Goudreauy; Kevin Langz; Satish B Raox; Torsten Suel; Thanasis Tsantilask,Abstract The Bulk-Synchronous Parallel (BSP) model was proposed by Valiant as astandard interface between parallel software and hardware. In theory; the BSP model hasbeen shown to allow the asymptotically optimal execution of architecture-independentsoftware on a variety of architectures. Our goal in this work is to experimentally examine thepractical use of the BSP model on current parallel architectures. We describe the design andimplementation of the Green BSP Library; a small library of functions that implement the BSPmodel; and of several applications that were written for this library. We then discuss theperformance of the library and application programs on several parallel architectures. Ourresults are positive; in that we demonstrate e ciency and portability over a range of parallelarchitectures; and show that the BSP cost model is useful for predicting performance …,*,1998,*
On randomization versus determinism in parallel sorting and routing problems,Torsten Suel,Skip to main content …,*,1998,*
On randomized and deterministic schemes for routing and sorting on fixed-connection networks,Torsten Suel,Abstract We give a high-level description of some fundamental randomized anddeterministic techniques for routing and sorting on fixed-connection networks such asmeshes; hypercubes or point-to-point networks. On the randomized side; we focus on thetechniques of randomized routing and random sampling and their use in many algorithms;while our presentation of deterministic algorithms uses the example of the Columnsortalgorithm to highlight techniques such as local sorting and deterministic sampling. We thendemonstrate that there is a close relationship between the randomized and deterministictechniques presented; and illustrate how this relationship can be used to transformrandomized into deterministic algorithms and vice versa. Our main objective here is toprovide a more unified perspective on many of the algorithms in the literature; and we do …,International Parallel Processing Symposium,1998,*
Nesetril; Jaroslav; 207 Newman; Ilan; 101,Takao Nishizeki; Ishai Ben Aroya; Yair Bartal; Phillip G Bradford; Gunnar Brinkmann; Artur Czumaj; David Eppstein; Rudolf Fleischer; Juan A Garay; Leszek Gasieniec; Michael T Goodrich; Inder S Gopal; Magnús M Halldórsson; Lisa Higham; David Kirkpatrick; Evangelos Kranakis; Danny Krizanc; Shay Kutten; Martin Loebl; Yishay Mansour; Marek Piotrów; Boris Pittel; C Greg Plaxton; Vijaya Ramachandran; Adi Rosén; Wojciech Rytter; Assaf Schuster; Akiyoshi Shioura; Michiel Smid; Torsten Suel; Hitoshi Suzuki; Roberto Tamassia; Mikkel Thorup; Ramakrishna Thurimella; Takeaki Uno; Biing-Feng Wang; Karsten Weihe; Robert S Weishaar; Moti Yung; Xiao Zhou,*,Journal of Algorithms,1997,*
Toward efficiency and portability,Torsten Suel; Mark Goudreau; Kevin Lang; Satish Rao; Thanasis Tsantilas,*,*,1996,*
Lower bounds for sorting networks,Torsten Suel; N Kahale; Tom Leighton; Yuan Ma; C Greg Plaxton; E Szemeredi,Suel; T; Kahale; N; Leighton; T; Ma; Y; Plaxton; CG & Szemeredi; E 1995; Lower bounds for sortingnetworks. in 27th ACM Symposium on the Theory of Computing (STOC '95'). pp. 437-446 …Suel T; Kahale N; Leighton T; Ma Y; Plaxton CG; Szemeredi E. Lower bounds for sortingnetworks. In 27th ACM Symposium on the Theory of Computing (STOC '95'). 1995. p.437-446 … Powered by Pure; Scopus & Elsevier Fingerprint Engine™ © 2017 Elsevier BV.,*,1995,*
Routing and Sorting of Fixed Typologies: PhD thesis,Torsten Suel,Suel; T. (1994). Routing and Sorting of Fixed Typologies: PhD thesis. Technical ReportTR-94-29: Department of Computer Science; University of Texas at Austin … Routing and Sortingof Fixed Typologies : PhD thesis. / Suel; Torsten … Technical Report TR-94-29 : Departmentof Computer Science; University of Texas at Austin; 1994 … Suel; T 1994; Routing and Sortingof Fixed Typologies: PhD thesis. Department of Computer Science; University of Texas atAustin; Technical Report TR-94-29 … Suel T. Routing and Sorting of Fixed Typologies: PhDthesis. Technical Report TR-94-29: Department of Computer Science; University of Texas atAustin; 1994 … Suel; Torsten. / Routing and Sorting of Fixed Typologies : PhD thesis. TechnicalReport TR-94-29 : Department of Computer Science; University of Texas at Austin; 1994 …Powered by Pure; Scopus & Elsevier Fingerprint Engine™ © 2018 Elsevier BV.,*,1994,*
A super-logarithmic lower bound for shuffle-unshuffle sorting networks,Torsten Suel; C Greg Plaxton,Suel; T & Plaxton; CG 1994; A super-logarithmic lower bound for shuffle-unshuffle sortingnetworks. in 21st International Colloquium on Automata; Languages and Programming (CALP'94). pp. 618-629 … Suel T; Plaxton CG. A super-logarithmic lower bound for shuffle-unshufflesorting networks. In 21st International Colloquium on Automata; Languages and Programming(CALP '94). 1994. p. 618-629 … Powered by Pure; Scopus & Elsevier Fingerprint Engine™© 2017 Elsevier BV.,*,1994,*
On the State-Change Complexity of Cellular Automata: Thesis (Diplomarbeit),Torsten Suel,Skip to main content …,*,1990,*
MOPS: A System for the Computer-Aided Verification of Programs Written in a Sublanguage of Modula-2 (in German),Torsten Suel; J Merker,Braunschweig; Germany : Institute for Programming Languages and Information Systems; TechnicalUniversity of Braunschweig. 1989 … Suel; T & Merker; J MOPS: A System for theComputer-Aided Verification of Programs Written in a Sublanguage of Modula-2 (inGerman) … Suel T; Merker J. MOPS: A System for the Computer-Aided Verification of ProgramsWritten in a Sublanguage of Modula-2 (in German). 1989 … Suel; Torsten ; Merker; J. /MOPS: A System for the Computer-Aided Verification of Programs Written in a Sublanguageof Modula-2 (in German) … Powered by Pure; Scopus & Elsevier Fingerprint Engine™ © 2018Elsevier BV.,*,1989,*
Improved Single-Round Protocols for Remote File Synchronization,Utku Irmak Svilen Mihaylov Torsten Suel,Abstract—Given two versions of a file; a current version located on one machine and anoutdated version known only to another machine; the remote file synchronization problem ishow to update the outdated version over a network with a minimal amount ofcommunication. In particular; when the versions are very similar; the total data transmittedshould be significantly smaller than the file size. File synchronization problems arise in manyapplication scenarios such as web site mirroring; file system backup and replication; andweb access over slow links. An open source tool for this problem; called rsync and includedin many Linux distributions; is widely used in such scenarios. rsync uses a single round ofmessages between the two machines. While recent research has shown that significantadditional savings in bandwidth consumption are possible through the use of optimized …,*,*,*
Compressing File Collections with a TSP-Based Approach,Dimitre Trendafilov Nasir Memon Torsten Suel,Abstract Delta compression techniques solve the problem of encoding a given target file withrespect to one or more reference files. Recent work in [15; 12; 7] has demonstrated thebenefits of using such techniques in the context of file collection compression. In thesescenarios; files are often better compressed by computing deltas with respect to other similarfiles from the same collection; as opposed to compressing each file by itself. It is known thatthe optimal set of such delta encodings; assuming that only a single reference file is used foreach target file; can be found by computing an optimal branching on a directed graph. In thispaper we propose two techniques for improving the compression of file collections. The firstone utilizes deltas computed with respect to more than one file; while the second oneimproves the compressibility of batched file collections; such as tar archives; using …,*,*,*
Theory and Practice of I/O-Efficient Algorithms for Multidimensional Batched Searching Problems (Extended Abstract) Lars Arge* Octavian Procopiuc† Sridhar Rama...,Torsten Suel; Jeffrey Scott Vitter,Abstract We describe a powerful framework for designing efficient batch algorithms forcertain large-scale dynamic problems that must be solved using external memory. The classof problems we consider; which we call colorable externaldecomposable problems; includerectangle intersection; orthogonal line segment intersection; range searching; and pointlocation. We are particularly interested in these problems in two and higher dimensions.They have numerous applications in geographic information systems (GIS); spatialdatabases; and VLSI and CAD design. We present simplified algorithms for problemspreviously solved by more complicated approaches (such as rectangle intersection); and wepresent efficient algorithms for problems not previously solved in an efficient way (such aspoint location and higherdimensional versions of range searching and rectangle …,*,*,*
1. Dissertation Research,Qingyang Wang; Constantinos Dimopoulos; Torsten Suel,My dissertation research shows that transient bottlenecks are an important contributingfactor to the wide response time variations of web-facing applications at high utilization. Thedissertation research consists of two parts: first; I identified several causes of transientbottlenecks from different system layers and explained why transient bottlenecks can lead towide range response time variations. Second; I designed and implemented practicaltechniques to detect transient bottlenecks and solutions to avoid or mitigate the negativeimpact of transient bottlenecks. The following paragraphs will discuss each of the two partsin detail. Transient bottlenecks can arise from a wide range of factors at different systemlayers. For example; I have identified several factors that can cause transient bottlenecks;ranging from CPU DVFS control at the architecture layer; to soft resource allocation at the …,*,*,*
Main Conference,Yiannis Cotronis; Masoud Daneshtalab; George Angelos Papadopoulos; Marco Aldinucci; Angelos Amanatiadis; Juan L Aragón; Mats Aspnäs; Rob Baxter; Julien Bourgeois; Peter Brauer; Massimo Canonico; Weiwei Chen; Zhezhe Chen; Andrea Clematis; Angelo Corana; Daniele D'Agostino; Donato D'Ambrosio; Marco Danelutto; Francisco De Sande; Cristian Dittamo; Didier El Baz; Christian Engelmann; Antonella Galizia; Manoj Gaur; Vladimir Getov; Horacio Gonzalez-Velez; Håkan Grahn; Armin Größlinger; Eric Heien; Tobias Hossfeld; Miaoqing Huang; Saurabh Hukerikar; Rika Ito; Adrian Jackson; Magnus Jahre; Jorn W Janneck; Gert Jervan; Gabriele Jost; Gregory Karagiorgos; Christos Kartsaklis; Christoph Kessler; Farshad Khunjush; Peter Kilpatrick; Konrad Kloeckner; Elias Konstantinidis; Sebastien Lafond; Khalid Latif; Vijay Laxmi; Coromoto Leon; Francesco Leporati; Huang Letian; Diego Lopez; Konstantinos Margaritis; Luisa Massari; Massimiliano Meneghin; Nikolaos Missirlis; Koji Nakano; Smail Niar; Tomas Nordström; Salvatore Orlando; Jin Park; Raffaele Perego; Radu Prodan; Kan Qiao; Amir Rahmani; Jean Roman; Hamid Sarbazi-Azad; Tobias Schuele; Georgios Ch Sirakoulis; Amund Skavhaug; Alejandro Soba; Fengguang Song; William Spataro; Ivor Spence; Achim Streit; Torsten Suel; Enqiang Sun; Domenico Talia; Jie Tao; Daniele Tessera; Francisco Tirado; Massimo Torquati; Pedro Trancoso; Giuseppe A Trunfio; Volodymyr Turchenko; Filippos Tzaferis; Zain Ul-Abdin; Kameswar Rao Vaddina; Frédéric Vivien; Roland Wismüller; Thomas Canhao Xu; Laurence T Yang,Yiannis Cotronis; National and Kapodistrian University of Athens Masoud Daneshtalab; KTHRoyal Institute of Technology George Angelos Papadopoulos; University of Cyprus … MarcoAldinucci Angelos Amanatiadis Juan L. Aragón Mats Aspnäs Rob Baxter Julien Bourgeois PeterBrauer Massimo Canonico Weiwei Chen Zhezhe Chen Andrea Clematis Angelo Corana DanieleD'Agostino Donato D'Ambrosio Marco Danelutto Francisco De Sande Cristian Dittamo DidierEl Baz Christian Engelmann Antonella Galizia Manoj Gaur Vladimir Getov HoracioGonzalez-Velez Håkan Grahn Armin Größlinger Eric Heien Tobias Hossfeld Miaoqing HuangSaurabh Hukerikar Rika Ito … Adrian Jackson Magnus Jahre Jorn W Janneck Gert Jervan GabrieleJost Gregory Karagiorgos Christos Kartsaklis Christoph Kessler Farshad Khunjush Peter KilpatrickKonrad Kloeckner Elias Konstantinidis Sebastien Lafond Khalid Latif Vijay Laxmi …,*,*,*
New Protocols for Remote File Synchronization Based on Erasure Codes Ѓ,Utku Irmak Svilen Mihaylov Torsten Suel,Abstract Given two versions of a file; a current version located on one machine and anoutdated version known only to another machine; the remote file synchronization problem ishow to update the outdated version over a network with a minimal amount ofcommunication. In particular; when the versions are very similar; the total data transmittedshould be significantly smaller than the file size. In this paper; we present a new approach tofile synchronization based on the use of erasure codes. Using this approach; we design asingle-round protocol that is provably efficient with respect to common measures of filedistance; and another optimized practical protocol that shows promising improvements overrsync on our data sets.,*,*,*
Interactive Wrapper Generation with Minimal User Effort,Utku Irmak Torsten Suel,Abstract While much of the data on the Web is unstructured in nature; there is also asignificant amount of embedded structured data; such as product information on e-commerce sites or stock data on financial sites. A large amount of research has focused onthe problem of generating wrappers; ie; software tools that allow easy and robust extractionof structured data from text and HTML sources. In many applications; such as comparisonshopping; data has to be extracted from many different sources; making manual coding of awrapper for each source impractical. On the other hand; fully automatic approaches areoften not reliable enough; resulting in low quality of the extracted data. We describe asystem for semi-automatic wrapper generation that can be trained on various data sourcesin a simple interactive manner. Our goal is to minimize the user effort for training reliable …,*,*,*
Calls for Papers,Torsten Suel,The widespread phenomenon of blogging demonstrates the power of citizen journalism andinstantaneous information sharing. Blogs act as conduits for information; propagating itsspread at an unprecedented pace. This creates an increasingly rich; open source of dataand presents a great opportunity for fresh research activities on influence; trust andreputation; privacy; search; spam; and group interaction. A prominent challenge lies inmodeling and mining this data to extract; represent; and exploit meaningful knowledge andleverage the structures and dynamics of emerging social networks in the blogosphere.Social computing that combines data mining with social network analysis is a promisingdirection and offers unique opportunities for developing novel algorithms and tools.,*,*,*
page 1,Torsten Suel,In the first part of the paper; we give improved deterministic algorithms for permutation routingand sorting. Among our results; we obtain two fairly simple algorithms for permutation routingon two-dimensional meshes with buses that achieve a running time of n + o(n) and a queue sizeof 2. We also describe an algorithm for routing on r-dimensional networks with a running timeof (2 ? 1=r)n + o(n) and a queue size of 2; and show how to obtain deterministic algorithms forsorting whose running times match those for permutation routing; within a lower order additiveterm. An interesting feature of our algorithms is that they can be efficiently implemented on avariety of different models of meshes with buses. The algorithms are obtained through a newtechnique that allows us to convert certain off-line routing schemes into deterministic on-linealgorithms … In the second part of the paper; we study the performance of meshes with …,*,*,*
DIMACS Working Group on The Burrows-Wheeler Transform: Ten Years Later,Mike Burrows; Nasir Memon; Raffaele Giancarlo; Marinella Sciortino; Peter Fenwick; Martin Farach-Colton; Kunihiko Sadakane; Paolo Ferragina; Veli Makinen; Ankur Gupta; Jeff Vitter; Torsten Suel; James J Hunt; Alberto Apostolico; S Cenk Sahinalp; Neva Cherniavsky; Serap Savari; Raanan Refua,DIMACS Working Group on The Burrows - Wheeler Transform: Ten Years Later. August 19 - 20;2004 DIMACS Center; CoRE Building; Rutgers University; Piscataway; NJ. Organizers: PaoloFerragina; University of Pisa Giovanni Manzini; University of Piemonte Orientale S. Muthukrishnan;Rutgers University; muthu@cs.rutgers.edu Presented under the auspices of the Special Focus onSpecial Focus on Data Analysis and Mining. Workshop Program: Thursday; August 19; 2004 8:15 -8:50 Breakfast and registration - CoRE Building; 4th Floor 8:50 - 9:00 Welcome and opening remarksS. Muthukrishnan; DIMACS and Rutgers University 9:00 - 10:00 The pre-history and future of theblock-sorting compression algorithm Mike Burrows; Google …,*,*,*
Low-Latency File Synchronization in Distributed Systems,Hao Yan; Utku Irmak; Torsten Suel,Abstract The remote file synchronization problem is how to update an outdated version of afile located on one machine to the current version located on another machine with aminimal amount of network communication. It arises in many scenarios including web sitemirroring; file system backup and replication; or web access over slow links. A widely usedopen-source tool called rsync uses a single round of messages between the two machinesto solve this problem (plus an initial round for exchanging meta information). While researchhas shown that significant additional savings in bandwidth are possible by using multiplerounds; such approaches are often not desirable due to network latencies; increasedprotocol complexity; or other overheads at the endpoints. In this paper; we study single-round synchronization techniques that offer significant benefits in bandwidth consumption …,*,*,*
Towards an Open and Highly Distributed Web Information Retrieval Architecture,Torsten Suel; Chandan Mathur; Jo-Wen Wu; Jiangong Zhang; Alex Delis; Mehdi Kharrazi; Xiaohui Long; Kulesh Shanmugasundaram,Abstract Due to the large size of the Web; users require specialized tools to navigate throughthe vast volumes of data; and a number of search engines and other IR tools have been builtto fill this need. The major engines are typically based on scalable clusters; ie; largenumbers of low-cost servers at a single location. Recent events have seen a concentrationin this market towards a small number of major players that offer their own proprietaryranking and user tools. For various reasons; these engines do not provide open interfaces tothe lower layers of their infrastructure; but offer a service that combines these layers withranking and user interface. We are proposing a two-tier model of web IR architectures thatseparates the lower layers of data acquisition; index construction; and index lookups fromthe higher layers of ranking and user interfaces. Under this model; we investigate the …,*,*,*
Is P2P a Suitable Architecture for Large-Scale Web Search? Ѓ,Torsten Suel,Abstract In this note we discuss the feasibility of a peer-to-peer based infrastructure for large-scale web search that could provide an alternative to current centralized engines such asGoogle. We first outline the structure and performance bottlenecks of current searchengines; and then discuss how to construct a peer-to-peer based engine. Our contention isthat for the time being; a peer-to-peer solution is unlikely to be truly competitive due toperformance and manageability issues.,*,*,*
