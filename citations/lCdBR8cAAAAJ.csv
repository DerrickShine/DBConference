Perm: Processing provenance and data on the same data model through query rewriting,Boris Glavic; Gustavo Alonso,Data provenance is information that describes how a given data item was produced. Theprovenance includes source and intermediate data as well as the transformations involvedin producing the concrete data item. In the context of a relational databases; the source andintermediate dataitems are relations; tuples and attribute values. The transformations areSQL queries and/or functions on the relational data items. Existing approaches captureprovenance information by extending the underlying data model. This has the intrinsicdisadvantage that the provenance must be stored and accessed using a different modelthan the actual data. In this paper; we present an alternative approach that uses queryrewriting to annotate result tuples with provenance information. The rewritten query and itsresult use the same model and can; thus; be queried; stored and optimized using …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,113
Data Provenance: A Categorization of Existing Approaches.,Boris Glavic; Klaus R Dittrich,Abstract: In many application areas like e-science and data-warehousing detailedinformation about the origin of data is required. This kind of information is often referred to asdata provenance or data lineage. The provenance of a data item includes information aboutthe processes and source data items that lead to its creation and current representation. Thediversity of data representation models and application domains has lead to a number ofmore or less formal definitions of provenance. Most of them are limited to a specialapplication domain; data representation model or data processing facility. Not surprisingly;the associated implementations are also restricted to some application domain and dependon a special data model. In this paper we give a survey of data provenance models andprototypes; present a general categorization scheme for provenance models and use this …,BTW,2007,87
Big data provenance: Challenges and implications for benchmarking,Boris Glavic,Abstract Data Provenance is information about the origin and creation process of data. Suchinformation is useful for debugging data and transformations; auditing; evaluating the qualityof and trust in data; modelling authenticity; and implementing access control for deriveddata. Provenance has been studied by the database; workflow; and distributed systemscommunities; but provenance for Big Data-which we refer to as Big Provenance-is a largelyunexplored field. This paper reviews existing approaches for large-scale distributedprovenance and discusses potential challenges for Big Data benchmarks that aim toincorporate provenance data/management. Furthermore; we will examine how Big Databenchmarking could benefit from different types of provenance information. We argue thatprovenance can be used for identifying and analyzing performance bottlenecks; to …,*,2014,43
A generic provenance middleware for database queries; updates; and transactions,Bahareh Arab; Dieter Gawlick; Venkatesh Radhakrishnan; Hao Guo; Boris Glavic,Abstract We present an architecture and prototype implementation for a generic provenancedatabase middleware (GProM) that is based on the concept of query rewrites; which areapplied to an algebraic graph representation of database operations. The system supports awide range of provenance types and representations for queries; updates; transactions; andoperations spanning multiple transactions. GProM supports several strategies forprovenance generation; eg; on-demand; rule-based; and “always on”. To the best of ourknowledge; we are the first to present a solution for computing the provenance of concurrentdatabase transactions. Our solution can retroactively trace transaction provenance as longas an audit log and time travel functionality are available (both are supported by mostDBMS). Other noteworthy features of GProM include: extensibility through a declarative …,Proceedings of TaPP,2014,33
Using sql for efficient generation and querying of provenance information,Boris Glavic; Renée J Miller; Gustavo Alonso,Abstract In applications such as data warehousing or data exchange; the ability to efficientlygenerate and query provenance information is crucial to understand the origin of data. In thischapter; we review some of the main contributions of Perm; a DBMS that generates differenttypes of provenance information for complex SQL queries (including nested and correlatedsubqueries and aggregation). The two key ideas behind Perm are representing data and itsprovenance together in a single relation and relying on query rewrites to generate thisrepresentation. Through this; Perm supports fully integrated; on-demand provenancegeneration and querying using SQL. Since Perm rewrites a query requesting provenanceinto a regular SQL query and generates easily optimizable SQL code; its performancegreatly benefits from the query optimization techniques provided by the underlying DBMS.,*,2013,30
The perm provenance management system in action,Boris Glavic; Gustavo Alonso,Abstract In this demonstration we present the Perm provenance management system (PMS).Perm is capable of computing; storing and querying provenance information for therelational data model. Provenance is computed by using query rewriting techniques toannotate tuples with provenance information. Thus; provenance data and provenancecomputations are represented as relational data and queries and; hence; can be queried;stored and optimized using standard relational database techniques. This demo shows thecomplete Perm system and lets attendants examine in detail the process of query rewritingand provenance retrieval in Perm; the most complete data provenance system availabletoday. For example; Perm supports lazy and eager provenance computation; externalprovenance and various contribution semantics.,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,30
TRAMP: Understanding the behavior of schema mappings through provenance,Boris Glavic; Gustavo Alonso; Renée J Miller; Laura M Haas,Abstract Though partially automated; developing schema mappings remains a complex andpotentially error-prone task. In this paper; we present TRAMP (TRAnsformation MappingProvenance); an extensive suite of tools supporting the debugging and tracing of schemamappings and transformation queries. TRAMP combines and extends data provenance withtwo novel notions; transformation provenance and mapping provenance; to explain therelationship between transformed data and those transformations and mappings thatproduced that data. In addition we provide query support for transformations; data; and allforms of provenance. We formally define transformation and mapping provenance; presentan efficient implementation of both forms of provenance; and evaluate the resulting systemthrough extensive experiments.,Proceedings of the VLDB Endowment,2010,27
Provenance for nested subqueries,Boris Glavic; Gustavo Alonso,Abstract Data provenance is essential in applications such as scientific computing; curateddatabases; and data warehouses. Several systems have been developed that provideprovenance functionality for the relational data model. These systems support only a subsetof SQL; a severe limitation in practice since most of the application domains that benefit fromprovenance information use complex queries. Such queries typically involve nestedsubqueries; aggregation and/or user defined functions. Without support for these constructs;a provenance management system is of limited use. In this paper we address this limitationby exploring the problem of provenance derivation when complex queries are involved.More precisely; we demonstrate that the widely used definition of Why-provenance fails inthe presence of nested subqueries; and show how the definition can be modified to …,Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology,2009,25
Clustering multidimensional sequences in spatial and temporal databases,Ira Assent; Ralph Krieger; Boris Glavic; Thomas Seidl,Abstract Many environmental; scientific; technical or medical database applications requireeffective and efficient mining of time series; sequences or trajectories of measurementstaken at different time points and positions forming large temporal or spatial databases.Particularly the analysis of concurrent and multidimensional sequences poses newchallenges in finding clusters of arbitrary length and varying number of attributes. Wepresent a novel algorithm capable of finding parallel clusters in different subspaces anddemonstrate our results for temporal and spatial applications. Our analysis of structuralquality parameters in rivers is successfully used by hydrologists to develop measures forriver quality improvements.,Knowledge and Information Systems,2008,20
Perm: Efficient provenance support for relational databases,Boris Glavic,Abstract In many application areas like scientific computing; data-warehousing; and dataintegration detailed information about the origin of data is required. This kind of informationis often referred to as data provenance. The provenance of a piece of data; a so-called dataitem; includes information about the source data from which it is derived and thetransformations that lead to its creation and current representation. In the context ofrelational databases; provenance has been studied both from a theoretical and algorithmicperspective. Yet; in spite of the advances made; there are very few practical systemsavailable that support generating; querying and storing provenance information (We refer tosuch systems as provenance management systems or PMS). These systems support only asubset of SQL; a severe limitation in practice since most of the application domains that …,*,2010,19
Ariadne: Managing fine-grained provenance on data streams,Boris Glavic; Kyumars Sheykh Esmaili; Peter Michael Fischer; Nesime Tatbul,Abstract Managing fine-grained provenance is a critical requirement for data streammanagement systems (DSMS); not only to address complex applications that requirediagnostic capabilities and assurance; but also for providing advanced functionality such asrevision processing or query debugging. This paper introduces a novel approach that usesoperator instrumentation; ie; modifying the behavior of operators; to generate and propagatefine-grained provenance through several operators of a query network. In addition toapplying this technique to compute provenance eagerly during query execution; we alsostudy how to decouple provenance computation from query processing to reduce run-timeoverhead and avoid unnecessary provenance retrieval. This includes computing a concisesuperset of the provenance to allow lazily replaying a query network and reconstruct its …,Proceedings of the 7th ACM international conference on Distributed event-based systems,2013,17
The iBench integration metadata generator,Patricia C Arocena; Boris Glavic; Radu Ciucanu; Renée J Miller,Abstract Given the maturity of the data integration field it is surprising that rigorous empiricalevaluations of research ideas are so scarce. We identify a major roadblock for empiricalwork-the lack of comprehensive metadata generators that can be used to createbenchmarks for different integration tasks. This makes it difficult to compare integrationsolutions; understand their generality; and understand their performance. We presentiBench; the first metadata generator that can be used to evaluate a wide-range of integrationtasks (data exchange; mapping creation; mapping composition; schema evolution; amongmany others). iBench permits control over the size and characteristics of the metadata itgenerates (schemas; constraints; and mappings). Our evaluation demonstrates that iBenchcan efficiently generate very large; complex; yet realistic scenarios with different …,Proceedings of the VLDB Endowment,2015,15
The Case for Fine-Grained Stream Provenance.,Boris Glavic; Kyumars Sheykh Esmaili; Peter M Fischer; Nesime Tatbul,Abstract: The current state of the art for provenance in data stream management systems(DSMS) is to provide provenance at a high level of abstraction (such as; from which sensorsin a sensor network an aggregated value is derived from). This limitation was imposed byhigh-throughput requirements and an anticipated lack of application demand for moredetailed provenance information. In this work; we first demonstrate by means of well-chosenuse cases that this is a misconception; ie; coarse-grained provenance is in fact insufficientfor many application domains. We then analyze the requirements and challenges involvedin integrating support for fine-grained provenance into a streaming system and outline ascalable solution for supporting tuple-level provenance in DSMS.,BTW Workshops,2011,15
Provenance for Data Mining,Boris Glavic; Javed Siddique; Periklis Andritsos; Renée J Miller,Abstract Data mining aims at extracting useful information from large datasets. Most datamining approaches reduce the input data to produce a smaller output summarizing themining result. While the purpose of data mining (extracting information) necessitates thisreduction in size; the loss of information it entails can be problematic. Specifically; the resultsof data mining may be more confusing than insightful; if the user is not able to understand onwhich input data they are based and how they were created. In this paper; we argue that theuser needs access to the provenance of mining results. Provenance; while extensivelystudied by the database; workflow; and distributed systems communities; has not yet beenconsidered for data mining. We analyze the differences between database; workflow; anddata mining provenance; suggest new types of provenance; and identify new usecases …,Presented as part of the 5th USENIX Workshop on the Theory and Practice of Provenance,2013,11
Reexamining Some Holy Grails of Data Provenance.,Boris Glavic; Renée J Miller,Abstract We reconsider some of the explicit and implicit properties that underlie well-established definitions of data provenance semantics. Previous work on comparingprovenance semantics has mostly focused on expressive power (does the provenancegenerated by a certain semantics subsume the provenance generated by other semantics)and on understanding whether a semantics is insensitive to query rewrite (ie; do equivalentqueries have the same provenance). In contrast; we try to investigate why certain semanticspossess specific properties (like insensitivity) and whether these properties are alwaysdesirable. We present a new property stability with respect to query language extension that;to the best of our knowledge; has not been isolated and studied on its own.,TaPP,2011,11
Value invention in data exchange,Patricia C Arocena; Boris Glavic; Renée J Miller,Abstract The creation of values to represent incomplete information; often referred to asvalue invention; is central in data exchange. Within schema mappings; Skolem functionshave long been used for value invention as they permit a precise representation of missinginformation. Recent work on a powerful mapping language called second-order tuplegenerating dependencies (SO tgds); has drawn attention to the fact that the use of arbitrarySkolem functions can have negative computational and programmatic properties in dataexchange. In this paper; we present two techniques for understanding when the Skolemfunctions needed to represent the correct semantics of incomplete information arecomputationally well-behaved. Specifically; we consider when the Skolem functions insecond-order (SO) mappings have a first-order (FO) semantics and are therefore …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,10
Messing up with BART: error generation for evaluating data-cleaning algorithms,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti; Donatello Santoro,Abstract We study the problem of introducing errors into clean databases for the purpose ofbenchmarking data-cleaning algorithms. Our goal is to provide users with the highestpossible level of control over the error-generation process; and at the same time developsolutions that scale to large databases. We show in the paper that the error-generationproblem is surprisingly challenging; and in fact; NP-complete. To provide a scalablesolution; we develop a correct and efficient greedy algorithm that sacrifices completeness;but succeeds under very reasonable assumptions. To scale to millions of tuples; thealgorithm relies on several non-trivial optimizations; including a new symmetry property ofdata quality constraints. The trade-off between control and scalability is the main technicalcontribution of the paper.,Proceedings of the VLDB Endowment,2015,9
Towards constraint-based explanations for answers and non-answers,Boris Glavic; Sven Köhler; Sean Riddle; Bertram Ludäscher,Abstract Explaining why an answer is present (traditional provenance) or absent (why-notprovenance) from a query result is important for many use cases. Most existing approachesfor positive queries use the existence (or absence) of input data to explain a (missing)answer. However; for realistically-sized databases; these explanations can be very largeand; thus; may not be very helpful to a user. In this paper; we argue that logical constraintsas a concise description of large (or even infinite) sets of existing or missing inputs canprovide a natural way of answering a why-or why-not provenance question. For instance;consider a query that returns the names of all cities which can be reached with at most onetransfer via train from Lyon in France. The provenance of a city in the result of this query; sayDijon; will contain a large number of train connections between Lyon and Dijon which …,Proceedings of the 7th USENIX Workshop on the Theory and Practice of Provenance (TaPP),2015,9
Reenactment for Read-Committed Snapshot Isolation,Boris Glavic Bahareh Sadat Arab; Dieter Gawlick; Vasudha Krishnaswamy; Venkatesh Radhakrishnan,*,CIKM,2016,8
LDV: light-weight database virtualization,Quan Pham; Tanu Malik; Boris Glavic; Ian Foster,We present a light-weight database virtualization (LDV) system that allows users to shareand re-execute applications that operate on a relational database (DB). Previous methodsfor sharing DB applications; such as companion websites and virtual machine images(VMIs); support neither easy and efficient re-execution nor the sharing of only a relevant DBsubset. LDV addresses these issues by monitoring application execution; including DBoperations; and using the resulting execution trace to create a lightweight re-executablepackage. A LDV package includes; in addition to the application; either the DB managementsystem (DBMS) and relevant data or; if the DBMS and/or data cannot be shared; just theapplication-DBMS communications for replay during re-execution. We introduce a linked DB-operating system provenance model and show how to infer data dependencies based on …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,8
Efficient stream provenance via operator instrumentation,Boris Glavic; Kyumars Sheykh Esmaili; Peter M Fischer; Nesime Tatbul,Abstract Managing fine-grained provenance is a critical requirement for data streammanagement systems (DSMS); not only for addressing complex applications that requirediagnostic capabilities and assurance; but also for providing advanced functionality; such asrevision processing or query debugging. This article introduces a novel approach that usesoperator instrumentation; that is; modifying the behavior of operators; to generate andpropagate fine-grained provenance through several operators of a query network. Inaddition to applying this technique to compute provenance eagerly during query execution;we also study how to decouple provenance computation from query processing to reduceruntime overhead and avoid unnecessary provenance retrieval. Our proposals includecomputing a concise superset of the provenance (to allow lazily replaying a query and …,ACM Transactions on Internet Technology (TOIT),2014,8
Interoperability for Provenance-aware Databases using PROV and JSON,Xing Niu; Raghav Kapoor; Boris Glavic; Dieter Gawlick; Zhen Hua Liu; Vasudha Krishnaswamy; Venkatesh Radhakrishnan,Abstract Since its inception; the PROV standard has been widely adopted as a standardizedexchange format for provenance information. Surprisingly; this standard is currently notsupported by provenanceaware database systems limiting their interoperability with otherprovenance-aware systems. In this work we introduce techniques for exporting databaseprovenance as PROV documents; importing PROV graphs alongside data; and linkingoutputs of an SQL operation to the imported provenance for its inputs. Our implementation inthe GProM system offloads generation of PROV documents to the backend database. Thisimplementation enables provenance tracking for applications that use a relational databasefor managing (part of) their data; but also execute some non-database operations.,Proceedings of the 7th USENIX Conference on Theory and Practice of Provenance; TaPP’15,2015,7
Adaptive schema databases,William Spoth; Bahareh Sadat Arab; Eric S Chan; Dieter Gawlick; Adel Ghoneimy; Boris Glavic; B Hammerschmidt; Oliver Kennedy; Seokki Lee; Zhen Hua Liu; Xing Niu; Ying Yang,ABSTRACT The rigid schemas of classical relational databases help users in specifyingqueries and inform the storage organization of data. However; the advantages of schemascome at a high upfront cost through schema and ETL process design. In this work; wepropose a new paradigm where the database system takes a more active role in schemadevelopment and data integration. We refer to this approach as adaptive schema databases(ASDs). An ASD ingests semi-structured or unstructured data directly using a pluggablecombination of extraction and data integration techniques. Over time it discovers and adaptsschemas for the ingested data using information provided by data integration andinformation extraction techniques; as well as from queries and user-feedback. In contrast torelational databases; ASDs maintain multiple schema workspaces that represent …,CIDR 2017; 8th Biennial Conference on Innovative Data Systems Research; Chaminade; CA; USA; January 8-11; 2017; Online Proceedings,2017,6
Spatial multidimensional sequence clustering,Ira Assent; Ralph Krieger; Boris Glavic; Thomas Seidl,Measurements at different time points and positions in large temporal or spatial databasesrequires effective and efficient data mining techniques. For several parallel measurements;finding clusters of arbitrary length and number of attributes; poses additional challenges. Wepresent a novel algorithm capable of finding parallel clusters in different structural qualityparameter values for river sequences used by hydrologists to develop measures for riverquality improvements,Data Mining Workshops; 2006. ICDM Workshops 2006. Sixth IEEE International Conference on,2006,6
Formal foundations of reenactment and transaction provenance,Bahareh Arab; Dieter Gawlick; Vasudha Krishnaswamy; Venkatesh Radhakrishnan; Boris Glavic,Abstract Provenance is essential for auditing; data debugging; understandingtransformations; and many additional use cases. All these use cases would benefit fromprovenance for transactional updates. We present a provenance model for snapshotisolation transactions extending the semiring framework with version annotations andupdates. Based on this model; we present the first solution for computing the provenance oftransactions. Our approach retroactively traces provenance using an audit log and timetravel functionality (supported by many DBMS) without having to store any additionalinformation. For a given transaction; we construct a reenactment query that simulates theeffect of the transaction. This query returns the updated versions of relations produced by thetransaction and has the same provenance as the transaction. Interestingly; such …,*,2016,5
Reenacting transactions to compute their provenance,Bahareh Arab; Dieter Gawlick; Vasudha Krishnaswamy; Venkatesh Radhakrishnan; Boris Glavic,ABSTRACT Database provenance is essential for auditing; data debugging; understandingtransformations; and many additional use cases. While these applications do benefit fromstate-ofthe-art provenance tracking for queries; most use cases also require provenance fortransactional updates. We present the first provenance model for concurrent databasetransactions. Our model extends the well-known semiring provenance framework withversion annotations and update operations. Based on this model; we present the firstsolution for computing the provenance of database transactions. Our approach canretroactively trace transaction provenance as long as an audit log and time travelfunctionality are available (both are supported by most DBMS) and without storing anyadditional information. For a given transaction; our approach constructs a reenactment …,Illinois Institute of Technology; Tech. Rep,2014,5
Provenance-aware query optimization,Xing Niu; Raghav Kapoor; Boris Glavic; Dieter Gawlick; Zhen Hua Liu; Vasudha Krishnaswamy; Venkatesh Radhakrishnan,Data provenance is essential for debugging query results; auditing data in cloudenvironments; and explaining outputs of Big Data analytics. A well-established technique isto represent provenance as annotations on data and to instrument queries to propagatethese annotations to produce results annotated with provenance. However; evensophisticated optimizers are often incapable of producing efficient execution plans forinstrumented queries; because of their inherent complexity and unusual structure. Thus;while instrumentation enables provenance support for databases without requiring anymodification to the DBMS; the performance of this approach is far from optimal. In this work;we develop provenancespecific optimizations to address this problem. Specifically; weintroduce algebraic equivalences targeted at instrumented queries and discuss …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,4
Mimir: Bringing ctables into practice,Arindam Nandi; Ying Yang; Oliver Kennedy; Boris Glavic; Ronny Fehling; Zhen Hua Liu; Dieter Gawlick,Abstract: The present state of the art in analytics requires high upfront investment of humaneffort and computational resources to curate datasets; even before the first query is posed.So-called pay-as-you-go data curation techniques allow these high costs to be spread out;first by enabling queries over uncertain and incomplete data; and then by assessing thequality of the query results. We describe the design of a system; called Mimir; around arecently introduced class of probabilistic pay-as-you-go data cleaning operators calledLenses. Mimir wraps around any deterministic database engine using JDBC; extending itwith support for probabilistic query processing. Queries processed through Mimir produceuncertainty-annotated result cursors that allow client applications to quickly assess resultquality and provenance. We also present a GUI that provides analysts with an interactive …,arXiv preprint arXiv:1601.00073,2016,4
Debugging data exchange with vagabond,Boris Glavic; Jiang Du; Renée J Miller; Gustavo Alonso; Laura M Haas,In this paper; we present Vagabond; a system that uses a novel holistic approach to helpusers to understand and debug data exchange scenarios. Developing such a scenario is acomplex and labor-intensive process where errors are often only revealed in the targetinstance produced as the result of this process. This makes it very hard to debug suchscenarios; especially for non-power users. Vagabond aides a user in debugging byautomatically generating possible explanations for target instance errors identified by theuser. Schema mappings are declarative constraints that model the relationship between asource and a target schema. Data exchange systems; such as Clio [6]; ORCHESTRA [5];and many others; use schema mappings to produce an instance of the target schema basedon an instance of the source schema. Creating a mapping between two schemata is a …,Proceedings of the VLDB Endowment,2011,4
A SQL-Middleware Unifying Why and Why-Not Provenance for First-Order Queries,Seokki Lee; Sven Köhler; Bertram Ludäscher; Boris Glavic,Explaining why an answer is in the result of a query or why it is missing from the result isimportant for many applications including auditing; debugging data and queries; andanswering hypothetical questions about data. Both types of questions; ie; why and why-notprovenance; have been studied extensively. In this work; we present the first practicalapproach for answering such questions for queries with negation (firstorder queries). Ourapproach is based on a rewriting of Datalog rules (called firing rules) that capturessuccessful rule derivations within the context of a Datalog query. We extend this rewriting tosupport negation and to capture failed derivations that explain missing answers. Given a(why or why-not) provenance question; we compute an explanation; ie; the part of theprovenance that is relevant to answer the question. We introduce optimizations that prune …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,3
Implementing Unified Why-and Why-Not Provenance Through Games,Seokki Lee; Sven Köhler; Bertram Ludäscher; Boris Glavic,Abstract Using provenance to explain why a query returns a result or why a result is missinghas been studied extensively. However; the two types of questions have been approachedindependently of each other. We present an efficient technique for answering both types ofquestions for Datalog queries based on a game-theoretic model of provenance calledprovenance games. Our approach compiles provenance requests into Datalog andtranslates the resulting query into SQL to execute it on a relational database backend. Weapply several novel optimizations to limit the computation to provenance relevant to a givenuser question.,International Provenance and Annotation Workshop,2016,3
Benchmarking Data Curation Systems.,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti; Donatello Santoro,Abstract Data curation includes the many tasks needed to ensure data maintains its valueover time. Given the maturity of many data curation tasks; including data transformation anddata cleaning; it is surprising that rigorous empirical evaluations of research ideas are soscarce. In this work; we argue that thorough evaluation of data curation systems imposesseveral major obstacles that need to be overcome. First; we consider the outputs generatedby a data curation system (for example; an integrated or cleaned database or a set ofconstraints produced by a schema discovery system). To compare the results of differentsystems; measures of output quality should be agreed upon by the community and; sincesuch measures can be quite complex; publicly available implementations of these measuresshould be developed; shared; and optimized. Second; we consider the inputs to the data …,IEEE Data Eng. Bull.,2016,3
Gain control over your integration evaluations,Patricia C Arocena; Radu Ciucanu; Boris Glavic; Renée J Miller,Abstract Integration systems are typically evaluated using a few real-world scenarios (eg;bibliographical or biological datasets) or using synthetic scenarios (eg; based on star-schemas or other patterns for schemas and constraints). Reusing such evaluations is acumbersome task because their focus is usually limited to showcasing a specific feature ofan approach. This makes it difficult to compare integration solutions; understand theirgenerality; and understand their performance for different application scenarios. Based onthis observation; we demonstrate some of the requirements for developing integrationbenchmarks. We argue that the major abstractions used for integration problems haveconverged in the last decade which enables the application of robust empirical methods tointegration problems (from schema evolution; to data exchange; to answering queries …,Proceedings of the VLDB Endowment,2015,3
A Primer on Database Provenance,Boris Glavic,LIMITED DISTRIBUTION NOTICE: The research presented in this report may be submittedas a whole or in parts for publication and will probably be copyrighted if accepted forpublication. It has been issued as a Technical Report for early dissemination of its contents.In view of the transfer of copyright to the outside publisher; its distribution outside of IIT-DBprior to publication should be limited to peer communications and specific requests. Afteroutside publication; requests should be filled only by reprints or legally obtained copies ofthe article (eg payment of royalties).,*,2014,3
Declarative serializable snapshot isolation,Christian Tilgner; Boris Glavic; Michael Böhlen; Carl-Christian Kanne,Abstract Snapshot isolation (SI) is a popular concurrency control protocol; but it permits non-serializable schedules that violate database integrity. The Serializable Snapshot Isolation(SSI) protocol ensures (view) serializability by preventing pivot structures in SI schedules. Inthis paper; we leverage the SSI approach and develop the Declarative SerializableSnapshot Isolation (DSSI) protocol; an SI protocol that guarantees serializable schedules.Our approach requires no analysis of application programs or changes to the underlyingDBMS. We present an implementation and prove that it ensures serializability.,East European Conference on Advances in Databases and Information Systems,2011,3
Data mining zur entscheidungsunterstützung in der hydrologie,Thomas Seidl; Ralph Krieger; Ira Assent; Boris Glavic; Heribert Nacken; Sabine Bartusseck; Hani Sewilam,Zusammenfassung Zur Umsetzung der europäischen Wasserrahmenrichtlinie (EG-WRRL)(Rat der Europäischen Union 2000) in NRW sollen Maßnahmenpläne zurVerbesserung des ökologischen Zustandes von Fließgewässern erarbeitet werden. Dazuwurde die Strukturgüte von ca. 1300 Gewässern kartiert und die Ergebnisse alsStrukturgüteklassen bezüglich verschiedener Kriterien mit einer Granularität von 100-Meter-Abschnitten erfasst. Mittels Data-Mining-Methoden wird die systematische Analyse derDaten sowie die Erstellung von Maßnahmenplänen unterstützt. GeeigneteVisualisierungstechniken bereiten die Ergebnisse auf und dienen der Vorbereitung einesDecision Support Systems.,Lehr-und Forschungsgebiet Ingenieurhydrologie der RWTH Aachen; Veranstaltung: Tag der Hydrologie,2005,3
DeepSea: Progressive Workload-Aware Partitioning of Materialized Views in Scalable Data Analytics.,Jiang Du; Renée J Miller; Boris Glavic; Wei Tan,ABSTRACT Selective materialization of intermediate query results as views is an effectivemethod for improving query performance. In this paper; we extend this technique toadaptively partition views based on the access patterns of a workload. That is; we collectinformation about the selection conditions of queries at runtime and utilize this information todetermine fragment boundaries for the initial partitioning when materializing a view.Furthermore; we refine view partitions over time based on the selection conditions ofincoming queries. We present a novel cost-benefit model for partitioned views; as well as acandidate view and fragment selection approach-both of which exploit the nature ofpartitioned views by taking the correlation among view fragments into account. Furthermore;we present DeepSea; an implementation of these techniques built on top of Hive. Our …,EDBT,2017,2
Provenance-aware versioned dataworkspaces,Xing Niu; B Arab; Dieter Gawlick; Zhen Hua Liu; Vasudha Krishnaswamy; Oliver Kennedy; Boris Glavic,Abstract Data preparation; curation; and analysis tasks are often exploratory in nature; withanalysts incrementally designing workflows that transform; validate; and visualize their inputsources. This requires frequent adjustments to data and workflows. Unfortunately; in currentdata management systems; even small changes can require time-and resource-heavyoperations like materialization; manual version management; and re-execution. This addedoverhead discourages exploration. We present Provenance-aware VersionedDataworkspaces (PVDs); our vision of a sandboxed environment in which users can apply—and more importantly; easily undo—changes to their data and workflows. A PVD keeps a logof the user's operations in a light-weight version graph structure. We describe a model forPVDs that admits efficient automatic refresh; merging of histories; reenactment; and …,Proceedings of the 8th USENIX Conference on Theory and Practice of Provenance,2016,2
The exception that improves the rule,Juliana Freire; Boris Glavic; Oliver Kennedy; Heiko Mueller,Abstract The database community has developed numerous tools and techniques for datacuration and exploration; from declarative languages; to specialized techniques for datarepair; and more. Yet; there is currently no consensus on how to best expose these powerfultools to an analyst in a simple; intuitive; and above all; flexible way. Thus; analysts continueto rely on tools such as spreadsheets; imperative languages; and notebook styleprogramming environments like Jupyter for data curation. In this work; we explore theintegration of spreadsheets; notebooks; and relational databases. We focus on a keyadvantage that both spreadsheets and imperative notebook environments have overclassical relational databases: ease of exception. By relying on set-at-a-time operations;relational databases sacrifice the ability to easily define singleton operations; exceptions …,Proceedings of the Workshop on Human-In-the-Loop Data Analytics; HILDA@SIGMOD                2016; San Francisco; CA; USA; June 26 - July 01; 2016,2016,2
Provenance and Annotation of Data and Processes,Marta Mattoso; Boris Glavic,This volume contains the proceedings of the 6th International Provenance and AnnotationWorkshop (IPAW); held June 7–8; 2016; at The MITRE Corporation in McLean; Virginia;USA. Following the successful inception of ProvenanceWeek in 2014; this year's installmentagain co-located the biennial IPAW workshop and the annual Workshop on the Theory andPractice of Provenance (TaPP). Together the two leading provenance workshops anchoredProvenanceWeek 2016; a full week of provenancerelated activities that included a sharedposter and demonstration session; and the PROV: Three Years Later and Provenance-based Security and Transparent Computing workshops. This year's installment of IPAW wasable to honor the extraordinary achievements of IPAW's authors through a best paper awardsponsored by Springer. We would like to use this forum to again congratulate Wellington …,6th International Provenance and Annotation Workshop (IPAW),2016,2
Sharing and reproducing database applications,Quan Pham; Severin Thaler; Tanu Malik; Ian Foster; Boris Glavic,Abstract Sharing and repeating scientific applications is crucial for verifying claims;reproducing experimental results (eg; to repeat a computational experiment described in apublication); and promoting reuse of complex applications. The predominant methods ofsharing and making applications repeatable are building a companion web site and/orprovisioning a virtual machine image (VMI). Recently; application virtualization (AV); hasemerged as a light-weight alternative for sharing and efficient repeatability. AV approachessuch as Linux Containers create a chroot-like environment [4]; while approaches such asCDE [1] trace system calls during application execution to copy all binaries; data; andsoftware dependencies into a self-contained package.,Proceedings of the VLDB Endowment,2015,2
iBench First Cut,Patricia C Arocena; Mariana D’Angelo; Boris Glavic; Renée J Miller,*,*,2015,2
Smile: enabling easy and fast development of domain-specific scheduling protocols,Christian Tilgner; Boris Glavic; Michael Böhlen; Carl-Christian Kanne,Abstract Modern server systems schedule large amounts of concurrent requests constrainedby; eg; correctness criteria and service-level agreements. Since standard databasemanagement systems provide only limited consistency levels; the state of the art is todevelop schedulers imperatively which is time-consuming and error-prone. In this poster; wepresent Smile (declarative Scheduling MIddLEware); a tool for developing domain-specificscheduling protocols declaratively. Smile decreases the effort to implement and adapt suchprotocols because it abstracts from low level scheduling details allowing developers to focuson the protocol implementation. We demonstrate the advantages of our approach byimplementing a domain-specific use case protocol.,British National Conference on Databases,2011,2
Correctness proof of the declarative SS2PL protocol implementation,Christian Tilgner; Boris Glavic; Michael Boehlen; Carl-Christian Kanne,In [1]; we proposed a novel approach for scheduling. The underlying idea of our declarativescheduling approach is to (1) treat sets of requests as data collections and to (2) employdatabase query processing techniques over this request data to produce high-qualityschedules in an efficient and flexible manner. Scheduling protocols are implemented asqueries that select those requests from the request data collections whose execution doesnot violate the scheduling constraints (eg correctness criteria and service-level agreements).We denote such queries implementing scheduling protocols declaratively as declarativeprotocol implementations (DPI). DPIs are much more concise; easier to understand andeasier to modify than an imperative scheduler implementation. Our approach allows for aneasy definition of scheduling protocols of different categories such as traditional …,*,2010,2
Carving database storage to detect and trace security breaches,James Wagner; Alexander Rasin; Boris Glavic; Karen Heart; Jacob Furst; Lucas Bressan; Jonathan Grier,Abstract Database Management Systems (DBMS) are routinely used to store and processsensitive enterprise data. However; it is not possible to secure data by relying on the accesscontrol and security mechanisms (eg; audit logs) of such systems alone–users may abusetheir privileges (no matter whether granted or gained illegally) or circumvent securitymechanisms to maliciously alter and access data. Thus; in addition to taking preventivemeasures; the major goal of database security is to 1) detect breaches and 2) to gatherevidence about attacks for devising counter measures. We present an approach thatevaluates the integrity of a live database; identifying and reporting evidence for logtampering. Our approach is based on forensic analysis of database storage and detection ofinconsistencies between database logs and physical storage state (disk and RAM). We …,Digital Investigation,2017,1
Debugging transactions and tracking their provenance with reenactment,Xing Niu; Bahareh Sadat Arab; Seokki Lee; Su Feng; Xun Zou; Dieter Gawlick; Vasudha Krishnaswamy; Zhen Hua Liu; Boris Glavic,Abstract Debugging transactions and understanding their execution are of immenseimportance for developing OLAP applications; to trace causes of errors in productionsystems; and to audit the operations of a database. However; debugging transactions ishard for several reasons: 1) after the execution of a transaction; its input is no longeravailable for debugging; 2) internal states of a transaction are typically not accessible; and3) the execution of a transaction may be affected by concurrently running transactions. Wepresent a debugger for transactions that enables non-invasive; postmortem debugging oftransactions with provenance tracking and supports what-if scenarios (changes totransaction code or data). Using reenactment; a declarative replay technique we havedeveloped; a transaction is replayed over the state of the DB seen by its original …,Proceedings of the VLDB Endowment,2017,1
Efficiently computing provenance graphs for queries with negation,Seokki Lee; Sven Koehler; Bertram Ludäscher; Boris Glavic,Abstract: Explaining why an answer is in the result of a query or why it is missing from theresult is important for many applications including auditing; debugging data and queries;and answering hypothetical questions about data. Both types of questions; ie; why and why-not provenance; have been studied extensively. In this work; we present the first practicalapproach for answering such questions for queries with negation (first-order queries). Ourapproach is based on a rewriting of Datalog rules (called firing rules) that capturessuccessful rule derivations within the context of a Datalog query. We extend this rewriting tosupport negation and to capture failed derivations that explain missing answers. Given a(why or why-not) provenance question; we compute an explanation; ie; the part of theprovenance that is relevant to answer the question. We introduce optimizations that prune …,arXiv preprint arXiv:1701.05699,2017,1
Answering Historical What-if Queries with Provenance; Reenactment; and Symbolic Execution,Bahareh Sadat Arab; Boris Glavic,Abstract What-if queries predict how the results of an analysis would change based onhypothetical changes to a database. While a what-if query determines the effect of ahypothetical change on a query's result; it is often unclear how such a change could havebeen achieved limiting the practical applicability of such queries. We propose an alternativemodel for what-if queries where the user proposes a hypothetical change to past updateoperations. Answering such a query amounts to determining the effect of a hypotheticalchange to past operations on the current database state (or a query's result). We argue thatsuch historical what-if queries are often easier to formulate for a user and lead to moreactionable insights. In this paper; we take a first stab at answering historical what-if queries.We use reenactment; a declarative replay technique for transactional histories; to …,9th USENIX Workshop on the Theory and Practice of Provenance (TaPP 17),2017,1
BARt in Action: Error Generation and Empirical Evaluations of Data-Cleaning Systems,Donatello Santoro; Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti,Abstract Repairing erroneous or conflicting data that violate a set of constraints is animportant problem in data management. Many automatic or semi-automatic data-repairingalgorithms have been proposed in the last few years; each with its own strengths andweaknesses. Bart is an open-source error-generation system conceived to support thoroughexperimental evaluations of these data-repairing systems. The demo is centered aroundthree main lessons. To start; we discuss how generating errors in data is a complex problem;with several facets. We introduce the important notions of detectability and repairability of anerror; that stand at the core of Bart. Then; we show how; by changing the features of errors; itis possible to influence quite significantly the performance of the tools. Finally; we concretelyput to work five data-repairing algorithms on dirty data of various kinds generated using …,Proceedings of the 2016 International Conference on Management of Data,2016,1
HRDBMS: Combining the Best of Modern and Traditional Relational Databases,Jason Arnold; Boris Glavic; Ioan Raicu,Abstract—HRDBMS is a novel distributed relational database that uses a hybrid modelcombining the best of traditional distributed relational databases and Big Data analyticsplatforms such as Hive. This allows HRDBMS to leverage years worth of research regardingquery optimization; while also taking advantage of the scalability of Big Data platforms. Thesystem uses an execution framework that is tailored for relational processing; thusaddressing some of the performance challenges of running SQL on top of platforms such asMapReduce and Spark. These include excessive materialization of intermediate results; lackof a global costbased optimization; unnecessary sorting; lack of index support; no statistics;no support for DML and ACID; and excessive communication caused by the rigidcommunication patterns enforced by these platforms.,Illinois Institute of Technology; Department of Computer Science; PhD Oral Qualifier,2015,1
Heuristic and cost-based optimization for provenance computation,Xing Niu; Raghav Kapoor; Boris Glavic,Motivation. Data Provenance; information about the origin of data and the transformationsused to produce it; has attracted attention by the database community in the recent yearsand has proven to be essential for a wide range of use cases including debugging of dataand queries; auditing; and establishing authorship. The de-facto standard for databaseprovenance is to model provenance as annotations on data and compute the provenancefor the outputs of an operation by propagating these annotations. Provenance systems usequery rewrite techniques to compile provenance computations into declarative queries (eg;SQL [1]). This approach has the important advantage that standard relational databases canbe used to compute provenance. However; these techniques generate queries with unusualaccess patterns and operator sequences. Even sophisticated databases are not capable …,*,2015,1
Error Generation for Evaluating Data Cleaning Algorithms,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti; Donatello Santoro,ABSTRACT We address the problem of generating errors within clean databases for thepurpose of benchmarking data-cleaning and data-repairing algorithms. Our goal is toprovide users with the highest possible level of control over the error generation processand at the same time develop a solution that scales to large databases. This is challenging;because the error generation problem is NP-complete. The main technical contribution ofthis paper is to develop an efficient PTIME algorithm which sacrifices completeness in anintelligent fashion that allows us to succeed under reasonable assumptions. However;scaling to databases with millions of tuples requires additional non-trivial optimizationsincluding exploiting a symmetry property of data quality constraints.,*,2015,1
Ariadne,Boris Glavic; Kyumars Sheykh Esmaili; Peter M Fischer; Nesime Tatbul,Abstract Managing fine-grained provenance is a critical requirement for data streammanagement systems (DSMS) to be able to address complex applications that requirediagnostic capabilities and assurance as well as serving as a supporting technology forother tasks such as revision processing. In this paper; based on an example use case; wemotivate the need for fine-grained provenance in stream processing and analyze itsrequirements. Inspired by these requirements; we investigate different techniques togenerate and retrieve stream provenance; and propose a new technique that is based onoperator instrumentation. Ariadne; our provenance-aware DSMS implements this techniqueon top of the Borealis system. We propose new optimization techniques to reduce thecomputational overhead of provenance generation and retrieval. Our experiments confirm …,Technical report/Systems Group; Department of Computer Science; ETH Zurich,2012,1
Formal Foundation of Contribution Semantics and Provenance Computation through Query Rewrite in TRAMP,Boris Glavic,In this report we present the theoretical foundation of TRAMP. TRAMP is a schema mappingdebugging system that uses provides provenance and query support as debuggingfunctality for schema mappings scenarios. TRAMP is an extension of Perm; a relationalprovenance management system developed at University of Zurich. In this report we are notfocussing on the debugging functionality added by TRAMP; but instead focus on thetheoretical foundation of the provenance types provided by the system. In chapter 2 wepresent the contribution semantics for data provenance; transformation provenance; andmapping provenance used by TRAMP. Contribution semantics define which parts of theinput (in case of data provenance) and which operators of a transformation (in case oftransformation provenance) belong to the provenance of an output of a transformation …,*,2010,1
sesam: Ensuring Privacy for a Interdisciplinary Longitudinal Study.,Boris Glavic; Klaus R Dittrich,*,GI Jahrestagung (1),2006,1
An Efficient Implementation Of Game Provenance In DBMS,Seokki Lee; Yuchen Tang; Sven Kohler; Bertram Ludäscher; Boris Glavic,Abstract—Explaining why a certain answer is in the result of a query or why it is missing fromthe result is important for many applications including auditing; debugging data; andanswering hypothetical questions about data. Both types of questions; ie; why provenanceand why-not (missing answer) provenance have been studied extensively. Provenancegames; a game-theoretic approach to provenance; can provide a unified view on why andwhy-not provenance. The power of provenance games; however; comes at the cost ofcomplexity: a direct approach for computing game provenance instantiates a large gamegraph with all possible tuples that can be formed from values of the active domain using therules of a query evaluation game. In this work; we present a new relational databaseapproach for computing a much smaller subgraph of the provenance game that is …,*,*,1
Provenance management for frequent itemsets,Javed Siddique; Boris Glavic; Renée J Miller,ABSTRACT Provenance has been studied extensively for relational queries and shown tobe important in revealing the origin and creation process of data that has been produced bypotentially complex relational transformations. Provenance for the results of data miningoperators in contrast has not been considered. We argue that provenance offers the samebenefits for mining as for relational queries; eg; it allows us to track errors caused byincorrect input data. We consider the most common mining operator; frequent itemsetmining; and introduce two types of provenance (why-and i-provenance) for this operator. Weargue that the concept of why-provenance for relational queries can be adapted for frequentitemsets; but that it poses new computational challenges due to the nature of itemset miningand the size of why-provenance. We address these challenges in two ways. First; we …,*,*,1
Using Reenactment to Retroactively Capture Provenance for Transactions,Bahareh Sadat Arab; Dieter Gawlick; Vasudha Krishnaswamy; Venkatesh Radhakrishnan; Boris Glavic,Database provenance explains how results are derived by queries. However; many usecases such as auditing and debugging of transactions require understanding of how thecurrent state of a database was derived by a transactional history. We present MV-semirings;a provenance model for queries and transactional histories that supports two common multi-version concurrency control protocols: snapshot isolation (SI) and read committed snapshotisolation (RC-SI). Furthermore; we introduce an approach for retroactively capturing suchprovenance using reenactment; a novel technique for replaying a transactional history withprovenance capture. Reenactment exploits the time travel and audit logging capabilities ofmodern DBMS to replay parts of a transactional history using queries. Importantly; ourtechnique requires no changes to the transactional workload or underlying DBMS and …,IEEE Transactions on Knowledge and Data Engineering,2018,*
Guest editorial: large-scale data curation and metadata management,Mohamed Eltabakh; Boris Glavic,We are delighted to present this special issue of the distributed and parallel databasesjournal (DPDB) on large-scale data curation and metadata management. Data curation andannotation are becoming essential mechanisms for capturing a wide variety of metadatarelated to data. This metadata may carry different semantics ranging from tracking the data'slineage and provenance; quality information; exchanging knowledge and discussionmessages among scientists; attaching related articles or documents; linking to relevantstatistics about the data; and highlighting erroneous or conflicting values. This metadata maybe represented in many different formats including free-text values; articles or binary files;images; structured information such as provenance; or semi-structured content such asemail messages. The creation and maintenance of annotated databases and metadata …,Distributed and Parallel Databases,2018,*
Integrating Approximate Summarization with Provenance Capture,Seokki Lee; Xing Niu; Bertram Ludaescher; Boris Glavic,Abstract How to use provenance to explain why a query returns a result or why a result ismissing has been studied extensively. Recently; we have demonstrated how to uniformlyanswer these types of provenance questions for first-order queries with negation and havepresented an implementation of this approach in our PUG (Provenance Unification throughGraphs) system. However; for realisticallysized databases; the provenance of answers andmissing answers can be very large; overwhelming the user with too much information andwasting computational resources. In this paper; we introduce an (approximate)summarization technique that generates compact representations of why and why-notprovenance. Our technique uses patterns as a summarized representation of sets ofelements from the provenance; ie; successful or failed derivations. We rank these patterns …,TaPP,2017,*
HRDBMS: A NewSQL Database for Analytics,Jason Arnold; Boris Glavic; Ioan Raicu,HRDBMS is a novel distributed relational database that uses a hybrid model combining thebest of traditional distributed relational databases and Big Data analytics platforms such asHive. This allows HRDBMS to leverage years worth of research regarding queryoptimization; while also taking advantage of the scalability of Big Data platforms. The systemuses an execution framework that is tailored for relational processing; thus addressing someof the performance challenges of running SQL on top of platforms such as MapReduce andSpark. These include excessive materialization of intermediate results; lack of a global cost-based optimization; unnecessary sorting; lack of index support; no statistics; no support forDML and ACID; and excessive communication caused by the rigid communication patternsenforced by these platforms.,Cluster Computing (CLUSTER); 2015 IEEE International Conference on,2015,*
Making Database Applications Shareable,Boris Glavic; Tanu Malik; Quan Pham,Motivation. Sharing and repeating applications that involve interactions with a database iscurrently a painful and cumbersome process. In this poster we discuss how to make sharingsuch applications trivial through the combination of three techniques and systems we haverecently developed: LDV (Light-weight database virtualization)[1] is a tool for sharing andrepeating applications accessing a relational database. LDV monitors the execution of anapplication including its database access and creates a reproducibility package that can beshared and reexecuted on a different machine. In addition to a provenance graph; such apackage contains all dependencies of the application (libraries; binaries; and data files) andthe relevant slice of the database required for reexecution. Running a packaged applicationrequires neither any manual installation nor setting up a database. We use LDV as a …,Proceedings of the 7th USENIX Workshop on the Theory and Practice of Provenance (TaPP)(Poster),2015,*
Subspace Sequence Clustering-Dataming zur Entscheidungsunterstützung in der Hydrologie.,Boris Glavic,Voraussetzungen für die Anwendbarkeit einer Maßnahme sind meist mehrereunterschiedliche Eigenschaften eines Flußabschnitts; die sich typischerweise nicht eins-zu-eins in die Güteklassen der funktionalen Einheiten übertragen lassen. Die dadurchentstehende Unschärfe wird noch verstärkt durch die Tatsache; dass die Kartierung undKlassifikation durch unterschiedliche Personen vorgenommen wurde. Deshalb werden dieMaßnahmenvoraussetzungen als Intervalle von Güteklassen einer oder mehrererfunktionaler Einheiten in Form von Regeln angegeben. Von Seiten der Ingenieurhydrologieist man unter anderem an folgenden Fragestellungen interessiert: Wieviele Einzelabschnittewerden durch eine Regel abgedeckt? Welche längeren Abschnitte sind durch eine Regelabgedeckt? Uber die vorgegebenen Regeln hinaus soll ein an der Struktur der Daten …,BTW Studierenden-Programm,2005,*
Letter from the Special Issue Editors,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renee J Miller; Paolo Papotti; Donatello Santoro,The prevalence of large volumes and varieties of accessible data is profoundly changing theway business; government and individuals approach decision making. Organizational bigdata investment strategies regarding what data to collect; clean; integrate; and analyze aretypically driven by some notion of perceived value. However; the value of the data isinescapably tied to the underlying quality of the data. Although for big data; value and qualitymay be correlated; they are conceptually different. For example; a complete and accurate listof the books read on April 1; 2016 by the special editors of this issue may not have muchvalue to anyone else. Whereas even partially complete and somewhat noisy GPS data frompublic transport vehicles may have a high perceived value for transport engineers and urbanplanners. In spite of significant advances in storage and compute capabilities; the time to …,*,*,*
Workshop Organization,Alberto HF Laender; Juliana Freire; Dan Suciu; Mirella M Moro; Vanessa Braganholo; Clodoveu Davis Jr; Marcos André Gonçalves; Francesco Bonchi; Angela Bonifati; Andrea Calì; Sara Cohen; Isabel Cruz; Wolfgang Gatterbauer; Boris Glavic; Claudio Gutierrez; Solmaz Kolahi; Dongwon Lee; Domenico Lembo; Marta Mattoso; Regina Motz; Frank Neven; Rachel Pottinger; Vibhor Rastogi; Altigran S da Silva; Cristina Sirangelo; Divesh Srivastava; Julia Stoyanovich; David Toman; Alejandro Vaisman; Stijn Vansummeren; Ke Yi; Daniel Oliveira,The Alberto Mendelzon International Workshop on Foundations of Data Management (AMW2012) held in Ouro Preto; Brazil; on June 27-30; 2012; is the sixth workshop of a serieswhich started in 2006; as part on an initiative of the Latin American community ofresearchers in data management to honor the memory of our friend; colleague and mentorAlberto Mendelzon. The AMW series has been a venue for high-quality research onfoundational aspects of data management and it has helped foster and solidify the researchin this area throughout Latin America. This event; as the previous ones; has encouraged theparticipation of Latin American graduate students and includes activities specially designedfor them. In addition; with sponsorship from the VLDB Endowment; travel grants have beenprovided for students to attend the event. The proceedings of the workshop consist of 14 …,*,*,*
Automatic Generation and Ranking of Explanations for Mapping Errors,Seokki Lee; Zhen Wang; Boris Glavic; Renée J Miller,ABSTRACT Data transformation is facilitated by the use of visual and logical specificationsof mappings between schemas. While easy to use; mappings are hard to design. Manytechniques have been proposed to help users understand and refine mappings. However;once an error in transformed data has been identified; these systems at best provide low-level dataflow style tracing or query language facilities to help a mapping developer tracethrough the massive space of possible reasons for the error. In this work; we present anapproach for systematically exploring the space of potential explanations (causes) for errorsin transformed data. Our system leverages data provenance in combination with informationabout the mapping to automatically generate possible explanations. Since the number ofpotential explanations for a set of data errors is exponential in the size of the set it is …,*,*,*
iBench First Cut [Technical Report],Patricia C Arocena; Mariana D’Angelo; Boris Glavic; Renée J Miller,ABSTRACT We present iBench; a metadata generator for creating arbitrarily large andcomplex mappings; schemas and schema constraints. iBench is rooted in the influentialSTBenchmark system; a benchmark designed for evaluating mapping systems. Weextended STBenchmark with several new features; including specialized support forgenerating logical mappings using the languages of source-to-target (st) tuple generatingdependencies (tgds) and plain second-order (SO) tgds. iBench can be used with a datagenerator to efficiently generate realistic data integration scenarios with varying degrees ofsize and complexity.,*,*,*
