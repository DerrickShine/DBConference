Integrating conflicting data: the role of source dependence,Xin Luna Dong; Laure Berti-Equille; Divesh Srivastava,Abstract Many data management applications; such as setting up Web portals; managingenterprise data; managing community data; and sharing scientific data; require integratingdata from multiple sources. Each of these sources provides a set of values and differentsources can often provide conflicting values. To present quality data to users; it is critical thatdata integration systems can resolve conflicts and discover true values. Typically; we expecta true value to be provided by more sources than any particular false one; so we can takethe value provided by the majority of the sources as the truth. Unfortunately; a false valuecan be spread through copying and that makes truth discovery extremely tricky. In this paper;we consider how to find true values from conflicting information when there are a largenumber of sources; among which some may copy from others. We present a novel …,Proceedings of the VLDB Endowment,2009,299
Truth discovery and copying detection in a dynamic world,Xin Luna Dong; Laure Berti-Equille; Divesh Srivastava,Abstract Modern information management applications often require integrating data from avariety of data sources; some of which may copy or buy data from other sources. When thesedata sources model a dynamically changing world (eg; people's contact informationchanges over time; restaurants open and go out of business); sources often provide out-of-date data. Errors can also creep into data when sources are updated often. Given out-of-date and erroneous data provided by different; possibly dependent; sources; it ischallenging for data integration systems to provide the true values. Straightforward ways toresolve such inconsistencies (eg; voting) may lead to noisy results; often with detrimentalconsequences. In this paper; we study the problem of finding true values and determiningthe copying relationship between sources; when the update history of the sources is …,Proceedings of the VLDB Endowment,2009,186
Global detection of complex copying relationships between sources,Xin Luna Dong; Laure Berti-Equille; Yifan Hu; Divesh Srivastava,Abstract Web technologies have enabled data sharing between sources but also simplifiedcopying (and often publishing without proper attribution). The copying relationships can becomplex: some sources copy from multiple sources on different subsets of data; some co-copy from the same source; and some transitively copy from another. Understanding suchcopying relationships is desirable both for business purposes and for improving many keycomponents in data integration; such as resolving conflicts across various sources;reconciling distinct references to the same real-world entity; and efficiently answeringqueries over multiple sources. Recent works have studied how to detect copying between apair of sources; but the techniques can fall short in the presence of complex copyingrelationships. In this paper we describe techniques that discover global copying …,Proceedings of the VLDB Endowment,2010,109
Sailing the information ocean with awareness of currents: Discovery and application of source dependence,Laure Berti-Equille; Anish Das Sarma; Amelie Marian; Divesh Srivastava,Abstract: The Web has enabled the availability of a huge amount of useful information; buthas also eased the ability to spread false information and rumors across multiple sources;making it hard to distinguish between what is true and what is not. Recent examples includethe premature Steve Jobs obituary; the second bankruptcy of United airlines; the creation ofBlack Holes by the operation of the Large Hadron Collider; etc. Since it is important to permitthe expression of dissenting and conflicting opinions; it would be a fallacy to try to ensurethat the Web provides only consistent information. However; to help in separating the wheatfrom the chaff; it is essential to be able to determine dependence between sources. Giventhe huge number of data sources and the vast volume of conflicting data available on theWeb; doing so in a scalable manner is extremely challenging and has not been …,arXiv preprint arXiv:0909.1776,2009,64
A framework for quality evaluation in data integration systems,J Akoka1a; L Berti-Equille; O Boucelma; M Bouzeghoub; I Comyn-Wattiau1ab; M Cosquer; V Goasdoué-Thion; Z Kedad; S Nugier; V Peralta; S Sisaid-Cherfi1a,Abstract: Ensuring and maximizing the quality and integrity of information is a crucialprocess for today enterprise information systems (EIS). It requires a clear understanding ofthe interdependencies between the dimensions characterizing quality of data (QoD); qualityof conceptual data model (QoM) of the database; keystone of the EIS; and quality of datamanagement and integration processes (QoP). The improvement of one quality dimension(such as data accuracy or model expressiveness) may have negative consequences onother quality dimensions (eg; freshness or completeness of data). In this paper we brieflypresent a framework; called QUADRIS; relevant for adopting a quality improvement strategyon one or many dimensions of QoD or QoM with considering the collateral effects on theother interdependent quality dimensions. We also present the scenarios of our ongoing …,9th International Conference on Entreprise Information Systems (ICEIS),2007,55
Don't be SCAREd: use SCalable Automatic REpairing with maximal likelihood and bounded changes,Mohamed Yakout; Laure Berti-Équille; Ahmed K Elmagarmid,Abstract Various computational procedures or constraint-based methods for data repairinghave been proposed over the last decades to identify errors and; when possible; correctthem. However; these approaches have several limitations including the scalability andquality of the values to be used in replacement of the errors. In this paper; we propose a newdata repairing approach that is based on maximizing the likelihood of replacement datagiven the data distribution; which can be modeled using statistical machine learningtechniques. This is a novel approach combining machine learning and likelihood methodsfor cleaning dirty databases by value modification. We develop a quality measure of therepairing updates based on the likelihood benefit and the amount of changes applied to thedatabase. We propose SCARE (SCalable Automatic REpairing); a systematic scalable …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,52
End-to-end quality of service provisioning through an integrated management system for multimedia content delivery,Toufik Ahmed; Abolghasem Hamid Asgari; Ahmed Mehaoua; Eugen Borcoci; Laure Berti-Equille; Kormentzas Georgios,Abstract Next generation networks will be complex; interconnecting different technologies;and architectures (IP; DVB-T/S; UMTS; GSM/GPRS; etc.) across a multitude of software andhardware platforms for offering a large number of value-added services includingmultimedia; audio–visual; etc. One of the major requirements for the successful and widedeployment of such services is the efficient transmission of performance sensitive digitalitems (ie; audio; video; and image) over a broad range of bandwidth-constrainedwired/wireless access/core/distribution networks. This paper presents an approach to themanagement and provisioning of end-to-end Quality of Service (QoS) in next generationnetworks (IP and non-IP networks) that is being developed in the IST ENTHRONE 1 project.The primary goal of this project is to provide a solution for seamless access to multimedia …,Computer Communications,2007,40
Truth discovery algorithms: An experimental evaluation,Dalia Attia Waguih; Laure Berti-Equille,Abstract: A fundamental problem in data fusion is to determine the veracity of multi-sourcedata in order to resolve conflicts. While previous work in truth discovery has proved to beuseful in practice for specific settings; sources' behavior or data set characteristics; there hasbeen limited systematic comparison of the competing methods in terms of efficiency;usability; and repeatability. We remedy this deficit by providing a comprehensive review of12 state-of-the art algorithms for truth discovery. We provide reference implementations andan in-depth evaluation of the methods based on extensive experiments on synthetic and real-world data. We analyze aspects of the problem that have not been explicitly studied before;such as the impact of initialization and parameter setting; convergence; and scalability. Weprovide an experimental framework for extensively comparing the methods in a wide …,arXiv preprint arXiv:1409.6428,2014,35
Discovery of complex glitch patterns: A novel approach to quantitative data cleaning,Laure Berti-Equille; Tamraparni Dasu; Divesh Srivastava,Quantitative Data Cleaning (QDC) is the use of statistical and other analytical techniques todetect; quantify; and correct data quality problems (or glitches). Current QDC approachesfocus on addressing each category of data glitch individually. However; in real-world data;different types of data glitches co-occur in complex patterns. These patterns and interactionsbetween glitches offer valuable clues for developing effective domain-specific quantitativecleaning strategies. In this paper; we address the shortcomings of the extant QDC methodsby proposing a novel framework; the DEC (Detect-Explore-Clean) framework. It is acomprehensive approach for the definition; detection and cleaning of complex; multi-typedata glitches. We exploit the distributions and interactions of different types of glitches todevelop data-driven cleaning strategies that may offer significant advantages over blind …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,35
Phytolith signal of aquatic plants and soils in Chad; Central Africa,Alice Novello; Doris Barboni; Laure Berti-Equille; Jean-Charles Mazur; Pierre Poilecot; Patrick Vignaud,Abstract To identify the phytolith signal of lacustrine environments; which are prone topreserving faunal remains including hominins; we analyzed the phytolith content of 46 grassand sedge species; and of 26 soil and mud samples. The samples were collected in Chad(Central Africa); in the Sudanian and Sahelian phytogeographical zones; near temporaryand permanent water-bodies (including Lake Chad) and in grass-dominated biomes on well-drained soils. Altogether; we observed and counted separately 80 different phytolith types;including 38 grass silica short cells (GSSCs). Phytolith type diversity and relativeabundances were analyzed in the botanical specimens to improve the phytolith taxonomicresolution. For the Poaceae; we used a value-test analysis to identify significant cohorts ofphytoliths to characterize aquatic; mesophytic; and xerophytic species. Our results show …,Review of Palaeobotany and Palynology,2012,34
Data quality awareness: a case study for cost optimal association rule mining,Laure Berti-Equille,Abstract The quality of discovered association rules is commonly evaluated byinterestingness measures (commonly support and confidence) with the purpose of supplyingindicators to the user in the understanding and use of the new discovered knowledge. Low-quality datasets have a very bad impact over the quality of the discovered association rules;and one might legitimately wonder if a so-called “interesting” rule noted LHS→ RHS ismeaningful when 30% of the LHS data are not up-to-date anymore; 20% of the RHS dataare not accurate; and 15% of the LHS data come from a data source that is well-known for itsbad credibility. This paper presents an overview of data quality characterization andmanagement techniques that can be advantageously employed for improving the qualityawareness of the knowledge discovery and data mining processes. We propose to …,Knowledge and information Systems,2007,28
Quality awareness for managing and mining data,Laure Berti-Équille,The maturity of database and web technologies has encouraged users to make data publiclyavailable in large quantities; opening up the possibility of large-scale searches andcomparative analyses over multi-source data. However; such analyses are possible in onlya small number of domains due to the practical difficulties involved in integrating andcomparing data from separately designed databases. Different storage technologies areused and different ways of representing the same data are adopted. To add to the problem;many of the available data sources overlap in terms of their content or purpose; making itdifficult for users: i) to select the most appropriate data sets for a specific analysis ordecisions; ii) to have clear means of distinguishing the various providers of the data sets; iii)to evaluate objectively the quality of the provided data at a given time; and iv) to make …,HDR; Rennes,2007,25
Solomon: Seeking the truth via copying detection,Xin Luna Dong; Laure Berti-Equille; Yifan Hu; Divesh Srivastava,Abstract We live in the Information Era; with access to a huge amount of information from avariety of data sources. However; data sources are of different qualities; often providingconflicting; out-of-date and incomplete data. Data sources can also easily copy; reformat andmodify data from other sources; propagating erroneous data. These issues make theidentification of high quality information and sources non-trivial. We demonstrate theSolomon system; whose core is a module that detects copying between sources. Wedemonstrate that we can effectively detect copying relationship between data sources;leverage the results in truth discovery; and provide a user-friendly interface to facilitate usersin identifying sources that best suit their information needs.,Proceedings of the VLDB Endowment,2010,24
Processing top-k join queries,Minji Wu; Laure Berti-Equille; Amélie Marian; Cecilia M Procopiuc; Divesh Srivastava,Abstract We consider the problem of efficiently finding the top-k answers for join queries overweb-accessible databases. Classical algorithms for finding top-k answers use branch-and-bound techniques to avoid computing scores of all candidates in identifying the top-kanswers. To be able to apply such techniques; it is critical to efficiently compute (lower andupper) bounds and expected scores of candidate answers in an incremental fashion duringthe evaluation. In this paper; we describe novel techniques for these problems. The firstcontribution of this paper is a method to efficiently compute bounds for the score of a queryresult when tuples in tables from the" FROM" clause are discovered incrementally; througheither sorted or random access. Our second contribution is an algorithm that; given a set ofpartially evaluated candidate answers; determines a good order in which to access the …,Proceedings of the VLDB Endowment,2010,23
Data fusion: resolving conflicts from multiple sources,Xin Luna Dong; Laure Berti-Equille; Divesh Srivastava,Abstract Many data management applications; such as setting up Web portals; managingenterprise data; managing community data; and sharing scientific data; require integratingdata from multiple sources. Each of these sources provides a set of values; and differentsources can often provide conflicting values. To present quality data to users; it is critical toresolve conflicts and discover values that reflect the real world; this task is called data fusion.Typically; we expect a true value to be provided by more sources than any particular falseone; so we can take the value provided by the largest number of sources as the truth.Unfortunately; a false value can be spread through copying and that makes truth discoveryextremely tricky. In this chapter; we consider how to find true values from conflictinginformation when there are a large number of sources; among which some may copy from …,*,2013,20
Measuring and modelling data quality for quality-awareness in data mining,Laure Berti-Equille,Summary. This chapter presents an overview of data quality management; data linkage anddata cleaning techniques that can be advantageously employed for improving the qualityawareness of the knowledge discovery process. Based on this database-oriented overviewof data quality management; this chapter also presents a pragmatic step-by-step frameworkfor data quality awareness and enhancement before warehousing and during theknowledge discovery process. Each step may use; combine and exploit the data qualitycharacterization; measurement and management methods; and the related techniquesproposed in the literature.,*,2007,20
Integrating and warehousing liver gene expression data and related biomedical resources in GEDAW,Emilie Guérin; Gwenaëlle Marquet; Anita Burgun; Olivier Loréal; Laure Berti-Equille; Ulf Leser; Fouzia Moussouni,Abstract Researchers at the medical research institute Inserm U522; specialized in the liver;use high throughput technologies to diagnose liver disease states. They seek to identify theset of dysregulated genes in different physiopathological situations; along with the molecularregulation mechanisms involved in the occurrence of these diseases; leading at mid-term tonew diagnostic and therapeutic tools. To be able to resolve such a complex question; onehas to consider both data generated on the genes by in-house transcriptome experimentsand annotations extracted from the many publicly available heterogeneous resources inBiomedicine. This paper presents GEDAW; a gene expression data warehouse that hasbeen developed to assist such discovery processes. The distinctive feature of GEDAW is thatit systematically integrates gene information from a multitude of structured data sources …,International Workshop on Data Integration in the Life Sciences,2005,20
Veracity of data: From truth discovery computation algorithms to models of misinformation dynamics,Laure Berti-Equille; Javier Borge-Holthoefer,Abstract On the Web; a massive amount of user-generated content is available throughvarious channels (eg; texts; tweets; Web tables; databases; multimedia-sharing platforms;etc.). Conflicting information; rumors; erroneous and fake content can be easily spreadacross multiple sources; making it hard to distinguish between what is true and what is not.This book gives an overview of fundamental issues and recent contributions for ascertainingthe veracity of data in the era of Big Data. The text is organized into six chapters; focusing onstructured data extracted from texts. Chapter 1 introduces the problem of ascertaining theveracity of data in a multi-source and evolving context. Issues related to informationextraction are presented in Chapter 2. Current truth discovery computation algorithms arepresented in details in Chapter 3. It is followed by practical techniques for evaluating data …,Synthesis Lectures on Data Management,2015,19
Quality-Aware Integration and Warehousing of Genomic Data.,Laure Berti-Equille; Fouzia Moussouni,*,IQ,2005,19
Quality and recommendation of multi-source data for assisting technological intelligence applications,Laure Berti,Abstract Due to its costly impact; data quality is becoming an emerging domain of research.Motivated by its stakes and issues; especially in the application domain of TechnologicalIntelligence; we propose a generic methodology for modeling and managing data quality inthe context of multiple information sources. Data quality has different categories of qualitycriteria and their evaluations enable the detection of errors and poor quality data. Weintroduce the notion of relative data quality when several data describe the same entity inthe real world but have contradictory values: homologous data. Our approach differs fromthe general approach for resolving extensional inconsistencies in integration ofheterogeneous systems. We cumulatively store homologous data and their quality metadataand we recommend dynamically data with the best quality and data which are the most …,Database and Expert Systems Applications DEXA,1999,18
Unsupervised Quantification of Under- and Over-Segmentation for Object-Based Remote Sensing Image Analysis,Andres Troya-Galvis; Pierre Gancarski; Nicolas Passat; Laure Berti-Equille,Object-based image analysis (OBIA) has been widely adopted as a common paradigm todeal with very high-resolution remote sensing images. Nevertheless; OBIA methods stronglydepend on the results of image segmentation. Many segmentation quality metrics have beenproposed. Supervised metrics give accurate quality estimation but require a ground-truthsegmentation as reference. Unsupervised metrics only make use of intrinsic image andsegment properties; yet most of them strongly depend on the application and do not dealwell with the variability of objects in remote sensing images. Furthermore; the few metricsdeveloped in a remote sensing context mainly focus on global evaluation. In this paper; wepropose a novel unsupervised metric; which evaluates local quality (per segment) byanalyzing segment neighborhood; thus quantifying under-and over-segmentation given a …,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2015,16
Assessment and analysis of information quality: a multidimensional model and case studies,Z Kedad L Berti-Equille; I Comyn-Wattiau; M Cosquer,Information quality is a complex and multidimensional notion. In the context of informationsystem engineering; it is also a transversal notion and to be fully understood; it needs to beevaluated jointly considering the quality of data; the quality of the underlying conceptualdata model and the quality of the software system that manages these data. This paperpresents a multidimensional model for exploring information in a multidimensional way;which aids in the navigation; filtering; and interpretation of quality measures; and thus in theidentification of the most appropriate actions to improve information quality. Two applicationscenarios are presented to illustrate and validate the multidimensional approach: the firstone concerns the quality of customer information at Electricité de France; a French electricitycompany; and the second concerns the quality of patient records at Institut Curie; a well …,International Journal of Information Quality,2011,15
Enriching multimedia content description for broadcast environments: from a unified metadata model to a new generation of authoring tool,Boris Rousseau; Wilfried Jouve; Laure Berti-Equille,In this paper; we propose a novel approach for authoring a diversity of multimedia resources(audio; video; text; images; etc). We introduce a prototype authoring tool (called M-Tool)relying on a metadata model that unifies MPEG-21 and TV-anytime descriptions to edit andenrich audiovisual contents with metadata. Additional innovative functionalities extendingthe M-Tool are also presented. This new generation of metadata authoring tools is designedand currently used for scenarios of TV and news broadcasting; and video on demandbroadcasting in the framework of the IST Integrated European Project ENTHRONE.,Multimedia; Seventh IEEE International Symposium on,2005,14
Potential application of macroinvertebrates indices in bioassessment of Mexican streams,Eva Carmina Serrano Balderas; Corinne Grac; Laure Berti-Equille; Ma Aurora Armienta Hernandez,Abstract Biomonitoring of surface waters using benthic macroinvertebrates is a commonpractice in developed countries. However; the use of biomonitoring metrics as part of regularmonitoring programs in developing countries is scarce. This study is aimed at identifying themacroinvertebrate-based monitoring approaches which can be potentially applied to assessthe ecological status of Mexican streams; and to provide guidelines and practical tools toimplement such approaches. In this study; a total of 35 biomonitoring metrics were selected;divided into the following classes: 5 metrics of richness; 11 metrics to list themacroinvertebrates organisms (enumeration metrics); 6 diversity and similarity indices; 7biotic indices; 5 functional feeding metrics and 1 multimetric approach. This selection wasmade using the a posteriori approach. Characteristics such as: sensitivity; ecological …,*,2016,13
Integration of biological data and quality-driven source negotiation,Laure Berti-Equille,Abstract Evaluation of data non-quality in database or datawarehouse systems is apreliminary stage before any data usage and analysis; moreover in the context of dataintegration where several sources provide more or less redundant or contradictoryinformation items and whose quality is often unknown; imprecise and very heterogeneous.Our application domain is bioinformatics where more than five hundred of semi-structureddatabanks propose biological information without any quality information (ie metadata andstatistics describing the production and the management of the biological data). In order tofacilitate the multi-source data integration in the context of distributed biological databanks;we propose a technique based on the concepts of quality contract and data sourcenegotiation for a standard wrapper-mediator architecture. A quality source contract allows …,International Conference on Conceptual Modeling,2001,12
A quality-aware spatial data warehouse for querying hydroecological data,L Berrahou; Nathalie Lalande; Eva Serrano; Guilhem Molla; Laure Berti-Équille; Sandro Bimonte; Sandra Bringay; Flavie Cernesson; Corinne Grac; Dino Ienco; Florence Le Ber; Maguelonne Teisseire,Abstract Addressing data quality issues in information systems remains a challenging task.Many approaches only tackle this issue at the extract; transform and load steps. Here wedefine a comprehensive method to gain greater insight into data quality characteristicswithin data warehouse. Our novel architecture was implemented for an hydroecologicalcase study where massive French watercourse sampling data are collected. The methodmodels and makes effective use of spatial; thematic and temporal accuracy; consistency andcompleteness for multidimensional data in order to offer analysts a “data quality” orientedframework. The results obtained in experiments carried out on the Saône River datasetdemonstrated the relevance of our approach.,Computers & Geosciences,2015,11
Quality-Adaptive Query Processing Over Distributed Sources.,Laure Berti-Equille,Abstract: For non-collaborative data sources; both cost estimate-based optimization andquality-driven query processing are difficult to achieve because the sources do not exportcost information nor data quality indicators. In this paper; we first propose an expressivequery language extension using QML1 syntax for defining in a flexible way dimensions;metrics of data quality and data source quality. We present a new framework for adaptivequery processing on quality-extended query declarations. This processing includes thenegotiation of quality contracts between the distributed data sources. The principle is to finddynamically the best trade-off between the cost of the query and the quality of the resultretrieved from several distributed sources.,IQ,2004,11
Un état de l'art sur la qualité des données,Laure Berti-Equille,*,Ingénierie des systèmes d'information,2004,11
Rheem: Enabling multi-platform task execution,Divy Agrawal; Lamine Ba; Laure Berti-Equille; Sanjay Chawla; Ahmed Elmagarmid; Hossam Hammady; Yasser Idris; Zoi Kaoudi; Zuhair Khayyat; Sebastian Kruse; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Mohammed J Zaki,Abstract Many emerging applications; from domains such as healthcare and oil & gas;require several data processing systems for complex analytics. This demo paper showcasessystem; a framework that provides multi-platform task execution for such applications. Itfeatures a three-layer data processing abstraction and a new query optimization approachfor multi-platform settings. We will demonstrate the strengths of system by using real-worldscenarios from three different applications; namely; machine learning; data cleaning; anddata fusion.,Proceedings of the 2016 International Conference on Management of Data,2016,10
AllegatorTrack: Combining and reporting results of truth discovery from multi-source data,Dalia Attia Waguih; Naman Goel; Hossam M Hammady; Laure Berti-Equille,In the Web; a massive amount of user-generated contents is available through variouschannels; eg; texts; tweets; Web tables; databases; multimedia-sharing platforms; etc.Conflicting information; rumors; erroneous and fake contents can be easily spread acrossmultiple sources; making it hard to distinguish between what is true and what is not. How doyou figure out that a lie has been told often enough that it is now considered to be true? Howmany lying sources are required to introduce confusion in what you knew before to be thetruth? To answer these questions; we present AllegatorTrack; a system that discovers trueclaims among conflicting data from multiple sources.,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,10
Dependency between sources in truth discovery,*,A method and system for truth discovery may implement a methodology that accounts foraccuracy of sources and dependency between sources. The methodology may be based onBayesian probability calculus for determining which data object values published by sourcesare likely to be true. The method may be recursive with respect to dependency; accuracy;and actual truth discovery for a plurality of sources.,*,2012,10
Data veracity estimation with ensembling truth discovery methods,Laure Berti-Equille,Estimation of data veracity is recognized as one of the grand challenges of big data.Typically; the goal of truth discovery is to determine the veracity of multi-source; conflictingdata and return; as outputs; a veracity label and a confidence score for each data value;along with the trustworthiness score of each source claiming it. Although a plethora ofmethods has been proposed; it is unlikely a technique dominates all others across all datasets. Furthermore; the performance evaluation of the methods entirely depends on theavailability of labeled ground truth data (ie; data whose veracity has been manuallychecked). In the context of Big Data; acquiring the complete ground truth data is out-of-reach. In this paper; we propose an ensembling method that mitigates the two problems ofmethod selection and ground truth data sparsity. Our approach combines the results of a …,Big Data (Big Data); 2015 IEEE International Conference on,2015,9
Multidimensional management and analysis of quality measures for CRM applications in an electricity company,Veronika Peralta; Virginie Thion; Zoubida Kedad; Laure Berti-Équille; Isabelle Comyn-Wattiau; Sylvaine Nugier; Samira Sisaid-Cherfi,This paper presents an approach integrating data quality into the business intelligencechain in the context of CRM applications at EDF (Electricite de France); the major electricitycompany in France. The main contribution of this paper is the definition and instantiation of ageneric multi-dimensional star-like model for storing; analyzing and capitalizing data qualityindicators; measurements and metadata. This approach is illustrated in one of EDF's CRMapplications implementing the data quality-driven information supply chain for businessintelligence where the role of the data quality expert is highly emphasized.,Proceedings of the International Conference on Information Quality (ICIQ),2009,9
Évaluation de la qualité des systèmes multisources Une approche par les patterns,Jacky Akoka; L Berti-Équille; Omar Boucelma; Mokrane Bouzeghoub; I Comyn-Wattiau; M Cosquer; V Goasdoué; Z Kedad; S Nugier; V Peralta; M Quafafou; S Sisaïd-Cherfi,Résumé. L'article décrit la problématique et les solutions proposées par le projet QUADRIS(ARA-05MMSA-0015) dont l'objectif est d'offrir un cadre d'évaluation de la qualité dans lessystèmes d'information multisources (SIM). Ce cadre a permis de définir un méta-modèlepour étudier en particulier les interdépendances entre les dimensions de la qualité d'unmodèle conceptuel de données et celles de la qualité des données instanciant ce modèle.Nous étudions la possibilité de définir des patterns d'évaluation de la qualité dans le but de:1) formaliser les corrélations entre les facteurs de qualité; 2) représenter les processus; et 3)analyser la qualité des données; du système et son évolution. Le projet QUADRIS s' estengagé à valider ses propositions dans les trois domaines d'application suivants: ledomaine biomédical; le domaine commercial et le domaine géographique.,Qualité des Données et des Connaissances,2008,9
Qualité des données,Laure Berti-Équille,Les problèmes de qualité des données stockées dans les bases et les entrepôts dedonnées se propagent de façon endémique à tous les types de données (structurées ounon) et dans tous les domaines d'application: données gouvernementales; commerciales;industrielles ou scientifiques. Il s' agit en particulier d'erreurs sur les données; de doublons;d'incohérences; de valeurs manquantes; incomplètes; incertaines; obsolètes; aberrantes oupeu fiables. Les conséquences de la non qualité des données (ou de leur qualité médiocre)sur les prises de décision et les coûts financiers qu'elle engendre sont considérables1. Avecla multiplication des sources d'informations disponibles et l'accroissement des volumes dedonnées potentiellement accessibles; la qualité des données et; plus largement; la qualitédes informations ont pris une place de premier plan; d'abord; au sein des entreprises et …,Techniques de l’ingénieur. Informatique HB4 (H3700),2006,9
Optimizing progressive query-by-example over pre-clustered large image databases,Annie Choupo; Anicet Kouomou and Berti-Equille; Laure and Morin,Abstract The typical mode for querying in an image content-based information system isquery-by-example; which allows the user to provide an image as a query and to search forsimilar images (ie; the nearest neighbors) based on one or a combination of low-levelmultidimensional features of the query example. Off-lime; this requires the time-consumingpre-computing of the whole set of visual descriptors over the image database. On-line; onemajor drawback is that multidimensional sequential NN-search is usually exhaustive overthe whole image set face to the user who has a very limited patience. In this paper; wepropose a technique for improving the performance of image query-by-example executionstrategies over multiple visual features. This includes first; the pre-clustering of the largeimage database and then; the scheduling of the processing of the feature clusters before …,Proceedings of the 2nd international workshop on Computer vision meets databases,2005,9
Annotation et recommandation collaboratives de documents selon leur qualité,Laure Berti-Equille,*,INGENIERIE DES SYSTEMS D INFORMATION,2002,8
A Masking Index for Quantifying Hidden Glitches,Laure Berti-Equille; Ji Meng Loh; Tamraparni Dasu,Abstract Data glitches are errors in a dataset. They are complex entities that often spanmultiple attributes and records. When they co-occur in data; the presence of one type ofglitch can hinder the detection of another type of glitch. This phenomenon is called masking.In this paper; we define two important types of masking and propose a novel; statisticallyrigorous indicator called masking index for quantifying the hidden glitches. We outline fourcases of masking: outliers masked by missing values; outliers masked by duplicates;duplicates masked by missing values; and duplicates masked by outliers. The maskingindex is critical for data quality profiling and data exploration. It enables a user to measurethe extent of masking and hence the confidence in the data. In this sense; it is a valuabledata quality index for choosing an anomaly detection method that is best suited for the …,ICDM 2013 IEEE International Conference on Data Mining,2013,7
La qualité des systèmes d? information? Vers une vision plus intégrée,Isabelle Comyn-Wattiau; Jacky Akoka; Laure Berti-Equille,Quality is a multidimensional concept encompassing different semantics related to differentcontexts. Information systems quality covers several aspects and dimensions includinghuman; technological and organizational dimensions. Evaluating the dimensions of aninformation system requires taking into account respectively data quality as well as modelsand software quality. The evaluation process is based on goals; quality factors; criteria andtheir associated metrics; and finally on quality models. Many models and frameworks havebeen proposed for evaluating data; models; and software quality. The aim of this paper is topresent a comprehensive state of the art of these approaches even if there is not acommonly accepted framework.,Revue des Sciences et Technologies de l'Information-Série ISI: Ingénierie des Systèmes d'Information,2010,6
Truth discovery and copying detection from source update history,D Srivastava X Dong; L Berti-Equille,*,*,2009,5
Report from the First and Second International Workshops on Information Quality in Information Systems: IQIS 2004 and IQIS 2005 in conjunction with ACM SIGMOD...,Monica Scannapieco; Laure Berti-Equille,Abstract This report summarizes the constructive discussions of the first two editions of theInternational Workshop on Information Quality in Information Systems; IQIS 2004 and IQIS2005; held respectively in Paris; France; on June 13; 2004 and in Baltimore; MD; USA; onJune 17; 2005.,ACM SIGMOD Record,2006,5
Contributions to Quality-Aware Online Query Processing,L Berti-Equille,Abstract For non-collaborative data sources; quality-aware query processing is difficult toachieve because the sources generally do not export data quality indicators. This paperpresents a prospective work on the declaration of metadata describing data quality and onthe adaptation of query processing for taking into account constraints on the quality of dataand finding dynamically the best trade-off between the cost of the query and the quality ofthe result.,IEEE Data Eng. Bull,2006,5
Multimedia indexing and retrieval with features association rules mining [image databases],Anicet Kouomou-Choupo; Laure Berti-Equille; Annie Morin,The administration of very large collections of images; accentuates the classical problems ofindexing and efficiently querying information. This paper describes a new method applied tovery large still image databases that combines two data mining techniques: clustering andassociation rules mining in order to better organize image collections and to improve theperformance of queries. The objective of our work is to exploit association rules discoveredby mining; global MPEG-7 features data and to adapt the query processing. In ourexperiment; we use five MPEG-7 features to describe several thousands of still images. Foreach feature; we initially determine several clusters of images by using a K-mean algorithm.Then; we generate association rules between different clusters of features and exploit theserules to rewrite the query and to optimize the query-by-content processing,Multimedia and Expo; 2004. ICME'04. 2004 IEEE International Conference on,2004,5
Vera: A platform for veracity estimation over web data,Mouhamadou Lamine Ba; Laure Berti-Equille; Kushal Shah; Hossam M Hammady,Abstract Social networks and the Web in general are characterized by multiple informationsources often claiming conflicting data values. Data veracity is hard to estimate; especiallywhen there is no prior knowledge about the sources or the claims and in time-dependentscenarios where initially very few observers can report first information. Despite the wide setof recently proposed truth discovery approaches;" no-one-fits-all" solution emerges forestimating the veracity of on-line information in open contexts. However; analyzing thespace of conflicting information and disagreeing sources might be relevant; as well asensembling multiple truth discovery methods. This demonstration presents VERA; a Web-based platform that supports information extraction from Web textual data and micro-textsfrom Twitter and estimates data veracity. Given a user query; VERA systematically …,Proceedings of the 25th International Conference Companion on World Wide Web,2016,4
Prediction of September–December Fire in New Caledonia (Southwestern Pacific) Using July Nino-4 Sea Surface Temperature Index,Vincent Moron; Renaud Barbero; Morgan Mangeas; Laurent Borgniet; Thomas Curt; Laure Berti-Equille,Abstract An empirical statistical scheme for predicting September–December fires in NewCaledonia in the southwestern Pacific Ocean region using a cross-validated generalizedlinear model has been developed for the 2000–10 period. The predictor employs July seasurface temperatures (SST) recorded over the Niño-4 box (5° S–5° N; 160°–210° E); whichare closely related to austral spring (September–November) rainfall anomalies across NewCaledonia. The correlation between the logarithm of observed and simulated total burnedareas across New Caledonia is 0.87. A decrease in the local-scale skill (median correlationbetween the log of observed and simulated total burned areas in a 20-km radius around arain gauge= 0.46) around the main town (Nouméa) and its suburbs in the southwest ofGrande Terre; and also in northern New Caledonia; could be associated either with a …,Journal of Applied Meteorology and Climatology,2013,4
Panorama des méthodes de détection et de traitement des anomalies,Laure Berti-Équille,–Kolmogorov-Smirnov Test: non-parametric test that quantifies the maximum distancebetween the empirical distribution function of the variable and the cdf of the normaldistribution–Anderson-Darling Test: variant of KS test weighting the tails of distributions–Lilliefors Test: variant of KS test for unknown mean and standard deviation–Shapiro-WilkTest: orders the sample values in ascending order and uses the correlation to detect smalldepartures from normality-not suitable for very large sample sizes (SAS proc UNIVARIATE),5èmes Journées thématiques d’Apprentissage Artificiel & Fouille de Données (AAFD’12),2012,4
DAQ_UWE: A framework for designing data quality aware web applications,César Guerra-García; Ismael Caballero; Laure Berti-Équille; Mario Piattini,Abstract: The use of Web applications in order to provide data with an acceptable level ofquality is currently of paramount importance for any enterprise that wishes its businessprocesses to succeed. The adequate management of the corresponding data resourcesthrough the introduction of all those aspects whose aim is to monitor the levels of quality forthe task in hand is therefore essential. We claim that the introduction of such elements andmechanisms should take place during the Web application development process. To thebest of our knowledge; there is still a lack of methodological and technological proposalswith which to design data quality aware applications in the field of Web applicationdevelopment. Based principally on the benefits provided by the Model Driven WebEngineering (MDWE); this paper proposes a metamodel and a UML profile (DAQ_UWE) …,International Conference of Information Quality; ICIQ,2011,4
Uguide: User-guided discovery of fd-detectable errors,Saravanan Thirumuruganathan; Laure Berti-Equille; Mourad Ouzzani; Jorge-Arnulfo Quiane-Ruiz; Nan Tang,Abstract Error detection is the process of identifying problematic data cells that are differentfrom their ground truth. Functional dependencies (FDs) have been widely studied in supportof this process. Oftentimes; it is assumed that FDs are given by experts. Unfortunately; it isusually hard and expensive for the experts to define such FDs. In addition; automatic dataprofiling over dirty data in order to find correct FDs is known to be a hard problem. In thispaper; we propose an end-to-end solution to detect FD-detectable errors from dirty data. Thebroad intuition is that given a dirty dataset; it is feasible to automatically find approximateFDs; as well as data that is possibly erroneous. Arguably; at this point; only experts canconfirm true FDs or true errors. However; in practice; experts never have enough budget tofind all errors. Hence; our problem is; given a limited budget of expert's time; which …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,3
Towards ontology reasoning for topological cluster labeling,Hatim Chahdi; Nistor Grozavu; Isabelle Mougenot; Younès Bennani; Laure Berti-Equille,Abstract In this paper; we present a new approach combining topological unsupervisedlearning with ontology based reasoning to achieve both:(i) automatic interpretation ofclustering; and (ii) scaling ontology reasoning over large datasets. The interest of suchapproach holds on the use of expert knowledge to automate cluster labeling and gives themhigh level semantics that meets the user interest. The proposed approach is based on twosteps. The first step performs a topographic unsupervised learning based on the SOM (Self-Organizing Maps) algorithm. The second step integrates expert knowledge in the map usingontology reasoning over the prototypes and provides an automatic interpretation of theclusters. We apply our approach to the real problem of satellite image classification. Theexperiments highlight the capacity of our approach to obtain a semantically labeled …,International Conference on Neural Information Processing,2016,3
Veracity of Big Data: Challenges of Cross-Modal Truth Discovery,Laure Berti-Equille; Mouhamadou Lamine Ba,As online user-generated content grows exponentially; the reliance on web and socialmedia data is increasing. Truth discovery from the web has significant practical importanceas online rumors and misinformation can have tremendous impacts on our society andeveryday life. One of the fundamental difficulties is that data can be biased; noisy; outdated;incorrect; misleading; and thus unreliable. Conflicting data from multiple sources amplifiesthis problem; and the veracity of data has to be estimated. Beyond the emerging field ofcomputational journalism and the success of online fact-checkers (eg; FactCheck 1ClaimBuster 2); truth discovery is a long-standing and challenging problem studied by manyresearch communities in artificial intelligence; databases; and complex systems; and undervarious names: fact-checking; data and knowledge fusion; information trustworthiness …,Journal of Data and Information Quality (JDIQ),2016,3
La qualité des données comme condition vers la qualité des connaissances: un état de l’art,L Berti-Equille,*,RNTÏ-EJ; pp 95f118,2004,3
From Data Source Quality to Information Quality: The Relative Dimension.,Laure Berti,Abstract This paper presents our current work concerning the characterization of theRelative Information Quality concept. In the context of over-and disinformation stressed byPush and Pull Technologies improvements; Information Quality judgment is very relative toData Supervisors and to Information lCustomers. We present the methods we are currentlybeing implementing and testing for user proﬁle acquisition; ﬂexible and personal InformationQuality configuration and Computer-aided Relative Information Quality Estimation. Thesefunctionalities will extend the VIGIWARE system we previously presented during the 1997Conference on Information Quality as an Information System specifically dedicated toTechnological Watch enabling the management and recommendation of contradictory multi-source information and their quality.,IQ,1998,3
On the Use of Ontology as a priori Knowledge into Constrained Clustering,Hatim Chahdi; Nistor Grozavu; Isabelle Mougenot; Laure Berti-Equille; Younes Bennani,Recent studies have shown that the use of a priori knowledge can significantly improve theresults of unsupervised classification. However; capturing and formatting such knowledge asconstraints is not only very expensive requiring the sustained involvement of an expert but itis also very difficult because some valuable information can be lost when it cannot beencoded as constraints. In this paper; we propose a new constraint-based clusteringapproach based on ontology reasoning for automatically generating constraints andbridging the semantic gap in satellite image labeling. The use of ontology as a prioriknowledge has many advantages that we leverage in the context of satellite imageinterpretation. The experiments we conduct have shown that our proposed approach candeal with incomplete knowledge while completely exploiting the available one.,Data Science and Advanced Analytics (DSAA); 2016 IEEE International Conference on,2016,2
Approche hybride à base d'ontologie pour le clustering par contraintes,Hatim Chahdi; Nistor Grozavu; Isabelle Mougenot; Laure Berti-Equille; Younès Bennani,Les méthodes de clustering semi supervisé font très souvent usage de contraintes afin derépondre au mieux aux besoins des utilisateurs. Cependant; dans des domaines comme letraitement et l'interprétation d'images satellites; la génération de ces contraintes estcoûteuse et demande une grande expertise du domaine. Dans cet article; nous présentonsune approche hybride qui exploite en amont les ontologies pour automatiser la générationdes contraintes pour le clustering et introduire une labellisation sémantique des clusters.L'objectif est de disposer d'une brique ontologique qui va permettre d'étiqueter sémantique-ment une partie du jeu de données en se basant sur des mécanismes de raison-nementdéductif; puis d'utiliser les résultats de ce raisonnement pour (i) générer des contraintes quivont guider le clustering;(ii) étiqueter sémantiquement les clusters obtenus avec les …,La conférence internationale francophone AAFD & SFC 2016,2016,2
La qualité et la gouvernance des données au service de la performance des entreprises,Laure Berti-Equille,Horizon / Plein textes La base de ressources documentaires de l'IRD. IRD. Publications desscientifiques de l'IRD. Berti-Equille Laure (ed.). (2012). La qualité et la gouvernance des donnéesau service de la performance des entreprises. Cachan : Lavoisier-Hermes Science; 388 p.(Informatique et SI). ISBN 978-2-7462-2510-7. Titre; La qualité et la gouvernance des donnéesau service de la performance des entreprises. Année de publication; 2012. Type de document;Ouvrage. Auteurs; Berti-Equille Laure (ed.). Source; Cachan : Lavoisier-Hermes Science; 2012;388 p. (Informatique et SI). ISBN 978-2-7462-2510-7. Plan de classement; Traitement del'information (y compris sources et recherche [124TRAINF] ; Applications diverses [122APPLIC] …,*,2012,2
QDex: a database profiler for generic bio-data exploration and quality aware integration,Fouzia Moussouni; Laure Berti-Equille; G Rozé; Olivier Loréal; Emilie Guérin,Abstract In human health and life sciences; researchers extensively collaborate with eachother; sharing genomic; biomedical and experimental results. This necessitates dynamicallyintegrating different databases into a single repository or a warehouse. The data integratedin these warehouses are extracted from various heterogeneous sources; having differentdegrees of quality and trust. Most of the time; they are neither rigorously chosen nor carefullycontrolled for data quality. Data preparation and data quality metadata are recommendedbut still insufficiently exploited for ensuring quality and validating the results of informationretrieval or data mining techniques. In a previous work; we built a data warehouse calledGEDAW (Gene Expression Data Warehouse) that stores various information: data on genesexpressed in the liver during iron overload and liver diseases; relevant information from …,International Conference on Web Information Systems Engineering,2007,2
Cost of Low-Quality Data over Association Rules Discovery,Laure Berti-Équille,Abstract. Quality in data mining critically depends on the preparation and on the quality ofprocessed data sets. Indeed data mining processes and applications require various formsof data preparation (and repair) with several data formatting and cleaning techniques;because the data input to the mining algorithms is assumed to conform to nice datadistributions; containing no missing; inconsistent or incorrect values. This leaves a large gapbetween the available dirty data and the available machinery to process and analyze thedata for discovering knowledge. This paper presents a theoretical probabilistic framework formodeling the cost of low-quality data on discovered association rules.,Proceedings of International Symposium on Applied Stochastic Models and Data Analysis (AM SDA 2005) Brest; France,2005,2
Assurer la qualité des données: un défi permanent pour les systèmes d'information; bases et entrepôts de données,Laure BERTI-EQUILLE,*,Génie logiciel,2005,2
Quality-based recommendation of XML documents,Laure Berti-Equille,Abstract. Information quality is multidimensional and can be modeled as an ordered set ofweighted criteria. For a collection of XML documents; our approach consists firstly inharvesting and generating information quality indicators and enriching the meta-descriptionof XML documents. Quality metadata are then exploited within the query processing formetadata-driven information retrieval and filtering in order to propose quality-adaptiverecommendation strategies (with a quality view as a result) of the queried XML documents.This quality view depends on the user's profile and on his specific information qualityrequirements. The paper describes the architecture of a quality-based recommender systemfor XML documents.,Journal of Digital Information Management,2003,2
Combining the power of query languages and search engines for on-line document and information retrieval: the QIRi@ D Environment,Laure Berti; Jean-Luc Damoiseaux; Elisabeth Murisasco,Abstract In order to retrieve an item of information on the Web; many search engines havebeen proposed. They are rarely efficient at the first attempt: the display of results “forces” theuser to navigate. In parallel; Web query languages have been developed to avoid these twosequential phases: research then navigation. In this context; the QIRI@ D experimentalplatform; based on the functional programming language SgmlQL; enables both informationretrieval and manipulation of distributed semi-structured documents published on a sub-network made up of the sites where QIRI@ D is running. It is possible in an unique query tospecify criteria to find a document; to filter it; to extract parts of it for building the result. Anautomatical enrichment of any published document is used to improve the search efficiency.,International Workshop on Principles of Digital Document Processing,1998,2
Out of Overinformation by Information Filtering and Information Quality Weighting.,Laure Berti,Abstract This paper presents our approach concerning the management of contradictoryinformation in multi-source information fusion process. We break down the process of datacapture which alterates information quality and; as compensation; we propose a value-adding process for information. More semantics of information is caught via quality meta-data Weighting the impact of data quality criteria on user's satisfaction allows an ordering ofcontradictory data according to their relative quality. The ultimate objective is to accomplisha personal; adaptative information recommendation process with reduction of information1055 for Technological Watch applications.,IQ,1997,2
Remote sensing image analysis by aggregation of segmentation-classification collaborative agents,Andrés Troya-Galvis; Pierre Gançarski; Laure Berti-Équille,Abstract In this article we present two different approaches for automatic remote sensingimage interpretation which are based on a multi-paradigm collaborative framework whichuses classification in order to guide the segmentation process. The first approach appliessequentially many one-vs-all class extractors in a manner inspired by cascading techniquesin machine learning. The second approach applies many collaborating one-vs-all classextractors in parallel. We show that the collaboration of the segmentation and classificationparadigms result in a remarkable reduction of segmentation errors but also in better objectclassification in comparison to a hybrid pixel-object approach as well as a deep learningapproach.,Pattern Recognition,2018,1
Profiling DRDoS Attacks with Data Analytics Pipeline,Laure Berti-Equille; Yury Zhauniarovich,Abstract A large amount of Distributed Reflective Denial-of-Service (DRDoS) attacks arelaunched every day; and our understanding of the modus operandi of their perpetrators isyet very limited as we are submerged with so Big Data to analyze and do not have reliableand complete ways to validate our findings. In this paper; we propose a first analytic pipelinethat enables us to cluster and characterize attack campaigns into several main profiles thatexhibit similarities. These similarities are due to common technical properties of theunderlying infrastructures used to launch these attacks. Although we do not have access tothe ground truth and we do not know how many perpetrators are acting behind the scene;we can group their attacks based on relevant commonalities with cluster ensembling toestimate their number and capture their profiles over time. Specifically; our results show …,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,2017,1
Collaborative segmentation and classification for remote sensing image analysis,Andrés Troya-Galvis; Pierre Gançarski; Laure Berti-Équille,In this article we present CoSC; a generic framework for collaborative segmentation andclassification. The framework is guided by both radiometric homogeneity based criteria andimplicit semantic criteria to segment and extract the objects of a given thematic class. Wepresent a proof-of-concept case-study and show that CoSC is able to reach higherconfidence for object classification and results in significant improvement of the wholesegmentation.,Pattern Recognition (ICPR); 2016 23rd International Conference on,2016,1
Using Twitter to Understand Public Interest in Climate Change: The Case of Qatar.,Sofiane Abbar; Tahar Zanouda; Laure Berti-Equille; Javier Borge-Holthoefer,Abstract Climate change has received an extensive attention from public opinion in the lastcouple of years; after being considered for decades as an exclusive scientific debate.Governments and world-wide organizations such as the United Nations are working morethan ever on raising and maintaining public awareness toward this global issue. In thepresent study; we examine and analyze Climate Change conversations in Qatar'sTwittersphere; and sense public awareness towards this global and shared problem ingeneral; and its various related topics in particular. Such topics include but are not limited topolitics; economy; disasters; energy and sandstorms. To address this concern; we collectand analyze a large dataset of 109 million tweets posted by 98K distinct users living in Qatar–one of the largest emitters of co2 worldwide. We use a taxonomy of climate change topics …,EcoMo@ ICWSM,2016,1
Génération de contraintes pour le clustering à partir d'une ontologie-Application à la classification d'images satellites,Hatim Chahdi; Nistor Grozavu; Isabelle Mougenot; Laure Berti-Equille; Younès Bennani,L'utilisation des connaissances a priori peut fortement améliorer la classification non-supervisée. L'injection de ces connaissances sous forme de contraintes sur les donnéesfigure parmi les techniques les plus efficaces de la littérature. Cependant; la génération descontraintes est très coûteuse et demande l'intervention de l'expert; la sémantique apportéepar l'étiquetage de l'expert est aussi perdue dans ce type de techniques; seuls lescontraintes sont retenues par le clustering. Dans cet article; nous proposons une nouvelleapproche hy-bride exploitant le raisonnement à base d'ontologie pour générer automatique-ment des contraintes permettant de guider et améliorer le clustering. L'utilisation d'uneontologie comme connaissance a priori a plusieurs avantages. Elle permet l'interprétationautomatisée des connaissances; ajoute de la modularité dans la chaîne de traitement et …,Extraction et Gestion des Connaissances (EGC),2016,1
Tracing Data Pollution in Large Business Applications.,Laure Berti-Équille,Abstract: In large business applications; various data processing activities can be donelocally or outsourced; split or combined and the resulting data flows have to be exchanged;shared or integrated from multiple data processing units. There are indeed variousalternative paths for data processing and data consolidation. But some data flows and dataprocessing applications are most likely exposed to generating and propagating data errors;some of them are more critical too. Actually; we usually ignore the impact of data errors inlarge and complex business applications because: 1) it is often very difficult to systematicallyaudit data; detect and trace data errors in such large applications; 2) we usually don't havethe complete picture of all the data processing units involved in every data processing paths;they are viewed as black-boxes; and 3) we usually ignore the total cost of detecting and …,ICIQ,2008,1
Measuring and Constraining Data Quality with Analytic Workflows.,Laure Berti-Equille,ABSTRACT One challenging aspects of data quality modeling and management is toprovide flexible; declarative and appropriate ways to express requirements on the quality ofdata. The paper presents a framework for specifying and checking constraints on dataquality in RDBMS. The evaluation of quality of data (QoD) is based on the declaration ofdata quality metrics that are computed and combined into so-called QoD analytic workflows.These workflows are designed as a composition of statistical methods and data miningtechniques used to detect patterns of anomalies in the data sets. As metadata they are usedto characterize various quantifiable dimensions of data quality (eg; completeness; freshness;consistency; accuracy). The paper proposes a query language extension for constrainingdata quality when querying both data and its associated QoD metadata. Probabilistic …,QDB/MUD,2008,1
Quality-Extended. Query. Processing. for. Mediation. Systems,Laure Berti-Équille; France IRISA,Abstract For noncollaborative distributed data sources; quality-driven query processing isdifficult to achieve because the sources generally do not export data quality indicators. Thischapter deals with the extension and adaptation of query processing for taking into accountconstraints on quality of distributed data. This chapter presents a novel framework foradaptive query processing on quality-extended query declarations. It proposes anexpressive query language extension combining SQL and QML; the quality of servicemodeling language proposed by Frølund and Koistinen (1998) for defining; in a flexible way;dimensions; and metrics on data; source; and service quality. The originality of the approachis to include the negotiation of quality contracts between the distributed data sourcescompeting for answering the query. The principle is to find dynamically the best trade-off …,*,2007,1
Quality-Aware association rule mining,Laure Berti-Équille,Abstract The quality of discovered association rules is commonly evaluated byinterestingness measures (commonly support and confidence) with the purpose of supplyingsubsidies to the user in the understanding and use of the new discovered knowledge. Low-quality datasets have a very bad impact over the quality of the discovered association rules;and one might legitimately wonder whether a so-called “interesting” rule noted LHS-> RHSis meaningful when 30% of LHS data are not up-to-date anymore; 20% of RHS data are notaccurate; and 15% of LHS data come from a data source that is well-known for its badcredibility. In this paper we propose to integrate data quality measures for effective andquality-aware association rule mining and we propose a cost-based probabilistic model forselecting legitimately interesting rules. Experiments on the challenging KDD-CUP-98 …,Pacific-Asia Conference on Knowledge Discovery and Data Mining,2006,1
Recherche dans de grandes bases d'images fixes: une nouvelle approche guidée par les règles d'association.,Anicet Kouomou Choupo; Annie Morin; Laure Berti-Équille,Résumé. Une base d'images fixes peut être décrite de plusieurs façons; notamment par desdescripteurs visuels globaux de couleur; de texture; ou de forme. Les requêtes les plusfréquentes impliquent et combinent les résultats de plusieurs types de descripteurs: parexemple;” retrouver toutes les images ayant une couleur et une texture semblablesa cellesd'une image requête donnée”. Pour retrouver plus efficacement et plus rapidement uneimage dans une grande base; nous exploitons des combinaisons appropriées dedescripteurs et étudions l'intérêt des regles d'association entre clusters de descripteurs pouraccélérer le temps de réponsea des requêtes sur de grandes bases d'images fixes.,EGC,2004,1
Renseigner la qualité des connaissances par la fusion d'indicateurs sur la qualité des données.,Laure Berti-Equille,RÉSUMÉ. L'objet de cet article est de présenter des travaux en cours sur la mesured'incidence que peut avoir la qualité des données sur la qualité des règles d'associationextraites. Dans un premier temps; l'article présente un bref panorama des travaux sur laqualité des données; travaux non négligeables dès lors que l'on s' intéresse à qualifier et àaméliorer la qualité des connaissances extraites à partir des données; une seconde partieprésente une méthodologie pour contrôler la qualité des données dans le processusd'extraction de connaissances à partir des données. Nous proposons ensuite de fusionnerles indicateurs de la qualité des données afin de renseigner la qualité des règles extraites.Notre approche offre de nombreux avantages dans l'étape de qualification; de validation etde rejet de règles. ABSTRACT. The aim of the article is to present our current works on …,EGC,2003,1
Integration of biological data on transcriptome,Laure Berti-Equille; Fouzia Moussouni; Anne Arcade,ABSTRACT: A major concern in modern biology and medical research consists of the use ofa “high flow” technology named bio-arrays or DNA chips that allows the study of thousandsof genes simultaneously. The medical research institute; INSERM U522; specialized in theliver; uses the transcriptome techniques to diagnose liver disease states and to point theway towards new therapies. For this sake; the design of a bioinformatic integratedenvironment; named Gedaw (Gene Expression DAta Warehouse) has been initiated forstoring; managing and analyzing such specific data. As an object-oriented data warehouse;it includes knowledge and complex data on genes expressed in the liver. The concept ofontology is the keystone of the application for integrating both genomic data available onpublic databanks; as well as experimental data on genes delivered from laboratory …,Ingénierie des Systèmes d'Information,2001,1
A Collaborative Framework for Joint Segmentation and Classification of Remote Sensing Images,Andrés Troya-Galvis; Pierre Gançarski; Laure Berti-Équille,Abstract In this article; we present a collaborative framework for joint segmentation andclassification. The framework is guided by and aware of the quality of each segment at everystage; it allows the consideration of both homogeneity based criteria as well as implicitsemantic criteria to extract the objects belonging to a given thematic class. We apply theproposed framework to vegetation extraction in a very high spatial resolution image ofStrasbourg. We compare our results to a pixel-based method; an object-based method anda hybrid segmentation-classification method. The experiments show that the proposedmethod reaches good classification results while remarkably improving the segmentationresults.,*,2018,*
On the Role of Political Affiliation in Human Perception The Case of Delhi OddEven Experiment,Tahar Zanouda; Sofiane Abbar; Laure Berti-Equille; Kushal Shah; Abdelkader Baggag; Sanjay Chawla; Jaideep Srivastava,Abstract In an effort to curb air pollution; the city of Delhi (India); known to be one of the mostpopulated; polluted; and congested cities in the world has run a trial experiment in twophases of 15 days intervals. During the experiment; most of four-wheeled vehicles wereconstrained to move on alternate days based on whether their plate numbers ended withodd or even digits. While the local government of Delhi represented by A. Kejriwal (leader ofAAP party) advocated for the benefits of the experiment; the prime minister of India; N. Modi(former leader of BJP) defended the inefficiency of the initiative. This later has led to a strongpolarization of public opinion towards OddEven experiment. This real-world urbanexperiment provided the scientific community with a unique opportunity to study the impactof political leaning on humans perception at a large-scale. We collect data about pollution …,International Conference on Social Informatics,2017,*
Principled Data Preprocessing: Application to Biological Aquatic Indicators of Water Pollution,Eva C Serrano Balderas; Laure Berti-Equille; Maria Aurora Armienta Hernandez; Corinne Grac,In many biological studies; statistical and data mining methods are extensively used toanalyze the data and discover actionable knowledge. But; bad data quality causing incorrectanalysis results and wrong interpretations may induce misleading conclusions andinadequate decisions. To ensure the validity of the results; avoid bias and data misuse; it isnecessary to control not only the whole analytical pipeline; but most importantly the quality ofthe data with appropriate data preprocessing choices. Since various preprocessingtechniques and alternative strategies may lead to dramatically different outputs; it is crucialto rely on a principled and rigorous method to select the optimal set of data preprocessingsteps that depends both on the input data distributional characteristics and on the inherentcharacteristics of the targeted statistical or data mining methods. In this paper; we …,Database and Expert Systems Applications (DEXA); 2017 28th International Workshop on,2017,*
Scalable automatic data repair,*,A computer implemented method for generating a set of updates for a database comprisingmultiple records including erroneous; missing and inconsistent values; the methodcomprising using a set of partitioning functions for subdividing the records of the databaseinto multiple subsets of records; allocating respective ones of the records to at least onesubset according to a predetermined criteria for mapping records to subsets; applyingmultiple machine learning models to each of the subsets to determine respective candidatereplacement values representing a tuple repair for a record including a probability ofcandidate and current values for the record; computing probabilities to select replacementvalues for the record from among the candidate replacement values which maximise theprobability for values of the record for an updated database.,*,2017,*
Metabolomic Data Profiling for Diabetes Research in Qatar,Raghvendra Mall; Laure Berti-Equille; Halima Bensmail,Diabetes is a leading health problem inthe developed world. The recent surge of wealth inQatar has made it one of the most vulnerable nations to diabetes and related diseases.Recent technological advances in 1H nuclear magnetic resonance (NMR) spectroscopytechniques for metabolomics profiling offer a great opportunity for biomarkers discovery tobetter understand the disease. Using this technology; we present in this study; an integrativeapproach with a newly proposed algorithm named Kernel Spectral Clustering (KSC) todiscover new metabolites and possibly new biomarkers. We performed an integrativeanalysis of 1H NMR spectras measured in urine; from 348 participants of the QatarMetabolomics Study on Diabetes (QMDiab). Our analyses revealed grouped metabolitesthat correlate with diabetes and identified specific metabolites affected by antidiabetes …,Database and Expert Systems Applications (DEXA); 2016 27th International Workshop on,2016,*
Scaling up truth discovery,Laure Berti-Equille,The evolution of the Web from a technology platform to a social ecosystem has resulted inunprecedented data volumes being continuously generated; exchanged; and consumed.User-generated content on the Web is massive; highly dynamic; and characterized by acombination of factual data and opinion data. False information; rumors; and fake contentscan be easily spread across multiple sources; making it hard to distinguish between what istrue and what is not. Truth discovery (also known as fact-checking) has recently gained lot ofinterest from Data Science communities. This tutorial will attempt to cover recent work ontruth-finding and how it can scale Big Data. We will provide a broad overview with newinsights; highlighting the progress made on truth discovery from information extraction; dataand knowledge fusion; as well as modeling of misinformation dynamics in social networks …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,*
Qurb: Qatar Urban Analytics,Laure Berti-Equille; Sofiane Abbar; Jaideep Srivastava; Sanjay Chawla; Javier Borge-Holthoefer; Hossam M Hammady,Doha is one of the fastest growing cities of the world with a population that has increased bynearly 40% in the last five years. There are two significant trends that are relevant to ourproposal. First; the government of Qatar is actively engaged in embracing the use of fine-grained data to “sense” the city for maintaining current services and future planning toensure a high standard of living for its residents. In this line; QCRI has initiated severalresearch projects related to urban computing to better understand and predict traffic mobilitypatterns in the city of Doha [1]. Second trend is the high degree of social media participationof the populace; providing a significant amount of time-oriented social sensing of the alltypes of events unfolding in the city. A key element of our vision is to integrate data fromphysical and social sensing; into what we call socio-physical sensing. Another key …,Qatar Foundation Annual Research Conference Proceedings,2016,*
Discovering the Truth on the Web Data: One Facet of Data Forensics,Mouhamadou Lamine Ba; Laure Berti-Equille; Hossam M Hammady,Data Forensics with Analytics; or DAFNA for short; is an ambitious project initiated by theData Analytics Research Group in Qatar Computing Research Institute; Hamad Bin KhalifaUniversity. It main goal is to provide effective algorithms and tools for determining theveracity of structured information when they originate from multiple sources. The ability toefficiently estimate the veracity of data; along with the reliability level of the informationsources; is a challenging problem with many real-world use cases (eg; data fusion; socialdata analytics; rumour detection; etc.) in which users rely on a semi-automated dataextraction and integration process in order to consume high quality information for personalor business purposes. DAFNA's vision is to provide a suite of tools for Data Forensics andinvestigate various research topics such as fact-checking and truth discovery and their …,Qatar Foundation Annual Research Conference Proceedings,2016,*
VERA: A Platform for Estimating the Veracity of Web Information,Mouhamadou Lamine Ba; Laure Berti-Equille; Hossam M Hammady,ABSTRACT Multiple Web information sources often claim conflicting data. Estimating theveracity of data is difficult on the Web; in particular when no prior knowledge neither aboutthe sources nor about the claims is available. However; exploring the space of conflictinginformation and the polarity of Web sources claiming them may be relevant in this context; aswell as ensembling multiple truth discovery methods. This demo presents VERA; a Web-based platform that supports event; entity and relation extraction from the Web given a userquery; systematically processes the extracted claims; expands the search for gathering moreevidences and conflicting or confirming information. VERA combines multiple truth findingalgorithms with active learning and returns data veracity and controversy scores. It alsodetermines the trustworthiness scores of Web sources for the given query. VERA will be …,*,2016,*
Un cadre collaboratif pour la segmentation et la classification d'images de télédétection.,Andrés Troya-Galvis; Pierre Gançarski; Laure Berti-Équille,Résumé. Dans cet article nous présentons CoSC; un cadre collaboratif pour lasegmentation et la classification d'images de télédétection permettant d'extraire les objetsd'une classe thématique donnée. Le processus de collaboration est guidé par la qualité desdonnées évaluée par des critères d'homogénéité ainsi que des critères implicitement liés àla sémantique des objets afin d'extraire une classe thématique donnée. Nos expériencesmontrent que CoSC atteint des bons résultats en termes de classification; et améliorenotablement la segmentation de l'image de manière globale.,EGC,2016,*
Quality of Web Data and Quality of Big Data: Open Problems,Laure Berti-Equille Monica Scannapieco,Searching and using the information stored on billions of Web pages poses significantchallenges; because this information and related semantics are usually more complex and dynamicthan the information that traditional database management systems store [304; 506]. As an evolvingcollection of interrelated files on one or more Web servers; Web data is extremely rich anddiverse; combining multiple types of media and data … Limited access; due to the lack ofhigh-quality keyword-based searches and the lack of deep Web access [423; 506]: users canhardly know which identifiers are used and are available for the construction of their queries.Furthermore; domain experts might not be able to express their queries in a structured form; althoughthey have a very precise idea of what kind of results they would like to retrieve [599; 600;679] … Limited knowledge of the various IQ problems existing in the Web data: for …,*,2016,*
Rheem: Enabling Multi-Platform Task Execution,Mohammed J. Zaki Divy Agrawal; Lamine Ba; Laure Berti-Equille; Sanjay Chawl; Ahmed Elmagarmid;Hossam Hammady;Yasser Idris Zoi Kaoudi; Zuhair Khayyat; Sebastian Kruse; Mourad Ouzzani;Paolo Papotti;Jorge-Arnulfo Quiané-Ruiz;Nan Tang,*,SIGMOD 2016,2016,*
Towards Principled Data Science Assessment-The Personal Data Science Process (PdsP).,Ismael Caballero; Laure Berti-Equille; Mario Piattini,Data scientist is considered the sexiest job in the world of the 21st century (Davenport andPatil 2012). With the growth of Big Data and the increasing demand of Big Dataprofessionals; it is becoming more important than ever to describe the required skills thatany worker should have in order to perform successfully the functions related to DataScience.(Partnership 2014) classifies these skills into two groups: Hard Skills (Subjectmatter expertise; math and statistics knowledge and data and technical skills) and soft skills(problem solving; storytelling; collaboration; creativity; communication andcuriosity)(Partnership 2014) Due to the great impact that any decision taken on data couldhave for the organization; it is paramount to make available data with adequate levels ofquality for the tasks at hand. This is not only a matter of reactively cleaning the data; but to …,ICEIS (1),2015,*
Data processing for controlling data quality on surface water quality assessment,Eva Carmina Serrano Balderas; Laure Berti-Equille; Corinne Grac; Maria Aurora Armienta Hernandez,Toggle navigation. HAL: HAL; HALSHS; TEL; MédiHAL; Liste des portails; AURéHAL; API;Documentation. Episciences.org; Sciencesconf.org; Support. Connexion: Connexion; Créerun compte; Mot de passe oublié ? Login oublié ? fr; en. Accueil; Dépôt; Consultation: Lesderniers dépôts; Par type de publication; Par discipline; Par année de publication; Parstructure de recherche; Les portails de l'archive; Les collections. Recherche; Documentation:Tutoriels; Compte et profil: Pourquoi créer un compte et un profil dans HAL; Créer son compteet son profil dans HAL; Modifier son compte ou son profil dans HAL; Modifier son mot depasse; Login ou mot de passe oublié; Les droits associés au profil. Déposer: Avant decommencer; Les types de publication acceptés …,Inforsid-Atelier SI et Environnement,2014,*
Cleaning; Integrating; and Warehousing Genomic Data from Biomedical Resources,Fouzia Moussouni; Laure Berti-Équille; France le Développement,Four biotechnological advances have been accomplished in the last decade:(i) sequencingof whole genomes giving rise to the discovery of thousands of genes;(ii) functional genomicsusing high-throughput DNA microarrays to measure the expression of each of these genesin multiple physiological and environmental conditions;(iii) scaling of proteins usingProteome to map all the proteins produced by a genome; and (iv) the dynamics of thesegenes and proteins in a network of interactions that gives life to any biological activity andphenotype. These major breakthroughs resulted in the massive collection of data in the fieldof life sciences. Considerable efforts have been made to sort; curate; and integrate everyrelevant piece of information from multiple information sources in order to understandcomplex biological phenomena. Biomedical researchers spend a phenomenal time to …,Biological Knowledge Discovery Handbook: Preprocessing; Mining; and Postprocessing of Biological Data,2013,*
Proceedings of the 17th International Conference on Information Quality,Laure Berti-Equille; Isabelle Comyn-Wattiau; Monica Scannapieco,Conférence ICIQ 2012.,*,2012,*
Multi-scale data integration challenges in the observational science data space,Laure Berti-Équille,Abstract In Europe; more than one thousand of laboratories intensively collect data tomeasure various properties of the Earth. Scientists observe environmental conditions;ecosystems and biological species. The ability to understand complex phenomena (eg;global warming) and predict trends from spatio-temporal data becomes a major issue inobservational science. However; theoretical and technical advances in multi-scale dataintegration are necessary to achieve this. This paper will present some challenging researchdirections for integrating such massive multi-scale scientific data. Zusammenfassung InEuropa sammeln mehr als tausend Laboratorien Daten; um verschiedene Eigenschaftender Erde zu messen. Die Fähigkeit; komplexe Phänomene (z. B. die globale Erwärmung) zuverstehen und Trends aus räumlich-zeitlichen Daten zu prognostizieren ist ein wichtiges …,it-Information Technology,2012,*
ADVISU: interactive visualization of anomalies and dependencies from massive scientific datasets,Noël Novelli; Laure Berti-Équille; Christophe Hurter,In this demo; we present ADVISU (Anomaly and Dependency VISUalization); a powerfulinteractive system for visual analytics from massive datasets. ADVISU efficiently computesdifferent types of dependencies (FDs; CFDs) and detects data anomalies from databases oflarge size; ie; up to several thousands of attributes and millions of records. Real-time andscalable computational methods have been implemented in ADVISU to ensure interactivityand the demonstration is intended to show how these methods scale up for realworldmassive scientific datasets in astrophysical and oceanographic application domains.ADVISU provides the users informative and interactive graphical interfaces for visualizingdata dependencies and anomalies. It enables the analysis to be refined interactively whilerecomputing the dependencies and anomalies in user selected subspaces with good …,EGC 2012; 12ème Conférence Internationale Francophone sur l'Extraction et la Gestion des Connaissances,2012,*
Application de mesures de distance pour la détection de problèmes de qualité de données,Laure Berti-Equille,Berti-Equille Laure (ed.).(2012). Application de mesures de distance pour la détection deproblèmes de qualité de données. In: Herschel M.; Berti-Equille Laure. La qualité et lagouvernance des données au service de la performance des entreprises. Cachan: Lavoisier-Hermes Science; 145-177.(Informatique et SI). ISBN 978-2-7462-2510-7,*,2012,*
Mesurer et évaluer la qualité des données et des connaissances,Jérôme Azé; Nicolas Béchet; Laure Berti-Équille; Sylvie Guillaume; Mathieu Roche; Fatiha Sais,Depuis plusieurs années; les ateliers QDC (Qualité des Données et des Connaissances) etEvalECD (évaluation des méthodes d'Extraction de Connaissances dans les Données) sontorganisés dans le cadre de la conférence EGC (Extraction et Gestion des Connaissances).Lors de l'édition 2011; nous avons rassemblé ces ateliers autour d'un thème unique lié à laqualité des données et des connaissances et à leur évaluation. Cette problématique estdevenue un des sujets d'intérêt tout à la fois émergent dans le domaine de la recherche etcritique pour les entreprises. Dans le même objectif; le numéro spécial de cette revue RNTIdécrit des approches d'analyse et d'évaluation de la qualité au sens large; tant en fouille dedonnées qu'en gestion des connaissances:-préparation des données (analyse de la qualitédes données; nettoyage des données; méthodologies de prétraitement; métriques d' …,*,2012,*
Application de mesures de distance pour la détection de problèmes de qualité de données,Melanie Herschel; Laure Berti-Équille,Avec la multiplication des sources d'informations disponibles et l'accroissement desvolumes et flux de données potentiellement accessibles; la qualité des données et; au senslarge; la qualité des informations n'ont cessé de prendre une place de premier plan tant auniveau académique qu'au sein des entreprises. Si l'analyse des données; l'extraction deconnaissances à partir des données et la prise de décision peuvent être réalisées sur desdonnées inexactes; incomplètes; ambiguës et de qualité médiocre; on peut alors s'interroger sur le sens à donner aux résultats de ces analyses et remettre en cause; à justetitre; la qualité des connaissances ainsi" élaborées"; tout comme le bien-fondé desdécisions prises. Aujourd'hui; il n'est donc plus question de négliger les données mais; bienau contraire; d'évaluer et de contrôler leur qualité dans les systèmes d'information; les …,*,2012,*
La qualité des systèmes d'information-analyse et évaluation,Jacky Akoka; Laure Berti-Equille; Isabelle Comyn-Wattiau,L? ampleur des investissements des entreprises dans leurs systèmes d? information et leurdépendance vis-à-vis de ces derniers sont de nature à engendrer une action spécifique demanagement de la qualité de ces systèmes d? information. L? objectif de cet article est defaire un rapide tour d? horizon des différents aspects couverts par la qualité des systèmesd? information. Avec la multiplication des sources d'informations disponibles etl'accroissement des volumes et flux de données potentiellement accessibles; la qualité desdonnées est aujourd? hui de première importance. Si l? analyse des données; l? extractionde connaissances et la prise de décision peuvent être facilement réalisées à partir dedonnées inexactes; incomplètes; ambiguës et de qualité médiocre; on peut s? interroger surle sens à donner à leurs résultats et remettre en cause; à juste titre; la qualité des …,*,2010,*
Challenges of quality-driven resource discovery,Bernd Amann; Laure Berti-Equille; Zoé Lacroix; María-Esther Vidal,Abstract We report on the concluding panel of the Third International Workshop on ResourceDiscovery. The panel followed two invited presentations that addressed the problem ofquality in the context of resource discovery. They are Assuring Quality of Service and Qualityof Data: New Challenges for Service and Resource Discovery by Laure Berti-Equille andOptimization Techniques for QoS-Aware Workflow Realization in Web Services Context byJoyce El Haddad. The questions discussed by the panelists covered modeling issues;formats; languages; semantics; applications; and benchmarks.,International Workshop on Resource Discovery,2010,*
Assuring Quality of Service and Quality of Data: New Challenges for Service and Resource Discovery,Laure Berti-Équille; Zoé Lacroix; Maria-Esther Vidal,*,Zoè Lacroix,2010,*
Modèle décisionnel basé sur la qualité des données pour sélectionner les règles d'associations légitimement intéressantes.,Laure Berti-Équille; Campus Universitaire de Beaulieu IRISA,Résumé. Dans cet article nous proposons d'exploiter des mesures décrivant la qualité desdonnées pour définir la qualité des règles d'associations résultant d'un processus de fouille.Nous proposons un modèle décisionnel probabiliste basé sur le coût de la sélection derègles légitimement; potentiellement intéressantes ou inintéressantes si la qualité desdonnées à l'origine de leur calcul est bonne; moyenne ou douteuse. Les expériences sur lesdonnées de KDD-CUP-98 montrent que les 10 meilleures règles sélectionnées d'aprèsleurs mesures de support et confiance ne sont intéressantes que dans le cas où la qualitéde leurs données est correcte voire améliorée.,EGC,2006,*
QUADRIS: Qualité des données dans les systèmes d'informations multi-sources,Laure Berti-Équille; J Akoka; O Boucelma; Mokrane Bouzeghoub; I Comyn-Wattiau; C Cosquer; Françoise Guisnel; Zoubida Kedad; N Nugier; Veronika Peralta; S Sisaid-Cherfi; V Thion-Goasdoué,Toggle navigation. HAL: HAL; HALSHS; TEL; MédiHAL; Liste des portails; AURéHAL; API;Documentation. Episciences.org; Sciencesconf.org; Support. Connexion: Connexion; Créerun compte; Mot de passe oublié ? Login oublié ? fr; en. Accueil; Dépôt; Consultation: Lesderniers dépôts; Par type de publication; Par discipline; Par année de publication; Parstructure de recherche; Les portails de l'archive; Les collections. Recherche; Documentation:Tutoriels; Compte et profil: Pourquoi créer un compte et un profil dans HAL; Créer son compteet son profil dans HAL; Modifier son compte ou son profil dans HAL; Modifier son mot depasse; Login ou mot de passe oublié; Les droits associés au profil. Déposer: Avant decommencer; Les types de publication acceptés …,*,2006,*
Proceedings of the 2nd Workshop on Data and Knowledge Quality,Laure Berti-Équille; Fabrice Guillet,Toggle navigation. HAL: HAL; HALSHS; TEL; MédiHAL; Liste des portails; AURéHAL; API;Documentation. Episciences.org; Sciencesconf.org; Support. Connexion: Connexion; Créerun compte; Mot de passe oublié ? Login oublié ? fr; en. Accueil; Dépôt; Consultation: Lesderniers dépôts; Par type de publication; Par discipline; Par année de publication; Parstructure de recherche; Les portails de l'archive; Les collections. Recherche; Documentation:Tutoriels; Compte et profil: Pourquoi créer un compte et un profil dans HAL; Créer son compteet son profil dans HAL; Modifier son compte ou son profil dans HAL; Modifier son mot depasse; Login ou mot de passe oublié; Les droits associés au profil. Déposer: Avant decommencer; Les types de publication acceptés …,*,2006,*
Qualité des données multi-sources: un aperçu des techniques issues du monde académique,L Berti-Equille,*,Journées CRM; CNAM,2005,*
Nettoyage des données XML: combien ça coûte?,Laure Berti-Équille,Résumé. L'objectif de cet article est de présenter un travail en cours qui consiste à proposer;implanter et valider expérimentalement un modèle pour estimer le coût d'un processus denettoyage de documents XML. Notre approche de calcul de coût est basée sur une méthodepar calibration selon une analyse probabiliste. Pour cela; nous proposons de calculer desprobabilités de pollution et au préalable de détection des différents types de pollutions. Pourvalider notre modèle; nous avons choisi de polluer artificiellement une collection dedonnées XML avec l'ensemble des types d'erreurs possibles (erreurs typographiques; ajoutde doublons; de valeurs manquantes; tronquées; censurées; etc.) et d'estimer; grâce aumodèle proposé; le nombre et le coût des opérations nécessaires au nettoyage desdonnées afin de proposer des stratégies de réparation ciblées et économes. Les …,EGC (Ateliers),2005,*
Proceedings of the 1st Workshop on Data and Knowledge Quality (DKQ 2005),Laure Berti-Équille; Fabrice Guillet,Toggle navigation. HAL: HAL; HALSHS; TEL; MédiHAL; Liste des portails; AURéHAL; API;Documentation. Episciences.org; Sciencesconf.org; Support. Connexion: Connexion; Créerun compte; Mot de passe oublié ? Login oublié ? fr; en. Accueil; Dépôt; Consultation: Lesderniers dépôts; Par type de publication; Par discipline; Par année de publication; Parstructure de recherche; Les portails de l'archive; Les collections. Recherche; Documentation:Tutoriels; Compte et profil: Pourquoi créer un compte et un profil dans HAL; Créer son compteet son profil dans HAL; Modifier son compte ou son profil dans HAL; Modifier son mot depasse; Login ou mot de passe oublié; Les droits associés au profil. Déposer: Avant decommencer; Les types de publication acceptés …,*,2005,*
Recherche par le contenu dans une base d’images fixes: l’intérêt des regles d’association,Anicet Kouomou Choupo; Annie Morin; Laure Berti-Equille,Résumé. Une base d'images fixes peut être décrite de plusieurs façons; notamment par desdescripteurs visuels globaux de couleur; de texture; ou de forme (niveau pixel). Lesrequêtes les plus fréquentes impliquent et combinent les résultats de plusieurs types dedescripteurs: par exemple;” retrouver toutes les images ayant une couleur et une texturesemblablesa celles d'une image requête donnée”. Pour retrouver plus efficacement et plusrapidement une image dans une base d'images fixes; nous exploitons des combinaisonsappropriées de descripteurs. Dans ce papier; nous étudions l'intérêt des reglesd'association entre clusters de descripteurs lors de la recherche dans une base d'imagesfixes. Apres avoir décrit le systeme de recherche qui nous sert de cadre d'étude; nous nousfocalisons essentiellement sur le temps de réponsea des requêtes puis nous confrontons …,Atelier n 6,2004,*
Ingénierie des systèmes d’information,JACKY AKOKA; LAURE BERTI-ÉQUILLE; ISABELLE COMYN-WATTIAU; PABLO BECKER; HERNAN MOLINA; LUIS OLSINA; MALIKA GRIM-YEFSAH; CAMILLE ROSENTHAL-SABROUX; VIRGINIE THION-GOASDOUÉ; FABIAN PANSE; NORBERT RITTER; FERNANDO LEMOS; MOHAMED REDA BOUADJENEK,9 La qualité des systèmes d'information. Vers une vision plus intégrée Towards an integratedvision of information system quality ISABELLE COMYN-WATTIAU; JACKY AKOKA; LAUREBERTI-ÉQUILLE … 33 Measurement and evaluation as a quality driver Mesure et évaluationdes outils de la qualité PABLO BECKER; HERNAN MOLINA; LUIS OLSINA … 63 Évaluationde la qualité d'un processus métier à l'aide d'informations issues de réseaux informels Businessprocess quality evaluation with informations of an informal network MALIKA GRIM-YEFSAH;CAMILLE ROSENTHAL-SABROUX; VIRGINIE THION-GOASDOUÉ … 85 Relational data completenessin the presence of maybe-tuples Complétude des données relationnelles en présence de «may-be » tuples FABIAN PANSE; NORBERT RITTER … 105 Using the QBox platform to assessquality in data integration systems Utilisation de la plateforme QBox pour l'évaluation de …,Revue des sciences et technologies de l'information,2004,*
Multi-Source Model and Architecture for Quality Negotiation and Integration of Biological Data,Laure Berti-Equille; Campus Universitaire de Beaulieu IRISA,Maintaining a certain level of quality of data and data sources is challenging in distributedmultiple source environment. In practice; assessing data quality in database systems ismainly conducted by professional assessors with more and more cost-competitive auditingpractices. Well-known approaches from industrial quality management and software qualityassessment have been adapted for data quality and came up with an extension of metadatamanagement 23; 14; 30; 32; 29]. Classically; the database literature refers to data qualitymanagement as ensuring: 1) syntactic correctness (eg constraints enforcement; that prevent"garbage data" from being entered into the database) and 2) semantic correctness (ie data inthe database truthfully re ect the real world situation). This traditional approach of dataquality management has lead to techniques such as integrity constraints; concurrency …,*,2001,*
Documents; données et méta-données: une approche mixte pour un système de veille,Laure BERTI-EQUILLE; David GRAVELEAU,*,Veille stratégique scientifique et technologique. Colloque,2001,*
Merging an active database and a reflective system: Modelling a new several active meta-levels architecture,Laure Berti,Abstract. The complexity of implementing; debugging and maintaining large numbers ofrules in active database systems and the need for understanding the behaviour of rules inspite of their non-deterministic execution make us suggest applying the reflective systems'approach to an active database system. The control of execution cycles; rule's flow andrelating transactions processing can be made dynamically and introspectively by the activesystem which monitors itself [2]. Beyond actual debugging; visualisation and explanationtools-DEAR [4]; REFLEX Visual Supervisor [7]; Sentinel Debugger [3]; OLAF [10],British National Conference on Databases,1997,*
Water Quality Data Analytics,Eva Carmina Serrano Balderas; Laure Berti-Equille; Ma Aurora Armienta Hernandez; Jean-Christophe Desconnets,Abstract: Water quality monitoring is a regular practice to assess the presence of pollutantsin the water. The importance of monitoring is justified by the need to know the current state ofaquatic ecosystems to design appropriate conservative and protective actions (SerranoBalderas et al.; 2015). Data from water quality monitoring may be prone to have variousproblems (ie; incomplete; inconsistent; inaccurate; or outlying data) that may result inmisleading analysis interpretation (Berrahou et al.; 2015). Incomplete data for instance; canbe replaced by imputed values so that the statistical methods commonly used to describepatterns on water quality assessment (such as PCA; Hierarchical Classification; Kohonen-SOM) can be achieved. But imputation of missing values may impact statistical results. In thisstudy; our goal is to assess the impact of imputation methods; and more generally of pre …,*,*,*
IEEE Transactions on Big Data Special Issue Call for Papers Data Quality in Big Data: Problems and Solutions,Laure Berti-Equille,The recent emergence of Big Data ushered in many applications that are vast and varied.Though data quality problems existed before; the advent of Big Data adds new dimensionsas well as exacerbates data quality problems. We seek submissions for the September 2016special issue on Data Quality in Big Data: Problems and Solutions. The guest editors solicitpapers covering all areas of data quality issues in the context of Big Data including dataacquisition; data cleaning; semantics and meta data generation; transformations and multi-modal data fusion; data modeling and storage; query execution and workflow optimization;and analytics. Suggested topics include; but are not limited to; the following in Big Datacontext:,*,*,*
Veracity of Big Data,Laure Berti-Equille; Javier Borge-Holthoefer,Page 1. Veracity of Big Data L B tiE ill dJ i B H lth f Veracity of Big Data Laure Berti-Equille andJavier Borge-Holthoefer Qatar Computing Research Institute {lberti;jborge}@qf.org.qa Page 2.Disclaimer Aim of the tutorial: Get the big picture The algorithms of the basic approaches will besketched Please don't mind if your favorite algorithm is missing y g g The revised version of thetutorial will be available at: http://daqcri.github.io/dafna/tutorial_cikm2015/index.html 2 October19; 2015 CIKM 2015 2 Page 3. Many sources of information ay sou ces o o at o available onlineAre all these sources equally t - accurate - up-to-date up to date - and trustworthy? October 19;2015 CIKM 2015 3 Page 4. Accurate? Deep Web data quality is low X. Li; XL Dong; K. Lyons;W. Meng; and D. Srivastava. Truth Finding on the Deep Web: Is the Problem Solved? PVLDB6(2):97–108 2012 PVLDB; 6(2):97–108; 2012. October 19; 2015 …,*,*,*
Searching for the Optimal Data Preprocessing Strategy,Laure Berti-Equille,Given a dirty; large dataset; and a variety of detection/repair methods how to efficiently: 1.Detect the anomalies 2. Correct them with minimal cost (domain expert intervention; time;external master data; etc.) 3. Evaluate the impact of preparation/repair choices over ML andanalytics results.,*,*,*
Le projet SABRE: de l’ontologie à l’inférence,Jean André Benvenuti; Laure Berti–Équille; Éric Jacopin,Résumé Le projet SABRE a pour objet le développement d'un didacticiel destiné à faciliterl'apprentissage des comportements militaires dans les écoles de formation de l'Armée deTerre. La formation militaire générale des élèves officiers s' appuie sur un corpus de textesde référence et sur l'utilisation de cas concrets issus du retour d'expérience (fichesstructurées en XML); qui permettent aux formateurs d'animer une séance pédagogiquevisant à l'appropriation des comportements militaires. La conception du système a débutépar la création d'une ontologie; préalable à la réalisation d'un analyseur syntaxiquepermettant d'extraire des règles d'inférence à partir des documents XML.,*,*,*
When Protégé and Rules becomes Parsing for Learning,Jean André Benvenuti; Laure Berti–Équille; Éric Jacopin,*,*,*,*
Ontologies and mapping rules for merging data from public databanks and gene expression experiments,Laure Berti-Equille; Campus Universitaire de Beaulieu IRISA; Fouzia Moussouni,Abstract The bio-array technology allows the analysis of thousands of genes simultaneously.It gives a quite big turn to the study of genes; and allows delivering new knowledge on theirdynamics and interrelationships. However the massive production of data involvesdifficulties in their management and their analysis. Biologists currently gather; seek andcompare heterogeneous information from different sources to carry out this analysis. Theyspend a considerable time to select tools and sources; phrase their questions and decipherthe results received from each source. This process requiring a considerable manual effortmay make barrier to progress. It may also be plagued with erroneous data ormisinterpretation.,*,*,*
Veracity of Big Data: Challenges,LAURE BERTI-EQUILLE; MOUHAMADOU LAMINE BA; HOSSAM M HAMMADY,*,*,*,*
Discovering the truth in the Web Data,Mouhamadou Lamine Ba; Laure Berti-Equille; Hossam M Hammady,Data Forensics with Analytics; or DAFNA for short; is an ambitious project initiated by theData Analytics Research Group in Qatar Computing Research Institute; Hamad Bin KhalifaUniversity. It main goal is to provide effective algorithms and tools for determining theveracity of structured information when they originate from multiple sources. The ability toefficiently estimate the veracity of data; along with the reliability level of the informationsources; is a challenging problem with many real-world use cases (eg; data fusion; socialdata analytics; rumour detection; etc.) in which users rely on a semi-automated dataextraction and integration process in order to consume high quality information for personalor business purposes. DAFNA's vision is to provide a suite of tools for Data Forensics andinvestigate various research topics such as fact-checking and truth discovery and their …,*,*,*
Lessons Learned From Ontology Design (Extended Abstract—Submitted to Protégé 2006),Jean-André Benvenuti; Laure Berti–Équille; Éric Jacopin,Conclusion The set of functions presented in this paper greatly helped to automaticallydetect the need to design rules to interpret ambiguous values in our ontology (in the settingof the use of this ontology in a pedagogical session) and ultimately to complete design.These functions run in polynomial time in the number of values of the same slot across allthe instances of the same class. In the final paper; we will describe in details the lessonslearned from the design of the French Military Training ontology design and; using theprevious functions; formalize the concept of minimality for an ontology that we have beenunable to present here due to space limitations.,*,*,*
