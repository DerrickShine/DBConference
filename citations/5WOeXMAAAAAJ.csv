Column-oriented storage techniques for MapReduce,Avrilia Floratou; Jignesh M Patel; Eugene J Shekita; Sandeep Tata,Abstract Users of MapReduce often run into performance problems when they scale up theirworkloads. Many of the problems they encounter can be overcome by applying techniqueslearned from over three decades of research on parallel DBMSs. However; translating thesetechniques to a Map-Reduce implementation such as Hadoop presents unique challengesthat can lead to new design choices. This paper describes how column-oriented storagetechniques can be incorporated in Hadoop in a way that preserves its popular programmingAPIs. We show that simply using binary storage formats in Hadoop can provide a 3xperformance boost over the naive use of text files. We then introduce a column-orientedstorage format that is compatible with the replication and scheduling constraints of Hadoopand show that it can speed up MapReduce jobs on real workloads by an order of …,Proceedings of the VLDB Endowment,2011,152
Can the elephants handle the nosql onslaught?,Avrilia Floratou; Nikhil Teletia; David J DeWitt; Jignesh M Patel; Donghui Zhang,Abstract In this new era of" big data"; traditional DBMSs are under attack from two sides. Atone end of the spectrum; the use of document store NoSQL systems (eg MongoDB)threatens to move modern Web 2.0 applications away from traditional RDBMSs. At the otherend of the spectrum; big data DSS analytics that used to be the domain of parallel RDBMSsis now under attack by another class of NoSQL data analytics systems; such as Hive onHadoop. So; are the traditional RDBMSs; aka" big elephants"; doomed as they arechallenged from both ends of this" big data" spectrum? In this paper; we compare onerepresentative NoSQL system from each end of this spectrum with SQL Server; and analyzethe performance and scalability aspects of each of these approaches (NoSQL vs. SQL) ontwo workloads (decision support analysis and interactive data-serving) that represent the …,Proceedings of the VLDB Endowment,2012,99
SQL-on-Hadoop:Full Circle Back to Shared-Nothing Database Architectures,Avrilia Floratou; Umar Farooq Minhas; Fatma Ozcan,Abstract SQL query processing for analytics over Hadoop data has recently gainedsignificant traction. Among many systems providing some SQL support over Hadoop; Hive isthe first native Hadoop system that uses an underlying framework such as MapReduce orTez to process SQL-like statements. Impala; on the other hand; represents the newemerging class of SQL-on-Hadoop systems that exploit a shared-nothing parallel databasearchitecture over Hadoop. Both systems optimize their data ingestion via columnar storage;and promote different file formats: ORC and Parquet. In this paper; we compare theperformance of these two systems by conducting a set of cluster experiments using a TPC-Hlike benchmark and two TPC-DS inspired workloads. We also closely study the I/O efficiencyof their columnar formats using a set of micro-benchmarks. Our results show that Impala is …,Proceedings of the VLDB Endowment,2014,93
Efficient and Accurate Discovery of Patterns in Sequence Datasets,Avrilia Floratou; Sandeep Tata; Jignesh M. Patel,Existing sequence mining algorithms mostly focus on mining for subsequences. However; alarge class of applications; such as biological DNA and protein motif mining; require efficientmining of “approximate” patterns that are contiguous. The few existing algorithms that can beapplied to find such contiguous approximate pattern mining have drawbacks like poorscalability; lack of guarantees in finding the pattern; and difficulty in adapting to otherapplications. In this paper; we present a new algorithm called FLexible and Accurate MotifDEtector (FLAME). FLAME is a flexible suffix-tree-based algorithm that can be used to findfrequent patterns with a variety of definitions of motif (pattern) models. It is also accurate; as italways finds the pattern if it exists. Using both real and synthetic data sets; we demonstratethat FLAME is fast; scalable; and outperforms existing algorithms on a variety of …,IEEE 26th International Conference on Data Engineering (ICDE),2010,69
When free is not really free: What does it cost to run a database workload in the cloud?,Avrilia Floratou; Jignesh M Patel; Willis Lang; Alan Halverson,Abstract The current computing trend towards cloud-based Database-as-a-Service (DaaS)as an alternative to traditional on-site relational database management systems (RDBMSs)has largely been driven by the perceived simplicity and cost-effectiveness of migrating to aDaaS. However; customers that are attracted to these DaaS alternatives may find that therange of different services and pricing options available to them add an unexpected level ofcomplexity to their decision making. Cloud service pricing models are typically 'pay-as-you-go'in which the customer is charged based on resource usage such as CPU and memoryutilization. Thus; customers considering different DaaS options must take into account howthe performance and efficiency of the DaaS will ultimately impact their monthly bill. In thispaper; we show that the current DaaS model can produce unpleasant surprises–for …,Technology Conference on Performance Evaluation and Benchmarking,2011,20
Athena: An ontology-driven system for natural language querying over relational data stores,Diptikalyan Saha; Avrilia Floratou; Karthik Sankaranarayanan; Umar Farooq Minhas; Ashish R Mittal; Fatma Özcan,Abstract In this paper; we present ATHENA; an ontology-driven system for natural languagequerying of complex relational databases. Natural language interfaces to databases enableusers easy access to data; without the need to learn a complex query language; such asSQL. ATHENA uses domain specific ontologies; which describe the semantic entities; andtheir relationships in a domain. We propose a unique two-stage approach; where the inputnatural language query (NLQ) is first translated into an intermediate query language overthe ontology; called OQL; and subsequently translated into SQL. Our two-stage approachallows us to decouple the physical layout of the data in the relational store from thesemantics of the query; providing physical independence. Moreover; ontologies providericher semantic information; such as inheritance and membership relations; that are lost …,Proceedings of the VLDB Endowment,2016,11
Dhalion: self-regulating stream processing in heron,Avrilia Floratou; Ashvin Agrawal; Bill Graham; Sriram Rao; Karthik Ramasamy,Abstract In recent years; there has been an explosion of large-scale real-time analyticsneeds and a plethora of streaming systems have been developed to support suchapplications. These systems are able to continue stream processing even when faced withhardware and software failures. However; these systems do not address some crucialchallenges facing their operators: the manual; time-consuming and error-prone tasks oftuning various configuration knobs to achieve service level objectives (SLO) as well as themaintenance of SLOs in the face of sudden; unpredictable load variation and hardware orsoftware performance degradation. In this paper; we introduce the notion of self-regulatingstreaming systems and the key properties that they must satisfy. We then present the designand evaluation of Dhalion; a system that provides self-regulation capabilities to …,Proceedings of the VLDB Endowment,2017,9
Replica Placement in Multi-Tenant Database Environments,Avrilia Floratou; Jignesh M. Patel,Database-as-a-service providers typically use replication to meet the performance andavailability guarantees demanded by their customers. A crucial problem in this context is thatof placing the replicas on machines in such a way as to meet these guarantees whileoptimally utilizing the available resources; despite having incomplete or erroneous a prioriknowledge of the workload characteristics. In contrast to previous work; we incorporate thisuncertainty by proposing online algorithms that make little or no assumptions aboutworkload characteristics. In this paper; we provide a formal definition of variants of thereplica placement problem; as well as a wide spectrum of criteria to evaluate proposedsolutions. We also designed and evaluated a number of new algorithms for the onlinereplica placement problem. We show that one of our algorithms; RkC; which is based on …,IEEE BigData Congress,2015,5
Benchmarking SQL-on-Hadoop Systems: TPC or not TPC?,Avrilia Floratou; Fatma Ozcan; Berni Schiefer,Abstract Benchmarks are important tools to evaluate systems; as long as their results aretransparent; reproducible and they are conducted with due diligence. Today; many SQL-on-Hadoop vendors use the data generators and the queries of existing TPC benchmarks; butfail to adhere to the rules; producing results that are not transparent. As the SQL-on-Hadoopmovement continues to gain more traction; it is important to bring some order to this “wildwest” of benchmarking. First; new rules and policies should be defined to satisfy thedemands of the new generation SQL systems. The new benchmark evaluation schemesshould be inexpensive; effective and open enough to embrace the variety of SQL-on-Hadoop systems and their corresponding vendors. Second; adhering to the new standardsrequires industry commitment and collaboration. In this paper; we discuss the problems …,In Proceedings of WBDB,2014,5
Adaptive caching in Big SQL using the hdfs cache,Avrilia Floratou; Nimrod Megiddo; Navneet Potti; Fatma Özcan; Uday Kale; Jan Schmitz-Hermes,Abstract The memory and storage hierarchy in database systems is currently undergoing aradical evolution in the context of Big Data systems. SQL-on-Hadoop systems share datawith other applications in the Big Data ecosystem by storing their data in HDFS; using openfile formats. However; they do not provide automatic caching mechanisms for storing data inmemory. In this paper; we describe the architecture of IBM Big SQL and its use of the HDFScache as an alternative to the traditional buffer pool; allowing in-memory data to be sharedwith other Big Data applications. We design novel adaptive caching algorithms for Big SQLtailored to the challenges of such an external cache scenario. Our experimental evaluationshows that only our adaptive algorithms perform well for diverse workload characteristics;and are able to adapt to evolving data access patterns. Finally; we discuss our …,Proceedings of the Seventh ACM Symposium on Cloud Computing,2016,4
Towards building wind tunnels for data center design,Avrilia Floratou; Frank Bertsch; Jignesh M Patel; Georgios Laskaris,Abstract Data center design is a tedious and expensive process. Recently; this process hasbecome even more challenging as users of cloud services expect to have guaranteed levelsof availability; durability and performance. A new challenge for the service providers is tofind the most cost-effective data center design and configuration that will accommodate theusers' expectations; on ever-changing workloads; and constantly evolving hardware andsoftware components. In this paper; we argue that data center design should become asystematic process. First; it should be done using an integrated approach that takes intoaccount both the hardware and the software interdependencies; and their impact on users'expectations. Second; it should be performed in a" wind tunnel"; which uses large-scalesimulation to systematically explore the impact of a data center configuration on both the …,Proceedings of the VLDB Endowment,2014,4
Twitter heron: Towards extensible streaming engines,Maosong Fu; Ashvin Agrawal; Avrilia Floratou; Bill Graham; Andrew Jorgensen; Mark Li; Neng Lu; Karthik Ramasamy; Sriram Rao; Cong Wang,Twitter's data centers process billions of events per day the instant the data is generated. Toachieve real-time performance; Twitter has developed Heron; a streaming engine thatprovides unparalleled performance at large scale. Heron has been recently open-sourcedand thus is now accessible to various other organizations. In this paper; we discuss thechallenges we faced when transforming Heron from a system tailored for Twitter'sapplications and software stack to a system that efficiently handles applications with diversecharacteristics on top of various Big Data platforms. Overcoming these challenges requireda careful design of the system using an extensible; modular architecture which providesflexibility to adapt to various environments and applications. Further; we describe the variousoptimizations that allow us to gain this flexibility without sacrificing performance. Finally …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,3
Finding Hidden Patterns in Sequences,Avrilia Floratou; Sandeep Tata; Jignesh M Patel,*,Sciences-New York,2010,2
Adaptive Caching Algorithms for Big Data Systems,Avrilia Floratou; Nimrod Megiddo; Navneet Potti; Fatma Özcan; Uday Kale; Jan Schmitz-Hermes,Abstract Today's Big Data platforms have enabled the democratization of data by allowingdata sharing among various data processing frameworks and applications that run in thesame platform. This data and resource sharing; combined with the fact that most applicationstend to access a hot set of the data has led to the development of external; in-memory;distributed caching frameworks. In this paper; we develop online; adaptive algorithms forexternal caches. Our caching algorithms take into account the workload access pattern; andthe cost of insertions in the external caching framework when making cache insertion andreplacement decisions. We provide both a detailed simulation study as well as clusterexperiments on IBM Big SQL; and show that only our adaptive algorithms perform well fordifferent workload characteristics; are able to adapt to evolving workload access patterns …,*,2015,1
Transforming an ontology query to an sql query,*,Abstract A computer-implemented method according to one embodiment includes receivingan ontology language query; receiving a mapping of an ontology to a relational database;and generating a structured query language (SQL) query; utilizing the ontology languagequery and the mapping of the ontology to the relational database.,*,2018,*
Caching policies for selection and replacement of objects,*,In one embodiment; a computer-implemented method includes inserting a set of accessedobjects into a cache; where the set of accessed objects varies in size. An object includes aset of object components; and responsive to receiving a request to access the object; it isdetermined that the object does not fit into the cache given the set of accessed objects and atotal size of the cache. A heuristic algorithm is applied; by a computer processor; to identifyin the set of object components one or more object components for insertion into the cache.The heuristic algorithm considers at least a priority of the object compared to priorities of oneor more objects in the set of accessed objects. The one or more object components areinserted into the cache.,*,2017,*
No data left behind: real-time insights from a complex data ecosystem,Manos Karpathiotakis; Avrilia Floratou; Fatma Özcan; Anastasia Ailamaki,Abstract The typical enterprise data architecture consists of several actively updated datasources (eg; NoSQL systems; data warehouses); and a central data lake such as HDFS; inwhich all the data is periodically loaded through ETL processes. To simplify queryprocessing; state-of-the-art data analysis approaches solely operate on top of the local;historical data in the data lake; and ignore the fresh tail end of data that resides in theoriginal remote sources. However; as many business operations depend on real-timeanalytics; this approach is no longer viable. The alternative is hand-crafting the analysis taskto explicitly consider the characteristics of the various data sources and identify optimizationopportunities; rendering the overall analysis non-declarative and convoluted. Based on ourexperiences operating in data lake environments; we design System-PV; a real-time …,Proceedings of the 2017 Symposium on Cloud Computing,2017,*
Self-Regulating Streaming Systems: Challenges and Opportunities,Avrilia Floratou; Ashvin Agrawal,Abstract In recent years; stream processing systems have been deployed in almost everyorganization due to the explosion of large-scale analytics applications. Our discussions withusers of these systems within Microsoft and Twitter have revealed that a major challengewith these frameworks is to tune them in order to meet the required performance and alsomaintain this level of performance over time. In this paper; we present the open problemsand challenges in supporting streaming systems that self-regulate. Such systemsautomatically adjust their configuration to meet service level objectives (SLOs) even in thepresence of external load variations or internal faults such as slow hardware. To addresssome of these challenges; we propose using machine learning techniques such assupervised learning and reinforcement learning which can potentially further improve the …,Proceedings of the International Workshop on Real-Time Business Intelligence and Analytics,2017,*
Towards Systematic Data Center Design,Avrilia Floratou,Data centers are changing fast. The rapid increase of reliance on cloud computing;particularly in the context of “big data” applications; inevitably leads to the following fact: datacenters must constantly update and adapt to the increased user requirements. This factbecomes more evident if we consider today's cloud environments. The users of theseenvironments have very strict requirements: they expect to have access to specific hardwareresources (IOPs; memory capacity; number of CPU cores; network bandwidth); they demanddata availability and durability guarantees; defined quantitatively in Service-LevelAgreements (SLAs)[1; 2]; and they will soon expect to get concrete performance guaranteesdefined in performance-based SLAs (eg [4; 5]). Given these user expectations; the naturalquestion to ask is the following:“How should we design the data centers of the future?” …,7th biennial Conference on Innovative Data Systems Research (CIDR),2014,*
Robust Data Center Design Using Large-scale Simulation,Frank Bertsch; Avrilia Floratou; Jignesh M. Patel,*,ACM Symposium on Cloud Computing,2014,*
Online Replica Placement in Cloud Environments,Avrilia Floratou; Navneet Potti; Jignesh M. Patel,*,ACM Symposium on Cloud Computing,2014,*
High-performance cloud data management,Avrilia Floratou,Abstract Over the past decade; as industry in nearly every sector of the economy has movedto a data-driven world; there has been an explosion in the volume of data and the need forricher and more flexible data processing tools and environments. Processing and analyzinglarge volumes of data has become easier over the last few years; and today there are manydistributed data processing tools and cloud computing platforms that can be readily used toanalyze large databases. In other words; users now have access to a variety of open-sourcetools for" big data" management; tailored for different types of workloads. Most of thesesystems can be easily deployed to large clusters and can scale to thousands of nodes.Moreover; users do not need to purchase expensive hardware to deploy their applications.There are a variety of cloud computing platforms; adjusted for different types of workloads …,*,2013,*
