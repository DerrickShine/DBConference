Self-tuning histograms: Building histograms without looking at data,Ashraf Aboulnaga; Surajit Chaudhuri,Abstract In this paper; we introduce self-tuning histograms. Although similar in structure totraditional histograms; these histograms infer data distributions not by examining the data ora sample thereof; but by using feedback from the query execution engine about the actualselectivity of range selection operators to progressively refine the histogram. Since the costof building and maintaining self-tuning histograms is independent of the data size; self-tuning histograms provide a remarkably inexpensive way to construct histograms for largedata sets with little up-front costs. Self-tuning histograms are particularly attractive as analternative to multi-dimensional traditional histograms that capture dependencies betweenattributes but are prohibitively expensive to build and maintain. In this paper; we describethe techniques for initializing and refining self-tuning histograms. Our experimental results …,ACM SIGMOD Record,1999,324
CORDS: automatic discovery of correlations and soft functional dependencies,Ihab F Ilyas; Volker Markl; Peter Haas; Paul Brown; Ashraf Aboulnaga,Abstract The rich dependency structure found in the columns of real-world relationaldatabases can be exploited to great advantage; but can also cause query optimizers---whichusually assume that columns are statistically independent---to underestimate theselectivities of conjunctive predicates by orders of magnitude. We introduce CORDS; anefficient and scalable tool for automatic discovery of correlations and soft functionaldependencies between columns. CORDS searches for column pairs that might haveinteresting and useful dependency relations by systematically enumerating candidate pairsand simultaneously pruning unpromising candidates using a flexible set of heuristics. Arobust chi-squared analysis is applied to a sample of column values in order to identifycorrelations; and the number of distinct values in the sampled columns is analyzed to …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,275
Estimating the selectivity of XML path expressions for internet scale applications,Ashraf Aboulnaga; Alaa R Alameldeen; Jeffrey F Naughton,Abstract Data on the Internet is increasingly presented in XML format. This allows for novelapplications that query all this data using some XML query language. All XML querylanguages use path expressions to navigate through the tree structure of the data.Estimating the selectivity of these path expressions is therefore essential for optimizingqueries in these languages. In this paper; we propose two techniques for capturing thestructure of complex large-scale XML data as would be handled by Internet-scaleapplications in a small amount of memory for estimating the selectivity of XML pathexpressions: summarized path trees and summarized Markov tables. We experimentallydemonstrate the accuracy of our proposed techniques; and explore the different situationsthat would favor one technique over the other. We also demonstrate that our proposed …,VLDB,2001,263
The Niagara internet query system,Jeffrey F.  Naughton; David J.  DeWitt; David Maier; Ashraf Aboulnaga; Jianjun Chen; Leonidas Galanis; Jaewoo Kang; Rajasekar Krishnamurthy; Qiong Luo; Naveen Prakash; Ravishankar Ramamurthy; Jayavel Shanmugasundaram; Feng Tian; Kristin Tufte; Stratis Viglas; Yuan Wang; Chun Zhang; Bruce Jackson; Anurag Gupta; Rushan Chen,Abstract Recently; there has been a great deal of research into XML query languages toenable the execution of database-style queries over XML files. However; merely being anXML query-processing engine does not render a system suitable for querying the Internet. Auseful system must provide mechanisms to (a) find the XML files that are relevant to a givenquery; and (b) deal with remote data sources that either provide unpredictable data accessand transfer rates; or are infinite streams; or both. The Niagara Internet Query System wasdesigned from the bottom-up to provide these mechanisms. In this article we describe theoverall Niagara architecture; and how Niagara finds relevant XML documents by using acollaboration between the Niagara XML-QL query processor and the Niagara “text-in-context” XML search engine. The Niagara Internet Query System is public domain …,IEEE Data Eng. Bull.,2001,256
Automatic virtual machine configuration for database workloads,Ahmed A Soror; Umar Farooq Minhas; Ashraf Aboulnaga; Kenneth Salem; Peter Kokosielis; Sunil Kamath,Abstract Virtual machine monitors are becoming popular tools for the deployment ofdatabase management systems and other enterprise software. In this article; we consider acommon resource consolidation scenario in which several database management systeminstances; each running in a separate virtual machine; are sharing a common pool ofphysical computing resources. We address the problem of optimizing the performance ofthese database management systems by controlling the configurations of the virtualmachines in which they run. These virtual machine configurations determine how the sharedphysical resources will be allocated to the different database system instances. Weintroduce a virtualization design advisor that uses information about the anticipatedworkloads of each of the database systems to recommend workload-specific …,ACM Transactions on Database Systems (TODS),2010,220
Query optimization by sub-plan memoization,*,Database system query optimizers use several techniques such as histograms andsampling to estimate the result sizes of operators and sub-plans (operator trees) and thenumber of distinct values in their outputs. Instead of estimates; the invention uses the exactactual values of the result sizes and the number of distinct values in the outputs of sub-plansencountered by the optimizer. This is achieved by optimizing the query in phases. In eachphase; newly encountered sub-plans are recorded for which result size and/or distinct valueestimates are required. These sub-plans are executed at the end of the phase to determinetheir actual result sizes and the actual number of distinct values in their outputs. Insubsequent phases; the optimizer uses these actual values when it encounters the samesub-plan again.,*,2005,116
ReStore: reusing results of MapReduce jobs,Iman Elghandour; Ashraf Aboulnaga,Abstract Analyzing large scale data has emerged as an important activity for manyorganizations in the past few years. This large scale data analysis is facilitated by theMapReduce programming and execution model and its implementations; most notablyHadoop. Users of MapReduce often have analysis tasks that are too complex to express asindividual MapReduce jobs. Instead; they use high-level query languages such as Pig; Hive;or Jaql to express their complex tasks. The compilers of these languages translate queriesinto workflows of MapReduce jobs. Each job in these workflows reads its input from thedistributed file system used by the MapReduce system and produces output that is stored inthis distributed file system and read as input by the next job in the workflow. The currentpractice is to delete these intermediate results from the distributed file system at the end of …,Proceedings of the VLDB Endowment,2012,112
Xseed: Accurate and fast cardinality estimation for xpath queries,Ning Zhang; M Tamer Ozsu; Ashraf Aboulnaga; Ihab F Ilyas,We propose XSEED; a synopsis of path queries for cardinality estimation that is accurate;robust; efficient; and adaptive to memory budgets. XSEED starts from a very small kernel;and then incrementally updates information of the synopsis. With such an incrementalconstruction; a synopsis structure can be dynamically configured to accommodate differentmemory budgets. Cardinality estimation based on XSEED can be performed very efficientlyand accurately. Extensive experiments on both synthetic and real data sets show that evenwith less memory; XSEED could achieve accuracy that is an order of magnitude better thanthat of other synopsis structures. The cardinality estimation time is under 2% of the actualquerying time for a wide range of queries in all test cases.,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,106
Deploying database appliances in the cloud.,Ashraf Aboulnaga; Kenneth Salem; Ahmed A Soror; Umar Farooq Minhas; Peter Kokosielis; Sunil Kamath,Page 1. 本翻译论文源于厦门大学计算机系数据库实验室林子雨老师的云数据库技术资料专区http://dblab.xmu.edu.cn/cloud_database_view 第1 页/共12 页翻译：厦门大学计算机系教师林子雨http://www.cs.xmu.edu.cn/linziyu Deploying Database Appliances in the Cloud Ashraf Aboulnaga;Kenneth Salem; Ahmed A. Soror; Umar Farooq Minhas; Peter Kokosielis; Sunil Kamath …,IEEE Data Eng. Bull.,2009,98
Accurate estimation of the cost of spatial selections,Ashraf Aboulnaga; Jeffrey F Naughton,Optimizing queries that involve operations on spatial data requires estimating the selectivityand cost of these operations. In this paper; we focus on estimating the cost of spatialselections; or window queries; where the query windows and data objects are generalpolygons. Cost estimation techniques previously proposed in the literature only handlerectangular query windows over rectangular data objects; thus ignoring the very significantcost of exact geometry comparison (the refinement step in a" filter and refine" queryprocessing strategy). The cost of the exact geometry comparison depends on the selectivityof the filtering step and the average number of vertices in the candidate objects identified bythis step. In this paper; we introduce a new type of histogram for spatial data that capturesthe complexity and size of the spatial objects as well as their location. Capturing these …,Data Engineering; 2000. Proceedings. 16th International Conference on,2000,83
Remusdb: Transparent high availability for database systems,Umar Farooq Minhas; Shriram Rajagopalan; Brendan Cully; Ashraf Aboulnaga; Kenneth Salem; Andrew Warfield,Abstract In this paper; we present a technique for building a high-availability (HA) databasemanagement system (DBMS). The proposed technique can be applied to any DBMS withlittle or no customization; and with reasonable performance overhead. Our approach isbased on Remus; a commodity HA solution implemented in the virtualization layer; that usesasynchronous virtual machine state replication to provide transparent HA and failovercapabilities. We show that while Remus and similar systems can protect a DBMS; databaseworkloads incur a performance overhead of up to 32% as compared to an unprotectedDBMS. We identify the sources of this overhead and develop optimizations that mitigate theproblems. We present an experimental evaluation using two popular database systems andindustry standard benchmarks showing that for certain workloads; our optimized …,The VLDB Journal,2013,71
Automated statistics collection in DB2 UDB,Ashraf Aboulnaga; Peter Haas; Mokhtar Kandil; Sam Lightstone; Guy Lohman; Volker Markl; Ivan Popivanov; Vijayshankar Raman,Abstract The use of inaccurate or outdated database statistics by the query optimizer in arelational DBMS often results in a poor choice of query execution plans and henceunacceptably long query processing times. Configuration and maintenance of thesestatistics has traditionally been a time-consuming manual operation; requiring that thedatabase administrator (DBA) continually monitor query performance and data changes inorder to determine when to refresh the statistics values and when and how to adjust the setof statistics that the DBMS maintains. In this paper we describe the new Automated StatisticsCollection (ASC) component of IBM® DB2® Universal Database TM (DB2 UDB). Thisautonomic technology frees the DBA from the tedious task of manually supervising thecollection and maintenance of database statistics. ASC monitors both the update-delete …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,68
Generating Synthetic Complex-Structured XML Data.,Ashraf Aboulnaga; Jeffrey F Naughton; Chun Zhang,Abstract Synthetically generated data has always been important for evaluating andunderstanding new ideas in database research. In this paper; we describe a data generatorfor generating synthetic complex-structured XML data that allows for a high level of controlover the characteristics of the generated data. This data generator is certainly not theultimate solution to the problem of generating synthetic XML data; but we have found it veryuseful in our research on XML data management; and we believe that it can also be usefulto other researchers. Furthermore; we hope that this paper starts a discussion in the XMLcommunity about characterizing and generating XML data; and that it may serve as a firststep towards developing a commonly accepted XML data generator for our community.,WebDB,2001,68
Second-Tier Cache Management Using Write Hints.,Xuhui Li; Ashraf Aboulnaga; Kenneth Salem; Aamer Sachedina; Shaobo Gao,Abstract Storage servers; as well as storage clients; typically have large memories in whichthey cache data blocks. This creates a two-tier cache hierarchy in which the presence of afirst-tier cache (at the storage client) makes it more difficult to manage the second-tier cache(at the storage server). Many techniques have been proposed for improving themanagement of second-tier caches; but none of these techniques use the information that isprovided by writes of data blocks from the first tier to help manage the second-tier cache. Inthis paper; we illustrate how the information contained in writes from the first tier can be usedto improve the performance of the second-tier cache. In particular; we argue that there arevery different reasons why storage clients write data blocks to storage servers (eg; cleaningdirty blocks vs. limiting the time to recover from failure). These different types of writes can …,FAST,2005,64
Self-tuning histogram and database modeling,*,Building histograms by using feedback information about the execution of query workloadrather than by examining the data helps reduce the cost of building and maintaininghistograms. A method of maintaining self-tuning histograms updates histograms based onfeedback about the execution of a user query. A histogram may be initialized using anassumption of uniform distribution of data or by combining existing histograms. A histogramtuner accesses and estimated result in response to a user query generated by using thehistogram. The histogram tuner calculates an estimation error based on the result of the userquery and the estimated result. The frequencies of histogram buckets are refined based onthe estimation error. The bucket bounds of the histogram are restructured based on therefined frequencies. The method may be performed on-line after a user query or off-line …,*,2002,63
Case study of scientific data processing on a cloud using hadoop,Chen Zhang; Hans De Sterck; Ashraf Aboulnaga; Haig Djambazian; Rob Sladek,Abstract With the increasing popularity of cloud computing; Hadoop has become a widelyused open source cloud computing framework for large scale data processing. However;few efforts have been made to demonstrate the applicability of Hadoop to various real-worldapplication scenarios in fields other than server side computations such as web indexing;etc. In this paper; we use the Hadoop cloud computing framework to develop a userapplication that allows processing of scientific data on clouds. A simple extension toHadoop's MapReduce is described which allows it to handle scientific data processingproblems with arbitrary input formats and explicit control over how the input is split. Thisapproach is used to develop a Hadoop-based cloud computing application that processessequences of microscope images of live cells; and we test its performance. It is discussed …,*,2010,58
Modeling and exploiting query interactions in database systems,Mumtaz Ahmad; Ashraf Aboulnaga; Shivnath Babu; Kamesh Munagala,Abstract The typical workload in a database system consists of a mixture of multiple queriesof different types; running concurrently and interacting with each other. Hence; optimizingperformance requires reasoning about query mixes and their interactions; rather thanconsidering individual queries or query types. In this paper; we show the significant impactthat query interactions can have on workload performance. We present a new approachbased on planning experiments and statistical modeling to capture the impact of queryinteractions. This approach requires no prior assumptions about the internal workings of thedatabase system or the nature or cause of query interactions; making it portable acrosssystems. As a concrete demonstration of the potential of capturing; modeling; and exploitingquery interactions; we develop a novel interaction-aware query scheduler that targets …,Proceedings of the 17th ACM conference on Information and knowledge management,2008,58
Scalable maximum clique computation using mapreduce,Jingen Xiang; Cong Guo; Ashraf Aboulnaga,We present a scalable and fault-tolerant solution for the maximum clique problem based onthe MapReduce framework. The key contribution that enables us to effectively useMapReduce is a recursive partitioning method that partitions the graph into severalsubgraphs of similar size. After partitioning; the maximum cliques of the different partitionscan be computed independently; and the computation is sped up using a branch and boundmethod. Our experiments show that our approach leads to good scalability; which isunachievable by other partitioning methods since they result in partitions of different sizesand hence lead to load imbalance. Our method is more scalable than an MPI algorithm; andis simpler and more fault tolerant.,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,55
E-store: Fine-grained elastic partitioning for distributed transaction processing systems,Rebecca Taft; Essam Mansour; Marco Serafini; Jennie Duggan; Aaron J Elmore; Ashraf Aboulnaga; Andrew Pavlo; Michael Stonebraker,Abstract On-line transaction processing (OLTP) database management systems (DBMSs)often serve time-varying workloads due to daily; weekly or seasonal fluctuations in demand;or because of rapid growth in demand due to a company's business success. In addition;many OLTP workloads are heavily skewed to" hot" tuples or ranges of tuples. For example;the majority of NYSE volume involves only 40 stocks. To deal with such fluctuations; anOLTP DBMS needs to be elastic; that is; it must be able to expand and contract resources inresponse to load fluctuations and dynamically balance load as hot tuples vary over time.This paper presents E-Store; an elastic partitioning framework for distributed OLTP DBMSs.It automatically scales resources in response to demand spikes; periodic events; andgradual changes in an application's workload. E-Store addresses localized bottlenecks …,Proceedings of the VLDB Endowment,2014,54
Fix: Feature-based indexing technique for xml documents,Ning Zhang; M Tamer Özsu; Ihab F Ilyas; Ashraf Aboulnaga,Abstract Indexing large XML databases is crucial for efficient evaluation of XML twig queries.In this paper; we propose a feature-based indexing technique; called FIX; based on spectralgraph theory. The basic idea is that for each twig pattern in a collection of XML documents;we calculate a vector of features based on its structural properties. These features are usedas keys for the patterns and stored in a B+ tree. Given an XPath query; its feature vector isfirst calculated and looked up in the index. Then a further refinement phase is performed tofetch the final results. We experimentally study the indexing technique over both syntheticand real data sets. Our experiments show that FIX provides great pruning power and couldgain an order of magnitude performance improvement for many XPath queries over existingevaluation techniques.,Proceedings of the 32nd international conference on Very large data bases,2006,53
Arabesque: a system for distributed graph mining,Carlos HC Teixeira; Alexandre J Fonseca; Marco Serafini; Georgos Siganos; Mohammed J Zaki; Ashraf Aboulnaga,Abstract Distributed data processing platforms such as MapReduce and Pregel havesubstantially simplified the design and deployment of certain classes of distributed graphanalytics algorithms. However; these platforms do not represent a good match for distributedgraph mining problems; as for example finding frequent subgraphs in a graph. Given aninput graph; these problems require exploring a very large number of subgraphs and findingpatterns that match some" interestingness" criteria desired by the user. These algorithms arevery important for areas such as social networks; semantic web; and bioinformatics. In thispaper; we present Arabesque; the first distributed data processing platform for implementinggraph mining algorithms. Arabesque automates the process of exploring a very largenumber of subgraphs. It defines a high-level filter-process computational model that …,Proceedings of the 25th Symposium on Operating Systems Principles,2015,50
Predicting completion times of batch query workloads using interaction-aware models and simulation,Mumtaz Ahmad; Songyun Duan; Ashraf Aboulnaga; Shivnath Babu,Abstract A question that database administrators (DBAs) routinely need to answer is howlong a batch query workload will take to complete. This question arises; for example; whileplanning the execution of different report-generation workloads to fit within available timewindows. To answer this question accurately; we need to take into account that the typicalworkload in a database system consists of mixes of concurrent queries. Interactions amongdifferent queries in these mixes need to be modeled; rather than the conventional approachof considering each query separately. This paper presents a new approach for estimatingworkload completion times that takes the significant impact of query interactions intoaccount. This approach builds performance models using an experiment-driven technique;by sampling the space of possible query mixes and fitting statistical models to the …,Proceedings of the 14th International Conference on Extending Database Technology,2011,48
Interaction-aware scheduling of report-generation workloads,Mumtaz Ahmad; Ashraf Aboulnaga; Shivnath Babu; Kamesh Munagala,Abstract The typical workload in a database system consists of a mix of multiple queries ofdifferent types that run concurrently. Interactions among the different queries in a query mixcan have a significant impact on database performance. Hence; optimizing databaseperformance requires reasoning about query mixes rather than considering queriesindividually. Current database systems lack the ability to do such reasoning. We propose anew approach based on planning experiments and statistical modeling to capture the impactof query interactions. Our approach requires no prior assumptions about the internalworkings of the database system or the nature and cause of query interactions; making itportable across systems. To demonstrate the potential of modeling and exploiting queryinteractions; we have developed a novel interaction-aware query scheduler for report …,The VLDB Journal,2011,46
Q-Cop: Avoiding bad query mixes to minimize client timeouts under heavy loads,Sean Tozer; Tim Brecht; Ashraf Aboulnaga,In three-tiered web applications; some form of admission control is required to ensure thatthroughput and response times are not significantly harmed during periods of heavy load.We propose Q-Cop; a prototype system for improving admission control decisions thatconsiders a combination of the load on the system; the number of simultaneous queriesbeing executed; the actual mix of queries being executed; and the expected time a user maywait for a reply before they or their browser give up (ie; time out). Using TPC-W queries; weshow that the response times of different types of queries can vary significantly dependingnot just on the number of queries being processed but on the mix of other queries that arerunning simultaneously. We develop a model of expected query execution times thataccounts for the mix of queries being executed and integrate this model into a three-tiered …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,46
Database systems on virtual machines: How much do you lose?,Umar Farooq Minhas; Jitendra Yadav; Ashraf Aboulnaga; Kenneth Salem,Virtual machine technologies offer simple and practical mechanisms to address manymanageability problems in database systems. For example; these technologies allow forserver consolidation; easier deployment; and more flexible provisioning. Therefore;database systems are increasingly being run on virtual machines. This offers manyopportunities for researchers in self-managing database systems; but it is also important tounderstand the cost of virtualization. In this paper; we present an experimental study of theoverhead of running a database workload on a virtual machine. We show that the averageoverhead is less than 10%; and we present details of the different causes of this overhead.Our study shows that the manageability benefits of virtualization come at an acceptable cost.,Data Engineering Workshop; 2008. ICDEW 2008. IEEE 24th International Conference on,2008,44
A bayesian approach to online performance modeling for database appliances using gaussian models,Muhammad Bilal Sheikh; Umar Farooq Minhas; Omar Zia Khan; Ashraf Aboulnaga; Pascal Poupart; David J Taylor,Abstract In order to meet service level agreements (SLAs) and to maintain peak performancefor database management systems (DBMS); database administrators (DBAs) need toimplement policies for effective workload scheduling; admission control; and resourceprovisioning. Accurately predicting response times of DBMS queries is necessary for a DBAto effectively achieve these goals. This task is particularly challenging due to the fact that adatabase workload typically consists of many concurrently running queries and an accuratemodel needs to capture their interactions. Additional challenges are introduced whenDBMSes are run in dynamic cloud computing environments; where workload; data; andphysical resources can change frequently; on-the-fly. Building an efficient and highlyaccurate online DBMS performance model that is robust in the face of changing …,Proceedings of the 8th ACM international conference on Autonomic computing,2011,40
Query interactions in database workloads,Mumtaz Ahmad; Ashraf Aboulnaga; Shivnath Babu,Abstract Database workloads consist of mixes of queries that run concurrently and interactwith each other. In this paper; we demonstrate that query interactions can have a significantimpact on database system performance. Hence; we argue that it is important to take theseinteractions into account when characterizing workloads; designing test cases; ordeveloping performance tuning algorithms for database systems. To capture and modelquery interactions; we propose using an experimental approach that is based on samplingthe space of possible interactions and fitting statistical models to the sampled data. Wediscuss using such an approach for database testing and tuning; and we present someopportunities and research challenges.,Proceedings of the Second International Workshop on Testing Database Systems,2009,37
Robustness in automatic physical database design,Kareem El Gebaly; Ashraf Aboulnaga,Abstract Automatic physical database design tools rely on" what-if" interfaces to the queryoptimizer to estimate the execution time of the training query workload under differentcandidate physical designs. The tools use these what-if interfaces to recommend physicaldesigns that minimize the estimated execution time of the input training workload. In thispaper; we argue that minimizing estimated execution time alone can lead to designs withinherent problems. In particular; if the optimizer makes an error in estimating the executiontime of some workload queries; then the recommended physical design may actually harmthe workload instead of benefiting it. In this sense; the physical design is risky. Moreover; ifthe production queries are slightly different from the training queries; the recommendedphysical design may not benefit them at all. In this sense; the physical design is not …,Proceedings of the 11th international conference on Extending database technology: Advances in database technology,2008,37
Towards cloud-based analytics-as-a-service (claaas) for big data analytics in the cloud,Farhana Zulkernine; Patrick Martin; Ying Zou; Michael Bauer; Femida Gwadry-Sridhar; Ashraf Aboulnaga,Data Analytics has proven its importance in knowledge discovery and decision support indifferent data and application domains. Big data analytics poses a serious challenge interms of the necessary hardware and software resources. The cloud technology today offersa promising solution to this challenge by enabling ubiquitous and scalable provisioning ofthe computing resources. However; there are further challenges that remain to be addressedsuch as the availability of the required analytic software for various application domains;estimation and subscription of necessary resources for the analytic job or workflow;management of data in the cloud; and design; verification and execution of analyticworkflows. We present a taxonomy for analytic workflow systems to highlight the importantfeatures in existing systems. Based on the taxonomy and a study of the existing analytic …,Big Data (BigData Congress); 2013 IEEE International Congress on,2013,36
System and method for updating database statistics according to query feedback,*,An autonomic tool that supervises the collection and maintenance of database statistics forquery optimization by transparently deciding what statistics to gather; when and in whatdetail to gather them. Feedback from data-driven statistics collection is simultaneouslycombined with feedback from query-driven learning-based statistics collection; to betterprocess both rapidly changing data and data that is queried frequently. The inventionmonitors table activity and decides if the data in a table has changed sufficiently to require arefresh of invalid statistics. The invention determines if the invalidity is due to correlationbetween purportedly independent data; outdated statistics; or statistics that have too fewfrequent values. Tables and column groups are ranked in order of statistical invalidity; and alimited computational budget is prioritized by ranking subsequent gathering of improved …,*,2010,36
Query optimization by sub-plan memoization,*,Database system query optimizers use several techniques such as histograms andsampling to estimate the result sizes of operators and sub-plans (operator trees) and thenumber of distinct values in their outputs. Instead of estimates; the invention uses the exactactual values of the result sizes and the number of distinct values in the outputs of sub-plansencountered by the optimizer. This is achieved by optimizing the query in phases. In eachphase; newly encountered sub-plans are recorded for which result size and/or distinct valueestimates are required. These sub-plans are executed at the end of the phase to determinetheir actual result sizes and the actual number of distinct values in their outputs. Insubsequent phases; the optimizer uses these actual values when it encounters the samesub-plan again.,*,2007,36
Schema clustering and retrieval for multi-domain pay-as-you-go data integration systems,Hatem A Mahmoud; Ashraf Aboulnaga,Abstract A data integration system offers a single interface to multiple structured datasources. Many application contexts (eg; searching structured data on the web) involve theintegration of large numbers of structured data sources. At web scale; it is impractical to usemanual or semi-automatic data integration methods; so a pay-as-you-go approach is moreappropriate. A pay-as-you-go approach entails using a fully automatic approximate dataintegration technique to provide an initial data integration system (ie; an initial mediatedschema; and initial mappings from source schemas to the mediated schema); and thenrefining the system as it gets used. Previous research has investigated automaticapproximate data integration techniques; but all existing techniques require the schemasbeing integrated to belong to the same conceptual domain. At web scale; it is impractical …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,32
spca: Scalable principal component analysis for big data on distributed platforms,Tarek Elgamal; Maysam Yabandeh; Ashraf Aboulnaga; Waleed Mustafa; Mohamed Hefeeda,Abstract Web sites; social networks; sensors; and scientific experiments currently generatemassive amounts of data. Owners of this data strive to obtain insights from it; often byapplying machine learning algorithms. Many machine learning algorithms; however; do notscale well to cope with the ever increasing volumes of data. To address this problem; weidentify several optimizations that are crucial for scaling various machine learning algorithmsin distributed settings. We apply these optimizations to the popular Principal ComponentAnalysis (PCA) algorithm. PCA is an important tool in many areas including imageprocessing; data visualization; information retrieval; and dimensionality reduction. We referto the proposed optimized PCA algorithm as scalable PCA; or sPCA. sPCA achievesscalability via employing efficient large matrix operations; effectively leveraging matrix …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,31
Database virtualization: A new frontier for database tuning and physical design,Ahmed A Soror; Ashraf Aboulnaga; Kenneth Salem,Resource virtualization is currently being employed at all levels of the IT infrastructure toimprove provisioning and manageability; with the goal of reducing total cost of ownership.This means that database systems will increasingly be run in virtualized environments;inside virtual machines. This has many benefits; but it also introduces new tuning andphysical design problems that are of interest to the database research community. In thispaper; we discuss how virtualization can benefit database systems; and we present thetuning problems it introduces; which relate to setting the new" tuning knobs" that controlresource allocation to virtual machines in the virtualized environment. We present aformulation of the visualization design problem; which focuses on setting resource allocationlevels for different database workloads statically at deployment and configuration time. An …,Data Engineering Workshop; 2007 IEEE 23rd International Conference on,2007,31
μbe: User guided source selection and schema mediation for internet scale data integration,Ashraf Aboulnaga; Kareem El Gebaly,The typical approach to data integration is to start by defining a common mediated schema;and then to map the data sources being integrated to this schema. In Internet-scale dataintegration tasks; where there may be hundreds or thousands of data sources providing dataof relevance to a particular domain; a better approach is to allow the user to discover themediated schema and the set of sources to use through an iterative exploration of the spaceof possible schemas and sources. In this paper; we present μBE; a data integration tool thathelps in this iterative exploratory process by automatically choosing the data sources toinclude in a data integration system and defining a mediated schema on these sources. Thedata integration system desired by the user may depend on several subjective and objectivecriteria; and the user guides μBE towards finding this system by iteratively solving a series …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,29
Accordion: Elastic scalability for database systems supporting distributed transactions,Marco Serafini; Essam Mansour; Ashraf Aboulnaga; Kenneth Salem; Taha Rafiq; Umar Farooq Minhas,Abstract Providing the ability to elastically use more or fewer servers on demand (scale outand scale in) as the load varies is essential for database management systems (DBMSes)deployed on today's distributed computing platforms; such as the cloud. This requiressolving the problem of dynamic (online) data placement; which has so far been addressedonly for workloads where all transactions are local to one sever. In DBMSes where ACIDtransactions can access more than one partition; distributed transactions represent a majorperformance bottleneck. Scaling out and spreading data across a larger number of serversdoes not necessarily result in a linear increase in the overall system throughput; becausetransactions that used to access only one server may become distributed. In this paper wepresent Accordion; a dynamic data placement system for partition-based DBMSes that …,Proceedings of the VLDB Endowment,2014,28
Elastic scale-out for partition-based database systems,Umar Farooq Minhas; Rui Liu; Ashraf Aboulnaga; Kenneth Salem; Jonathan Ng; Sean Robertson,An important goal for database systems today is to provide elastic scale-out; ie; the ability togrow and shrink processing capacity on demand; with varying load. Database systems aredifficult to scale since they are stateful-they manage a large database; and it is importantwhen scaling to multiple server machines to provide mechanisms so that these machinescan collaboratively manage the database and maintain its consistency. Databasepartitioning is often used to solve this problem; with each server machine being responsiblefor one partition. In this paper; we propose that the flexibility provided by a partitioned;shared nothing parallel database system can be exploited to provide elastic scale-out. Theidea is to start with a small number of server machines that manage all partitions; and toelastically scale out by dynamically adding new server machines and redistributing …,Data Engineering Workshops (ICDEW); 2012 IEEE 28th International Conference on,2012,28
Window query processing in linear quadtrees,Ashraf Aboulnaga; Walid G Aref,Abstract The linear quadtree is a spatial access method that is built by decomposing thespatial objects in a database into quadtree blocks and storing these quadtree blocks in a B-tree. The linear quadtree is very useful for geographic information systems because itprovides good query performance while using existing B-tree implementations. An algorithmand a cost model are presented for processing window queries in linear quadtrees. Thealgorithm can handle query windows of any shape in the general case of spatial databaseswith overlapping objects. The algorithm recursively decomposes the space into quadtreeblocks; and uses the quadtree blocks overlapping the query window to search the B-tree.The cost model estimates the I/O cost of processing window queries using the algorithm. Thecost model is also based on a recursive decomposition of the space; and it uses very …,Distributed and Parallel Databases,2001,26
Qshuffler: Getting the query mix right,Mumtaz Ahmad; Ashraf Aboulnaga; Shivnath Babu; Kamesh Munagala,The typical workload in a database system consists of a mixture of multiple queries ofdifferent types; running concurrently and interacting with each other. Hence; optimizingperformance requires reasoning about query mixes and their interactions; rather thanconsidering individual queries or query types. In this paper; we use such a reasoningapproach to develop a query scheduler. We treat the database system as a black box andexperimentally build a model to estimate the performance of different query mixes. Ourscheduler uses this model to decide which query mixes to schedule; with the goal ofmaximizing throughput. We experimentally demonstrate the effectiveness of our schedulerusing queries from the TPC-H benchmark on DB2.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,22
Interaction-aware prediction of business intelligence workload completion times,Mumtaz Ahmad; Songyun Duan; Ashraf Aboulnaga; Shivnath Babu,While planning the execution of report-generation workloads; database administrators oftenneed to know how long different query workloads will take to run. Database systems runmixes of multiple queries of different types concurrently. Hence; estimating the completiontime of a query workload requires reasoning about query mixes and inter-query interactionsin the mixes; rather than considering queries or query types in isolation. This paper presentsa novel approach for estimating workload completion time based on experiment-drivenmodeling and simulation of the impact of inter-query interactions. A preliminary evaluation ofthis approach with TPC-H queries on IBM DB2 shows how our approach can consistentlypredict workload completion times with good accuracy.,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,19
Packing the most onto your cloud,Ashraf Aboulnaga; Ziyu Wang; Zi Ye Zhang,Abstract Parallel dataflow programming frameworks such as Map-Reduce are increasinglybeing used for large scale data analysis on computing clouds. It is therefore becomingimportant to automatically optimize the performance of these frameworks. In this paper; wedeal with one particular optimization problem; namely scheduling sets of Map-Reduce jobson a cluster of machines. We present a scheduler that takes job characteristics into accountand finds a schedule that minimizes the total completion time of the set of jobs. Ourscheduler decides on the number of machines to assign to each job; and it tries to pack asmany jobs on the machines as the machine resources can support. To enable flexibleassignment of jobs onto machines; we run the Map-Reduce jobs in virtual machines. Ourscheduling problem is formulated as a constrained optimization problem; and we …,Proceedings of the first international workshop on Cloud data management,2009,18
Records retention in relational database systems,Ahmed A Ataullah; Ashraf Aboulnaga; Frank Wm Tompa,Abstract The recent introduction of several pieces of legislation mandating minimum andmaximum retention periods for corporate records has prompted the Enterprise ContentManagement (ECM) community to develop various records retention solutions. Recordsretention is a significant subfield of records management; and legal records retentionrequirements apply over corporate records regardless of their shape or form. Unfortunately;the scope of existing solutions has been largely limited to proper identification; classificationand retention of documents; and not of data more generally. In this paper we address theproblem of managed records retention in the context of relational database systems. Theproblem is significantly more challenging than it is for documents for several reasons.Foremost; there is no clear definition of what constitutes a business record in relational …,Proceedings of the 17th ACM conference on Information and knowledge management,2008,18
CLIC: CLient-Informed Caching for Storage Servers.,Xin Liu; Ashraf Aboulnaga; Kenneth Salem; Xuhui Li,Abstract Traditional caching policies are known to perform poorly for storage server caches.One promising approach to solving this problem is to use hints from the storage clients tomanage the storage server cache. Previous hinting approaches are ad hoc; in that apredefined reaction to specific types of hints is hard-coded into the caching policy. With adhoc approaches; it is difficult to ensure that the best hints are being used; and it is difficult toaccommodate multiple types of hints and multiple client applications. In this paper; wepropose CLient-Informed Caching (CLIC); a generic hint-based policy for managing storageserver caches. CLIC automatically interprets hints generated by storage clients andtranslates them into a server caching policy. It does this without explicit knowledge of theapplication-specific hint semantics. We demonstrate using trace-based simulation of …,FAST,2009,16
Virtualization and databases: state of the art and research challenges,Ashraf Aboulnaga; Cristiana Amza; Kenneth Salem,Abstract There is currently a lot of interest in resource virtualization as an importanttechnique for addressing the problems of manageability; reliability; and security in computersystems. Resource virtualization decouples the user's perception of hardware and softwareresources from the actual implementation of these resources. It adds a flexible andprogrammable layer of software between user applications (such as database systems) andthe resources that they use. This layer of software maps the virtual resources perceived bythe applications to real physical resources. An example of this layer of software is a virtualmachine monitor; which partitions the resources of a machine (CPU; disk; memory; network;etc.) into multiple virtual machines; and independent operating systems and applicationscan be installed on each virtual machine. The power of resource virtualization comes from …,Proceedings of the 11th international conference on Extending database technology: Advances in database technology,2008,16
A demonstration of the solid platform for social web applications,Essam Mansour; Andrei Vlad Sambra; Sandro Hawke; Maged Zereba; Sarven Capadisli; Abdurrahman Ghanem; Ashraf Aboulnaga; Tim Berners-Lee,Abstract Solid is a decentralized platform for social Web applications. In the Solid platform;users' data is managed independently of the applications that create and consume this data.Each user stores their data in a Web-accessible personal online datastore (or pod). Eachuser can have one or more pods from different pod providers; and can easily switch betweenproviders. Applications access data in users' pods using well defined protocols; and adecentralized authentication and access control mechanism guarantees the privacy of thedata. In this decentralized architecture; applications can operate on users' data wherever it isstored. Users control access to their data; and have the option to switch betweenapplications at any time. We will demonstrate the utility of Solid and how it is experiencedfrom the point of view of end users and application developers. For this; we will use a set …,Proceedings of the 25th International Conference Companion on World Wide Web,2016,14
Scalable matrix inversion using mapreduce,Jingen Xiang; Huangdong Meng; Ashraf Aboulnaga,Abstract Matrix operations are a fundamental building block of many computational tasks infields as diverse as scientific computing; machine learning; and data mining. Matrixinversion is an important matrix operation; but it is difficult to implement in today's popularparallel dataflow programming systems; such as MapReduce. The reason is that eachelement in the inverse of a matrix depends on multiple elements in the input matrix; so thecomputation is not easily partitionable. In this paper; we present a scalable and efficienttechnique for matrix inversion in MapReduce. Our technique relies on computing the LUdecomposition of the input matrix and using that decomposition to compute the requiredmatrix inverse. We present a technique for computing the LU decomposition and the matrixinverse using a pipeline of MapReduce jobs. We also present optimizations of this …,Proceedings of the 23rd international symposium on High-performance parallel and distributed computing,2014,12
Workload management for big data analytics,Ashraf Aboulnaga; Shivnath Babu,Parallel database systems and MapReduce systems (most notably Hadoop) are essentialcomponents of today's infrastructure for Big Data analytics. These systems process multipleconcurrent workloads consisting of complex user requests; where each request isassociated with an (explicit or implicit) service level objective. For example; the workload ofa particular user or application may have a higher priority than other workloads. Or; aparticular workload may have strict deadlines for the completion of its requests. Theresearch area of Workload Management focuses on ensuring that the system meets theservice level objectives of various requests while at the same time minimizing the resourcesrequired to achieve this goal. At a high level; workload management can be viewed aslooking beyond the performance of an individual request to the performance of an entire …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,12
An XML index advisor for DB2,Iman Elghandour; Ashraf Aboulnaga; Daniel C Zilio; Fei Chiang; Andrey Balmin; Kevin Beyer; Calisto Zuzarte,Abstract XML database systems are expected to handle increasingly complex queries overincreasingly large and highly structured XML databases. An important problem that needs tobe solved for these systems is how to choose the best set of indexes for a given workload.We have developed an XML Index Advisor that solves this XML index recommendationproblem and is tightly coupled with the query optimizer of the database system. We haveimplemented our XML Index Advisor for DB2. In this demonstration we showcase the newquery optimizer modes that we added to DB2; the index recommendation process; and theeffectiveness of the recommended indexes.,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,11
NoSE: Schema design for NoSQL applications,Michael Joseph Mior; Kenneth Salem; Ashraf Aboulnaga; Rui Liu,Database design is critical for high performance in relational databases and a myriad oftools exist to aid application designers in selecting an appropriate schema. While theproblem of schema optimization is also highly relevant for NoSQL databases; existing toolsfor relational databases are inadequate in that setting. Application designers wishing to usea NoSQL database instead rely on rules of thumb to select an appropriate schema. Wepresent a system for recommending database schemas for NoSQL applications. Our cost-based approach uses a novel binary integer programming formulation to guide the mappingfrom the application's conceptual data model to a database schema. We implemented aprototype of this approach for the Cassandra extensible record store. Our prototype; theNoSQL Schema Evaluator (NoSE) is able to capture rules of thumb used by expert …,IEEE Transactions on Knowledge and Data Engineering,2017,10
Building XML statistics for the hidden web,Ashraf Aboulnaga; Jeffrey F Naughton,Abstract There have been several techniques proposed for building statistics for static XMLdata. However; very little work has been done in the area of building XML statistics for datasources that export XML views of data that is stored in relational or other databases. Forsuch data sources; we need statistics that are built in an on-line manner; by observing theXML queries to the data sources and their results. In this paper; we present a technique forbuilding on-line XML statistics by observing the XPath queries issued to a data source andtheir result sizes. These XPath queries select parts of the virtual XML document representingthe XML view of the data at the data source. We convert these XPath queries to a moreabstract and generalized form that we call annotated path expressions. We present atechnique for storing these annotated path expressions and information about their …,Proceedings of the twelfth international conference on Information and knowledge management,2003,10
Dax: a widely distributed multitenant storage service for dbms hosting,Rui Liu; Ashraf Aboulnaga; Kenneth Salem,Abstract Many applications hosted on the cloud have sophisticated data management needsthat are best served by a SQL-based relational DBMS. It is not difficult to run a DBMS in thecloud; and in many cases one DBMS instance is enough to support an application'sworkload. However; a DBMS running in the cloud (or even on a local server) still needs away to persistently store its data and protect it against failures. One way to achieve this is toprovide a scalable and reliable storage service that the DBMS can access over a network.This paper describes such a service; which we call DAX. DAX relies on multi-masterreplication and Dynamo-style flexible consistency; which enables it to run in multiple datacenters and hence be disaster tolerant. Flexible consistency allows DAX to control theconsistency level of each read or write operation; choosing between strong consistency at …,Proceedings of the VLDB Endowment,2013,9
Recommending XMLTable views for XQuery workloads,Iman Elghandour; Ashraf Aboulnaga; Daniel C Zilio; Calisto Zuzarte,Abstract Physical structures; for example indexes and materialized views; can improve queryexecution performance by orders of magnitude. Hence; it is important to choose the rightconfiguration of these physical structures for a given database. In this paper; we discuss thetypes of materialized views that are suitable for an XML database. We then focus onXMLTable materialized views and present a procedure to recommend them given an XMLdatabase and a workload of XQuery queries. We have implemented our XMLTable ViewAdvisor in a prototype version based on IBM® DB2® V9. 7; which supports both relationaland XML data; and we experimentally demonstrate the effectiveness of our advisor'srecommendations.,International XML Database Symposium,2009,8
Method; system and program for prioritizing maintenance of database tables,*,There is disclosed a data processing system implemented method; a data processingsystem; and an article of manufacture for directing a data processing system to maintain adatabase table associated with an initial maintenance scheduling interval. The dataprocessing system implemented method includes selecting a randomizing factor; andselecting a new maintenance scheduling interval for the database table based on the initialmaintenance scheduling interval and the selected randomizing factor.,*,2008,8
Automatic relationship discovery in self-managing database systems,Ihab Ilyas; Volker Markl; P Haas; Paul G Brown; Ashraf Aboulnaga,In this paper; we describe CORDS; an algorithm that automatically discovers correlationsand soft functional dependencies (FDs) between pairs of columns and; based on theserelationships; determines a set of statistics to maintain. This data-driven technology is anessential complement to query-driven approaches such as LEO; helping to ensureacceptable performance during slow learning periods. CORDS focuses on column pairsbecause this greatly simplifies the algorithms; and experiments have shown that themarginal benefit of capturing n-way dependencies for n> 2 is relatively small.,Autonomic Computing; 2004. Proceedings. International Conference on,2004,8
Recommending XML physical designs for XML databases,Iman Elghandour; Ashraf Aboulnaga; Daniel C Zilio; Calisto Zuzarte,Abstract Database systems employ physical structures such as indexes and materializedviews to improve query performance; potentially by orders of magnitude. It is thereforeimportant for a database administrator to choose the appropriate configuration of thesephysical structures for a given database. XML database systems are increasingly beingused to manage semi-structured data; and XML support has been added to commercialdatabase systems. In this paper; we address the problem of automatic physical design forXML databases; which is the process of automatically selecting the best set of physicalstructures for a database and a query workload. We focus on recommending two types ofphysical structures: XML indexes and relational materialized views of XML data. We presenta design advisor for recommending XML indexes; one for recommending materialized …,The VLDB Journal,2013,7
XML index recommendation with tight optimizer coupling,Iman Elghandour; Ashraf Aboulnaga; Daniel C Zilio; Fei Chiang; Andrey Balmin; Kevin Beyer; Calisto Zuzarte,XML database systems are expected to handle increasingly complex queries overincreasingly large and highly structured XML databases. An important problem that needs tobe solved for these systems is how to choose the best set of indexes for a given workload. Inthis paper; we present an XML Index Advisor that solves this XML index recommendationproblem and has the key characteristic of being tightly coupled with the query optimizer. Werely on the optimizer to enumerate index candidates and to estimate the benefit gained frompotential index configurations. We expand the set of candidate indexes obtained from thequery optimizer to include more general indexes that can be useful for queries other thanthose in the training workload. To recommend an index configuration; we introduce two newsearch algorithms. The first algorithm finds the best set of indexes for the specific training …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,6
Clay: fine-grained adaptive partitioning for general database schemas,Marco Serafini; Rebecca Taft; Aaron J Elmore; Andrew Pavlo; Ashraf Aboulnaga; Michael Stonebraker,Abstract Transaction processing database management systems (DBMSs) are critical fortoday's data-intensive applications because they enable an organization to quickly ingestand query new information. Many of these applications exceed the capabilities of a singleserver; and thus their database has to be deployed in a distributed DBMS. The key factoraffecting such a system's performance is how the database is partitioned. If the database ispartitioned incorrectly; the number of distributed transactions can be high. Thesetransactions have to synchronize their operations over the network; which is considerablyslower and leads to poor performance. Previous work on elastic database repartitioning hasfocused on a certain class of applications whose database schema can be represented in ahierarchical tree structure. But many applications cannot be partitioned in this manner …,Proceedings of the VLDB Endowment,2016,5
Method; system and program for prioritizing maintenance of database tables,*,There is disclosed a data processing system implemented method; a data processingsystem; and an article of manufacture for directing a data processing system to maintain adatabase table associated with an initial maintenance scheduling interval. The dataprocessing system implemented method includes selecting a randomizing factor; andselecting a new maintenance scheduling interval for the database table based on the initialmaintenance scheduling interval and the selected randomizing factor.,*,2012,4
Arabesque: A system for distributed graph mining-Extended version,Carlos HC Teixeira; Alexandre J Fonseca; Marco Serafini; Georgos Siganos; Mohammed J Zaki; Ashraf Aboulnaga,Abstract: Distributed data processing platforms such as MapReduce and Pregel havesubstantially simplified the design and deployment of certain classes of distributed graphanalytics algorithms. However; these platforms do not represent a good match for distributedgraph mining problems; as for example finding frequent subgraphs in a graph. Given aninput graph; these problems require exploring a very large number of subgraphs and findingpatterns that match some" interestingness" criteria desired by the user. These algorithms arevery important for areas such as social net-works; semantic web; and bioinformatics. In thispaper; we present Arabesque; the first distributed data processing platform for implementinggraph mining algorithms. Arabesque automates the process of exploring a very largenumber of subgraphs. It defines a high-level filter-process computational model that …,arXiv preprint arXiv:1510.04233,2015,3
ALEX: Automatic link exploration in linked data,Ahmed El-Roby; Ashraf Aboulnaga,Abstract There has recently been an increase in the number of RDF knowledge basespublished on the Internet. These rich RDF data sets can be useful in answering manyqueries; but much more interesting queries can be answered by integrating information fromdifferent data sets. This has given rise to research on automatically linking different RDFdata sets representing different knowledge bases. This is challenging due to their scale andsemantic heterogeneity. Various approaches have been proposed; but there is room forimproving the quality of the generated links. In this paper; we present ALEX; a system thataims at improving the quality of links between RDF data sets by using feedback provided byusers on the answers to linked data queries. ALEX starts with a set of candidate linksobtained using any automatic linking algorithm. ALEX utilizes user feedback to discover …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,3
Semantics and Encoding of the kell-m Calculus,Rolando Blanco; Paulo Alencar; Rolando Blanco; Paulo Alencar; Ning Zhang; Ihab F Ilyas; M Tamer Özsu; Jiewen Wu; Ihab Ilyas; Grant Weddell; Shahram Esmaeilsabzali; Nancy A Day; Farhad Mavaddat; Carol Fung; Quanyan Zhu; Raouf Boutaba; Tamar Basar; Spyros Angelopoulos; Alejandro Lopez-Ortiz; Konstantinos Panagiotou; Therese Biedl; Stephane Durocher; Celine Engelbeen; Samuel Fiorini; Maxwell Young; Maxwell Young; Raouf Boutaba; Cristina Ribeiro; Gaurav Mehrotra; Gregory Vey; Areej Alhothali; Chrysanne DiMarco; Md J Alam; T Biedl; S Felsner; M Kaufmann; SG Kobourov; Alejandro Lopez-Ortiz; Alejandro Salinger; Muhammad Bilal Sheikh; Umar Farooq Minhas; Omar Zia Khan; Ashraf Aboulnaga; Pascal Poupart; David J Taylor; Eduardo S Barrenechea; Paulo SC Alencar; Rolando Blanc; Don Cowan; Arash Farzan; Shahin Kamali; Adam Fourney; Richard Mann; Michael Terry; Soroush Alamdari; Therese Biedl,Abstract We present kell-m; an asynchronous higher-order process algebra with hierarchicallocalities. The main focus of this report is on the operational semantics and behaviouralequivalences for kell-m. The operational semantics determine how systems representedusing kell-m evolve; the behavioural equivalences determine what it means for two kell-mprocesses to behave similarly. We also present and encoding of kell-m into MMC; thevariation of the-calculus as implemented in the Mobility Model Checker (MMC).,*,2011,3
Report: 4th Int'l Workshop on Self-Managing Database Systems (SMDB 2009).,Ashraf Aboulnaga; Kenneth Salem,The Fourth International Workshop on Self-Managing Database Systems took place onMarch 29th; 2009 in Shanghai; China; on the day before ICDE. The SMDB workshops bringtogether researchers and practitioners interested in making data management systemseasier to deploy and operate effectively. Topics of interest range from “traditional”management issues; such as database physical design; system tuning; and resourceallocation to more recent challenges around scalable and highly-available data services incloud computing environments. The SMDB Workshops are sponsored by the IEEE TCDEWorkgroup on Self-Managing Database Systems. The SMDB 2009 program began with akeynote presentation by James Hamilton; Vice President and Distinguished Engineer withAmazon Web Services. This was followed by five technical papers; presented in two …,IEEE Data Eng. Bull.,2009,3
Index selection for XML database systems,*,A method; computer-implemented system; and computer program product for creatingindexes over XML data managed by a database system are provided. The method;computer-implemented system; and computer program product provide for receiving aworkload for the XML data; the workload including one or more database statements;utilizing an optimizer of the database system to enumerate a set of one or more pathexpressions by creating a virtual universal index based on the workload received andmatching a path expression to the virtual universal index; and recommending one or morepath expressions from the set of one or more candidate path expressions to create theindexes over the XML data.,*,2017,2
CORDS,Ihab Ilyas; V Markl; P Haas; P Brown; A Aboulnaga,• A simple solution: build statistics on groups … • Relation R with two columns A;B … – Comparethe actual selectivity to the estimated … • The goal is to collect useful joint … – Apply pruningrules to limit # for Phase 2 … – Across all joinable tables (PK-FK pairing rule) … IF“skewed”: FILTER S with the frequent values … Build a (skew-dependent) contingency tablefor A × B from S … If correlated; RETURN (“Correlated with degree of correlation = x”) … • Falsepositives are ok (if not many) … • Basic CGS is the number of distinct … • CGS gives a betterestimate of joint … • Error due to faulty independence assumption is eliminated … – Inpractice; most error is due to independence assumption … – Future work: exploit column groupdistribution statistics … • Rank Soft FD's by their Strength … 0.9 0.92 0.94 0.96 0.98 0.99 1 24 6 … C G S (s e c o n d s ) … 100 1000 10000 100000 1000000 10000000 Sample Size,Proceedings of the 2004 ACM SIGMOD international conference on Management of data-SIGMOD,*,2
A Demonstration of Lusail: Querying Linked Data at Scale,Essam Mansour; Ibrahim Abdelaziz; Mourad Ouzzani; Ashraf Aboulnaga; Panos Kalnis,Abstract There has been a proliferation of datasets available as interlinked RDF dataaccessible through SPARQL endpoints. This has led to the emergence of variousapplications in life science; distributed social networks; and Internet of Things that need tointegrate data from multiple endpoints. We will demonstrate Lusail; a system that supportsthe need of emerging applications to access tens to hundreds of geo-distributed datasets.Lusail is a geo-distributed graph engine for querying linked RDF data. Lusail delivers out-standing performance using (i) a novel locality-aware query decomposition technique thatminimizes the intermediate data to be accessed by the subqueries; and (ii) selectivity-awareness and parallel query execution to reduce network latency and to increaseparallelism. During the demo; the audience will be able to query actually deployed RDF …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,1
Query Optimizations Over Decentralized RDF Graphs,Ibrahim Abdelaziz; Essam Mansour; Mourad Ouzzani; Ashraf Aboulnaga; Panos Kalnis,Applications in life sciences; decentralized social networks; Internet of Things; and statisticallinked dataspaces integrate data from multiple decentralized RDF graphs via SPARQLqueries. Several approaches have been proposed to optimize query processing over asmall number of heterogeneous data sources by utilizing schema information. In the case ofschema similarity and interlinks among sources; these approaches cause unnecessary dataretrieval and communication; leading to poor scalability and response time. This paperaddresses these limitations and presents Lusail; a system for scalable and efficient SPARQLquery processing over decentralized graphs. Lusail achieves scalability and low queryresponse time through various optimizations at compile and run times. At compile time; weuse a novel locality-aware query decomposition technique that maximizes the number of …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,1
A method and system for processing data,*,A system of redistributing partitions across servers having multiple partitions that eachprocess transactions. Where the transactions are related to one another and the transactionsare able to access one or a set of partitions simultaneously. The system comprising: amonitoring module operable to determine a transaction rate of the number of transactionsprocessed by the multiple partitions on the first server; an affinity module operable todetermine affinity between partitions; wherein the affinity being a measure of how oftengroup transactions access sets of respective partitions; a partition placement moduleoperable to determine a partition mapping in response to a change in a transactionworkload on at least one partition on the first server; the partition placement moduleoperable to receive input from at least one of: a server capacity estimator module; …,*,2016,1
Sapphire: Querying RDF data made simple,Ahmed El-Roby; Khaled Ammar; Ashraf Aboulnaga; Jimmy Lin,Abstract There is currently a large amount of publicly accessible structured data available asRDF data sets. For example; the Linked Open Data (LOD) cloud now consists of thousandsof RDF data sets with over 30 billion triples; and the number and size of the data sets iscontinuously growing. Many of the data sets in the LOD cloud provide public SPARQLendpoints to allow issuing queries over them. These end-points enable users to retrievedata using precise and highly expressive SPARQL queries. However; in order to do so; theuser must have sufficient knowledge about the data sets that she wishes to query; that is; thestructure of data; the vocabulary used within the data set; the exact values of literals; theirdata types; etc. Thus; while SPARQL is powerful; it is not easy to use. An alternative toSPARQL that does not require as much prior knowledge of the data is some form of …,Proceedings of the VLDB Endowment,2016,1
Database high availability using SHADOW systems,Jaemyung Kim; Kenneth Salem; Khuzaima Daudjee; Ashraf Aboulnaga; Xin Pan,Abstract Hot standby techniques are widely used to implement highly available databasesystems. These techniques make use of two separate copies of the database; an active copyand a backup that is managed by the standby. The two database copies are storedindependently and synchronized by the database systems that manage them. However;database systems deployed in computing clouds often have access to reliable persistentstorage that can be shared by multiple servers. In this paper we consider how hot standbytechniques can be improved in such settings. We present SHADOW systems; a novelapproach to hot standby high availability. Like other database systems that use sharedstorage; SHADOW systems push the task of managing database replication out of thedatabase system and into the underlying storage service; simplifying the database system …,Proceedings of the Sixth ACM Symposium on Cloud Computing,2015,1
CIMBA-Client-Integrated MicroBlogging Architecture.,Andrei Vlad Sambra; Sandro Hawke; Tim Berners-Lee; Lalana Kagal; Ashraf Aboulnaga,Abstract. Personal data ownership and interoperability for decentralized social Webapplications are currently two debated topics; especially when taking into consideration theaspects of privacy and access control. To increase data ownership; users should have thefreedom to choose where their data resides and who is allowed access to it by decouplingdata storage from the application that consumes it. Through CIMBA; we propose adecentralized architecture based on Web standards; which puts users back in control of theirown data.,International Semantic Web Conference (Posters & Demos),2014,1
RACE: a scalable and elastic parallel system for discovering repeats in very long sequences,Essam Mansour; Ahmed El-Roby; Panos Kalnis; Aron Ahmadia; Ashraf Aboulnaga,Abstract A wide range of applications; including bioinformatics; time series; and log analysis;depend on the identification of repetitions in very long sequences. The problem of findingmaximal pairs subsumes most important types of repetition-finding tasks. Existing solutionsrequire both the input sequence and its index (typically an order of magnitude larger thanthe input) to fit in memory. Moreover; they are serial algorithms with long execution time.Therefore; they are limited to small datasets; despite the fact that modern applicationsdemand orders of magnitude longer sequences. In this paper we present RACE; a parallelsystem for finding maximal pairs in very long sequences. RACE supports parallel executionon stand-alone multicore systems; in addition to scaling to thousands of nodes on clusters orsupercomputers. RACE does not require the input or the index to fit in memory; therefore …,Proceedings of the VLDB Endowment,2013,1
Towards building XML statistics for the hidden web,Ashraf Aboulnaga; Jeffrey F Naughton,Abstract There is currently a lot of interest in developing Internet query processors that canpose elaborate queries on XML data on the Web. Such query processors can query datasources that have static XML files; but they should also be able to query “hidden Web” datasources that export an XML view of data stored in a database. To optimize queries thatinvolve these hidden Web data sources; we need to have XML statistics that can be used toestimate the selectivity of queries posed to these sources. Since we can only access thedata at a hidden Web data source by issuing queries; we need to develop on-line XMLstatistics that are built by observing queries to a hidden Web data source and their resultsizes. In this paper; we assume that queries to a hidden Web data source are XPathselections from a virtual XML document that represents all the data at this source. We …,*,2003,1
Cost estimation techniques for database systems,Ashraf Aboulnaga,Abstract This dissertation is about developing advanced selectivity and cost estimationtechniques for query optimization in database systems. It addresses the following threeissues related to current trends in database research: estimating the cost of spatialselections; building histograms without looking at data; and estimating the selectivity of XMLpath expressions. The first part of this dissertation deals with estimating the cost of spatialselections; or window queries; where the query windows and the data objects are generalpolygons. Previously proposed cost estimation techniques only handle rectangular querywindows over rectangular data objects; thus ignoring the significant cost of exact geometrycomparison (the refinement step in a “filter and refine” query processing strategy). The costof the exact geometry comparison depends on the selectivity of the filtering step and the …,*,2002,1
Lusail: a system for querying linked data at scale,Ibrahim Abdelaziz; Essam Mansour; Mourad Ouzzani; Ashraf Aboulnaga; Panos Kalnis,Abstract The RDF data model allows publishing interlinked RDF datasets; where eachdataset is independently maintained and is queryable via a SPARQL endpoint. Manyapplications would benefit from querying the resulting large; decentralized; geo-distributedgraph through a federated SPARQL query processor. A crucial factor for good performancein federated query processing is pushing as much computation as possible to the localendpoints. Surprisingly; existing federated SPARQL engines are not effective at this tasksince they rely only on schema information. Consequently; they cause unnecessary dataretrieval and communication; leading to poor scalability and response time. This paperaddresses these limitations and presents Lusail; a scalable and efficient federated SPARQLsystem for querying large RDF graphs that are geo-distributed on different endpoints …,Proceedings of the VLDB Endowment,2017,*
UFeed: Refining Web Data Integration Based on User Feedback,Ahmed El-Roby; Ashraf Aboulnaga,Abstract One of the main challenges in large-scale data integration for relational schemas iscreating an accurate mediated schema; and generating accurate semantic mappingsbetween heterogeneous data sources and this mediated schema. Some applications canstart with a moderately accurate mediated schema and mappings and refine them over time;which is referred to as the pay-as-you-go approach to data integration. Creating themediated schema and mappings automatically to bootstrap the pay-as-you-go approach hasbeen extensively studied. However; refining the mediated schema and mappings is still anopen challenge because the data sources are usually heterogeneous and use diverse andsometimes ambiguous vocabularies. In this paper; we introduce UFeed; a system thatrefines relational mediated schemas and mappings based on user feedback over query …,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,2017,*
Graph Data Mining with Arabesque,Eslam Hussein; Abdurrahman Ghanem; Vinicius Vitor dos Santos Dias; Carlos HC Teixeira; Ghadeer AbuOda; Marco Serafini; Georgos Siganos; Gianmarco De Francisci Morales; Ashraf Aboulnaga; Mohammed Zaki,Abstract Graph data mining is defined as searching in an input graph for all subgraphs thatsatisfy some property that makes them interesting to the user. Examples of graph datamining problems include frequent subgraph mining; counting motifs; and enumeratingcliques. These problems differ from other graph processing problems such as PageRank orshortest path in that graph data mining requires searching through an exponential number ofsubgraphs. Most current parallel graph analytics systems do not provide good support forgraph data mining. One notable exception is Arabesque; a system that was built specificallyto support graph data mining. Arabesque provides a simple programming model to expressgraph data mining computations; and a highly scalable and efficient implementation of thismodel; scaling to billions of subgraphs on hundreds of cores. This demonstration will …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Special section on data-intensive cloud infrastructure,Ashraf Aboulnaga; Beng Chin Ooi; Patrick Valduriez,More and more individuals; companies and organizations are relying on the cloud to storeand manage their data; which translates into increasing pressure on the cloud infrastructure.Cloud data can be very diverse; including a wide variety of personal data collections; verylarge multimedia content repositories and very large datasets. Users and applicationdevelopers can be in very high numbers; with little DBMS expertise. Data-intensiveapplications can be very diverse too; with requirements ranging from basic databasecapabilities to complex analytics over big data. In particular; the pay-as-you-go model makesthe cloud attractive for supporting novel large-scale elastic applications. NoSQL solutions forthe cloud; for instance; have traded consistency and transactional guarantees for scalability.However; the grand challenge for a data-intensive cloud infrastructure is to provide ease …,The VLDB Journal,2014,*
ReStore: Reusing results of MapReduce jobs,Ashraf Aboulnaga; Dr Iman Elghandour,'Big Data'analysis has become a central activity in business and science. Companies suchas Facebook; Yahoo; and Google now own petabyte-scale data warehouses that areaccessed on a regular basis. Terabyte-scale data warehouses are now common in manysmaller organizations. This big data analysis is mostly supported by the MapReduceprogramming and execution model and its implementations; most notably Hadoop which isnow one of the major big data platforms. Users of MapReduce often have analysis tasks thatare too complex to express as one MapReduce job. Instead; they often use high-level querylanguages such as Pig Latin; Hive; or Jaql to express their complex analysis tasks. Thecompilers of these query languages translate queries into workflows of MapReduce jobs.Each job in such a workflow produces an output that is stored in the distributed file system …,Qatar Foundation Annual Research Forum,2012,*
Accurate Query Optimization by Sub-plan Memoization,Ashraf Aboulnaga; Surajit Chaudhuri,Estimating the cost of candidate query execution plans is an essential part of queryoptimization. The cost models used by query optimizers to estimate the cost of candidatequery execution plans usually require estimating the result sizes (or selectivities) of theoperators constituting these plans and the number of distinct values in the output of theseoperators. Result size and distinct value estimates play a very important role in costestimation. More accurate information about these two quantities typically results in moreaccurate cost estimates; which helps the optimizer choose more efficient query executionplans. The result size of an operator and the number of distinct values in its output dependon the data distribution of its inputs. To estimate these two quantities; database systems usevarious techniques to approximate input data distributions; such as histograms [PIHS96] …,*,1999,*
Solid: A Platform for Decentralized Social Applications Based on Linked Data,Andrei Vlad Sambra; Essam Mansour; Sandro Hawke; Maged Zereba; Nicola Greco; Abdurrahman Ghanem; Dmitri Zagidulin; Ashraf Aboulnaga; Tim Berners-Lee,Abstract. This paper presents Solid; a decentralized platform for social Web applications. InSolid; users' data is managed independently of the applications that create and consumethis data. The user's data is stored in a Web-accessible personal online datastore (or pod).Solid allows users to have one or more pods from different pod providers; while at the sametime enabling users to easily switch between providers. Developers can use Solid protocols;which is based on existing W3C recommendations; for reading; writing and access control ofthe contents of users' pods. In Solid architecture; applications can operate over data ownedby the user or the user has access to regardless the location of this data on the Web. Userscan also control access to their data; and have the option to switch between applications atany time. This is paradigm shift in integrating social features into Web applications. Our …,*,*,*
Report on the Second International Workshop on Data Management in the Cloud (DMC 2013),Ashraf Aboulnaga; Carlo Curino,The Second International Workshop on Data Management in the Cloud took place on April8; 2013 in Brisbane; Australia; on the day before ICDE. The DMC workshop aims to bringresearchers and practitioners in cloud computing and data management systems together todiscuss the research issues at the intersection of these two areas; and also to draw moreattention from the larger data management and systems research communities to this newand highly promising field. The DMC Workshops are sponsored by the IEEE TCDEWorkgroup on Cloud Computing. The DMC 2013 program began with a keynotepresentation by Amr El Abbadi; Professor at the University of California; Santa Barbara. Thiswas followed by six technical papers presented in two sessions. The workshop concludedwith a panel discussion on research challenges in data management for the cloud.,*,*,*
What is Virtualization?,Ashraf Aboulnaga,Page 1. 1 Virtualization and Databases: State of the Art and Research Challenges AshrafAboulnaga University of Waterloo prepared jointly with Cristiana Amza Kenneth Salem Universityof Toronto University of Waterloo What is Virtualization? ● Separating the abstract view ofcomputing resources from the implementation of these resources ● A layer of indirection betweenabstract view and implementation of resources − Hides implementation details C tl if bttitil t ti −Controls mapping from abstract view to implementation "any problem in computer science canbe solved with another layer of indirection" – David Wheeler Page 2. 2 App 1 App 3 App 2 Example:Virtual Machines Machine CPU CPU Mem Operating System Virtual Machine CPU CPU MemNet Physical Machine Layer of Indirection (VMM) Example: Virtual Storage App 1 App 3 App 2Virtual Disk Machine CPU CPU Mem Operating System …,*,*,*
论文翻译: Deploying Database Appliances in the Cloud,Ashraf Aboulnaga; Kenneth Salem; Ahmed A Soror; Umar Farooq Minhas; Peter Kokosielis; Sunil Kamath,*,*,*,*
Report on the Fourth International Workshop on Self-Managing Database Systems (SMDB 2009),Ashraf Aboulnaga; Kenneth Salem,The Fourth International Workshop on Self-Managing Database Systems took place onMarch 29th; 2009 in Shanghai; China; on the day before ICDE. The SMDB workshops bringtogether researchers and practitioners interested in making data management systemseasier to deploy and operate effectively. Topics of interest range from “traditional”management issues; such as database physical design; system tuning; and resourceallocation to more recent challenges around scalable and highly-available data services incloud computing environments. The SMDB Workshops are sponsored by the IEEE TCDEWorkgroup on Self-Managing Database Systems. The SMDB 2009 program began with akeynote presentation by James Hamilton; Vice President and Distinguished Engineer withAmazon Web Services. This was followed by five technical papers; presented in two …,*,*,*
