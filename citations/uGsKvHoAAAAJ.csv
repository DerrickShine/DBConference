Similarity search for web services,Xin Dong; Alon Halevy; Jayant Madhavan; Ema Nemes; Jun Zhang,Abstract Web services are loosely coupled software components; published; located; andinvoked across the web. The growing number of web services available within anorganization and on the Web raises a new and challenging search problem: locatingdesired web services. Traditional keyword search is insufficient in this context: the specifictypes of queries users require are not captured; the very small text fragments in web servicesare unsuitable for keyword search; and the underlying structure and semantics of the webservices are not exploited. We describe the algorithms underlying the Woogle search enginefor web services. Woogle supports similarity search for web services; such as finding similarweb-service operations and finding operations that compose with a given one. We describenovel techniques to support these types of searches; and an experimental study on a …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,896
Reference reconciliation in complex information spaces,Xin Dong; Alon Halevy; Jayant Madhavan,Abstract Reference reconciliation is the problem of identifying when different references (ie;sets of attribute values) in a dataset correspond to the same real-world entity. Most previousliterature assumed references to a single class that had a fair number of attributes (eg;research publications). We consider complex information spaces: our references belong tomultiple related classes and each reference may have very few attribute values. A primeexample of such a space is Personal Information Management; where the goal is to providea coherent view of all the information on one's desktop. Our reconciliation algorithm hasthree principal features. First; we exploit the associations between references to design newmethods for reference comparison. Second; we propagate information betweenreconciliation decisions to accumulate positive and negative evidences. Third; we …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,636
Knowledge vault: A web-scale approach to probabilistic knowledge fusion,Xin Dong; Evgeniy Gabrilovich; Geremy Heitz; Wilko Horn; Ni Lao; Kevin Murphy; Thomas Strohmann; Shaohua Sun; Wei Zhang,Abstract Recent years have witnessed a proliferation of large-scale knowledge bases;including Wikipedia; Freebase; YAGO; Microsoft's Satori; and Google's Knowledge Graph.To increase the scale even further; we need to explore automatic methods for constructingknowledge bases. Previous approaches have primarily focused on text-based extraction;which can be very noisy. Here we introduce Knowledge Vault; a Web-scale probabilisticknowledge base that combines extractions from Web content (obtained via analysis of text;tabular data; page structure; and human annotations) with prior knowledge derived fromexisting knowledge repositories. We employ supervised machine learning methods forfusing these distinct information sources. The Knowledge Vault is substantially bigger thanany previously published structured knowledge repository; and features a probabilistic …,Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,2014,561
Web-scale data integration: You can only afford to pay as you go,Jayant Madhavan; Shawn R Jeffery; Shirley Cohen; Xin Dong; David Ko; Cong Yu; Alon Halevy,ABSTRACT The World Wide Web is witnessing an increase in the amount of structuredcontent–vast heterogeneous collections of structured data are on the rise due to the DeepWeb; annotation schemes like Flickr; and sites like Google Base. While this phenomenon iscreating an opportunity for structured data management; dealing with heterogeneity on theweb-scale presents many new challenges. In this paper; we highlight these challenges intwo scenarios–the Deep Web and Google Base. We contend that traditional data integrationtechniques are no longer valid in the face of such heterogeneity and scale. We propose anew data integration architecture; PAYGO; which is inspired by the concept of dataspacesand emphasizes pay-as-you-go data management as means for achieving web-scale dataintegration.,*,2007,429
Integrating conflicting data: the role of source dependence,Xin Luna Dong; Laure Berti-Equille; Divesh Srivastava,Abstract Many data management applications; such as setting up Web portals; managingenterprise data; managing community data; and sharing scientific data; require integratingdata from multiple sources. Each of these sources provides a set of values and differentsources can often provide conflicting values. To present quality data to users; it is critical thatdata integration systems can resolve conflicts and discover true values. Typically; we expecta true value to be provided by more sources than any particular false one; so we can takethe value provided by the majority of the sources as the truth. Unfortunately; a false valuecan be spread through copying and that makes truth discovery extremely tricky. In this paper;we consider how to find true values from conflicting information when there are a largenumber of sources; among which some may copy from others. We present a novel …,Proceedings of the VLDB Endowment,2009,297
Bootstrapping pay-as-you-go data integration systems,Anish Das Sarma; Xin Dong; Alon Halevy,Abstract Data integration systems offer a uniform interface to a set of data sources. Despiterecent progress; setting up and maintaining a data integration application still requiressignificant upfront effort of creating a mediated schema and semantic mappings from thedata sources to the mediated schema. Many application contexts involving multiple datasources (eg; the web; personal information management; enterprise intranets) do not requirefull integration in order to provide useful services; motivating a pay-as-you-go approach tointegration. With that approach; a system starts with very few (or inaccurate) semanticmappings and these mappings are improved over time as deemed necessary. This paperdescribes the first completely self-configuring data integration system. The goal of our workis to investigate how advanced of a starting point we can provide a pay-as-you-go system …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,280
The Piazza peer data management project,Igor Tatarinov; Zachary Ives; Jayant Madhavan; Alon Halevy; Dan Suciu; Nilesh Dalvi; Xin Luna Dong; Yana Kadiyska; Gerome Miklau; Peter Mork,Abstract A major problem in today's information-driven world is that sharing heterogeneous;semantically rich data is incredibly difficult. Piazza is a peer data management system thatenables sharing heterogeneous data in a distributed and scalable way. Piazza assumes theparticipants to be interested in sharing data; and willing to define pairwise mappingsbetween their schemas. Then; users formulate queries over their preferred schema; and aquery answering system expands recursively any mappings relevant to the query; retrievingdata from other peers. In this paper; we provide a brief overview of the Piazza projectincluding our work on developing mapping languages and query reformulation algorithms;assisting the users in defining mappings; indexing; and enforcing access control overshared data.,ACM Sigmod Record,2003,263
Data integration with uncertainty,Xin Dong; Alon Y Halevy; Cong Yu,Abstract This paper reports our first set of results on managing uncertainty in dataintegration. We posit that data-integration systems need to handle uncertainty at three levels;and do so in a principled fashion. First; the semantic mappings between the data sourcesand the mediated schema may be approximate because there may be too many of them tobe created and maintained or because in some domains (eg; bioinformatics) it is not clearwhat the mappings should be. Second; queries to the system may be posed with keywordsrather than in a structured form. Third; the data from the sources may be extracted usinginformation extraction techniques and so may yield imprecise data. As a first step to buildingsuch a system; we introduce the concept of probabilistic schema mappings and analyzetheir formal foundations. We show that there are two possible semantics for such …,Proceedings of the 33rd international conference on Very large data bases,2007,251
Big data integration,Xin Luna Dong; Divesh Srivastava,The Big Data era is upon us: data is being generated; collected and analyzed at anunprecedented scale; and data-driven decision making is sweeping through all aspects ofsociety. Since the value of data explodes when it can be linked and fused with other data;addressing the big data integration (BDI) challenge is critical to realizing the promise of BigData. BDI differs from traditional data integration in many dimensions:(i) the number of datasources; even for a single domain; has grown to be in the tens of thousands;(ii) many of thedata sources are very dynamic; as a huge amount of newly collected data are continuouslymade available;(iii) the data sources are extremely heterogeneous in their structure; withconsiderable variety even for substantially similar entities; and (iv) the data sources are ofwidely differing qualities; with significant differences in the coverage; accuracy and …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,237
A platform for personal information management and integration,Xin Luna Dong; Alon Halevy,Abstract The explosion of the amount of information available in digital form has madesearch a hot research topic for the Information Management Community. While most of theresearch on search is focused on the WWW; individual computer users have developed theirown vast collections of data on their desktops; and these collections are in critical need forgood search tools. We study the Personal Information Management (PIM) problem from thedata management point of view. We argue that the key for building a successful PIM systemis to provide a logical view of one's personal information; consisting of semanticallymeaningful objects and associations. The thesis of this research is to build a prototype of aPIM system based upon this logical view; and demonstrate how we can leverage such aview to address the many PIM challenges.,CIDR,2005,226
Truth discovery and copying detection in a dynamic world,Xin Luna Dong; Laure Berti-Equille; Divesh Srivastava,Abstract Modern information management applications often require integrating data from avariety of data sources; some of which may copy or buy data from other sources. When thesedata sources model a dynamically changing world (eg; people's contact informationchanges over time; restaurants open and go out of business); sources often provide out-of-date data. Errors can also creep into data when sources are updated often. Given out-of-date and erroneous data provided by different; possibly dependent; sources; it ischallenging for data integration systems to provide the true values. Straightforward ways toresolve such inconsistencies (eg; voting) may lead to noisy results; often with detrimentalconsequences. In this paper; we study the problem of finding true values and determiningthe copying relationship between sources; when the update history of the sources is …,Proceedings of the VLDB Endowment,2009,186
Indexing dataspaces,Xin Dong; Alon Halevy,Abstract Dataspaces are collections of heterogeneous and partially unstructured data.Unlike data-integration systems that also offer uniform access to heterogeneous datasources; dataspaces do not assume that all the semantic relationships between sources areknown and specified. Much of the user interaction with dataspaces involves exploring thedata; and users do not have a single schema to which they can pose queries. Consequently;it is important that queries are allowed to specify varying degrees of structure; spanningkeyword queries to more structure-aware queries. This paper considers indexing support forqueries that combine keywords and structure. We describe several extensions to invertedlists to capture structure when it is present. In particular; our extensions incorporate attributelabels; relationships between data items; hierarchies of schema elements; and synonyms …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,178
Truth finding on the deep web: Is the problem solved?,Xian Li; Xin Luna Dong; Kenneth Lyons; Weiyi Meng; Divesh Srivastava,Abstract The amount of useful information available on the Web has been growing at adramatic pace in recent years and people rely more and more on the Web to fulfill theirinformation needs. In this paper; we study truthfulness of Deep Web data in two domainswhere we believed data are fairly clean and data quality is important to people's lives: Stockand Flight. To our surprise; we observed a large amount of inconsistency on data fromdifferent sources and also some sources with quite low accuracy. We further applied onthese two data sets state-of-the-art data fusion methods that aim at resolving conflicts andfinding the truth; analyzed their strengths and limitations; and suggested promising researchdirections. We wish our study can increase awareness of the seriousness of conflicting dataon the Web and in turn inspire more research in our community to tackle this problem.,Proceedings of the VLDB Endowment,2012,174
Data fusion: resolving data conflicts for integration,Xin Luna Dong; Felix Naumann,Abstract The amount of information produced in the world increases by 30% every year andthis rate will only go up. With advanced network technology; more and more sources areavailable either over the Internet or in enterprise intranets. Modern data managementapplications; such as setting up Web portals; managing enterprise data; managingcommunity data; and sharing scientific data; often require integrating available data sourcesand providing a uniform interface for users to access data from different sources; suchrequirements have been driving fruitful research on data integration over the last twodecades [11; 13].,Proceedings of the VLDB Endowment,2009,167
Data integration with uncertainty,Xin Luna Dong; Alon Halevy; Cong Yu,Abstract This paper reports our first set of results on managing uncertainty in dataintegration. We posit that data-integration systems need to handle uncertainty at three levelsand do so in a principled fashion. First; the semantic mappings between the data sourcesand the mediated schema may be approximate because there may be too many of them tobe created and maintained or because in some domains (eg; bioinformatics) it is not clearwhat the mappings should be. Second; the data from the sources may be extracted usinginformation extraction techniques and so may yield erroneous data. Third; queries to thesystem may be posed with keywords rather than in a structured form. As a first step tobuilding such a system; we introduce the concept of probabilistic schema mappings andanalyze their formal foundations. We show that there are two possible semantics for such …,The VLDB Journal,2009,140
Personal information management with SEMEX,Yuhan Cai; Xin Luna Dong; Alon Halevy; Jing Michelle Liu; Jayant Madhavan,Abstract The explosion of information available in digital form has made search a hotresearch topic for the Information Management Community. While most of the research onsearch is focused on the WWW; individual computer users have developed their own vastcollections of data on their desktops; and these collections are in critical need for goodsearch and query tools. The problem is exacerbated by the proliferation of varied electronicdevices (laptops; PDAs; cellphones) that are at our disposal; which often hold subsets orvariations of our data. In fact; several recent venues have noted Personal InformationManagement (PIM) as an area of growing interest to the data management community [1; 8;6],Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,129
Global detection of complex copying relationships between sources,Xin Luna Dong; Laure Berti-Equille; Yifan Hu; Divesh Srivastava,Abstract Web technologies have enabled data sharing between sources but also simplifiedcopying (and often publishing without proper attribution). The copying relationships can becomplex: some sources copy from multiple sources on different subsets of data; some co-copy from the same source; and some transitively copy from another. Understanding suchcopying relationships is desirable both for business purposes and for improving many keycomponents in data integration; such as resolving conflicts across various sources;reconciling distinct references to the same real-world entity; and efficiently answeringqueries over multiple sources. Recent works have studied how to detect copying between apair of sources; but the techniques can fall short in the presence of complex copyingrelationships. In this paper we describe techniques that discover global copying …,Proceedings of the VLDB Endowment,2010,109
Less is more: Selecting sources wisely for integration,Xin Luna Dong; Barna Saha; Divesh Srivastava,Abstract We are often thrilled by the abundance of information surrounding us and wish tointegrate data from as many sources as possible. However; understanding; analyzing; andusing these data are often hard. Too much data can introduce a huge integration cost; suchas expenses for purchasing data and resources for integration and cleaning. Furthermore;including low-quality data can even deteriorate the quality of integration results instead ofbringing the desired quality gain. Thus;" the more the better" does not always hold for dataintegration and often" less is more". In this paper; we study how to select a subset of sourcesbefore integration such that we can balance the quality of integrated data and integrationcost. Inspired by the Marginalism principle in economic theory; we wish to integrate a newsource only if its marginal gain; often a function of improved integration quality; is higher …,Proceedings of the VLDB Endowment,2012,108
Linking temporal records,Pei Li; X Dong; Andrea Maurino; Divesh Srivastava,ABSTRACT Many data sets contain temporal records over a long period of time; each recordis associated with a time stamp and describes some aspects of a realworld entity at thatparticular time (eg; author information in DBLP). In such cases; we often wish to identifyrecords that describe the same entity over time and so be able to enable interestinglongitudinal data analysis. However; existing record linkage techniques ignore the temporalinformation and can fall short for temporal data. This paper studies linking temporal records.First; we apply time decay to capture the effect of elapsed time on entity value evolution.Second; instead of comparing each pair of records locally; we propose clustering methodsthat consider time order of the records and make global decisions. Experimental resultsshow that our algorithms significantly outperform traditional linkage methods on various …,Proceedings of the VLDB Endowment,2011,95
Knowledge-based trust: Estimating the trustworthiness of web sources,Xin Luna Dong; Evgeniy Gabrilovich; Kevin Murphy; Van Dang; Wilko Horn; Camillo Lugaresi; Shaohua Sun; Wei Zhang,Abstract The quality of web sources has been traditionally evaluated using exogenoussignals such as the hyperlink structure of the graph. We propose a new approach that relieson endogenous signals; namely; the correctness of factual information provided by thesource. A source that has few false facts is considered to be trustworthy. The facts areautomatically extracted from each source by information extraction methods commonly usedto construct knowledge bases. We propose a way to distinguish errors made in theextraction process from factual errors in the web source per se; by using joint inference in anovel multi-layer probabilistic model.,Proceedings of the VLDB Endowment,2015,92
From data fusion to knowledge fusion,Xin Luna Dong; Evgeniy Gabrilovich; Geremy Heitz; Wilko Horn; Kevin Murphy; Shaohua Sun; Wei Zhang,Abstract The task of data fusion is to identify the true values of data items (eg; the true date ofbirth for Tom Cruise) among multiple observed values drawn from different sources (eg; Websites) of varying (and unknown) reliability. A recent survey [20] has provided a detailedcomparison of various fusion methods on Deep Web data. In this paper; we study theapplicability and limitations of different fusion techniques on a more challenging problem:knowledge fusion. Knowledge fusion identifies true subject-predicate-object triples extractedby multiple information extractors from multiple information sources. These extractorsperform the tasks of entity linkage and schema alignment; thus introducing an additionalsource of noise that is quite different from that traditionally considered in the data fusionliterature; which only focuses on factual errors in the original sources. We adapt state-of …,Proceedings of the VLDB Endowment,2014,90
Fusing data with correlations,Ravali Pochampally; Anish Das Sarma; Xin Luna Dong; Alexandra Meliou; Divesh Srivastava,Abstract Many applications rely on Web data and extraction systems to accomplishknowledge-driven tasks. Web information is not curated; so many sources provideinaccurate; or conflicting information. Moreover; extraction systems introduce additionalnoise to the data. We wish to automatically distinguish correct data and erroneous data forcreating a cleaner set of integrated data. Previous work has shown that a naive votingstrategy that trusts data provided by the majority or at least a certain number of sources maynot work well in the presence of copying between the sources. However; correlationbetween sources can be much broader than copying: sources may provide data fromcomplementary domains (negative correlation); extractors may focus on different types ofinformation (negative correlation); and extractors may apply common rules in extraction …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,74
Sailing the information ocean with awareness of currents: Discovery and application of source dependence,Laure Berti-Equille; Anish Das Sarma; Amelie Marian; Divesh Srivastava,Abstract: The Web has enabled the availability of a huge amount of useful information; buthas also eased the ability to spread false information and rumors across multiple sources;making it hard to distinguish between what is true and what is not. Recent examples includethe premature Steve Jobs obituary; the second bankruptcy of United airlines; the creation ofBlack Holes by the operation of the Large Hadron Collider; etc. Since it is important to permitthe expression of dissenting and conflicting opinions; it would be a fallacy to try to ensurethat the Web provides only consistent information. However; to help in separating the wheatfrom the chaff; it is essential to be able to determine dependence between sources. Giventhe huge number of data sources and the vast volume of conflicting data available on theWeb; doing so in a scalable manner is extremely challenging and has not been …,Arxiv preprint arXiv:0909.1776,2009,64
Incremental record linkage,Anja Gruenheid; Xin Luna Dong; Divesh Srivastava,Abstract Record linkage clusters records such that each cluster corresponds to a singledistinct real-world entity. It is a crucial step in data cleaning and data integration. In the bigdata era; the velocity of data updates is often high; quickly making previous linkage resultsobsolete. This paper presents an end-to-end framework that can incrementally andefficiently update linkage results when data updates arrive. Our algorithms not only allowmerging records in the updates with existing clusters; but also allow leveraging newevidence from the updates to fix previous linkage errors. Experimental results on three realand synthetic data sets show that our algorithms can significantly reduce linkage timewithout sacrificing linkage quality.,Proceedings of the VLDB Endowment,2014,61
Method and apparatus for updating XML views of relational data,*,A method and apparatus are provided for updating XML views of relational data. Thepresent invention translates an update to an XML view of a relational database into updateoperations to be performed on the underlying relational database itself. The disclosed XMLview update manager can perform updates in the context of an underlying relationaldatabase that serves the XML-based application; as well as traditional relational databasemanagement system (RDBMS) applications. Given a pre-existing underlying relationaldatabase schema and an XML view defined on it; the present invention provides aframework for generating update plans to perform an update without introducing side-effectsto other parts of the view.,*,2005,60
Answering Structured Queries on Unstructured Data.,Jing Liu; Xin Dong; Alon Y Halevy,Abstract There is growing number of applications that require access to both structured andunstructured data. Such collections of data have been referred to as dataspaces; andDataspace Support Platforms (DSSPs) were proposed to offer several services overdataspaces; including search and query; source discovery and categorization; indexing andsome forms of recovery. One of the key services of a DSSP is to provide seamless queryingon the structured and unstructured data. Querying each kind of data in isolation has beenthe main subject of study for the fields of databases and information retrieval. Recently thedatabase community has studied the problem of answering keyword queries on structureddata such as relational data or XML data. The only combination that has not been fullyexplored is answering structured queries on unstructured data. This paper explores an …,WebDB,2006,55
Visualization of heterogeneous data,Mike Cammarano; Xin Dong; Bryan Chan; Jeff Klingner; Justin Talbot; Alon Halevey; Pat Hanrahan,Both the resource description framework (RDF); used in the semantic web; and Maya Viz u-forms represent data as a graph of objects connected by labeled edges. Existing systems forflexible visualization of this kind of data require manual specification of the possiblevisualization roles for each data attribute. When the schema is large and unfamiliar; thisrequirement inhibits exploratory visualization by requiring a costly up-front data integrationstep. To eliminate this step; we propose an automatic technique for mapping data attributesto visualization attributes. We formulate this as a schema matching problem; findingappropriate paths in the data model for each required visualization attribute in avisualization template.,IEEE Transactions on Visualization and Computer Graphics,2007,54
Structured data meets the Web: a few observations.,Jayant Madhavan; Alon Y Halevy; Shirley Cohen; Xin Luna Dong; Shawn R Jeffery; David Ko; Cong Yu,Abstract The World Wide Web is witnessing an increase in the amount of structured content–vast heterogeneous collections of structured data are on the rise due to the Deep Web;annotation schemes like Flickr; and sites like Google Base. While this phenomenon iscreating an opportunity for structured data management; dealing with heterogeneity on theweb-scale presents many new challenges. In this paper we articulate challenges based onour experience with addressing them at Google; and offer some principles for addressingthem in a general fashion.,IEEE Data Eng. Bull.,2006,52
Containment of nested XML queries,Xin Dong; Alon Y Halevy; Igor Tatarinov,Abstract Query containment is the most fundamental relationship between a pair of databasequeries: a query Q is said to be contained in a query Q′ if the answer for Q is always asubset of the answer for Q′; independent of the current state of the database. Querycontainment is an important problem in a wide variety of data management applications;including verification of integrity constraints; reasoning about contents of data sources indata integration; semantic caching; verification of knowledge bases; determining queriesindependent of updates; and most recently; in query reformulation for peer datamanagement systems. Query containment has been studied extensively in the relationalcontext and for XPath queries; but not for XML queries with nesting. We consider thetheoretical aspects of the problem of query containment for XML queries with nesting. We …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,50
Online data fusion,Xuan Liu; Xin Luna Dong; Beng Chin Ooi; Divesh Srivastava,ABSTRACT The Web contains a significant volume of structured data in various domains;but a lot of data are dirty and erroneous; and they can be propagated through copying. Whiledata integration techniques allow querying structured data on the Web; they take the unionof the answers retrieved from different sources and can thus return conflicting information.Data fusion techniques; on the other hand; aim to find the true values; but are designed foroffline data aggregation and can take a long time. This paper proposes SOLARIS; the firstonline data fusion system. It starts with returning answers from the first probed source; andrefreshes the answers as it probes more sources and applies fusion techniques on theretrieved data. For each returned answer; it shows the likelihood that the answer is correct;and stops retrieving data for it after gaining enough confidence that data from the …,Proceedings of the VLDB Endowment,2011,49
Characterizing and selecting fresh data sources,Theodoros Rekatsinas; Xin Luna Dong; Divesh Srivastava,Abstract Data integration is a challenging task due to the large numbers of autonomous datasources. This necessitates the development of techniques to reason about the benefits andcosts of acquiring and integrating data. Recently the problem of source selection (ie;identifying the subset of sources that maximizes the profit from integration) was introducedas a preprocessing step before the actual integration. The problem was studied for staticsources and used the accuracy of data fusion to quantify the integration profit. In this paper;we study the problem of source selection considering dynamic data sources whose contentchanges over time. We define a set of time-dependent metrics; including coverage;freshness and accuracy; to characterize the quality of integrated data. We show howstatistical models for the evolution of sources can be used to estimate these metrics …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,46
Record linkage with uniqueness constraints and erroneous values,Songtao Guo; Xin Luna Dong; Divesh Srivastava; Remi Zajac,Abstract Many data-management applications require integrating data from a variety ofsources; where different sources may refer to the same real-world entity in different waysand some may even provide erroneous data. An important task in this process is torecognize and merge the various references that refer to the same entity. In practice; someattributes satisfy a uniqueness constraint---each real-world entity (or most entities) has aunique value for the attribute (eg; business contact phone; address; and email). Traditionaltechniques tackle this case by first linking records that are likely to refer to the same real-world entity; and then fusing the linked records and resolving conflicts if any. Such methodscan fall short for three reasons: first; erroneous values from sources may prevent correctlinking; second; the real world may contain exceptions to the uniqueness constraints and …,Proceedings of the VLDB Endowment,2010,46
Functional Dependency Generation and Applications in Pay-As-You-Go Data Integration Systems.,Daisy Zhe Wang; Xin Luna Dong; Anish Das Sarma; Michael J Franklin; Alon Y Halevy,ABSTRACT Recently; the opportunity of extracting structured data from the Web has beenidentified by a number of research projects. One such example is that millions of relational-style HTML tables can be extracted from the Web. Traditional data integration approachesdo not scale over such corpora with hundreds of small tables in one domain. To solve thisproblem; previous work has proposed pay-as-you-go data integration systems to provide;with little up-front cost; base services over loosely-integrated information. One keycomponent of such systems; which has received little attention to date; is the need for aframework to gauge and improve the quality of the integration. We propose a frameworkbased on functional dependencies (FDs). Unlike in traditional database design; where FDsare specified as statements of truth about all possible instances of the database; in web …,WebDB,2009,44
Malleable Schemas: A Preliminary Report.,Xin Dong; Alon Y Halevy,ABSTRACT Large-scale information integration; and in particular; search on the World WideWeb; is pushing the limits on the combination of structured data and unstructured data. By itsvery nature; as we combine a large number of information sources; our ability to model thedomain in a completely structured way diminishes. We argue that in order to buildapplications that combine structured and unstructured data; there is a need for a newmodeling tool. We consider the question of modeling an application domain whose datamay be partially structured and partially unstructured. In particular; we are concerned withapplications where the border between the structured and unstructured parts of the data isnot well defined; not well known in advance; or may evolve over time. We propose theconcept of malleable schemas as a modeling tool that enables incorporating both …,WebDB,2005,38
Data modeling in dataspace support platforms,Anish Das Sarma; Xin Luna Dong; Alon Y Halevy,Abstract Data integration has been an important area of research for several years.However; such systems suffer from one of the main drawbacks of database systems: theneed to invest significant modeling effort upfront. Dataspace Support Platforms (DSSP)envision a system that offers useful services on its data without any setup effort; and improvewith time in a pay-as-you-go fashion. We argue that in order to support DSSPs; the systemneeds to model uncertainty at its core. We describe the concepts of probabilistic mediatedschemas and probabilistic mappings as enabling concepts for DSSPs.,*,2009,34
Semex: Toward on-the-fly personal information integration,Xin Dong; Alon Halevy; Ema Nemes; Stephan Sigurdsson; Pedro Domingos,Abstract On-the-fly information integration attempts to change the basic cost-benefit equationassociation with building information integration applications. This paper argues that on-the-fly can be supported by extending one's personal information space. As a first step in thisdirection; we describe the Semex system that provides a logical and integrated view of one'spersonal information.,Proc. of the VLDB IIWeb Workshop,2004,34
Data integration with dependent sources,Anish Das Sarma; Xin Luna Dong; Alon Halevy,Abstract Data integration systems offer users a uniform interface to a set of data sources.Previous work has typically assumed that the data sources are independent of each other;however; in scenarios involving large numbers of sources; such as the Web or largeenterprises; there is an eco-system of dependent sources; where some sources copy partsof their data from others. This paper considers the new optimization problems that arisewhile answering queries over large number of dependent sources. These are the (1) cost-minimization problem: what is the minimum cost we must incur to get all answer tuples;(2)maximum-coverage problem: given a bound on the cost; how can we get the maximumpossible coverage; and (3) the source-ordering problem: for a set of data sources; what isthe best order to query them so as to retrieve answer tuples as fast as possible.,Proceedings of the 14th International Conference on Extending Database Technology,2011,33
Finding Quality in Quantity: The Challenge of Discovering Valuable Sources for Integration.,Theodoros Rekatsinas; Xin Luna Dong; Lise Getoor; Divesh Srivastava,ABSTRACT Data is becoming a commodity of tremendous value for many domains. This isleading to a rapid increase in the number of data sources and public access data services;such as cloud-based data markets and data portals; that facilitate the collection; publishingand trading of data. Data sources typically exhibit wide variety and heterogeneity in thetypes or schemas of the data they provide; their quality; and the fees they charge foraccessing their data. Users who want to build upon such publicly available data; must (i)discover sources that are relevant to their applications;(ii) identify sources that collectivelysatisfy the quality and budget requirements of their applications; with few effective cluesabout the quality of the sources; and (iii) repeatedly invest many person-hours in assessingthe eventual usefulness of data sources. All three steps require investigating the content …,CIDR,2015,31
Mining structures for semantics,Xin Dong; Jayant Madhavan; Alon Halevy,Abstract Online data is available in two avors: unstructured data that resides as free text inHTML pages; and structured data that resides in databases and knowledge bases.Unstructured data is easily accessed as human-readable text on a browser; while structureddata is hidden behind web query interfaces (web forms); web services; and customdatabase APIs. Access to this data; popularly referred to as the hidden web; entailssubmitting correctly completed web forms or writing code to access web services usingprotocols such as SOAP.,ACM SIGKDD Explorations Newsletter,2004,28
Solomon: Seeking the truth via copying detection,Xin Luna Dong; Laure Berti-Equille; Yifan Hu; Divesh Srivastava,Abstract We live in the Information Era; with access to a huge amount of information from avariety of data sources. However; data sources are of different qualities; often providingconflicting; out-of-date and incomplete data. Data sources can also easily copy; reformat andmodify data from other sources; propagating erroneous data. These issues make theidentification of high quality information and sources non-trivial. We demonstrate theSolomon system; whose core is a module that detects copying between sources. Wedemonstrate that we can effectively detect copying relationship between data sources;leverage the results in truth discovery; and provide a user-friendly interface to facilitate usersin identifying sources that best suit their information needs.,Proceedings of the VLDB Endowment,2010,24
Dexter: large-scale discovery and extraction of product specifications on the web,Disheng Qiu; Luciano Barbosa; Xin Luna Dong; Yanyan Shen; Divesh Srivastava,Abstract The web is a rich resource of structured data. There has been an increasing interestin using web structured data for many applications such as data integration; web search andquestion answering. In this paper; we present Dexter; a system to find product sites on theweb; and detect and extract product specifications from them. Since product specificationsexist in multiple product sites; our focused crawler relies on search queries and backlinks todiscover product sites. To perform the detection; and handle the high diversity ofspecifications in terms of content; size and format; our system uses supervised learning toclassify HTML fragments (eg; tables and lists) present in web pages as specifications or not.To perform large-scale extraction of the attribute-value pairs from the HTML fragmentsidentified by the specification detector; D exter adopts two lightweight strategies: a …,Proceedings of the VLDB Endowment,2015,23
Data x-ray: A diagnostic tool for data errors,Xiaolan Wang; Xin Luna Dong; Alexandra Meliou,Abstract A lot of systems and applications are data-driven; and the correctness of theiroperation relies heavily on the correctness of their data. While existing data cleaningtechniques can be quite effective at purging datasets of errors; they disregard the fact that alot of errors are systematic; inherent to the process that produces the data; and thus will keepoccurring unless the problem is corrected at its source. In contrast to traditional datacleaning; in this paper we focus on data diagnosis: explaining where and how the errorshappen in a data generative process. We develop a large-scale diagnostic framework calledDATA X-RAY. Our contributions are three-fold. First; we transform the diagnosis problem tothe problem of finding common properties among erroneous elements; with minimal domain-specific assumptions. Second; we use Bayesian analysis to derive a cost model that …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,20
Data fusion: resolving conflicts from multiple sources,Xin Luna Dong; Laure Berti-Equille; Divesh Srivastava,Abstract Many data management applications; such as setting up Web portals; managingenterprise data; managing community data; and sharing scientific data; require integratingdata from multiple sources. Each of these sources provides a set of values; and differentsources can often provide conflicting values. To present quality data to users; it is critical toresolve conflicts and discover values that reflect the real world; this task is called data fusion.Typically; we expect a true value to be provided by more sources than any particular falseone; so we can take the value provided by the largest number of sources as the truth.Unfortunately; a false value can be spread through copying and that makes truth discoveryextremely tricky. In this chapter; we consider how to find true values from conflictinginformation when there are a large number of sources; among which some may copy from …,*,2013,19
Uncertainty in data integration,Anish Das Sarma; Xin Dong; Alon Halevy,Abstract Data integration has been an important area of research for several years. In thischapter; we argue that supporting modern data integration applications requires systems tohandle uncertainty at every step of integration. We provide a formal framework for dataintegration systems with uncertainty. We define probabilistic schema mappings andprobabilistic mediated schemas; show how they can be constructed automatically for a set ofdata sources; and provide techniques for query answering. The foundations laid out in thischapter enable bootstrapping a pay-as-you-go integration system completely automatically.,in" Managing and Mining Uncertain Data" Ed. Charu Aggarwal; Springer; 2008,2008,19
Scaling up copy detection,Xian Li; Xin Luna Dong; Kenneth B Lyons; Weiyi Meng; Divesh Srivastava,Recent research shows that copying is prevalent for Deep-Web data and consideringcopying can significantly improve truth finding from conflicting values. However; existingcopy detection techniques do not scale for large sizes and numbers of data sources; so truthfinding can be slowed down by one to two orders of magnitude compared with thecorresponding techniques that do not consider copying. In this paper; we study how toimprove scalability of copy detection on structured data. Our algorithm builds an invertedindex for each shared value and processes the index entries in decreasing order of howmuch the shared value can contribute to the conclusion of copying. We show how we usethe index to prune the data items we consider for each pair of sources; and to incrementallyrefine our results in iterative copy detection. We also apply a sampling strategy with which …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,17
Keys for graphs,Wenfei Fan; Zhe Fan; Chao Tian; Xin Luna Dong,Abstract Keys for graphs aim to uniquely identify entities represented by vertices in a graph.We propose a class of keys that are recursively defined in terms of graph patterns; and areinterpreted with subgraph isomorphism. Extending conventional keys for relations and XML;these keys find applications in object identification; knowledge fusion and social networkreconciliation. As an application; we study the entity matching problem that; given a graph Gand a set Σ of keys; is to find all pairs of entities (vertices) in G that are identified by keys in Σ.We show that the problem is intractable; and cannot be parallelized in logarithmic rounds.Nonetheless; we provide two parallel scalable algorithms for entity matching; in MapReduceand a vertex-centric asynchronous model. Using real-life and synthetic data; weexperimentally verify the effectiveness and scalability of the algorithms.,Proceedings of the VLDB Endowment,2015,14
Compact explanation of data fusion decisions,Xin Luna Dong; Divesh Srivastava,Abstract Despite the abundance of useful information on the Web; different Web sourcesoften provide conflicting data; some being out-of-date; inaccurate; or erroneous. Data fusionaims at resolving conflicts and finding the truth. Advanced fusion techniques apply iterativeMAP (Maximum A Posteriori) analysis that reasons about trustworthiness of sources andcopying relationships between them. Providing explanations for such decisions is importantfor a better understanding; but can be extremely challenging because of the complexity ofthe analysis during decision making. This paper proposes two types of explanations for data-fusion results: snapshot explanations take the provided data and any other decision inferredfrom the data as evidence and provide a high-level understanding of a fusion decision;comprehensive explanations take only the data as evidence and provide an in-depth …,Proceedings of the 22nd international conference on World Wide Web,2013,14
CHRONOS: Facilitating History Discovery by Linking Temporal Records,Pei Li; Haidong Wang; Christina Tziviskou; Xin Luna Dong; Xiaoguang Liu; Andrea Maurino; Divesh Srivastava,Abstract Many data sets contain temporal records over a long period of time; each record isassociated with a time stamp and describes some aspects of a real-world entity at thatparticular time. From such data; users often wish to search for entities in a particular periodand understand the history of one entity or all entities in the data set. A major challenge forenabling such search and exploration is to identify records that describe the same real-worldentity over a long period of time; however; linking temporal records is hard given that thevalues that describe an entity can evolve over time (eg.; a person can move from oneaffiliation to another). We demonstrate the Chronos system which offers users the useful toolfor finding real-world entities over time and understanding history of entities in thebibliography domain. The core of Chronos is a temporal record-linkage algorithm; which …,VLDB,2012,14
Structure everything,Tiziana Catarci; Luna Dong; Alon Halevy; Antonella Poggi,The availability of vast amounts of information on the Internet and the sharp decrease in thecost of digital storage has enabled today's desktops to become personal informationarchives. Desktops contain collections of work-and hobbyrelated data as well as richcollections of personal multimedia files. However; the typical lack of structure of one'sdesktop data makes it difficult for the user to easily search and benefit from the information itcontains. Consider an example; a scenario where Alex's laptop contains a huge amount ofinformation that is stored in several different formats; including emails; pictures; textdocuments; media files; address books; and more. In particular; Alex uses email extensivelyto communicate with many friends; his mother; Connie; and his sister; Brooke. When hereceives a voicemail from Connie asking him to help plan a surprise birthday party for …,Personal information management,2007,13
TimeMachine: Timeline generation for knowledge-base entities,Tim Althoff; Xin Luna Dong; Kevin Murphy; Safa Alai; Van Dang; Wei Zhang,Abstract We present a method called TIMEMACHINE to generate a timeline of events andrelations for entities in a knowledge base. For example for an actor; such a timeline shouldshow the most important professional and personal milestones and relationships such asworks; awards; collaborations; and family relationships. We develop three orthogonaltimeline quality criteria that an ideal timeline should satisfy:(1) it shows events that arerelevant to the entity;(2) it shows events that are temporally diverse; so they distribute alongthe time axis; avoiding visual crowding and allowing for easy user interaction; such aszooming in and out; and (3) it shows events that are content diverse; so they contain manydifferent types of events (eg; for an actor; it should show movies and marriages and awards;not just movies). We present an algorithm to generate such timelines for a given time …,Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2015,12
Online ordering of overlapping data sources,Mariam Salloum; Xin Luna Dong; Divesh Srivastava; Vassilis J Tsotras,Abstract Data integration systems offer a uniform interface for querying a large number ofautonomous and heterogeneous data sources. Ideally; answers are returned as sources arequeried and the answer list is updated as more answers arrive. Choosing a good ordering inwhich the sources are queried is critical for increasing the rate at which answers arereturned. However; this problem is challenging since we often do not have complete orprecise statistics of the sources; such as their coverage and overlap. It is further exacerbatedin the Big Data era; which is witnessing two trends in Deep-Web data: first; obtaining a fullcoverage of data in a particular domain often requires extracting data from thousands ofsources; second; there is often a big variation in overlap between different data sources. Inthis paper we present OASIS; an Online query Answering System for overlappIng …,Proceedings of the VLDB Endowment,2013,10
Dependency between sources in truth discovery,*,A method and system for truth discovery may implement a methodology that accounts foraccuracy of sources and dependency between sources. The methodology may be based onBayesian probability calculus for determining which data object values published by sourcesare likely to be true. The method may be recursive with respect to dependency; accuracy;and actual truth discovery for a plurality of sources.,*,2012,10
Large-scale copy detection,Xin Luna Dong; Divesh Srivastava,Abstract The Web has enabled the availability of a vast amount of useful information inrecent years. However; the web technologies that have enabled sources to share theirinformation have also made it easy for sources to copy from each other and often publishwithout proper attribution. Understanding the copying relationships between sources hasmany benefits; including helping data providers protect their own rights; improving variousaspects of data integration; and facilitating in-depth analysis of information flow. Theimportance of copy detection has led to a substantial amount of research in many disciplinesof Computer Science; based on the type of information considered; such as text; images;videos; software code; and structured data. This tutorial explores the similarities anddifferences between the techniques proposed for copy detection across the different types …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,10
Providing best-effort services in dataspace systems,Xin Dong,Nowadays many data sharing applications need to manage a dataspace [68]; whichcontains a number of heterogeneous data sources and partially unstructured data. Suchscenarios include large enterprises; collaborative scientific projects; digital libraries;personal information; and the Web. Understanding the relationships between the datasources requires specifying schema mappings; such as one stating that full-name in onedata source corresponds to the concatenation of first-name and last-name in another datasource. However; the data sources in a dataspace are only loosely coupled; so we may nothave schema mappings specified up front. This dissertation studies how to provide best-effort search; querying and browsing services in a dataspace system; even when preciseschema mappings are not present. To provide useful services over all data in a …,*,2007,10
The elephant in the room: getting value from Big Data,Serge Abiteboul; Luna Dong; Oren Etzioni; Divesh Srivastava; Gerhard Weikum; Julia Stoyanovich; Fabian M Suchanek,Big Data; and its 4 Vs–volume; velocity; variety; and veracity–have been at the forefront ofsocietal; scientific and engineering discourse. Arguably the most important 5th V; value; isnot talked about as much. How can we make sure that our data is not just big; but alsovaluable? WebDB 2015 1 has as its theme “Freshness; Correctness; Quality of Informationand Knowledge on the Web”. The workshop attracted 31 submissions; of which the best 9were selected for presentation at the workshop; and for publication in the proceedings. Toset the stage; we have interviewed several prominent members of the data managementcommunity; soliciting their opinions on how we can ensure that data is not just available inquantity; but also in quality. In this interview Serge Abiteboul; Oren Etzioni; DiveshSrivastava with Luna Dong; and Gerhard Weikum shared with us their motivation for …,Proceedings of the 18th international workshop on web and databases,2015,9
A time machine for information: Looking back to look forward,Xin Luna Dong; Wang-Chiew Tan,Abstract With the abundant availability of information one can mine from the Web today;there is increasing interest to develop a complete understanding of the history of an entity(ie; a person; a company; a music genre; a country; etc.)(see; for example;[7; 9; 10; 11]) andto depict trends over time [5; 12; 13]. This; however; remains a largely difficult and manualtask despite more than a couple of decades of research in the areas of temporal databasesand data integration.,Proceedings of the VLDB Endowment,2015,7
We challenge you to certify your updates,Su Chen; Xin Luna Dong; Laks VS Lakshmanan; Divesh Srivastava,Abstract Correctness of data residing in a database is vital. While integrity constraintenforcement can often ensure data consistency; it is inadequate to protect against updatesthat involve careless; unintentional errors; eg; whether a specified update to an employee'srecord was for the intended employee. We propose a novel approach that is complementaryto existing integrity enforcement techniques; to guard against such erroneous updates. Ourapproach is based on (a) updaters providing an update certificate with each databaseupdate; and (b) the database system verifying the correctness of the update certificateprovided before performing the update. We formalize a certificate as a (challenge; response)pair; and characterize good certificates as those that are easy for updaters to provide and;when correct; give the system enough confidence that the update was indeed intended …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,7
Knowledge curation and knowledge fusion: challenges; models and applications,Xin Luna Dong; Divesh Srivastava,Abstract Large-scale knowledge repositories are becoming increasingly important as afoundation for enabling a wide variety of complex applications. In turn; building high-qualityknowledge repositories critically depends on the technologies of knowledge curation andknowledge fusion; which share many similar goals with data integration; while facing evenmore challenges in extracting knowledge from both structured and unstructured data; acrossa large variety of domains; and in multiple languages. Our tutorial highlights the similaritiesand differences between knowledge management and data integration; and has two goals.First; we introduce the Database community to the techniques proposed for the problems ofentity linkage and relation extraction by the Knowledge Management; Natural LanguageProcessing; and Machine Learning communities. Second; we give a detailed survey of …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,4
ROLEX: relational on-line exchange with XML,Philip Bohannon; Xin Luna Dong; Sumit Ganguly; Henry F Korth; Chengkai Li; PPS Narayan; Pradeep Shenoy,To achieve this; ROLEX is integrated tightly with both the DBMS and the application througha standard interface supported by most XML parsers; the Document Object Model (DOM).Thus; in general; an application need not be modified to be used with ROLEX. With theDBMS providing performance qualitatively similar to cached data; XML applications can relyon it for concurrency control and recovery services. To support our integration model andperformance goals; ROLEX is built on the DataBlitz TM Main-Memory Database System;allowing us to capitalize on low-latency access to data while still providing concurrencycontrol and recovery [1]. The DOM interface supports the expected navigation functions:parent-to-child; child-to-parent; and sibling-to-sibling. Adom interface to an XML view querysupports all the DOM operations and behaves as if the user were navigating the XML …,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,4
Mining summaries for knowledge graph search,Qi Song; Yinghui Wu; Xin Luna Dong,Mining and searching heterogeneous and large knowledge graphs is challenging underreal-world resource constraints such as response time. This paper studies a framework thatdiscover to facilitate knowledge graph search. 1) We introduce a class of summariescharacterized by graph patterns. In contrast to conventional summaries defined by frequentsubgraphs; the summaries are capable of adaptively summarize entities with similarneighbors up to a bounded hop. 2) We formulate the computation of graph summarization asa bi-criteria pattern mining problem. Given a knowledge graph G; the problem is to discoverk diversified summaries that maximizes the informativeness measure. Although this problemis NP-hard; we show that it is 2-approximable. We also introduce an online mining algorithmthat trade-off speed and accuracy; under given resource constraints. 3) We develop query …,Data Mining (ICDM); 2016 IEEE 16th International Conference on,2016,3
Detecting clones; copying and reuse on the web,Xin Luna Dong; Divesh Srivastava,The Web has enabled the availability of a vast amount of useful information in recent years.However; the web technologies that have enabled sources to share their information havealso made it easy for sources to copy from each other and often publish without properattribution. Understanding the copying relationships between sources has many benefits;including helping data providers protect their own rights; improving various aspects of dataintegration; and facilitating in-depth analysis of information flow. The importance of copydetection has led to a substantial amount of research in many disciplines of ComputerScience; based on the type of information considered; such as text; images; videos; softwarecode; and structured data. This seminar explores the similarities and differences betweenthe techniques proposed for copy detection across the different types of information. We …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,3
Minimal difference query and view matching,*,The subject disclosure pertains to efficient computation of the difference between queries byexploiting commonality between them. A minimal difference query (MDQ) is generated thatroughly corresponds to removal of as many joins as possible while still accuratelyrepresenting the query difference. The minimal difference can be employed to furthersubstantially the scope of view matching where a query is not wholly subsumed by a view.Additionally; the minimal difference query can be employed as an analytical tool in variouscontexts.,*,2007,3
Knowledge verification for long-tail verticals,Furong Li; Xin Luna Dong; Anno Langen; Yang Li,Abstract Collecting structured knowledge for real-world entities has become a critical task formany applications. A big gap between the knowledge in existing knowledge repositoriesand the knowledge in the real world is the knowledge on tail verticals (ie; less populardomains). Such knowledge; though not necessarily globally popular; can be personalhobbies to many people and thus collectively impactful. This paper studies the problem ofknowledge verification for tail verticals; that is; deciding the correctness of a given triple.Through comprehensive experimental study we answer the following questions. 1) Can wefind evidence for tail knowledge from an extensive set of sources; including knowledgebases; the web; and query logs? 2) Can we judge correctness of the triples based on thecollected evidence? 3) How can we further improve knowledge verification on tail …,Proceedings of the VLDB Endowment,2017,2
A time machine for information: Looking back to look forward,Xin Luna Dong; Anastasios Kementsietsidis; Wang-Chiew Tan,Abstract Historical data (also called long data) holds the key to understanding when factsare true. It is through long data that one can understand the trends that have developed inthe past; form the audit trails needed for justification; and make predictions about the future.For searching; there is also increasing interest to develop search capabilities over long data.In this article; we first motivate the need to develop a time machine for information that willhelp people" look back" so as to" look forward". We will overview key ideas on threecomponents (extraction; linking; and cleaning) that we believe are central to thedevelopment of any time machine for information. Finally; we conclude with our thoughts onwhat we believe are some interesting open research problems. This article is based on thematerial presented in a tutorial at VLDB 2015.,ACM SIGMOD Record,2016,2
Sourcesight: enabling effective source selection,Theodoros Rekatsinas; Amol Deshpande; Xin Luna Dong; Lise Getoor; Divesh Srivastava,Abstract Recently there has been a rapid increase in the number of data sources and dataservices; such as cloud-based data markets and data portals; that facilitate the collection;publishing and trading of data. Data sources typically exhibit large heterogeneity in the typeand quality of data they provide. Unfortunately; when the number of data sources is large; itis difficult for users to reason about the actual usefulness of sources for their applicationsand the trade-offs between the benefits and costs of acquiring and integrating sources. Inthis demonstration we present\textsc {SourceSight}; a system that allows users tointeractively explore a large number of heterogeneous data sources; and discover valuablesets of sources for diverse integration tasks.\textsc {SourceSight}~ uses a novel multi-levelsource quality index that enables effective source selection at different granularity levels …,Proceedings of the 2016 International Conference on Management of Data,2016,2
Error diagnosis and data profiling with data X-ray,Xiaolan Wang; Mary Feng; Yue Wang; Xin Luna Dong; Alexandra Meliou,Abstract The problem of identifying and repairing data errors has been an area of persistentfocus in data management research. However; while traditional data cleaning techniquescan be effective at identifying several data discrepancies; they disregard the fact that manyerrors are systematic; inherent to the process that produces the data; and thus will keepoccurring unless the root cause is identified and corrected. In this demonstration; we willpresent a large-scale diagnostic framework called D ata XR ay. Like a medical X-ray thataids the diagnosis of medical conditions by revealing problems underneath the surface; Data XR ay reveals hidden connections and common properties among data errors. Thus; incontrast to traditional cleaning methods; which treat the symptoms; our system investigatesthe underlying conditions that cause the errors.,Proceedings of the VLDB Endowment,2015,2
Selection and ordering of candidate documents for effective query answering in XML databases,Mariam Salloum; Vassilis J Tsotras; Divesh Srivastava; Luna Dong,*,Fifth International Workshop on Ranking in Databases,2011,2
Minimal difference query and view matching,*,The subject disclosure pertains to efficient computation of the difference between queries byexploiting commonality between them. A minimal difference query (MDQ) is generated thatroughly corresponds to removal of as many joins as possible while still accuratelyrepresenting the query difference. The minimal difference can be employed to furthersubstantially the scope of view matching where a query is not wholly subsumed by a view.Additionally; the minimal difference query can be employed as an analytical tool in variouscontexts.,*,2009,2
Discovering multiple truths with a hybrid model,Furong Li; Xin Luna Dong; Anno Langen; Yang Li,Abstract: Many data management applications require integrating information from multiplesources. The sources may not be accurate and provide erroneous values. We thus have toidentify the true values from conflicting observations made by the sources. The problem isfurther complicated when there may exist multiple truths (eg; a book written by severalauthors). In this paper we propose a model called Hybrid that jointly makes two decisions:how many truths there are; and what they are. It considers the conflicts between values asimportant evidence for ruling out wrong values; while keeps the flexibility of allowing multipletruths. In this way; Hybrid is able to achieve both high precision and high recall. Subjects:Databases (cs. DB) Cite as: arXiv: 1705.04915 [cs. DB](or arXiv: 1705.04915 v1 [cs. DB] forthis version) Submission history From: Furong Li [view email][v1] Sun; 14 May 2017 04 …,arXiv preprint arXiv:1705.04915,2017,1
Leave no valuable data behind: The crazy ideas and the business,Xin Luna Dong,Abstract With the mission" leave no valuable data behind"; we developed techniques forknowledge fusion to guarantee the correctness of the knowledge. This talk starts withdescribing a few crazy ideas we have tested. The first; known as" Knowledge Vault"; used 15extractors to automatically extract knowledge from 1B+ Webpages; obtaining 3B+ distinct(subject; predicate; object) knowledge triples and predicting well-calibrated probabilities forextracted triples. The second; known as" Knowledge-Based Trust"; estimated thetrustworthiness of 119M webpages and 5.6 M websites based on the correctness of theirfactual information. We then present how we bring the ideas to business in filling the gapbetween the knowledge at Google Knowledge Graph and the knowledge in the world.,Proceedings of the VLDB Endowment,2016,1
Robust group linkage,Pei Li; Xin Luna Dong; Songtao Guo; Andrea Maurino; Divesh Srivastava,Abstract We study the problem of group linkage: linking records that refer to multiple entitiesin the same group. Applications for group linkage include finding businesses in the samechain; finding social network users from the same organization; and so on. Group linkagefaces new challenges compared to traditional entity resolution. First; although differentmembers in the same group can share some similar global values of an attribute; theyrepresent different entities so can also have distinct local values for the same or differentattributes; requiring a high tolerance for value diversity. Second; we need to be able todistinguish local values from erroneous values. We present a robust two-stage algorithm: thefirst stage identifies pivots--maximal sets of records that are very likely to belong to the samegroup; while being robust to possible erroneous values; the second stage collects strong …,Proceedings of the 24th International Conference on World Wide Web,2015,1
Probabilistic Models for Collective Entity Resolution Between Knowledge Graphs,Jay Pujara; Kevin Murphy; Xin Luna Dong; Curtis Janssen,Abstract The growing popularity of structured knowledge bases such as knowledge graphsnecessitates integrating multiple knowledge sources. A key component of this integration isentity resolution (ER); reconciling instances of a single entity occurring in differentknowledge graphs. In contrast to the conventional ER problem setting; we consider thescenario where ER judgments for related entities are made collectively while alsodetermining when a new entity should be added to the graph. Our approach uses hinge-lossMarkov random fields to define a joint probability distribution over entity coreferences. Weapply this model to two publicly-available knowledge graphs; MusicBrainz and Freebasewhere relational structure allows us to collectively resolve musical artists and albums;achieving an F1 of 0.84.,Bay Area Machine Learning Symposium,2014,1
10th international workshop on quality in databases: QDB 2012,Xin Luna Dong; Eduard Constantin Dragut,The problem of low-quality data in databases; data warehouses; and information systemssignificantly and indistinctly affects every application domain. Many data processing tasks(such as information integration; data sharing; information retrieval; and knowledgediscovery from databases) require various forms of data preparation and consolidation withcomplex data processing techniques. These tasks usually assume that the data inputconforms to nice data distributions; containing no missing; inconsistent or incorrect values.This leaves a large gap between the available “dirty” data and the available machinery toeffectively process the data for the application purposes. The term data quality denotes; in abroad sense; a set of properties of the data that indicates various types of error conditions.The Quality in Databases (QDB) workshop is focused on discussing various issues …,ACM SIGMOD Record,2013,1
13th international workshop on the web and databases: Webdb 2010,Xin Luna Dong; Felix Naumann,The WebDB workshop has been held thirteen times so far: the first WebDB workshop wascolocated with EDBT'1998; whereas the other twelve were co-located with the annualSIGMOD/PODS conference. The WebDB workshop provides a forum where researchers;theoreticians; and practitioners can share their knowledge and opinions about problems andsolutions at the intersection of data management and the Web. WebDB has had a highimpact and has been a forum in which a number of seminal papers have been presented.Since 2002; the workshop always had a theme. In 2010 WebDB focused on Quality of WebData and on Linked Data; but papers on all aspects of the web and databases weresolicited; such as unstructured and semi-structured data management; data-extraction;-integration;-cleansing; and-mining; web applications and privacy; search and information …,ACM SIGMOD Record,2011,1
Letter from the Special Issue Editors.,Xin Luna Dong; Wang Chiew Tan,The quality of data has always been important to businesses and intelligence; as policiesand business decisions are often made based on analysis performed on data. This issuecontains articles that explore different aspects of data quality that frequently arise in thecontext of multiple (Web) data sources providing overlapping; complementary; andsometimes contradictory information about the same concept or real world entity. It istherefore important to provide techniques for understanding the truthfulness andtrustworthiness of data sources to the extent possible; and reconcile their differences inorder to create a clean integrated view of the underlying data sources. Techniques for entityresolution; mapping; fusion; and data cleaning are important “ingredients” towards achievingsuch a clean unified view. The first three articles in this issue describe different …,IEEE Data Eng. Bull.,2011,1
Data Quality: The Role of Empiricism,Shazia Sadiq; Tamraparni Dasu; Xin Luna Dong; Juliana Freire; Ihab F Ilyas; Sebastian Link; Miller J Miller; Felix Naumann; Xiaofang Zhou; Divesh Srivastava,Abstract We outline a call to action for promoting empiricism in data quality research. Theaction points result from an analysis of the landscape of data quality research. Thelandscape exhibits two dimensions of empiricism in data quality research relating to type ofmetrics and scope of method. Our study indicates the presence of a data continuum rangingfrom real to synthetic data; which has implications for how data quality methods areevaluated. The dimensions of empiricism and their inter-relationships provide a means ofpositioning data quality research; and help expose limitations; gaps and opportunities.,ACM SIGMOD Record,2018,*
Providing user-interactive graphical timelines,*,Methods; systems; and apparatus; including computer programs encoded on computerstorage media; for combining authentication and application shortcut. An example methodincludes responsive to a user request identifying an entity: identifying a first time periodassociated with the entity based at least on a type of the entity; determining; within the firsttime period; a plurality of first candidate entities associated with the first entity; selecting firstentities in the plurality of first candidate entities according to one or more selection criteria;and providing; for presentation to the user; first user-selectable graphical elements on a firstgraphical user-interactive timeline. Each first user-selectable graphical element identifies acorresponding first entity in the first entities.,*,2016,*
Online data fusion,*,An online data fusion system receives a query; probes a first source for an answer to thequery; returns the answer from the first source; refreshes the answer while probing anadditional source; and applies fusion techniques on data associated with an answer that isretrieved from the additional source. For each retrieved answer; the online data fusionsystem computes the probability that the answer is correct and stops retrieving data for theanswer after gaining enough confidence that data retrieved from the unprocessed sourcesare unlikely to change the answer. The online data fusion system returns correct answersand terminates probing additional sources in an expeditious manner without sacrificing thequality of the answers.,*,2016,*
Method and apparatus for exploring and selecting data sources,*,A system and method for choosing data sources for use in a data repository first chooses aninitial selection of data sources based on keywords. An exploration tool is provided toorganize the sources according to content and other attributes. The tool is used to pre-selectdata sources. The sources to include in the data repository are then selected based on amarginalism economic theory that considers both costs and quality of data.,*,2013,*
Securing database content,*,A method for securing content in a database includes identifying a challenge columnassociated with a database column referenced in an update query. A challenge value for thechallenge column may be received and resolved for a match with a corresponding valuestored in the challenge column. In case of a match; the update query may be certified forexecution on the database; otherwise; the update query may be prevented from executing.Challenge columns may be determined by an analysis of the database on the basis ofdiscriminating power; description complexity; and/or diversity.,*,2012,*
WebDB 2010 at ACM SIGMOD 2010 13th International Workshop on the Web and Databases,Xin Luna Dong; Felix Naumann,The WebDB workshop focuses on providing a forum where researchers; theoreticians; andpractitioners can share their knowledge and opinions about problems and solutions at theintersection of data management and the Web. WebDB has high impact and has been aforum in which a number of seminal papers have been presented. The workshop chairswelcome researchers and practitioners to register for the workshop at http://webdb2010.org/registration. html. The registration fee is $100; rebates for students ($65) and ACMmembers ($80) are available. Breakfast; lunch; and coffee breaks will be provided.,SIGMOD Record,2009,*
XML Indexing,Xin Luna Dong; Divesh Srivastava,XML access control refers to the practice of limiting access to (parts of) XML data to onlyauthorized users. Similar to access control over other types of data and resources; XMLaccess control is centered around two key problems:(i) the development of formal models forthe specification of access control policies over XML data; and (ii) techniques for efficientenforcement of access control policies over XML data.,*,2009,*
SEMEX: Mining for Personal Information Integration,Xin Dong; Alon Halevy; Ema Nemes; Stephan B Sigurdsson; Pedro Domingos,Abstract. Personal information management is one of the key applications of the semanticweb. Whereas today's devices store data according to applications; ideal personalinformation management system should treat all data as a set of meaningful objects andassociations between the objects. To ensure extensibility; a personal informationmanagement system should automatically incorporate associations generated in multipleways: mining specific personal data sources; or integrating with external data. As a first stepin this direction; we describe the Semex system that provides a logical and integrated viewof one's personal information.,Mining for and from the Semantic Web 2004 (SWM 2004),2004,*
Topology Transformation in a Peer Data Management System,Xin Luna Dong,*,*,*,*
Streamed Update Propagation in a Peer Data Management System,Xin Dong; Lin Liao; Zizhen Yao,Abstract Peer to peer distributed system has been widely studied in recent years because ofits appealing properties: scalability; robustness and no need for central control. Peer datamanagement system (PDMS) rises up above P2P system to exploit the power of availabledata management technology. The goal of PDMS is to facilitate wide-scale data sharingwithin and across communities. This paper focuses on one particular problem regarding todata sharing; the update propagation problem. The problem is defined as follows: givenmultiple data sources and materialized views based on these base data; all of which aredistributed on different hosts across Internet; how to synchronize the views when the datasource has been updated? In this paper; we proposes eager join; lazy merge policy thatpreserves resources without comprising the performance; and improves performance …,*,*,*
Querying and Searching a Dataspace,Xin Dong; Alon Halevy; Jing Liu,Many data management applications involve managing dataspaces; which are largecollections of highly heterogeneous data sources and partially unstructured data [5]. Thedistinguishing aspect of dataspaces from data integration systems is that dataspaces do notnecessarily include semantic mappings between data sources; and hence the data sourcesare only loosely coupled. Examples of dataspaces are ubiquitous; including enterprises andgovernment agencies; personal information on the desktop or “smart homes”; digital librariesand scientific data. The goal of Dataspace Support Platforms (DSSPs) is to provide a set ofservices that recur in many dataspace contexts; such as search and query; discovery ofsources; and dataspace evolution. The key idea behind DSSPs is a “pay as you go”approach to integration: provide some services from the outset; and evolve the …,*,*,*
Data Engineering,Philip A Bernstein; Nishant Dani; Badriddine Khessib; Ramesh Manne; David Shutt; Jayant Madhavan; Alon Halevy; Shirley Cohen; Xin Luna Dong; Shawn R Jeffery; David Ko; Cong Yu; Varun Bhagwan; Mike Ching; Alex Cozzi; Raj Desai; Daniel Gruhl; Kevin Haas; Linda Kato; Jeff Kusnitz; Bryan Langston; Ferdy Nagy; Linda Nguyen; Jan Pieper; Savitha Srinivasan; Anthony Stuart; Renjie Tang,The Bulletin of the Technical Committee on Data Engineering is published quarterly and isdistributed to all TC members. Its scope includes the design; implementation; modelling;theory and application of database systems and their technology. Letters; conferenceinformation; and news should be sent to the Editor-in-Chief. Papers for each issue aresolicited by and should be sent to the Associate Editor responsible for the issue. Opinionsexpressed in contributions are those of the authors and do not necessarily reflect thepositions of the TC on Data Engineering; the IEEE Computer Society; or the authors'organizations. Membership in the TC on Data Engineering is open to all current members ofthe IEEE Computer Society who are interested in database systems. There are two DataEngineering Bulletin web sites: http://www. research. microsoft. com/research/db/debull …,*,*,*
