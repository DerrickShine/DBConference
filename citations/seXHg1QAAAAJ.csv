Klee: A framework for distributed top-k query algorithms,Sebastian Michel; Peter Triantafillou; Gerhard Weikum,Abstract This paper addresses the efficient processing of top-k queries in wide-areadistributed data repositories where the index lists for the attribute values (or text terms) of aquery are distributed across a number of data peers and the computational costs includenetwork latency; bandwidth consumption; and local peer work. We present KLEE; a novelalgorithmic framework for distributed top-k queries; designed for high performance andflexibility. KLEE makes a strong case for approximate top-k algorithms over widelydistributed data sources. It shows how great gains in efficiency can be enjoyed at low result-quality penalties. Further; KLEE affords the query-initiating peer the flexibility to trade-offresult quality and expected performance and to trade-off the number of communicationphases engaged during query execution versus network bandwidth performance. We …,Proceedings of the 31st international conference on Very large data bases,2005,297
Minerva: Collaborative p2p search,Matthias Bender; Sebastian Michel; Peter Triantafillou; Gerhard Weikum; Christian Zimmer,Abstract This paper proposes the live demonstration of a prototype of MINERVA; a novelP2P Web search engine. The search engine is layered on top of a DHT-based overlaynetwork that connects an a-priori unlimited number of peers; each of which maintains apersonal local database and a local search facility. Each peer posts a small amount ofmetadata to a physically distributed directory that is used to efficiently select promising peersfrom across the peer population that can best locally execute a query. The proposeddemonstration serves as a proof of concept for P2P Web search by deploying the project onstandard notebook PCs and also invites everybody to join the network by instantly installinga small piece of software from a USB memory stick.,Proceedings of the 31st international conference on Very large data bases,2005,160
Towards High Performance Peer-to-Peer Content and Resource Sharing Systems.,Peter Triantafillou; Chryssani Xiruhaki; Manolis Koubarakis; Nikos Ntarmos,Abstract Peer-to-peer sharing systems are becoming increasingly popular and an excitingnew class of innovative; internet-based data management systems. In these systems; userscontribute their own resources (processing units and storage devices) and content (ie;documents) to the P2P community. We focus on the management of content and resourcesin such systems. Our goal is to harness all available resources in the P2P network so thatthe users can access all available content efficiently. Efficiency is taken both from (i) thepoint of view of the system; in that we strive to ensure fair load distribution among all peernodes; and (ii) from the point of view of the users; in that we strive to ensure low user-requestresponse times. We propose a novel architecture for this new class of applications; whichdiffers drastically from what is either found currently in existing products or proposed in …,CIDR,2003,147
Improving collection selection with overlap awareness in p2p search engines,Matthias Bender; Sebastian Michel; Peter Triantafillou; Gerhard Weikum; Christian Zimmer,Abstract Collection selection has been a research issue for years. Typically; in related work;precomputed statistics are employed in order to estimate the expected result quality of eachcollection; and subsequently the collections are ranked accordingly. Our thesis is that thissimple approach is insufficient for several applications in which the collections typicallyoverlap. This is the case; for example; for the collections built by autonomous peers crawlingthe web. We argue for the extension of existing quality measures using estimators of mutualoverlap among collections and present experiments in which this combination outperformsCORI; a popular approach based on quality estimation. We outline our prototypeimplementation of a P2P web search engine; coined MINERVA; that allows handling largeamounts of data in a distributed and self-organizing manner. We conduct experiments …,Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,2005,129
Content-based publish-subscribe over structured P2P networks,Peter Triantafillou; Ioannis Aekaterinidis,Abstract In this work we leverage the advantages of the Chord DHT to build a content-basedpublish-subscribe system that is scalable; self-organizing; and well-performing. However;DHTs provide very good support only for exact-match; equality predicates and rangepredicates are expected to be very popular when specifying subscriptions in pub/subsystems We will thus also provide solutions supporting efficiently subscriptions with rangepredicates in Chordbased pub/sub systems.,Proceedings of the 4th International Workshop on Distributed Event-Based Systems,2004,123
Replication; load balancing and efficient range query processing in DHTs,Theoni Pitoura; Nikos Ntarmos; Peter Triantafillou,Abstract We consider the conflicting problems of ensuring data-access load balancing andefficiently processing range queries on peer-to-peer data networks maintained overDistributed Hash Tables (DHTs). Placing consecutive data values in neighboring peers isfrequently used in DHTs since it accelerates range query processing. However; such aplacement is highly susceptible to load imbalances; which are preferably handled byreplicating data (since replication also introduces fault tolerance benefits). In this paper; wepresent HotRoD; a DHT-based architecture that deals effectively with this combined problemthrough the use of a novel locality-preserving hash function; and a tunable data replicationmechanism which allows trading off replication costs for fair load distribution. Our detailedexperimentation study shows strong gains in both range query processing efficiency and …,International Conference on Extending Database Technology,2006,121
Pastrystrings: A comprehensive content-based publish/subscribe dht network,Ioannis Aekaterinidis; Peter Triantafillou,In this work we propose and develop a comprehensive infrastructure; coined PastryStrings;for supporting rich queries on both numerical (with range; and comparison predicates) andstring attributes;(accommodating equality; prefix; suffix; and containment predicates) overDHT networks utilising prefix-based routing. As event-based; publish/subscribe informationsystems are a champion application class; we formulate our solution in terms of thisenvironment.,Distributed Computing Systems; 2006. ICDCS 2006. 26th IEEE International Conference on,2006,114
Subscription summarization: A new paradigm for efficient publish/subscribe systems,Peter Triantafillou; Andreas Economides,We contribute a new paradigm for publish/subscribe systems. It is centered on the novelnotion of subscription summarization. We first present the summarization structures for abroker's subscriptions and accompanying algorithms; which operate on the summarystructures to match incoming events to the brokers with relevant subscriptions and for themaintenance of subscriptions in the face of updates. Second; we present novel algorithmsfor efficiently propagating subscription summaries to brokers. Finally; we present a novelalgorithm for the efficient distributed processing of incoming events; utilizing the propagatedsubscription summaries to route the events to brokers with matched subscriptions. We studythe performance of our contributions; comparing them against a baseline approach andagainst corresponding techniques employed in a well-known event-based distributed …,Distributed Computing Systems; 2004. Proceedings. 24th International Conference on,2004,109
Towards a unifying framework for complex query processing over structured peer-to-peer data networks,Peter Triantafillou; Theoni Pitoura,Abstract In this work we study how to process complex queries in DHT-based Peer-to-Peer(P2P) data networks. Queries are made over tuples and relations and are expressed in aquery language; such as SQL. We describe existing research approaches for queryprocessing in P2P systems; we suggest improvements and enhancements; and propose aunifying framework that consists of a modified DHT architecture; data placement and searchalgorithms; and provides efficient support for processing a variety of query types; includingqueries with one or more attributes; queries with selection operators (involving equality andrange queries); and queries with join operators. To our knowledge; this is the first work thatputs forth a framework providing support for all these query types.,International Workshop on Databases; Information Systems; and Peer-to-Peer Computing,2003,96
Principles of optimally placing data in tertiary storage libraries,Stavros Christodoulakis; Peter Triantafillou; Fenia A Zioga,Abstract Recently; technological advances have resulted in the wide availability ofcommercial products offering near-line; robot-based; tertiary storage libraries. Thus; suchlibraries have become a crucial component of modern largescale storage servers; given thevery large storage requirements of modern applications. Although the subject of optimal dataplacement (ODP) strategies has received considerable attention for other storage devices(such as magnetic and optical disks and disk arrays); the issue of optimal data placement intertiary libraries has been neglected. The latter issue is more critical since tertiary storageremains three orders of magnitude slower than secondary storage. In this paper; we addressthis issue by deriving such optimal placement algorithms. First; we study the ODP problem indisk libraries (jukeboxes) and subsequently; in tape libraries. In our studies; we consider …,VLDB,1997,62
P2P Content Search: Give the Web Back to the People.,Matthias Bender; Sebastian Michel; Peter Triantafillou; Gerhard Weikum; Christian Zimmer,ABSTRACT The proliferation of peer-to-peer (P2P) systems has come with variouscompelling applications including file sharing based on distributed hash tables (DHTs) orother kinds of overlay networks. Searching the content of files (especially Web Search)requires multi-keyword querying with scoring and ranking. Existing approaches have no wayof taking into account the correlation between the keywords in the query. This paperpresents our solution that incorporates the queries and behavior of the users in the P2Pnetwork such that interesting correlations can be inferred.,IPTPS,2006,59
SeAl: Managing accesses and data in peer-to-peer sharing networks,Nikos Ntarmos; Peter Triantafillou,We present SeAl; a novel data/resource and data-access management infrastructuredesigned for the purpose of addressing a key problem in P2P data sharing networks;namely the problem of wide-scale selfish peer behavior. Selfish behavior has beenmanifested and well documented and it is widely accepted that unless this is dealt with; thescalability; efficiency; and the usefulness of P2P sharing networks will be diminished. SeAlessentially consists of a monitoring/accounting subsystem; an auditing/verificationsubsystem; and incentive mechanisms. The monitoring subsystem facilitates theclassification of peers into selfish/altruistic. The auditing/verification layer provides a shieldagainst perjurer/slandering and colluding peers that may try to cheat the monitoringsubsystem. The incentives mechanisms effectively utilize these layers so to increase the …,Peer-to-Peer Computing; 2004. Proceedings. Proceedings. Fourth International Conference on,2004,54
Subscription summaries for scalability and efficiency in publish/subscribe systems,Peter Triantafillou; Andreas Economides,A key issue when designing and implementing largescale publish/subscribe systems is howto efficiently propagate subscriptions among the brokers of the system. Brokers require thisinformation in order to forward incoming events only to interested users; filtering outunrelated events; which can save significant overheads (particularly network bandwidth andprocessing time at the brokers). In this paper we contribute the notion of subscriptionsummaries; a mechanism appropriately compacting subscription information. We developthe associated data structures and matching algorithms. The proposed mechanism canhandle event/subscription schemata that are rich in terms of their attribute types andpowerful in terms of the allowed operations on them. Our major results are that the proposedmechanism (i) is scalable; with the bandwidth required to propagate subscriptions …,Distributed Computing Systems Workshops; 2002. Proceedings. 22nd International Conference on,2002,53
Discovering and exploiting keyword and attribute-value co-occurrences to improve P2P routing indices,Sebastian Michel; Matthias Bender; Nikos Ntarmos; Peter Triantafillou; Gerhard Weikum; Christian Zimmer,Abstract Peer-to-Peer (P2P) search requires intelligent decisions for query routing: selectingthe best peers to which a given query; initiated at some peer; should be forwarded forretrieving additional search results. These decisions are based on statistical summaries foreach peer; which are usually organized on a per-keyword basis and managed in adistributed directory of routing indices. Such architectures disregard the possiblecorrelations among keywords. Together with the coarse granularity of per-peer summaries;which are mandated for scalability; this limitation may lead to poor search result quality. Thispaper develops and evaluates two solutions to this problem; sk-STAT based on single-keystatistics only; and mk-STAT based on additional multi-key statistics. For both cases; hashsketch synopses are used to compactly represent a peer's data items and are efficiently …,Proceedings of the 15th ACM international conference on Information and knowledge management,2006,52
Longitudinal analytics on web archive data: it's about time!,Gerhard Weikum; Nikos Ntarmos; Marc Spaniol; Peter Triantafillou; András A Benczúr; Scott Kirkpatrick; Philippe Rigaux; Mark Williamson,ABSTRACT Organizations like the Internet Archive have been capturing Web contents overdecades; building up huge repositories of time-versioned pages. The timestamp annotationsand the sheer volume of multi-modal content constitutes a gold mine for analysts of all sorts;across different application areas; from political analysts and marketing agencies toacademic researchers and product developers. In contrast to traditional data analytics onclick logs; the focus is on longitudinal studies over very long horizons. This longitudinalaspect affects and concerns all data and metadata; from the content itself; to the indices andthe statistical metadata maintained for it. Moreover; advanced analysts prefer to deal withsemantically rich entities like people; places; organizations; and ideally relationships suchas company acquisitions; instead of; say; Web pages containing such references. For …,CIDR,2011,49
Disk scheduling for mixed-media workloads in a multimedia server,Y Rompogiannakis; Guido Nerjes; Peter Muth; Michael Paterakis; Peter Triantafillou; Gerhard Weikum,310st mutimedia appfic~. ons require storage and rm” ev~ of hge amounts of continuous andficrete data at v~ l high rat~ Disk driv= should be servicing such tid workloads achieving lowrqonse times for discrete requests; while guaruntea” ng the uninterrupted delivery of coIti.nuous data Disk scheduling algorithms for J- workloa&; &hough they play a central role inthis task have been overlooked by related mutimedia r~ earch efforts; which so far haveJmstfy concentrated on the scheduling of coI*” nuous requ- 03dv.~ e focus of this paper is one~ iti disk WO scheduling algorithms for _ workbads in a mutimedia storage server. Wepropose novel algorithms; a tonomy of r~~ mzt algorithms; and stud~'their p~ ormancethrough~ m. mentatiom Our resuti show that our proposed dgo~ hms offer drasticimprovements in discrete request average response times; low r~ onse-time vm” abifity …,Proceedings of the sixth ACM international conference on Multimedia,1998,49
Internet scale string attribute publish/subscribe data networks,Ioannis Aekaterinidis; Peter Triantafillou,Abstract With this work we aim to make a three-fold contribution. We first address the issue ofsupporting efficiently queries over string-attributes involving prefix; suffix; containment; andequality operators in large-scale data networks. Our first design decision is to employdistributed hash tables (DHTs) for the data network's topology; harnessing their desirableproperties. Our next design decision is to derive DHT-independent solutions; treating DHTas a black box. Second; we exploit this infrastructure to develop efficient content basedpublish/subscribe systems. The main contribution here are algorithms for the efficientprocessing of queries (subscriptions) and events (publications). Specifically; we show thatour subscription processing algorithms require O (logN) messages for a N-node network;and our event processing algorithms require O (lx logN) messages (with l being the …,Proceedings of the 14th ACM international conference on Information and knowledge management,2005,48
Overlay striping and optimal parallel I/O for modern applications,Peter Triantafillou; Christos Faloutsos,Abstract Disk array systems are rapidly becoming the secondary-storage media of choice formany emerging applications with large storage and high bandwidth requirements. Stripingdata across the disks of a disk array introduces significant performance benefits mainlybecause the effective transfer rate of the secondary storage is increased by a factor equal tothe stripe width. However; the choice of the optimal stripe width is an open problem: nogeneral formal analysis has been reported and intuition alone fails to provide goodguidelines. As a result one may find occasionally contradictory recommendations in theliterature. With this work we first contribute an analytical calculation of the optimal stripewidth. Second; we recognize that the optimal stripe width is sensitive to themultiprogramming level; which is not known a priori and fluctuates with time. Thus …,Parallel Computing,1998,46
Iqn routing: Integrating quality and novelty in p2p querying and ranking,Sebastian Michel; Matthias Bender; Peter Triantafillou; Gerhard Weikum,Abstract We consider a collaboration of peers autonomously crawling the Web. A pivotalissue when designing a peer-to-peer (P2P) Web search engine in this environment is queryrouting: selecting a small subset of (a potentially very large number of relevant) peers tocontact to satisfy a keyword query. Existing approaches for query routing work well ondisjoint data sets. However; naturally; the peers' data collections often highly overlap; aspopular documents are highly crawled. Techniques for estimating the cardinality of theoverlap between sets; designed for and incorporated into information retrieval engines arevery much lacking. In this paper we present a comprehensive evaluation of appropriateoverlap estimators; showing how they can be incorporated into an efficient; iterativeapproach to query routing; coined Integrated Quality Novelty (IQN). We propose to further …,International Conference on Extending Database Technology,2006,43
High performance data broadcasting systems,Peter Triantafillou; R Harpantidou; Michael Paterakis,Abstract Data broadcasting as a means of efficient data dissemination is a key technologyfacilitating ubiquitous computing. For this reason; broadcast scheduling algorithms havereceived a lot of attention. However; all existing algorithms make the core assumption thatthe data items to be broadcast are immediately available in the transmitter's queue; ignoringthe key role that the disk subsystem and the cache management play in the overallbroadcast system performance. With this paper we contribute a comprehensive system'sperspective towards the development of high performance broadcast systems; taking intoaccount how broadcast scheduling; disk scheduling; and cache management algorithmsaffect the overall performance. We contribute novel techniques that ensure an efficientinterplay between broadcast scheduling; cache management; and disk scheduling. We …,Mobile Networks and Applications,2002,42
Counting at large: Efficient cardinality estimation in internet-scale data networks,Nikos Ntarmos; Peter Triantafillou; Gerhard Weikum,Counting in general; and estimating the cardinality of (multi-) sets in particular; is highlydesirable for a large variety of applications; representing a foundational block for the efficientdeployment and access of emerging internetscale information systems. Examples of suchapplications range from optimizing query access plans in internet-scale databases; toevaluating the significance (rank/score) of various data items in information retrievalapplications. The key constraints that any acceptable solution must satisfy are:(i) efficiency:the number of nodes that need be contacted for counting purposes must be small in order toenjoy small latency and bandwidth requirements;(ii) scalability; seemingly contradicting theefficiency goal: arbitrarily large numbers of nodes nay need to add elements to a (multi-) set;which dictates the need for a highly distributed solution; avoiding server-based scalability …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,41
On-demand data elevation in a hierarchical multimedia storage server,Peter Triantafillou; Thomas Papadakis,Abstract Given the present cost of memories and the very large storage and bandwidthrequirements of large-scale multimedia databases; hierarchical storage servers (whichconsist of RAM; disk storage; and robot-based tertiary libraries) are becoming increasinglypopular. However; related research is scarce and employs tertiary storage for storageaugmentation purposes only. This work; exploiting the ever-increasing performance o eredby (particularly) modern tape library products; aims to utilize tertiary storage in order toaugment the system's performance. We consider the issue of elevating continuous data fromits permanent place in tertiary for display purposes. Our primary goals are to save on thesecondary storage bandwidth that traditional techniques require for the display ofcontinuous objects; while requiring no additional RAM bu er space. To this end we …,VLDB,1997,41
The location-based paradigm for replication: Achieving efficiency and availability in distributed systems,Peter Triantafillou; David J.  Taylor,Replication techniques for transaction-based distributed systems generally achieveincreased availability but with a significant performance penalty. We present a newreplication paradigm; the location-based paradigm; which addresses availability and otherperformance issues. It provides availability similar to quorum-based replication protocols butwith transaction-execution delays similar to one-copy systems. The paradigm further exploitsreplication to improve performance in two instances. First; it takes advantage of local ornearby replicas to further improve the response time of transactions; achieving smallerexecution delays than one-copy systems. Second; it takes advantage of replication tofacilitate the independent crash recovery of replica sites-a goal which is unattainable in one-copy systems. In addition to the above the location-based paradigm avoids bottlenecks …,IEEE Transactions on Software Engineering,1995,40
Minerva∞: A scalable efficient peer-to-peer search engine,Sebastian Michel; Peter Triantafillou; Gerhard Weikum,Abstract The promises inherent in users coming together to form data sharing networkcommunities; bring to the foreground new problems formulated over such dynamic; evergrowing; computing; storage; and networking infrastructures. A key open challenge is toharness these highly distributed resources toward the development of an ultra scalable;efficient search engine. From a technical viewpoint; any acceptable solution must fullyexploit all available resources dictating the removal of any centralized points of control;which can also readily lead to performance bottlenecks and reliability/availability problems.Equally importantly; however; a highly distributed solution can also facilitate pluralism ininforming users about internet content; which is crucial in order to preclude the formation ofinformation-resource monopolies and the biased visibility of content from economically …,Proceedings of the ACM/IFIP/USENIX 2005 International Conference on Middleware,2005,39
A comprehensive analytical performance model for disk devices under random workloads,Peter Triantafillou; Stavros Christodoulakis; Costas A Georgiadis,Our goal is to contribute a common theoretical framework for studying the performance ofdisk-storage devices. Understanding the performance behavior of these devices will allowprediction of the I/O cost in modern applications. Current disk technologies differ in terms ofthe fundamental modeling characteristics; which include the magnetic/optical nature;angular and linear velocities; storage capacities; and transfer rates. Angular and linearvelocities; storage capacities; and transfer rates are made constant or variable in differentexisting disk products. Related work in this area has studied Constant Angular Velocity(CAV) magnetic disks and Constant Linear Velocity (CLV) optical disks. We present acomprehensive analytical model; validated through simulations; for the random retrievalperformance of disk devices which takes into account all the above-mentioned …,IEEE Transactions on Knowledge and data Engineering,2002,37
Load distribution fairness in p2p data management systems,Theoni Pitoura; Peter Triantafillou,We address the issue of measuring storage; or query load distribution fairness in peer-to-peer data management systems. Existing metrics may look promising from the point of viewof specific peers; while in reality being far from optimal from a global perspective. Thus; firstwe define the requirements and study the appropriateness of various statistical metrics formeasuring load distribution fairness towards these requirements. The metric proposed asmost appropriate is the Gini coefficient (G). Second; we develop novel distributed samplingalgorithms to compute G on-line; with high precision; efficiently; and scalably. Third; weshow how G can readily be utilized on-line by higher-level algorithms which can now knowwhen to best intervene to correct load imbalances. Our analysis and experiments testify forthe efficiency and accuracy of these algorithms; permitting the online use of a rich and …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,35
Optimal data placement on disks: a comprehensive solution for different technologies,Peter Triantafillou; Stavros Christodoulakis; Costas Georgiadis,The problem of optimally placing data on disks (ODP) to maximize disk-access performancehas long been recognized as important. Solutions to this problem have been reported forsome widely available disk technologies; such as magnetic CAV and optical CLV disks.However; important new technologies such as multizoned magnetic disks; have beenrecently introduced. For such technologies no formal solution to the ODP problem has beenreported. In this paper; we first identify the fundamental characteristics of disk-devicetechnologies which influence the solution to the ODP problem. We develop acomprehensive solution to the problem that covers all currently available disk technologies.We show how our comprehensive solution can be reduced to the solutions for existing disktechnologies; contributing thus a solution to the ODP problem for multizoned disks. Our …,IEEE Transactions on Knowledge and Data Engineering,2000,35
Combining information extraction and human computing for crowdsourced knowledge acquisition,Sarath Kumar Kondreddi; Peter Triantafillou; Gerhard Weikum,Automatic information extraction (IE) enables the construction of very large knowledgebases (KBs); with relational facts on millions of entities from text corpora and Web sources.However; such KBs contain errors and they are far from being complete. This motivates theneed for exploiting human intelligence and knowledge using crowd-based humancomputing (HC) for assessing the validity of facts and for gathering additional knowledge.This paper presents a novel system architecture; called Higgins; which shows how toeffectively integrate an IE engine and a HC engine. Higgins generates game questionswhere players choose or fill in missing relations for subject-relation-object triples. Forgenerating multiple-choice answer candidates; we have constructed a large dictionary ofentity names and relational phrases; and have developed specifically designed statistical …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,34
Global document frequency estimation in peer-to-peer web search,Matthias Bender; Sebastian Michel; Peter Triantafillou; Gerhard Weikum,This paper presents an efficient solution for the problem of estimating global documentfrequencies in a large-scale P2P network with very high dynamics where peers can join andleave the network on short notice. In particular; the developed method takes into account thefact that the local document collections of autonomous peers may arbitrarily overlap; so thatglobal counting needs to be duplicateinsensitive. The method is based on hash sketches asa technique for compact data synopses. Experimental studies demonstrate the estimator'saccuracy; scalability; and ability to cope with high dynamics. Moreover; the benefit forranking P2P search results is shown by experiments with real-world Web data and queries.,Proc. of the 9th Int. Workshop on the web and databases,2006,34
eXO: Decentralized Autonomous Scalable Social Networking.,Andreas Loupasakis; Nikos Ntarmos; Peter Triantafillou; Darko Makreshanski,Social networks have been receiving increasing attention throughout recent years;specifically due to the success of sites like Facebook; Twitter; Youtube; Flickr; etc. As theirpopularity has increased; several issues have arised revolving in general aroundcentralized social networks as a type of social networks. First and foremost is the issue ofprivacy. All centralized social network sites require the user to upload the content they willshare to their site; which typically involves releasing of ownership of content from the user tothe service provider. Furthermore; there are the privacy statements which in many cases aredifficult to read and actually rarely read by users. Also; there are the laws in specificcountries requiring service providers to keep user data for several years even after the userhas deleted his account with obligation to provide the data to the authorities upon request …,CIDR,2011,33
Dynamic Web Service discovery architecture based on a novel peer based overlay network,Spyros Sioutas; Evangelos Sakkopoulos; Ch Makris; Bill Vassiliadis; A Tsakalidis; Peter Triantafillou,Abstract Service Oriented Computing and its most famous implementation technology WebServices (WS) are becoming an important enabler of networked business models. Discoverymechanisms are a critical factor to the overall utility of Web Services. So far; discoverymechanisms based on the UDDI standard rely on many centralized and area-specificdirectories; which poses information stress problems such as performance bottlenecks andfault tolerance. In this context; decentralized approaches based on Peer to Peer overlaynetworks have been proposed by many researchers as a solution. In this paper; we proposea new structured P2P overlay network infrastructure designed for Web Services Discovery.We present theoretical analysis backed up by experimental results; showing that theproposed solution outperforms popular decentralized infrastructures for web discovery …,Journal of Systems and Software,2009,33
AESOP: Altruism-Endowed self-organizing peers,Nikos Ntarmos; Peter Triantafillou,Abstract We argue the case for a new paradigm for architecting structured P2P overlaynetworks; coined AESOP. AESOP consists of 3 layers:(i) an architecture; PLANES; thatensures significant performance speedups; assuming knowledge of altruistic peers;(ii) anaccounting/auditing layer; AltSeAl; that identifies and validates altruistic peers; and (iii)SeAledPLANES; a layer that facilitates the coordination/collaboration of the previous twocomponents. We briefly present these components along with experimental and analyticaldata of the promised significant performance gains and the related overhead. In light ofthese very encouraging results; we put this three-layer architecture paradigm forth as theway to structure the P2P overlay networks of the future.,International Workshop on Databases; Information Systems; and Peer-to-Peer Computing,2004,30
Achieving strong consistency in a distributed file system,Peter Triantafillou; Carl Neilson,Distributed file systems need to provide for fault tolerance. This is typically achieved with thereplication of files. Existing approaches to the construction of replicated file systems sacrificestrong semantics (ie the guarantees the systems make to running computations whenfailures occur and/or files are accessed concurrently). This is done mainly for efficiencyreasons. This paper puts forward a replicated file system protocol that enforces strongconsistency semantics. Enforcing strong semantics allows for distributed systems to behavemore like their centralized counterparts-an essential feature in order to provide thetransparency that is so strived for in distributed computing systems. One characteristic of ourprotocol is its distributed nature. Because of it; the extra cost needed to ensure the strongerconsistency is kept low (since the bottleneck problem noticed in primary-copy systems is …,IEEE Transactions on Software Engineering,1997,28
Research and development issues for large-scale multimedia information systems,Stavros Christodoulakis; Peter Triantafillou,During the last few years a convergence in the industries of computers and software;telecommunications; publishing; and consumer electronics has been observed. This is ofmajor industrial importance. The convergence of the relevant technologies will allowinformation to be distributed on demand from very large information servers to homeelectronics devices(called Set-Top Units; STUS) through a variety of high-capacitytelecommunication media. Private and public organizations from these industries haveformed the Digital Audio-Visual Council(DAVIC). DAVIC has produced a draft standard thatdescribes the structure for information servers; telecommunication systems; STUS; and thecommunication protocols. The objective is to achieve“rapid; industry-driven consensus” inavoiding problems of incompatibility while facilitating the fast development of the …,ACM Computing Surveys (CSUR),1995,27
Video placement and configuration of distributed video servers on cable TV networks,Constantinos Vassilakis; Michael Paterakis; Peter Triantafillou,Abstract. A large-scale; distributed video-on-demand (VOD) system allows geographicallydispersed residential and business users to access video services; such as movies andother multimedia programs or documents on demand from video servers on a high-speednetwork. In this paper; we first demonstrate through analysis and simulation the need for ahierarchical architecture for the VOD distribution network. We then assume a hierarchicalarchitecture; which fits the existing tree topology used in today's cable TV (CATV) hybridfiber/coaxial (HFC) distribution networks. We develop a model for the video programplacement; configuration; and performance evaluation of such systems. Our approach takesinto account the user behavior; the fact that the user requests are transmitted over a sharedchannel before reaching the video server containing the requested program; the fact that …,Multimedia Systems,2000,26
Interval indexing and querying on key-value cloud stores,George Sfakianakis; Ioannis Patlakas; Nikos Ntarmos; Peter Triantafillou,Cloud key-value stores are becoming increasingly more important. Challengingapplications; requiring efficient and scalable access to massive data; arise every day. Wefocus on supporting interval queries (which are prevalent in several data intensiveapplications; such as temporal querying for temporal analytics); an efficient solution forwhich is lacking. We contribute a compound interval index structure; comprised of twotiers:(i) the MRSegmentTree (MRST); a key-value representation of the Segment Tree; and(ii) the Endpoints Index (EPI); a column family index that stores information for intervalendpoints. In addition to the above; our contributions include:(i) algorithms for efficientlyconstructing and populating our indices using MapReduce jobs;(ii) techniques for efficientand scalable index maintenance; and (iii) algorithms for processing interval queries. We …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,25
Distributed hash sketches: Scalable; efficient; and accurate cardinality estimation for distributed multisets,Nikos Ntarmos; Peter Triantafillou; Gerhard Weikum,Abstract Counting items in a distributed system; and estimating the cardinality of multisets inparticular; is important for a large variety of applications and a fundamental building block foremerging Internet-scale information systems. Examples of such applications range fromoptimizing query access plans in peer-to-peer data sharing; to computing the significance(rank/score) of data items in distributed information retrieval. The general formal problemaddressed in this article is computing the network-wide distinct number of items with someproperty (eg; distinct files with file name containing “spiderman”) where each node in thenetwork holds an arbitrary subset; possibly overlapping the subsets of other nodes. The keyrequirements that a viable approach must satisfy are:(1) scalability towards very largenetwork size;(2) efficiency regarding messaging overhead;(3) load balance of storage …,ACM Transactions on Computer Systems (TOCS),2009,25
Xl peer-to-peer pub/sub systems,Anne-Marie Kermarrec; Peter Triantafillou,Abstract Increasingly; one of the most prominent ways to disseminate information on theWeb is through “notifications”(also known as alerts); and as such they are at the core ofmany large-scale applications. For instance; users are notified of articles in which they areinterested through RSS feeds; of posts from their friends through social networks; or ofrecommendation generated by various sites. Event notification usually relies on the so-called Publish-Subscribe (P ub/S ub) communication paradigm. In P ub/S ub systems;subscribers sign up for events or classes of events in order to be asynchronously notifiedafterward by the system. The size of such systems (with respect to events and subscriptions)keeps growing; and providing scalable implementations of P ub/S ub systems is extremelychallenging. Although there exist popular examples of centralized P ub/S ub systems that …,ACM Computing Surveys (CSUR),2013,24
Distributed top-k aggregation queries at large,Thomas Neumann; Matthias Bender; Sebastian Michel; Ralf Schenkel; Peter Triantafillou; Gerhard Weikum,Abstract Top-k query processing is a fundamental building block for efficient ranking in alarge number of applications. Efficiency is a central issue; especially for distributed settings;when the data is spread across different nodes in a network. This paper introduces noveloptimization methods for top-k aggregation queries in such distributed environments. Theoptimizations can be applied to all algorithms that fall into the frameworks of the prior TPUTand KLEE methods. The optimizations address three degrees of freedom: 1) hierarchicallygrouping input lists into top-k operator trees and optimizing the tree structure; 2) computingdata-adaptive scan depths for different input sources; and 3) data-adaptive sampling of asmall subset of input sources in scenarios with hundreds or thousands of query-relevantnetwork nodes. All optimizations are based on a statistical cost model that utilizes local …,Distributed and Parallel Databases,2009,23
Scheduling strategies for mixed workloads in multimedia information servers,Guido Nerjes; Peter Muth; Michael Paterakis; Yannis Romboyannakis; Peter Triantafillou; Gerhard Weikum,In contrast to pure video servers; advanced applications such as digital libraries orteleteaching exhibit a mixed workload with massive access to conventional;" discrete" datasuch as text documents; images and indexes as well as requests for" continuous data". Inaddition to the service quality guarantees for continuous data requests; quality-consciousapplications require that the response time of the discrete data requests stay below someuser-tolerance threshold. We study the impact of different disk scheduling policies on theservice quality for both continuous and discrete data. We identify a number of critical issues;present a framework for describing the various policies in terms of few parameters andfinally provide experimental results; based on a detailed simulation testbed; that comparedifferent scheduling policies.,Research Issues In Data Engineering; 1998.'Continuous-Media Databases and Applications'. Proceedings.; Eighth International Workshop on,1998,23
Saturn: range queries; load balancing and fault tolerance in DHT data systems,Theoni Pitoura; Nikos Ntarmos; Peter Triantafillou,In this paper; we present Saturn; an overlay architecture for large-scale data networksmaintained over Distributed Hash Tables (DHTs) that efficiently processes range queriesand ensures access load balancing and fault-tolerance. Placing consecutive data values inneighboring peers is desirable in DHTs since it accelerates range query processing;however; such a placement is highly susceptible to load imbalances. At the same time;DHTs may be susceptible to node departures/failures and high data availability and faulttolerance are significant issues. Saturn deals effectively with these problems through theintroduction of a novel multiple ring; order-preserving architecture. The use of a novel order-preserving hash function ensures fast range query processing. Replication across andwithin data rings (termed vertical and horizontal replication) forms the foundation over …,IEEE Transactions on Knowledge and Data Engineering,2012,21
Self-join size estimation in large-scale distributed data systems,Theoni Pitoura; Peter Triantafillou,In this work we tackle the open problem of self-join size (SJS) estimation in a large-scaleDistributed Data System; where tuples of a relation are distributed over data nodes whichcomprise an overlay network. Our contributions include adaptations of five well-known SJSestimation centralized techniques (coined sequential; cross-sampling; adaptive; bifocal; andsample-count) to the network environment and a novel technique which is based on the useof the Gini coefficient. We develop analyses showing how Gini estimations can lead toestimations of the underlying Zipfian or power-law value distributions. We further contributedistributed sampling algorithms that can estimate accurately and efficiently the Ginicoefficient. Finally; we provide detailed experimental evidence testifying for the claimedincreased accuracy; precision; and efficiency of the proposed SJS estimation method …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,21
NIPPERS: network of Interpolated PeERS for Web Service discovery,Christos Makris; Evangelos Sakkopoulos; Spyros Sioutas; Peter Triantafillou; A Tsakalidis; Bill Vassiliadis,Web services are becoming an important enabler of the semantic Web. Besides the need fora rich description mechanism; Web service information should be made available in anaccessible way for machine processing. In this paper; we propose a new P2P basedapproach for Web services discovery. Peers that store Web services information; such asdata item descriptions; are efficiently located using a scalable and robust data indexingstructure for peer-to-peer data networks; NIPPERS. We present a theoretical analysis whichshows that the communication cost of the query and update operations scale double-logarithmically with the number of NIPPERS nodes. Furthermore; we show that the networkis robust with respect to failures fulfilling quality of Web services requirements.,Information Technology: Coding and Computing; 2005. ITCC 2005. International Conference on,2005,21
Rank join queries in nosql databases,Nikos Ntarmos; Ioannis Patlakas; Peter Triantafillou,Abstract Rank (ie; top-k) join queries play a key role in modern analytics tasks. However;despite their importance and unlike centralized settings; they have been completelyoverlooked in cloud NoSQL settings. We attempt to fill this gap: We contribute a suite ofsolutions and study their performance comprehensively. Baseline solutions are offeredusing SQL-like languages (like Hive and Pig); based on MapReduce jobs. We first providesolutions that are based on specialized indices; which may themselves be accessed usingeither MapReduce or coordinator-based strategies. The first index-based solution is basedon inverted indices; which are accessed with MapReduce jobs. The second index-basedsolution adapts a popular centralized rank-join algorithm. We further contribute a novelstatistical structure comprising histograms and Bloom filters; which forms the basis for the …,Proceedings of the VLDB Endowment,2014,20
Range query optimization leveraging peer heterogeneity,Nikos Ntarmos; Theoni Pitoura; Peter Triantafillou,Abstract. In this work we address the issue of efficient processing of range queries in DHT-based P2P data networks. The novelty of the proposed approach lies on architectures;algorithms; and mechanisms for identifying and appropriately exploiting powerful nodes insuch networks. The existence of such nodes has been well documented in the literature andplays a key role in the architecture of most successful real-world P2P applications. However;till now; this heterogeneity has not been taken into account when architecting solutions forcomplex query processing; especially in DHT networks. With this work we attempt to fill thisgap for optimizing the processing of range queries. Significant performance improvementsare achieved due to (i) ensuring a much smaller hop count performance for range queries;and (ii) avoiding the dangers and inefficiencies of relying for range query processing on …,In 3rd International Workshop on Databases; Information Systems and Peer-to-Peer Computing (DBISP2P,2005,20
Crowdsourcing taxonomies,Dimitris Karampinas; Peter Triantafillou,Abstract Taxonomies are great for organizing and searching web content. As such; manypopular classes of web applications; utilize them. However; their manual generation andmaintenance by experts is a time-costly procedure; resulting in static taxonomies. On theother hand; mining and statistical approaches may produce low quality taxonomies. We thuspropose a drastically new approach; based on the proven; increased human involvementand desire to tag/annotate web content. We define the required input from humans in theform of explicit structural; eg; supertype-subtype relationships between concepts. Hence weharvest; via common annotation practices; the collective wisdom of users with respect to the(categorization of) web content they share and access. We further define the principles uponwhich crowdsourced taxonomy construction algorithms should be based. The resulting …,Extended Semantic Web Conference,2012,19
PLANES: The next step in peer-to-peer network architectures,Peter Triantafillou,ABSTRACT We present PLANES; Peer-to-Peer; Layered; Altruism-inspired Networks.PLANES is a new paradigm for architecting structured P2P overlay networks. It leveragesthe coexistence of altruistic and selfish nodes in the network; manifested in severalapplications. We describe novel architectures; present the associated algorithms for routingand for adding/deleting nodes and content; and discuss the resulting efficiencyimprovements. Namely; in an N-node network in the steady-state case our architecture canensure routing in O (log (log N)) hops. Perhaps more importantly; in the highly-dynamic casePLANES can be configured to require from O (log N) to O (N) routing hops. This represents aspeedup factor of up to 2-to-4X (for expected values of N) for the steadystate case; and ofseveral orders of magnitude for the highlydynamic case. Furthermore; we show the …,Proceedings of SIGCOMM Workshop on Future Directions in Network Architectures,2003,17
Performance and scalability of indexed subgraph query processing methods,Foteini Katsarou; Nikos Ntarmos; Peter Triantafillou,Abstract Graph data management systems have become very popular as graphs are thenatural data model for many applications. One of the main problems addressed by thesesystems is subgraph query processing; ie; given a query graph; return all graphs that containthe query. The naive method for processing such queries is to perform a subgraphisomorphism test against each graph in the dataset. This obviously does not scale; assubgraph isomorphism is NP-Complete. Thus; many indexing methods have been proposedto reduce the number of candidate graphs that have to underpass the subgraphisomorphism test. In this paper; we identify a set of key factors-parameters; that influence theperformance of related methods: namely; the number of nodes per graph; the graph density;the number of distinct labels; the number of graphs in the dataset; and the query graph …,Proceedings of the VLDB Endowment,2015,16
NanoPeer networks and P2P worlds,Peter Triantafillou; Nikos Ntarmos; S Nikoletseas; P Spirakis,We present the NanoPeers architecture paradigm; a peer-to-peer network of lightweightdevices; lacking all or most of the capabilities of their computer-world counterparts. Weidentify the problems arising when we apply current routing and searching methods to thisnanoworld; and present some initial solutions; using a case study of a sensor networkinstance; Smart Dust. Furthermore; we propose the P2P Worlds framework as a hybrid P2Parchitecture paradigm; consisting of cooperating layers of P2P networks; populated bycomputing entities with escalating capabilities. Our position is that:(i) experience gainedthrough research and experimentation in the field of P2P computing; can be indispensablewhen moving down the stair of computing capabilities; and that (ii) the proposed frameworkcan be the basis of numerous real-world applications; opening up several challenging …,Peer-to-Peer Computing; 2003.(P2P 2003). Proceedings. Third International Conference on,2003,15
Clustered scheduling algorithms for mixed-media disk workloads in a multimedia server,Elias Balafoutis; Michael Paterakis; Peter Triantafillou; Guido Nerjes; Peter Muth; Gerhard Weikum,Abstract Divisible load scenarios occur in modern media server applications since mostmultimedia applications typically require access to continuous and discrete data. A highperformance Continuous Media (CM) server greatly depends on the ability of its disk IOsubsystem to serve both types of workloads efficiently. Disk scheduling algorithms for mixedmedia workloads; although they play a central role in this task; have been overlooked byrelated research efforts. These algorithms must satisfy several stringent performance goals;such as achieving low response time and ensuring fairness; for the discrete-data workload;while at the same time guaranteeing the uninterrupted delivery of continuous data; for thecontinuous-data workload. The focus of this paper is on disk scheduling algorithms for mixedmedia workloads in a multimedia information server. We propose novel algorithms …,Cluster Computing,2003,15
Independent recovery in large-scale distributed systems,P Triantafiliou,In large systems; replication can become important means to improve data access times andavailability. Existing recovery protocols; on the other hand; were proposed for small-scaledistributed systems. Such protocols typically update stale; newly-recovered sites withreplicated data and resolve the commit uncertainty of recovering sites. Thus; given that inlarge systems failures are more frequent and that data access times are costlier; suchprotocols can potentially introduce large overheads in large systems and must be avoided; ifpossible. We call these protocols dependent recovery protocols since they require arecovering site to consult with other sites. Independent recovery has been studied in thecontext of one-copy systems and has been proven unattainable. This paper offersindependent recovery protocols for large-scale systems with replicated data. It shows how …,IEEE Transactions on Software Engineering,1996,15
A new paradigm for high availability and efficiency in replicated distributed databases,Peter Triantafillou; David Taylor,The paper presents a new paradigm for replication. Its major goal is to achieve performancesimilar to systems that do not employ replication and; at the same time; to offer theavailability benefits that result from replication. The paradigm contributes two mechanisms.The first mechanism is an extended location service; for which it uses a logically centralizedimplementation. In addition; it modifies the traditional transaction-processing mechanism tointeract with the location service inexpensively during transaction execution. The secondmechanism is a priority-based; preemptive concurrency control algorithm which allows locksto be synchronously acquired at only a single replica. In addition; the paradigm exhibitsdesirable availability characteristics; satisfies the one-copy serializability correctnesscriterion and is easy to implement. For these reasons it is presented as a basis for …,Parallel and Distributed Processing; 1990. Proceedings of the Second IEEE Symposium on,1990,15
Indexing Query Graphs to Speed Up Graph Query Processing,Jing Wang; Nikos Ntarmos; Peter Triantafillou,Subgraph/supergraph queries although central to graph an-alytics; are costly as they entailthe NP-Complete problem of subgraph isomorphism. We present a fresh solution; the novelprinciple of which is to acquire and utilize knowledge from the results of previously executedqueries. Our ap-proach; iGQ; encompasses two component subindexes to identify if a newquery is a subgraph/supergraph of pre-viously executed queries and stores related keyinforma-tion. iGQ comes with novel query processing and index space managementalgorithms; including graph replacement policies. The end result is a system that leads tosignifi-cant reduction in the number of required subgraph isomor-phism tests and speedupsin query processing time. iGQ can be incorporated into any sub/supergraph queryprocessing method and help improve performance. In fact; it is the only contribution that …,*,2016,13
Incremental scheduling of mixed workloads in multimedia information servers,Guido Nerjes; Peter Muth; Michael Paterakis; Y Romboyannakis; Peter Triantafillou; Gerhard Weikum,Abstract In contrast to pure video servers; advanced multimedia applications such as digitallibraries or teleteaching exhibit a mixed workload with massive access toconventional;“discrete” data such as text documents; images and indexes as well asrequests for “continuous data”; like video and audio data. In addition to the service qualityguarantees for continuous data requests; quality-conscious applications require that theresponse time of the discrete data requests stay below some user-tolerance threshold. Inthis paper; we study the impact of different disk scheduling policies on the service quality forboth continuous and discrete data. We provide a framework for describing various policies interms of few parameters; and we develop a novel policy that is experimentally shown tooutperform all other policies.,*,2000,13
Multiclass replicated data management: exploiting replication to improve efficiency,Peter Triantafillou; David J Taylor,Research efforts in replication-control protocols primarily use replication as a means ofincreasing availability in distributed systems. It is well-known; however; that replication canreduce the costs of accessing remotely-stored data in distributed systems. We contribute aclassification of replicas and a replication-control protocol which introduce the availabilitybenefits of replication and; at the same time; exploit replication to improve performance; byreducing response time. Each replica class has different consistency requirements.Metareplicas keep track of up-to-date replicas for recently-accessed objects and help exploitdata-reference localities. Thus they allow many transaction operations to executesynchronously at only a single (and often local) replica. Pseudoreplicas are nonpermanentreplicas that facilitate" localized execution" of transaction operations. True replicas are …,IEEE Transactions on Parallel and Distributed Systems,1994,13
Efficiently maintaining availability in the presence of partitionings in distributed systems,Peter Triantafillou; David Taylor,A new approach is presented for handling partitionings in replicated distributed databases.Mechanisms are developed through which transactions can access replicated data objectsand observe delays similar to nonreplicated systems while enjoying the availability benefitsof replication. The replication control protocol; called VELOS; achieves optimal availability;according to a well-known metric; while ensuring one-copy serializability. It is shown toprovide better availability than other methods which meet the same optimality criterion. Itoffers these availability characteristics without relying on system transactions that mustexecute to restore availability; when failures and recoveries occur; but which introducesignificant delays to user transactions.,Data Engineering; 1991. Proceedings. Seventh International Conference on,1991,13
Towards self-organizing query routing and processing for peer-to-peer web search,Gerhard Weikum; Peter Triantafillou; David Hales; Christian Schindelhauer,Abstract The peer-to-peer computing paradigm is an intriguing alternative to Google-stylesearch engines for querying and ranking Web content. In a network with many thousands ormillions of peers the storage and access load requirements per peer are much lighter thanfor a centralized Google-like server farm; thus more powerful techniques from informationretrieval; statistical learning; computational linguistics; and ontological reasoning can beemployed on each peer's local search engine for boosting the quality of search results. Inaddition; peers can dynamically collaborate on advanced and particularly difficult queries.Moroever; a peer-to-peer setting is ideally suited to capture local user behavior; like querylogs and click streams; and disseminate and aggregate this information in the network; at thediscretion of the corresponding user; in order to incorporate richer cognitive models. This …,Proceedings of the European Conference on Complex Systems (ECCS'05); Nov. 14th; Paris; France. Publishers: i6doc; Belgium,2005,12
Fundamentals of scheduling and performance of video tape libraries,Costas Georgiadis; Peter Triantafillou; Christos Faloutsos,Abstract Robotic tape libraries are popular for applications with very high storagerequirements; such as video servers. Here; we study the throughput of a tape library system;we design a new scheduling algorithm; the so-called Relief; and compare it against someolder/straightforward ones; like FCFS; Maximum Queue Length (MQL) and an unfair one(Bypass); roughly equivalent to Shortest Job First. The proposed algorithm incorporates anaging mechanism in order to attain fairness and we prove that; under certain assumptions; itminimizes the average start-up latency. Extensive simulation experiments show that Reliefoutperforms its competitors (fair and unfair alike); with up to 203% improvement inthroughput; for the same rejection ratio.,Multimedia Tools and Applications,2002,12
Prefetching into smart-disk caches for high performance media servers,Peter Triantafillou; Stavros Harizopoulos,The paper presents techniques which exploit recent magnetic disk-drive technologicaldevelopments (such as the existence of embedded drive-level caches and powerfulcontrollers; and the ever-increasing transfer rates). It contributes prefetching techniques intohost-and drive-level caches to improve the maximum number of continuous data streamsthat a drive can support. We show how our techniques can achieve significant performanceimprovements while guaranteeing the uninterrupted display of the continuous data. Inaddition; despite our techniques' utilization of drive-level caches; the performanceimprovements do not come at the expense of additional cache memory (at the host and/orthe drive). Given current technology trends; the benefits of our techniques are expected tobecome even greater.,Multimedia Computing and Systems; 1999. IEEE International Conference on,1999,12
Web proxy cache replacement: do's; don'ts; and expectations,Peter Triantafillou; L Aekaterinidis,Numerous research efforts have produced a large number of algorithms and mechanismsfor web proxy caches. In order to build powerful web proxies and understand theirperformance; one must be able to appreciate the impact and significance of earliercontributions and how they can be integrated To do this we employ a cache replacementalgorithm;'CSP; which integrates key knowledge from previous work. CSP utilizes thecommunication Cost to fetch web objects; the objects' Sizes; their Popularifies; an auxiliarycache and a cache admission control algorithm. We study the impact of these componentswith respect to hit ratio; latency; and bandwidth requirements. Our results show that there areclear performance gains when utilizing the communication cost; the popularity of objects;and the auxiliary cache. In contrast; the size of objects and the admission controller have …,Network Computing and Applications; 2003. NCA 2003. Second IEEE International Symposium on,2003,11
Hierarchical caching and prefetching for continuous media servers with smart disks,Stavros Harizopoulos; Costas Harizakis; Peter Triantafillou,I/O controllers and SCSI controller as dif- ferent processing units; the data can flow between thedisk caches; the multiple disk controller buffer; and the host cache. We have developed severalalgorithms that exploit these coexisting elements in continuous media applications. Ouralgo- rithms work in parallel to retrieve; or “prefetch;” data from the disk surface to either the diskor the SCSI controller caches and concurrently transfer the data from the lower cache hierarchiesto the host cache. At the same time; the host streams the data from its RAM through the networkto the clients. As we will describe; this parallelism—which is mainly based on the intelligent controllersand the differ- ent caches—can significantly improve media server performance. We measureperformance in three ways: by the maximum number of continuous data streams that a drivecan support; the total RAM size requirements; and the start- up latency. Our caching and …,IEEE concurrency,2000,11
Hierarchical scheduling algorithms for near-line tape libraries,Peter Triantafillou; Ioannis Georgiadis,Robotic tape libraries (RTLs) currently enjoy a prominent place in the storage market; with areported average annual growth rate approaching 34%; primarily due to their low cost perMB figures. Given the ever-increasing requirements for storage of several modernapplications; this will continue to hold. However; despite this fact and the fact that theiraccess times continue to be very slow (eg; tens of seconds) the central issue of efficientlyscheduling accesses to RTLs has not received the attention it deserves. The papercontributes a study of efficient scheduling algorithms for tape based robotic storage libraries.It contributes novel algorithms and experimentally evaluates their performance; comparingthem against that of well-known algorithms found in other environments. The paper's maincontribution; hierarchical scheduling algorithms; can offer significant performance …,Database and Expert Systems Applications; 1999. Proceedings. Tenth International Workshop on,1999,11
Velos: a new approach for efficiently achieving high availability in partitioned distributed systems,Peter Triantafillou; David J Taylor,The work presents a new protocol; VELOS; for tolerating partitionings in distributed systemswith replicated data. Our primary goals were influenced by efficiency and availabilityconstraints. The proposed protocol achieves optimal availability; according to a well knownmetric; while ensuring one copy serializability. In addition; however; VELOS is designed toreduce the cost involved in achieving high availability. We have developed mechanismsthrough which transactions; in the absence of failures; can access replicated data objectsand observe shorter delays than related protocols; and impose smaller loads on the networkand the servers. Furthermore; VELOS offers high availability without relying on systemtransactions that must execute to restore availability when failures and recoveries occur.Such system transactions typically access all (replicas of all) data objects and thus …,IEEE transactions on knowledge and data engineering,1996,11
Using multiple replica classes to improve performance in distributed systems,Peter Triantafillou; David Taylor,Replication has been primarily used as a means of increasing availability in distributedsystems. It is known that replication can mitigate the costs of accessing remotely stored datain distributed systems. Replication control protocols in the literature have stopped short ofaddressing availability and performance concerns. These issues are addressed bycontributing a classification of replicas with each class having different consistencyrequirements. Metareplicas keep track of up-to-date replicas for recently accessed objectsand changes in data reference localities. Thus they allow many transaction operations tosynchronously execute at only a single (and often local) replica. Pseudoreplicas are non-permanent replicas that facilitate localized execution of transaction operations. True replicasare permanent replicas that increase the availability of operations and data. A replication …,Distributed Computing Systems; 1991.; 11th International Conference on,1991,11
Understanding information need: An fmri study,Yashar Moshfeghi; Peter Triantafillou; Frank E Pollick,Abstract The raison d'etre of IR is to satisfy human information need. But; do we reallyunderstand information need? Despite advances in the past few decades in both the IR andrelevant scientific communities; this question is largely unanswered. We do not reallyunderstand how an information need emerges and how it is physically manifested.Information need refers to a complex concept: at the very initial state of the phenomenon (ieat a visceral level); even the searcher may not be aware of its existence. This renders themeasuring of this concept (using traditional behaviour studies) nearly impossible. In thispaper; we investigate the connection between an information need and brain activity. Usingfunctional Magnetic Resonance Imaging (fMRI); we measured the brain activity of twentyfour participants while they performed a Question Answering (Q/A) Task; where the …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,10
Scaling out big data missing value imputations: pythia vs. godzilla,Christos Anagnostopoulos; Peter Triantafillou,Abstract Solving the missing-value (MV) problem with small estimation errors in big dataenvironments is a notoriously resource-demanding task. As datasets and their usercommunity continuously grow; the problem can only be exacerbated. Assume that it ispossible to have a single machine (Godzilla'); which can store the massive dataset andsupport an ever-growing community submitting MV imputation requests. Is it possible toreplace Godzilla by employing a large number of cohort machines so that imputations canbe performed much faster; engaging cohorts in parallel; each of which accesses muchsmaller partitions of the original dataset? If so; it would be preferable for obviousperformance reasons to access only a subset of all cohorts per imputation. In this case; canwe decide swiftly which is the desired subset of cohorts to engage per imputation? But …,Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,2014,10
ART: sub-logarithmic decentralized range query processing with probabilistic guarantees,Spyros Sioutas; Peter Triantafillou; George Papaloukopoulos; Evangelos Sakkopoulos; Kostas Tsichlas; Yannis Manolopoulos,Abstract We focus on range query processing on large-scale; typically distributedinfrastructures; such as clouds of thousands of nodes of shared-datacenters; of p2pdistributed overlays; etc. In such distributed environments; efficient range query processingis the key for managing the distributed data sets per se; and for monitoring theinfrastructure's resources. We wish to develop an architecture that can support rangequeries in such large-scale decentralized environments and can scale in terms of thenumber of nodes as well as in terms of the data items stored. Of course; in the last few yearsthere have been a number of solutions (mostly from researchers in the p2p domain) fordesigning such large-scale systems. However; these are inadequate for our purposes; sinceat the envisaged scales the classic logarithmic complexity (for point queries) is still too …,Distributed and Parallel Databases,2013,10
Statistical structures for Internet-scale data management,Nikos Ntarmos; Peter Triantafillou; Gerhard Weikum,Abstract Efficient query processing in traditional database management systems relies onstatistics on base data. For centralized systems; there is a rich body of research results onsuch statistics; from simple aggregates to more elaborate synopses such as sketches andhistograms. For Internet-scale distributed systems; on the other hand; statistics managementstill poses major challenges. With the work in this paper we aim to endow peer-to-peer datamanagement over structured overlays with the power associated with such statisticalinformation; with emphasis on meeting the scalability challenge. To this end; we firstcontribute efficient; accurate; and decentralized algorithms that can compute key aggregatessuch as Count; CountDistinct; Sum; and Average. We show how to construct several types ofhistograms; such as simple Equi-Width; Average-Shifted Equi-Width; and Equi-Depth …,The VLDB Journal,2009,9
Publish-subscribe information delivery with substring predicates,Ioannis Aekaterinidis; Peter Triantafillou,The content-based publish-subscribe (pub-sub) paradigm for system design is becomingincreasingly popular; offering unique benefits for many data-intensive applications. Coupledwith peer-to-peer technology; it can serve as a central building block for developing data-dissemination applications deployed over a large-scale network infrastructure. A key openproblem in creating large-scale content-based pub-sub infrastructures relates to efficientlyand accurately matching subscriptions with substring predicates to incoming events. Thiswork addresses this issue.,IEEE Internet Computing,2007,9
Substring matching in P2P publish/subscribe data management networks,Ioannis Aekaterinidis; Peter Triantafillou,The content-based publish/subscribe (pub/sub) paradigm for system design is becomingincreasingly popular; offering unique benefits for a large number of data-intensiveapplications. Coupled with the peer-to-peer technology; it can serve as a central buildingblock for such applications deployed over a large-scale network infrastructure. A keyproblem toward the creation of large-scale content-based pub/sub infrastructures relates todealing efficiently with continuous queries (subscriptions) with rich predicates on stringattributes; in this work we study the problem of efficiently and accurately matching substringqueries to incoming events.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,9
Recovering in large distributed systems with replicated data,Peter Triantafillou,The problem of recovery in large-scale transaction-based distributed systems with replicateddata is studied. In large distributed systems the cost of accessing data items may beconsiderably greater; because of the distances involved. It is thus important to exploitreplication to reduce data-access times. Also; in large systems; failure events are much morefrequent than in small systems. Therefore; executing costly recovery protocols; such as theones needed to update stale; newly-recovered replicas or to resolve the uncertainty ofrecovering replicas; must be avoided. These protocols are called dependent recoveryprotocols; since they require a recovering site to consult other sites before it can bereintegrated into the distributed system. Independent recovery has been provedunattainable in one-copy systems. It is shown that independent recovery is possible in …,Parallel and Distributed Information Systems; 1993.; Proceedings of the Second International Conference on,1993,9
High availability is not enough (distributed systems),Peter Triantafillou,The author mainly concentrates on transactional distributed systems. Most previousresearch on replication in such environments has concentrated in employing replication toachieve high availability. The position is that high availability along is not enough. First; it isimportant to consider the cost of providing high availability through replication. Second; onemust exploit the potential of replication as a means of improving performance. Thenperformance issues (in addition to availability) in which one is mostly interested are:transaction latency; bottlenecks and throughput; and scalability (in particular as it affects theformer issues). The author briefly outlines his related research efforts which can be classifiedin the following areas: replication-control protocols; recovery strategies; and studyingavailability in large-scale distributed systems.,Management of Replicated Data; 1992.; Second Workshop on the,1992,9
Pyracanthus: A scalable solution for dht-independent content-based publish/subscribe data networks,Ioannis Aekaterinidis; Peter Triantafillou,Abstract We study the problems associated with building large-scale; content-based;publish/subscribe networks. In particular; we focus on network-independent solutions; in aneffort to isolate and bypass the difficult problems of ensuring the desirable underlying-network properties of scalability; fault tolerance; high performance; and adaptability to userand network dynamics. For this reason; we assume a popular overlay network infrastructure;based on Distributed Hash Tables; over which our pub/sub system will be built. We presentPyracanthus which embodies and can be configured using a suite of novel alternativecomponents; which are categorized according to their ability to handle numerical and/orstring attribute predicates and according to whether they are stateful/stateless. We present indetail each component consisting of algorithms for in-network indexing and storing of …,Information Systems,2011,8
Brief announcement: ART--sub-logarithmic decentralized range query processing with probabilistic guarantees,Spyros Sioutas; George Papaloukopoulos; Evangelos Sakkopoulos; Kostas Tsichlas; Yannis Manolopoulos; Peter Triantafillou,Abstract We focus on range query processing on large-scale; typically distributedinfrastructures. In this work we present the ART (Autonomous Range Tree) structure; whichoutperforms the most popular decentralized structures; including Chord (and some of itssuccessors); BATON (and its successor) and Skip-Graphs. ART supports the join/leave andrange query operations in O (log log N) and O (log 2 b logN+| A|) expected whp number ofhops respectively; where the base b is a double-exponentially power of two; N is the totalnumber of peers and| A| the answer size.,Proceedings of the 29th ACM SIGACT-SIGOPS symposium on Principles of distributed computing,2010,7
Critique and apologetics: Jews; Christians and pagans in Antiquity,Anders-Christian Jacobsen; Jörg Ulrich,Historische Bibliographie. Jacobsen; Anders C.; Ulrich; Jörg (Hrsg.). Critique and apologetics.Jews; christians and pagans in antiquity. Frankfurt am Main [ua]: Lang; 2009; 327 S. (EarlyChristianity in the Context of Antiquity. 4). ISBN 978-3-631-58011-0 Titel im KVK (KarlsruherVirtueller Katalog) suchen. Gliederungsnummer; II.3.2.2 – Das römische Weltreich (30 v.-476n.Chr.). Gliederungsebenen; II. Chronologisch-systematischer Teil 3 Die antike Welt 2 Rom 2Das römische Weltreich (30 v.-476 n.Chr.). Sachgruppe; Religion und Kirche. Hierin enthalten:(sortiert nach Autoren). S.137-154; Aland; Barbara: Apologetic Motives in Gnostic Texts.S.155-176; Avemarie; Friedrich: Traces of Apologetics in Rabbinic Literature. S.265-282; Barclay;John MG: Josephus' Contra Apionem as Jewish Apologetics. S.111-136; Becker; Eve M.: Jewsand Christians in Conflict? Polemical and Satirical Elements in Revelation 2-3 …,*,2009,7
Design alternatives for large-scale web search: Alexander was great; aeneas a pioneer; and anakin has the force,Matthias Bender; Sebastian Michel; Peter Triantafillou; Gerhard Weikum,ABSTRACT Indexing the Web and meeting the throughput; responsetime; and failure-resilience requirements of a search engine requires massive storage and computationalresources and a careful system design for scalability. This is exemplified by the big datacenters of the leading commercial search engines. Various proposals and debates haveappeared in the literature as to whether Web indexes can be implemented in a fullydistributed or even peer-to-peer manner without impeding scalability; and differentpartitioning strategies have been worked out. In this paper; we resume this ongoingdiscussion by analyzing the design space for distributed Web indexing; considering theinfluence of partitioning strategies as well as different storage technologies including Flash-RAM. We outline and discuss the pros and cons of three fundamental alternatives; and …,Proceedings of the 1st LSDS-IR Workshop,2007,7
Continuous data block placement in and elevation from tertiary storage in hierarchical storage servers,Peter Triantafillou; Thomas Papadakis,Abstract Given the cost of memories and the very large storage and bandwidth requirementsof large-scale multimedia databases; hierarchical storage servers (which consist of disk-based secondary storage and tape-library-based tertiary storage) are becoming increasinglypopular. Such server applications rely upon tape libraries to store all media; exploiting theirexcellent storage capacity and cost per MB characteristics. They also rely upon disk arrays;exploiting their high bandwidth; to satisfy a very large number of requests. Given typicalaccess patterns and server configurations; the tape drives are fully utilized uploading datafor requests that “fall through” to the tertiary level. Such upload operations consumesignificant secondary storage device and bus bandwidth. In addition; with presenttechnology (and trends) the disk array can serve fewer requests to continuous objects …,Cluster Computing,2001,7
On mixed-workload multimedia storage servers with guaranteed performance and service quality,Guido Nerjes; Yannis Romboyannakis; Peter Muth; Michael Paterakis; Peter Triantafillou; Gerhard Weikum,Abstract An important issue in multimedia information systems that has receivedconsiderable attention is to provide performance and service quality guaranteesfor\continuous" streams of video/audio data; especially bounding the rate of non-timely datafragments; so-called\glitches"; under a given number of concurrently served streams. Anequally important but much harder problem that has been neglected so far is to extend suchguarantees to a mixed workload with both continuous-data streams and response-time-sensitive requests to conventional;\discrete" data. This paper develops an analyticperformance model for such a mixed workload. The model is a hierarchical one; where thehigher; macroscopic level addresses the mutual performance impacts of continuous-dataand discrete-data requests by means of an abstract Markov process model; and the lower …,Proceedings 3rd International Workshop on Multimedia Information Systems; Como; Italy,1997,7
Optimizing distributed top-k queries,Thomas Neumann; Matthias Bender; Sebastian Michel; Ralf Schenkel; Peter Triantafillou; Gerhard Weikum,Abstract Top-k query processing is a fundamental building block for efficient ranking in alarge number of applications. Efficiency is a central issue; especially for distributed settings;when the data is spread across different nodes in a network. This paper introduces noveloptimization methods for top-k aggregation queries in such distributed environments thatcan be applied to all algorithms that fall into the frameworks of the prior TPUT and KLEEmethods. The optimizations address 1) hierarchically grouping input lists into top-k operatortrees and optimizing the tree structure; and 2) computing data-adaptive scan depths fordifferent input sources. The paper presents comprehensive experiments with two differentreal-life datasets; using the ns-2 network simulator for a packet-level simulation of a largeInternet-style network.,International Conference on Web Information Systems Engineering,2008,6
Fair resource allocation in a simple multi-agent setting: search algorithms and experimental evaluation,Paraskevi Raftopoulou; Manolis Koubarakis; Kostas Stergiou; Peter Triantafillou,We study the problem of fair resource allocation in a simple cooperative multi-agent settingwhere we have k agents and a set of n objects to be allocated to those agents. Each objectis associated with a weight represented by a positive integer or real number. We would liketo allocate all objects to the agents so that each object is allocated to only one agent and theweight is distributed fairly. We adopt the fairness index popularized by the networkingcommunity as our measure of fairness; and study centralized algorithms for fair resourceallocation. Based on the relationship between our problem and number partitioning; wedevise a greedy algorithm for fair resource allocation that runs in polynomial time but is notguaranteed to find the optimal solution; and a complete anytime algorithm that finds theoptimal solution but runs in exponential time. Then we study the phase transition behavior …,International Journal on Artificial Intelligence Tools,2005,6
Optimal cache memory exploitation for continuous media: To cache or to prefetch?,Antonis Hondroulis; Costas Harizakis; Peter Triantafillou,Abstract Network continuous-media applications are emerging with a great pace. Cachememories have long been recognized as a key resource (along with network bandwidth)whose intelligent exploitation can ensure high performance for such applications. Cachememories exist at the continuous-media servers and their proxy servers in the network.Within a server; cache memories exist in a hierarchy (at the host; the storage-devices; and atintermediate multi-device controllers). Our research is concerned with how to best exploitthese resources in the context of continuous media servers and in particular; how to bestexploit the available cache memories at the drive; the disk array controller; and the hostlevels. Our results determine under which circumstances and system configurations it ispreferable to devote the available memory to traditional caching (aka “data sharing”) …,Multimedia Tools and Applications,2004,6
A cache engine for E-content integration,Peter Triantafillou; Nikos Ntarmos; John Yannakopoulos,Content-integration systems generally suffer performance bottlenecks due to networkoverhead. To address this problem; the authors developed the Data Integration CacheEngine (DICE); which uses summarization techniques (subqueries and Bloom filters) andsemantic metadata to achieve semantic active caching in the context of Internet-based dataintegration applications. The system uses algorithms and specialized data structures togenerate exact remainder queries to locate content that is missing from cache in case ofpartial hits. The authors' performance results indicate that DICE outperforms existing optionsin terms of response time; network overhead; and server load.,IEEE Internet Computing,2004,6
Supporting partial data accesses to replicated data,Peter Triantafillou; Feng Xiao,Partial data access operations occur frequently in distributed systems. This paper presentsnew approaches for efficiently supporting partial data access operations to replicated data.We propose the replica modularization (RM) technique which suggests partitioning replicasinto modules; which now become the minimum unit of data access. RM is shown to increasethe availability of both partial read and write operations and improves performance byreducing access delays and the size of data transfers occurring during operation executionon replicated data. In addition; we develop a new module-based protocol (MB) in whichdifferent replication protocols are used to access different sets of replicas; with each replicastoring different modules. The instance of MB we discuss here is a hybrid of the ROWA(Read One Write All) protocol and the MQ (Majority Quorum) protocol. MB allows a trade …,Data Engineering; 1994. Proceedings. 10th International Conference,1994,6
Subgraph querying with parallel use of query rewritings and alternative algorithms,Foteini Katsarou; Nikos Ntarmos; Peter Triantafillou,Subgraph queries are central to graph analytics and graph DBs. We analyze this problemand present key novel discoveries and observations on the nature of the problem which holdacross query sizes; datasets; and top-performing algorithms. Firstly; we show that algorithms(for both the decision and matching versions of the problem) suffer from straggler queries;which dominate query workload times. As related research caps query times not reportingresults for queries exceeding the cap; this can lead to erroneous conclusions of the methods'relative performance. Secondly; we study and show the dramatic effect that isomorphicgraph queries can have on query times. Thirdly; we show that for each query; isomorphicqueries based on proposed query rewritings can introduce large performance benefits.Fourthly; that straggler queries are largely algorithm-specific: many challenging queries to …,*,2017,5
Searching private data in a cloud encrypted domain,Bernardo Ferreira; Henrique Domingos,Abstract Cloud computing security and reliability are important challenges in the researchagenda. For some applications managing sensitive data; cloud security solutions and data-privacy management are the main concerns for organizations that are considering a move tothe cloud. The advantages of cloud computing include reduced costs; easy maintenanceand re-provisioning of resources; thereby also possibly increasing profits. But the adoptionof Cloud Computing solutions applies only if different security concerns are ensured. Thisarticle presents a solution for data storage and data management in Internet StorageClouds; preserving privacy conditions under the control of Cloud users. The proposedsolution supports operations over stored encrypted data; including reading; writing andsearching based on relevance ranking and multiple keywords. The approach is based on …,Proceedings of the 10th Conference on Open Research Areas in Information Retrieval,2013,5
Peer-to-peer publish-subscribe systems,Peter Triantafillou; Ioannis Aekaterinidis,P/FDM [5–7] integrated a functional data model with the logic programming language Prologfor general-purpose computation. The data model can be seen as an Entity-Relationshipdiagram with sub-types; much like a UML Class Diagram. The idea was for the user to beable to define a computation over objects in the diagram; instead of just using it as a schemadesign aid. Later versions of P/FDM included a graphic interface [2; 4] to build queries inDAPLEX syntax by clicking on the diagram and filling in values from menus.,*,2009,5
An approach to deadlock detection in multidatabases,Peter Triantafillou,Abstract In this paper we study the problem of deadlock detection in multidatabase systems.In such environments; global deadlock mechanisms perform poorly since they often detectphantom deadlocks and perform duplicate work. With this work we allow the implicitcollaboration between the global and the local deadlock resolution schemes; which avoidsduplicate work and we devise mechanisms which reduce the number of unnecessarytransaction abortions caused by phantom deadlocks. Also; our scheme introduces smalleroverhead; when compared to related work. Moreover; our scheme makes no unrealisticassumptions about the computational environment (unlike related work) and is thus ofpractical use. Finally; we present an empirical performance study through detailedsimulations of known approaches and study MDBS deadlock detection experimentally.,Information Systems,1997,5
Employing replication to achieve high availability and efficiency in distributed systems,Peter Triantafillou,Abstract The introduction of replication into distributed systems is necessary if they are toprovide availability and fault tolerance. Although considerable research effort has beendirected towards the design of replication-control protocols; replication is still viewed asa'necessary evil'; primarily because of the performance overhead imposed by theseprotocols. We believe that this is the principal reason for the lack of commercially-availablereplicated distributed systems. This thesis describes research which was principallymotivated by the perceived need to address the issues of availability and performance indistributed systems. The thesis presents a new approach to replication-control in distributedsystems. The major contribution of our research is the development of replication-controlprotocols the use of which allows distributed-system services to be made highly available …,*,1992,5
GraphCache: a caching system for graph queries,Jing Wang; Nikos Ntarmos; Peter Triantafillou,Graph query processing is essential for graph analytics; but can be very time-consuming asit entails the NP-Complete problem of subgraph isomorphism. Traditionally; caching plays akey role in expediting query processing. We thus put forth GraphCache (GC); the first full-edged caching system for general subgraph/supergraph queries. We contribute the overallsystem architecture and implementation of GC. We study a number of novel graph cachereplacement policies and show that different policies win over different graph datasetsand/or queries; we therefore contribute a novel hybrid graph replacement policy that isalways the best or near-best performer. Moreover; we discover the related problem of cachepollution and propose a novel cache admission control mechanism to avoid cache pollution.Furthermore; we show that GC can be used as a front end; complementing any graph …,*,2017,4
Learning set cardinality in distance nearest neighbours,Christos Anagnostopoulos; Peter Triantafillou,Distance-based nearest neighbours (dNN) queries and aggregations over their answer setsare important for exploratory data analytics. We focus on the Set Cardinality Prediction(SCP) problem for the answer set of dNN queries. We contribute a novel; query-drivenperspective for this problem; whereby answers to previous dNN queries are used to learnthe answers to incoming dNN queries. The proposed novel machine learning (ML) modellearns the dynamically changing query patterns space and thus it can focus only on theportion of the data being queried. The model enjoys several comparative advantages inprediction error and space requirements. This is in addition to being applicable inenvironments with sensitive data and/or environments where data accesses are too costly toexecute; where the data-centric state-of-the-art is inapplicable and/or too costly. A …,Data Mining (ICDM); 2015 IEEE International Conference on,2015,4
Disk scheduling for mixed-media workloads in multimedia servers,Y Rombogiannakis; G Nerjes; P Muth; M Paterakis; P Triantafillou; G Weikum,*,*,1998,4
Efficient scalable accurate regression queries in in-dbms analytics,Christos Anagnostopoulos; Peter Triantafillou,Recent trends aim to incorporate advanced data analytics capabilities within DBMSs. Linearregression queries are fundamental to exploratory analytics and predictive modeling.However; computing their exact answers leaves a lot to be desired in terms of efficiency andscalability. We contribute a novel predictive analytics model and associated regressionquery processing algorithms; which are efficient; scalable and accurate. We focus onpredicting the answers to two key query types that reveal dependencies between the valuesof different attributes:(i) mean-value queries and (ii) multivariate linear regression queries;both within specific data subspaces defined based on the values of other attributes. Ouralgorithms achieve many orders of magnitude improvement in query processing efficiencyand nearperfect approximations of the underlying relationships among data attributes.,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,3
Human computing games for knowledge acquisition,Sarath Kumar Kondreddi; Peter Triantafillou; Gerhard Weikum,Abstract Automatic information extraction techniques for knowledge acquisition are known toproduce noise; incomplete or incorrect facts from textual sources. Human computing offers anatural alternative to expand and complement the output of automated information extractionmethods; thereby enabling us to build high-quality knowledge bases. However; relyingsolely on human inputs for extraction can be prohibitively expensive in practice. Wedemonstrate human computing games for knowledge acquisition that employ humancomputing to overcome the limitations in automated fact acquisition methods. We provide acombined approach that tightly integrates automated extraction techniques with humancomputing for effective gathering of facts. The methods we provide gather facts in the form ofrelationships between entities. The games we demonstrate are specifically designed to …,Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,2013,3
Anthropocentric data systems,Peter Triantafillou,Arguably; it all started with Mike Dertouzos' vision on the Information Marketplace [2]. Then;an explosion occurred. Social networks. Social computing. Social software. Groupware.Shareware. Open-source software. Personalized query answering and personalizedinformation systems. Tagging. Folksonomies. Log and clickstream mining. Recommendersystems. Crowdsourcing. Human-in-the loop and humancentered systems. Provably; thesebuzzwords have dominated the academic landscape within the data systems (and not only)community. There is a fundamental paradigm shift going on here. The old world; where thehuman was simply a passive user; has given way to a new world where humans contributedata;(storage; communication; and compute) resources; and software. Further; recently;humans take on tasks that actually alleviate and improve the jobs performed by machines …,37th VLDB Conference (Visions and Challenges),2011,3
Distribution fairness in Internet-scale networks,Theoni Pitoura; Peter Triantafillou,Abstract We address the issue of measuring distribution fairness in Internet-scale networks.This problem has several interesting instances encountered in different applications;ranging from assessing the distribution of load between network nodes for load balancingpurposes; to measuring node utilization for optimal resource exploitation; and to guidingautonomous decisions of nodes in networks built with market-based economic principles.Although some metrics have been proposed; particularly for assessing load balancingalgorithms; they fall short. We first study the appropriateness of various known andpreviously proposed statistical metrics for measuring distribution fairness. We put forward anumber of required characteristics for appropriate metrics. We propose and comparativelystudy the appropriateness of the Gini coefficient (G) for this task. Our study reveals as …,ACM Transactions on Internet Technology (TOIT),2009,3
of Proceedings: Advances in Database Technology-EDBT 2006: 10th International Conference on Extending Database Technology,Ralf Schenkel; Martin Theobald,Abstract/Description: Relevance Feedback is an important way to enhance retrieval qualityby integrating relevance information provided by a user. In XML retrieval; feedback enginesusually generate an expanded query from the content of elements marked as relevant ornonrelevant. This approach that is inspired by text-based IR completely ignores thesemistructured nature of XML. This paper makes the important step from content-based tostructural feedback. It presents an integrated solution for expanding keyword queries withnew content; path; and document constraints. An extensible framework evaluates suchquery conditions with existing keyword-based XML search engines while allowing to easilyintegrate new dimensions of feedback. Extensive experiments with the established INEXbenchmark show the feasibility of our approach.,*,2006,3
Distributed name management in internet systems: A study of design and performance issues,Peter Triantafillou; Michael Bauer,Abstract This paper deals with the problems associated with developing a namemanagement system which will be an integral part of a special type of distributed system; aninternet system. An internet system is a software system that provides transparent access toresources and services distributed over a collection of single-site computing systems andlocal area networks. The responsibilities of an internet name management system (INMS)are to (1) provide the name service that locates named resources and binds resources tolocations within the internet and (2) manage the naming information. This work investigatesa distributed approach for maintaining the naming information. Various design issues andpossible solutions are discussed. In particular; this paper examines several significantfactors; including the design of the global name table; the effects of internet …,Journal of Parallel and Distributed Computing,1990,3
Learning to accurately COUNT with query-driven predictive analytics,Christos Anagnostopoulos; Peter Triantafillou,We study a novel solution to executing aggregation (and specifically COUNT) queries overlarge-scale data. The proposed solution is generally applicable; in the sense that it can bedeployed in environments in which data owners may or may not restrict access to their dataand allow onlyaggregation operators' to be executed over their data. For this; it is based onpredictive analytics; driven by queries and their results. We propose a machine learning(ML) framework for the task (which can be adapted for different aggregates as well). Wefocus on the widely used set-cardinality (ie; COUNT) aggregation operator; as it is afundamental operator for both internal data system optimisations and for aggregation-queryanalytics. We contribute a novel; query-driven ML model whose goals are to:(i) learn thequery space (access patterns);(ii) associate (complex) aggregation queries with the …,Big Data (Big Data); 2015 IEEE International Conference on,2015,2
Web Information Systems Engineering-WISE 2010,Lei Chen; Peter Triantafillou; Torsten Suel,*,Lecture Notes in Computer Science,2010,2
Bad: Bandwidth adaptive dissemination or (the case for bad trees),Manos Kapritsos; Peter Triantafillou,Abstract In this paper; we present BAD; an application-level multicast infrastructure. BAD isdesigned to improve the performance of multicast dissemination trees; under both a staticand a dynamic environment; where the effective bandwidth of the network links changeswith time. Its main goal is to improve the data rate that end users perceive during a multicastoperation. BAD can be used for the creation and management of multicast groups. It can bedeployed over any DHT retaining its fundamental advantages of bandwidth improvement.BAD consists of a suit of algorithms for node joins/leaves; bandwidth distribution toheterogeneous nodes; tree rearrangement and reduction of overhead. We haveimplemented BAD within the FreePastry system. We report on the results of a detailedperformance evaluation which testifies for BAD's efficiency and low overhead. Specifically …,Proceedings of the 2007 ACM/IFIP/USENIX international conference on Middleware companion,2007,2
Design alternatives for large-scale web search: Alexander was great; Aeneas a pioneer and Anakin has the force,S Michel; M Bender; P Triantafillou; G Weikum,*,*,2007,2
MINERVA: Collaborative P2P web search,M Bender; S Michel; P Triantafillou; G Weikum; C Zimmer,*,*,2005,2
The RangeGuard: Range query optimization in peer-to-peer data networks,Peter Triantafillou; Nikos Ntarmos,*,*,2004,2
Self-organization and volunteering: Engineering in very large scale sharing networks,Peter Triantafillou,*,*,2004,2
Modeling; design and performance evaluation of interactive distributed video-on-demand systems,ConStantinoS VaSSilakiS; Michael Paterakis; Peter Triantafillou,A large scale; distributed video-on-demand (VOD) system allows geographically dispersedresidential and business users to access video services; such as movies and othermultimedia programs or documents on demand from video servers on a high speed network.In this paper we demonstrate through analysis and simulation the need for a hierarchicalarchitecture for the video-on-demand distribution network. We assume a hierarchicalarchitecture; which fits the existing tree topology used in today's cable TV (CATV) hybridfiber/coaxial (HFC) distribution networks. We develop a model for the design; configuration;program placement and performance evaluation of such systems. The model takes intoaccount the user behavior; the fact that the user re quests are transmitted over a sharedchannel before reaching the video server containing the requested program; the finite …,High-Performance Communication Systems; 1997.(HPCS'97) The Fourth IEEE Workshop on,1997,2
Benchmarking and Performance Tuning of Multimedia Servers,S Christodoulakis; P Triantafillou; D Magoulioti,Search all the public and authenticated articles in CiteULike. Include unauthenticated resultstoo (may include "spam") Enter a search phrase. You can also specify a CiteULike article id(123456);. a DOI (doi:10.1234/12345678). or a PubMed ID (pmid:12345678). Click Help foradvanced usage. CiteULike; Group: VTCS_PIM2006_Annotation; Search; Register; Log in …,*,1997,2
Availability and performance limitations in multidatabases,Peter Triantafillou,Abstract There are several available paradigms to use when constructing the distributedsystems of the future. Multidatabase systems (MDBSs) are becoming an increasinglypopular paradigm: they offer a new promise for easing the construction of new distributedsystems. Instead of developing a whole new distributed system; only a small portion needsto be developed; which integrates existing systems. We believe it is important to investigatethe appropriateness of this approach to the construction of future distributed systems. In thispaper we begin this investigation by focusing on one of the most important benefits ofdistributed systems: high availability. We study the problem of replication control inmultidatabase systems in which members are autonomous. The requirement for localautonomy creates the need for a new model for replicating data in a MDBS. In this paper …,Information Systems,1996,2
String Attribute Query Processing over DHTs: The Publish-Subscriber Case,I Aekarteinidis; Peter Triantafillou,Abstract In this paper; we present and study solutions for the efficient processing of queriesover string attributes in a large P2P data network implemented with DHTs. The proposedsolutions support queries with equality; prefix; suffix; and containment predicates over stringattributes. Currently; no known solution to this problem exists. We propose and studyalgorithms for processing such queries and their optimizations. As event-based;Publish/Subscribe information systems are a champion application class where stringattribute (continuous) queries are very common; we pay particular attention to this type ofdata networks; formulating our solution in terms of this environment. A major design decisionbehind the proposed solution is our intention to provide a solution that is general (DHT-independent); capable of being implemented on top of any particular DHT.,International Workshop on Distributed Event Based Systems (DEBS04); IEEE; online version: http://serl. cs. colorado. edu/∼ carzanig/debs04/proceedings. html,*,2
Query-driven learning for predictive analytics of data subspace cardinality,Christos Anagnostopoulos; Peter Triantafillou,Abstract Fundamental to many predictive analytics tasks is the ability to estimate thecardinality (number of data items) of multi-dimensional data subspaces; defined by queryselections over datasets. This is crucial for data analysts dealing with; eg; interactive datasubspace explorations; data subspace visualizations; and in query processing optimization.However; in many modern data systems; predictive analytics may be (i) too costly money-wise; eg; in clouds;(ii) unreliable; eg; in modern Big Data query engines; where accuratestatistics are difficult to obtain/maintain; or (iii) infeasible; eg; for privacy issues. Wecontribute a novel; query-driven; function estimation model of analyst-defined data subspacecardinality. The proposed estimation model is highly accurate in terms of prediction andaccommodating the well-known selection queries: multi-dimensional range and distance …,ACM Transactions on Knowledge Discovery from Data (TKDD),2017,1
Ensuring Consistency in Graph Cache for Graph-Pattern Queries,Jing Wang; Nikolaos Ntarmos; Peter Triantafillou,Graph queries are costly; as they entail the NP-Complete subgraph isomorphism problem.Graph caching had been recently suggested; showing the potential to significantlyaccelerate subgraph/supergraph queries. Subsequently; Graph-Cache; the first full-fledgedgraph caching system was put forth. However; when the underlying dataset changes con-currently with the query workload proceeding; how to ensure the graph cache consistencybecomes an issue. The current work provides a systematic solution to address this problem;by presenting an upgraded GraphCache system coined GraphCache+ (abbreviated asGC+). We develop two GC+ exclusive models that employ different approaches to deal withthe consistency issue. Moreover; we present the logic of GC+ in expediting queries; bundledwith the formally proved correctness. We evaluate the performance of GC+ by a real …,*,2017,1
Scalable data quality for big data: the pythia framework for handling missing values,Atoshum Cahsai; Christos Anagnostopoulos; Peter Triantafillou,Abstract Solving the missing-value (MV) problem with small estimation errors in large-scaledata environments is a notoriously resource-demanding task. The most widely used MVimputation approaches are computationally expensive because they explicitly depend onthe volume and the dimension of the data. Moreover; as datasets and their user communitycontinuously grow; the problem can only be exacerbated. In an attempt to deal with such aproblem; in our previous work; we introduced a novel framework coined Pythia; whichemploys a number of distributed data nodes (cohorts); each of which contains a partition ofthe original dataset. To perform MV imputation; the Pythia; based on specific machine andstatistical learning structures (signatures); selects the most appropriate subset of cohorts toperform locally a missing value substitution algorithm (MVA). This selection relies on the …,Big data,2015,1
HIGGINS: knowledge acquisition meets the crowds,Sarath Kumar Kondreddi; Peter Triantafillou; Gerhard Weikum,Abstract We present HIGGINS; a system for Knowledge Acquisition (KA); placing emphasison its architecture. The distinguishing characteristic and novelty of HIGGINS lies in itsblending of two engines: an automated Information Extraction (IE) engine; aided by semanticresources and statistics; and a game-based Human Computing (HC) engine. We focus onKA from web pages and text sources and; in particular; on deriving relationships betweenentities. As a running application we utilize movie narratives; from which we wish to deriverelationships among movie characters.,Proceedings of the 22nd International Conference on World Wide Web,2013,1
D-Hive: Data Bees Pollinating RDF; Text; and Time,Srikanta Bedathur; Klaus Berberich; Ioannis Patlakas; Peter Triantafillou; Gerhard Weikum,ABSTRACT Although the problem of integrating IR and DB solutions is considered “old”; theincreasing importance of big data analytics and its formidable demands for both enrichedfunctionality and scalable performance creates the need to revisit the problem itself and tosee possible solutions from a new perspective. Our goal is to develop a system that willmake large corpora aware of entities and relationships (ER); addressing the challenges insearching and analyzing ER patterns in web data and social media. We put forward D-Hive;a system facilitating analytics over RDF-style (SPO) triples augmented with text and(validity/transaction) time capable of addressing the functionality and scalabilityrequirements which current solutions cannot meet. We consider various alternatives for thedata modeling; storage; indexing; and query processing engines of D-Hive paying …,Sixth Biennial Conference on Innovative Data Systems Research,2013,1
Overlap-aware global df estimation in distributed information retrieval systems,Matthias Bender; Sebastian Michel; Gerhard Weikum; Peter Triantafilou,Zusammenfassung Peer-to-Peer (P2P) search engines and other forms of distributedinformation retrieval (IR) are gaining momentum. Unlike in centralized IR; it is difficult andexpensive to compute statistical measures about the entire document collection as it iswidely distributed across many computers in a highly dynamic network. On the other hand;such network-wide statistics; most notably; global document frequencies of the individualterms; would be highly beneficial for ranking global search results that are compiled fromdifferent peers. This paper develops an efficient and scalable method for estimating globaldocument frequencies in a large-scale; highly dynamic P2P network with autonomouspeers. The main difficulty that is addressed in this paper is that the local collections ofdifferent peers may arbitrarily overlap; as many peers may choose to gather popular …,*,2006,1
P2P Web Search with MINERVA: How do you want to search tomorrow?,Sebastian Michel; Matthias Bender; Peter Triantafillou; Gerhard Weikum; Christian Zimmer,Abstract. MINERVA1 is a novel approach towards P2P Web search that connects an a-prioriunlimited number of peers; each of which maintains a personal local database and a localsearch facility. Each peer posts a small amount of metadata to a physically distributeddirectory layered on top of a DHT-based overlay network that is used to efficiently selectpromising peers from across the peer population that can best locally execute a query. Thispaper proposes a live demonstration of MINERVA; showcasing the full information lifecycle:crawling web pages; disseminating metadata to a distributed directory; and executingqueries online. We additionally invite all visitors to instantly join the network by executing asmall piece of software.,Untitled Event,2005,1
HotRoD: Load Balancing and Efficient Range Query Processing in Peer-to-Peer Data Networks,Theoni Pitoura; Nikos Ntarmos; Peter Triantafillou,Abstract. We consider the conflicting problems of ensuring data-access load balancing andefficiently processing range queries on peer-to-peer data net-works maintained overDistributed Hash Tables (DHTs). Placing consecutive data values in neighboring peers isfrequently used in DHTs since it accelerates range query processing. However; such aplacement is highly susceptible to load imbalances; which are preferably handled byreplicating data (since repli-cation also introduces fault tolerance benefits). In this paper; wepresent HotRoD; a DHT-based architecture that deals effectively with this combined problemthrough the use of a novel locality-preserving hash function; and a tun-able data replicationmechanism which allows trading off replication costs for fair load distribution. Our detailedexperimentation study shows strong gains in both range query processing efficiency and …,*,2004,1
The RangeGuard: Range Query Optimization in Peer-to-Peer Data Networks,Nikos Ntarmos; Theoni Pitoura; Peter Triantafillou,*,*,2004,1
Efficient distributed event processing using subscription summaries in large scale publish/subscribe systems,Peter Triantafillou; Andreas Economides,Abstract A key issue when designing and implementing large-scale publish/subscribesystems is how to efficiently propagate user subscriptions among the brokers of the system.In this paper we contribute the notion of broker subscription summaries and accompanyingdistributed and scalable algorithms for subscription summary propagation and event filteringand routing. In addition we present a performance analysis; quantifying the associatedbenefits. Our results show that the proposed mechanism (i) introduces significantperformance gains in terms of saved network bandwidth (up to orders of magnitude);required storage space; and processing capacity requirements at each broker; and (ii) ishighly scalable; with the bandwidth required to propagate subscriptions increasing onlyslightly; even at huge-scales.,IEEE Workshop on Distributed Event-based Systems,2002,1
Benchmarking and performance tuning of multimedia servers,Peter Triantafillou; Stavros Christodoulakis; Theodora Magoulioti,Abstract In this paper we study the issues related to the development of a multimedia serverbenchmark. Multimedia benchmarks can be used to compare the performance of existingmultimedia servers and to fine-tune the performance of servers under development. Themodeling of multimedia servers includes the modeling of databases; multi-user workloads;user behavior; and the definition of the relevant performance metrics. The proposedbenchmark has been implemented and has been used to fine-tune the performance of amultimedia server; called KYDONIA. We present the results of testing KYDONIA andrepresentative instances of fine-tuning KYDONIA's performance.,European Conference on Parallel Processing,1997,1
Configuring and performance modeling of multimedia servers with mixed workload,G Nerjes; Y Rombogiannakis; P Muth; M Paterakis; P Triantafillou; G Weikum,*,*,1997,1
Issues of name management in internet systems,Peter Triantafillou,*,*,1987,1
Hybrid algorithms for subgraph pattern queries in graph databases,Foteini Katsarou; Nikos Ntarmos; Peter Triantafillou,Numerous methods have been proposed over the years for subgraph query processing; as itis central to graph analytics. Existing work is fragmented into two major categories. Methodsin the filter-then-verify (FTV) category first construct an index of the DB graphs. Given aquery; the index is used to filter out graphs that cannot contain the query. On the remaininggraphs; a subgraph isomorphism algorithm is applied to verify whether each graph indeedcontains the query. A second category of algorithms is mainly concerned with optimizing theSubgraph Isomorphism (SI) testing process (an NP-Complete problem) in order to find alloccurrences of the query within each DB graph; also known as the matching problem. Thecurrent research trend is to totally dismiss FTV methods; because SI methods have beenshown to enjoy much shorter query execution times and because of the alleged high …,Big Data (Big Data); 2017 IEEE International Conference on,2017,*
Scalable aggregation predictive analytics,Christos Anagnostopoulos; Fotis Savva; Peter Triantafillou,Abstract We introduce a predictive modeling solution that provides high quality predictiveanalytics over aggregation queries in Big Data environments. Our predictive methodology isgenerally applicable in environments in which large-scale data owners may or may notrestrict access to their data and allow only aggregation operators like COUNT to beexecuted over their data. In this context; our methodology is based on historical queries andtheir answers to accurately predict ad-hoc queries' answers. We focus on the widely usedset-cardinality; ie; COUNT; aggregation query; as COUNT is a fundamental operator for bothinternal data system optimizations and for aggregation-oriented data exploration andpredictive analytics. We contribute a novel; query-driven Machine Learning (ML) modelwhose goals are to:(i) learn the query-answer space from past issued queries;(ii) …,Applied Intelligence,2017,*
Topic detection and tracking on heterogeneous information,Long Chen; Huaizhi Zhang; Joemon M Jose; Haitao Yu; Yashar Moshfeghi; Peter Triantafillou,Abstract Given the proliferation of social media and the abundance of news feeds; asubstantial amount of real-time content is distributed through disparate sources; whichmakes it increasingly difficult to glean and distill useful information. Although combiningheterogeneous sources for topic detection has gained attention from several researchcommunities; most of them fail to consider the interaction among different sources and theirintertwined temporal dynamics. To address this concern; we studied the dynamics of topicsfrom heterogeneous sources by exploiting both their individual properties (includingtemporal features) and their inter-relationships. We first implemented a heterogeneous topicmodel that enables topic–topic correspondence between the sources by iteratively updatingits topic–word distribution. To capture temporal dynamics; the topics are then correlated …,Journal of Intelligent Information Systems,2017,*
Scaling k-Nearest Neighbours Queries (The Right Way),Atoshum Cahsai; Nikos Ntarmos; Christos Anagnostopoulos; Peter Triantafillou,Recently parallel/distributed processing approaches have been proposed for processing k-Nearest Neighbours (kNN) queries over very large (multidimensional) datasets aiming toensure scalability. However; this is typically achieved at the expense of efficiency. With thispaper we offer a novel approach that alleviates the performance problems associated withstate of the art methods. The essence of our approach; which differentiates it from relatedresearch; rests on (i) adopting a coordinator-based distributed processing algorithm; insteadof those employed over data-parallel executionengines (such as Hadoop/MapReduce orSpark); and (ii) on a way to organize data; to structure computation; and to index the storeddatasets that ensures that only a very small number of data items are retrieved from theunderlying data store; communicated over the network; and processed by the …,Distributed Computing Systems (ICDCS); 2017 IEEE 37th International Conference on,2017,*
A Task Completion Engine to Enhance Search Session Support for Air Traffic Work Tasks,Yashar Moshfeghi; Raoul Rothfeld; Leif Azzopardi; Peter Triantafillou,Abstract Providing support for users during their search sessions has been hailed as a majorchallenge in interactive information retrieval (IIR). Providing such support requiresconsidering the context of the search and facilitating the work task at hand. In this paper; weconsider the work tasks associated with air traffic analysts; who perform numerous searchesusing a multifaceted search interface in order to acquire business intelligence regardingparticular events and situations. In particular; we develop a novel task completion engineand seamlessly incorporated it within a current air traffic search system to facilitate thecomparison of information objects found. In a study with 24 participants; we found that theycompleted the complex work task faster using the comparison feature; but for simple worktasks; participants were slower. However; participants reported (statistically) significantly …,European Conference on Information Retrieval,2017,*
Towards Hybrid Methods for Graph Pattern Queries.,Foteini Katsarou; Nikos Ntarmos; Peter Triantafillou,ABSTRACT In the subgraph querying problem; given a query graph and a dataset of graphs;we want to locate which graphs in the dataset contain the query and/or find all itsoccurrences. Over the years; numerous methods; fragmented in 2 categories; wereproposed to tackle the problem. In the first category; methods follow the filter-then-verify(FTV) paradigm where an index is used to filter out graphs that definitely do not contain thequery as an answer. On the remaining set of graphs; a subgraph isomorphism algorithm isapplied to verify the graphs that contain the query graph. A second category; so called no-filter; verify (NFV); invested in optimizing the subgraph isomorphism process; by employing alightweight index primarily to locate candidate vertices on the graphs in the dataset. Thecurrent trend is to totally dismiss the FTV methods and employ NFV methods instead. With …,EDBT/ICDT Workshops,2017,*
Improving Search Results with Prior Similar Queries,Yashar Moshfeghi; Kristiyan Velinov; Peter Triantafillou,Abstract This paper describes a novel approach to re-ranking search engine result pages(SERP): Its fundamental principle is to re-rank results to a given query; based on exploitingevidence gathered from past similar search queries. Our approach is inspired bycollaborative filtering; with the main challenge being to find the set of similar queries; whilealso taking efficiency into account. In particular; our approach aims to address this challengeby proposing a combination of a similarity graph and a locality sensitive hashing scheme.We construct a set of features from our similarity graph and build a prediction model usingthe Hoeffding decision tree algorithm. We have evaluated the effectiveness of our model interms of P@ 1; MAP@ 10; and nDCG@ 10; using the Yandex Data Challenge data set. Wehave compared the performance of our model against two baselines; namely; the Yandex …,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,2016,*
Algorithmic Aspects of Cloud Computing,Ioannis Karydis; Spyros Sioutas; Peter Triantafillou; Dimitrios Tsoumakos,The International Workshop on Algorithmic Aspects of Cloud Computing (ALGOCLOUD) isan annual event aiming to tackle the diverse new topics in the emerging area of algorithmicaspects of computing and data management in the cloud. The increasing adoption of cloudcomputing introduces a variety of parallel and distributed algorithmic models andarchitectures. To leverage elastic cloud resources; scalability has to be a fundamentalarchitectural design trait of new cloud databases. This challenge is manifested in new datamodels (NoSQL); replication; caching and partitioning schemes; relaxed consistency andtransaction guarantees; as well as new protocols; APIs; indexing and storage services. Theaim of the workshop is to bring together researchers and practitioners in cloud computingalgorithms; service design; and data architectures to exchange ideas and contribute to …,*,2016,*
Towards a subgraph/supergraph cached query-graph index,Jing Wang; Nikos Ntarmos; Peter Triantafillou,Many modern big data applications deal with graph structured data; such as databases ofmolecular compounds represented as graphs of atoms and bonds; or “structured interactionnetworks” in biological and social networks; where nodes refer to entities (proteins; people;etc.) and edges represent their relationships. Central to high performance graph analyticsover such data; is to locate patterns in dataset graphs. Informally; given a graph dataset anda query (aka pattern) graph g; the goal is to return stored graphs that contain g (subgraphquerying) or are contained in g (supergraph querying). These operations are costly; as theyentail the NPComplete subgraph isomorphism problem [1]. This is further aggravated whenthe dataset consists of a large number of graphs; as testing g for subgraph isomorphismagainst all of them would require a very large amount of time.,Big Data (Big Data); 2015 IEEE International Conference on,2015,*
UCUI'15: Proceedings of the ACM First International Workshop on Understanding the City with Urban Informatics,Yashar Moshfeghi; Iadh Ounis; Craig Macdonald; Joemon Jose; Peter Triantafillou; Mark Livingston; Piyushimita Thakuriah,*,*,2015,*
UCUI'15: The 1st International Workshop on Understanding the City with Urban Informatics,Yashar Moshfeghi; Iadh Ounis; Craig Macdonald; Joemon M Jose; Peter Triantafillou; Mark Livingston; Piyushimita Thakuriah,Abstract Urban Informatics aims to exploit the large quantities of information produced bymodern cities in order to gain insights into how they function. These insights lay thefoundation for improving the lives of citizens; by improving the efficacy and efficiency ofpublic services; and satisfying complex information needs arising within this context. Thegoal of the workshop is to provide a multidisciplinary forum which brings togetherresearchers in Big Data (BD); Information Retrieval (IR); Data Mining; and Urban Studies; toexplore novel solutions to the numerous theoretical; practical and ethical challenges arisingin this context. These include difficulties in collecting city data; creating data managementinfrastructures; and providing new effective and efficient information access techniques to asmany users as possible in the context of a smart city. To foster the development of new …,Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,2015,*
Combining information extraction and human computing for crowdsourced knowledge acquisition,S Kumar Kondredi; P Triantafillou; G Weikum,*,*,2014,*
HIGGINS: where knowledge acquisition meets the crowds,S Kumar Kondreddi; Peter Triantafillou; Gerhard Weikum,We present HIGGINS; an engine for high quality Knowl-edge Acquisition (KA); placingspecial emphasis on its ar-chitecture. The distinguishing characteristic and novelty ofHIGGINS lies in its special blending of two engines: An automated Information Extraction (IE)engine; aided by semantic resources; and a game-based; Human Computing engine (HC).We focus on KA from web data and text sources and; in particular; on deriving relationshipsbetween enti-ties. As a running application we utilise movie narratives; using which we wishto derive relationships among movie characters.,*,2013,*
D-hive: Data bees pollinating RDF and text,S Bedathur; K Berberich; Y Patlakas; P Triantafillou; G Weikum,*,*,2013,*
Middleware 2012: ACM/IFIP/USENIX 13th International Middleware Conference; Montreal; Canada; December 3-7; 2012. Proceedings,Priya Narasimhan; Peter Triantafillou,This book constitutes the refereed proceedings of the ACM/IFIP/USENIX 13th InternationalMiddleware Conference; held in Montreal; Canada; in December 2012. The 24 revised fullpapers presented were carefully reviewed and selected from 125 submissions. The papersare organized in topical sections on mobile middleware; tracing and diagnosis; architectureand performance; publish/subscribe middleware; and big-data and cloud computing;availability; security and privacy.,*,2012,*
Rank Join Queries in Cloud Stores,N Ntarmos; I Patlakas; G Sfakianakis; P Triantafillou,*,*,2012,*
SNFS: The design and implementation of a Social Network File System,Charalabos Kaidos; Andreas Pasiopoulos; Peter Triantafillou; Nikos Ntarmos,Abstract Social network systems and services have become amazingly popular in recentyears. This has resulted in huge amounts of data being published by users. At the sametime; a great number of relationships between users; user groups; and (collections of) dataitems are constantly being established based on highly dynamic tagging activities by users.With this work we present the design and implementation of a special-purpose user-level filesystem; coined SNFS; designed to manage social-network entities (data items; users andtheir profiles; and tags) and their relationships. At the core of our approach lie tagging;indexing; and ranked retrieval (top-k) algorithms; allowing the key functionality to beprovided in a timely manner. We discuss the core design and implementation features ofSNFS and present a performance evaluation; exposing the key performance costs; and …,Proceedings of the 4th Workshop on Social Network Systems,2011,*
Joins and Selects on Clouds,A Altanis; I Patlakas; N Ntarmos; P Triantafillou,*,*,2011,*
Decentralized Rank Join Queries,N Ntarmos; I Patlakas; P Triantafillou,*,*,2011,*
Interval Queries on the Cloud,G Sfakianakis; I Patlakas; N Ntarmos; P Triantafillou,*,*,2011,*
Web Information Systems Engineering-WISE 2010: 11th International Conference; Hong Kong; China; December 12-14; 2010; Proceedings,Lei Chen; Peter Triantafillou; Torsten Suel,th Welcome to the Proceedings of WISE 2010—the 11 International Conference on WebInformation Systems Engineering. This year; WISE returned to the place where the inauguralconference was held in 2000; Hong Kong. WISE has also been held in: 2001 Kyoto (Japan);2002 Singapore; 2003 Rome (Italy); 2004 Brisbane (Australia); 2005 New York (USA); 2006Wuhan (China); 2007 Nancy (France); 2008 Auckland (New Zealand); and 2009 Poznan(Poland). Continuing its trend; this year's WISE provided a forum for engineers and scientiststo present their latest findings in Web-related technologies and solutions. The submittedcontributions address challenging issues in Web services; search; modeling;recommendation and data mining; as well as keyword search; social network analysis;query languages; and information retrieval and extraction. This year; WISE received 170 …,*,2010,*
Décentralisation des applications sociales,Pascal FELBER; Maarten VAN STEEN; Peter TRIANTAFILLOU; Sihem AMER-YAHIA; Anne-Marie KERMARREC; Marin BERTIER,The so-called Web 2.0 revolution has fundamentally changed the way people interact withthe Internet. The Web has turned from a read-only infrastructure to a collaborative platform.The users are now active participants; and contribute to the content of the websites byexpressing their opinions and sharing information. Social networks Social network websites;such as Facebook; were originally created to keep track of friends; and share informationand pictures with them. On these platforms; users can publish information to explicitlydesignated friends. It is also possible; for example; to associate a friend with a picture onwhich she appears. Facebook has proved to be a very efficient mean to quickly reachthousands of people and organize events.,*,2010,*
of Proceedings: Web Information Systems Engineering–WISE 2008: 9th International Conference,Thomas Neumann; Matthias Bender; Sebastian Michel; Ralf Schenkel; Peter Triantafillou; Gerhard Weikum,Abstract/Description: Top-k query processing is a fundamental building block for efficientranking in a large number of applications. Efficiency is a central issue; especially fordistributed settings; when the data is spread across different nodes in a network. This paperintroduces novel optimization methods for top-k aggregation queries in such distributedenvironments that can be applied to all algorithms that fall into the frameworks of the priorTPUT and KLEE methods. The optimizations address 1) hierarchically grouping input listsinto top-k operator trees and optimizing the tree structure; and 2) computing data-adaptivescan depths for different input sources. The paper presents comprehensive experiments withtwo different real-life datasets; using the ns-2 network simulator for a packet-level simulationof a large Internet-style network.,*,2008,*
Middleware 2007 Works in Progress,MJ Arif; S Karunasekera; S Kulkarni,We propose a Web-service-based P2P architecture for VoIP called service-oriented VoIP.SOVoIP provides interoperability between protocols from both telephony and data networksusing the converging behavior of Web services while ensuring security; extendability; andmobility. We also address other critical issues related to VoIP such as network addresstranslation and firewall traversal; Enhanced 911; and the Communication Assistance for LawEnforcement Act. SOVoIP also provides modularity and reusability; making clientdevelopment easier. Extendability in the Web service architecture is transparent to the client;so frequent client updates aren't required in order to consume new features. This reducesthe time and money spent on upgrades.,IEEE Distributed Systems Online,2007,*
BAD: Bandwidth Adaptive Dissemination,Manos Kapritsos; Peter Triantafillou,ABSTRACT In this paper; we present BAD; an application-level multicast infrastructure. BADis designed to improve the performance of multicast dissemination trees; under both a staticand a dynamic environment; where the effective bandwidth of the network links changeswith time. Its main goal is to improve the data rate that end users perceive during a multicastoperation. BAD can be used for the creation and management of multicast groups. It can bedeployed over any DHT retaining its fundamental advantages of bandwidth improvement.BAD consists of a suit of algorithms for node joins/leaves; bandwidth distribution toheterogeneous nodes; tree rearrangement and reduction of overhead. We haveimplemented BAD within the FreePastry system. We report on the results of a detailedperformance evaluation which testifies for BAD's efficiency and low overhead. Specifically …,*,2007,*
202 Michel; S. 30 Milani; A. 3 98 249 249 301 Miranda; H. Monod; M.,F Morabito; M Bender; R Beraldi; V Bezençon; D Calvanese; I Carreras; I Chlamtac; A Corsaro; G Cortese; P Cudré-Mauroux; F Davide; G De Giacomo; F De Pellegrini; V Dohnal; P Eugster; P Felber; R Guerraoui; I Gupta; F Heine; C Kiraly; S Leggio; D Lembo; M Lenzerini; V Martins; D Novak; E Pacitti; T Pitoura; V Quema; L Rodrigues; S Scipioni; S Sivasubramanian; M Szymaniak; P Triantafillou; S Tucci Piergiovanni; P Valduriez; M van Steen; A Virgillito; G Weikum; P Zezula; C Zimmer,Global Data Management 359 R. Baldoni et al.(Eds.) IOS Press; 2006 © 2006 The authors. Allrights reserved. Aberer; K. Aekaterinidis; I. Akbarinia; R. Altherr; P. Baehni; S. Bender; M.Beraldi; R. Bezençon; V. Calvanese; D. Carreras; I. Chlamtac; I. Corsaro; A. Cortese; G.Cudré-Mauroux; P. Davide; F. De Giacomo; G. De Pellegrini; F. Dohnal; V. Eugster; P.Felber; P. Guerraoui; R. Gupta; I. Heine; F. Kiraly; C. Leggio; S. Lembo; D. Lenzerini; M.Martins; V. Author Index 202 Michel; S. 30 Milani; A. 3 98 249 249 301 Miranda; H. Monod;M. Morabito; F. Novak; D. 53 249 Ntarmos; N. Pacitti; E. 177 146 146 Pierre; G. Pitoura; T.Quema; V …,Global Data Management,2006,*
''To Infinity and Beyond'': P2P Web Search with Minerva and Minerva∞,Matthias Bender; Sebastian Michel; Peter Triantafillou; Gerhard Weikum; Christian Zimmer; Roberto Baldoni; Giovanni Cortese; Fabrizio Davide; Angelo Melpignano,Abstract Peer-to-peer (P2P) computing is an intriguing paradigm for Web search for severalreasons: 1) the computational resources of a huge computer network can facilitate richermathematical and linguistic models for ranked retrieval; 2) the network provides acollaborative infrastructure where recommendations of many users and the communitybehavior can be leveraged for better search result quality; and 3) the decentralizedarchitecture of a P2P search engine is a great alternative to the de-facto monopoly of the fewlarge-scale commercial search services with the potential risk of information bias or evencensorship. The challenges of implementing this visionary approach lie in coping with thehuge scale and high dynamics of P2P networks. This paper discusses the architecturaldesign space for a scalable P2P Web search engine and presents two specific …,*,2006,*
Statistical Structures for Internet-Scale Query Optimization,N Ntarmos; P Triantafillou,*,*,2006,*
Global document frequency estimation in peer-to-peer web search,S Michel; M Bender; P Triantafillou; G Weikum,*,*,2006,*
3.2." To Infinity and Beyond": P2P Web Search with Minerva and Minerva,M Bender; S Michel; P Triantafillou; G Weikum; C Zimmer,*,EMERGING COMMUNICATION,2006,*
Towards Efficient Complex Data Management Services in Peer-to-Peer Networks,Ioannis Aekaterinidis; Nikos Ntarmos; Theoni Pitoura; Peter Triantafillou,Abstract Building efficient internet-scale data management services is the main focus of thischapter. In particular; we aim to show how to leverage DHT technology and extend it withnovel algorithms and architectures in order to (i) improve efficiency and reliability fortraditional DHT (exact-match) queries; particularly exploiting the abundance of altruismwitnessed in real-life P2P networks;(ii) speedup range queries for data stored on DHTs; and(iii) support efficiently and scalably the publish/subscribe paradigm over DHTs; whichcrucially depends on algorithms for supporting rich queries on string-attribute data.,*,2006,*
Approximate top-k query algorithms,S Michel; P Triantafillou; G Weikum,*,*,2005,*
Towards Peer-to-Peer Web Search,Gerhard Weikum; Holger Bast; Geoffrey Canright; David Hales; Christian Schindelhauer; Peter Triantafillou,The peer-to-peer (P2P) computing paradigm is an intriguing alternative to Google-stylesearch engines for querying and ranking Web content. In a network with many thousands ormillions of peers the storage and access load requirements per peer are much lighter thanfor a centralized Google-like server farm; thus more powerful techniques from informationretrieval; statistical learning; computational linguistics; and ontological reasoning can beemployed on each peer's local search engine for boosting the quality of search results [1; 2;10–12; 26]. In addition; peers can dynamically collaborate on advanced and particularlydifficult queries. Moroever; a peer-to-peer setting is ideally suited to capture local userbehavior; like query logs and click streams; and disseminate and aggregate this informationin the network; at the discretion of the corresponding user; in order to incorporate richer …,Untitled Event,2005,*
of Proceedings: Middleware 2005: ACM; IFIP; USENIX 6th International Middleware Conference,Sebastian Michel; Peter Triantafillou; Gerhard Weikum,*,*,2005,*
of Proceedings: European Conference on Complex Systems (ECCS'05) Workshop on Peer-to-peer Data Management in the Complex Systems Perspective,Gerhard Weikum; Holger Bast; Geoffrey Canright; David Hales; Christian Schindelhauer; Peter Triantafillou,Abstract/Description: The peer-to-peer computing paradigm is an intriguing alternative toGoogle-style search engines for querying and ranking Web content. In a network with manythousands or millions of peers the storage and access load requirements per peer are muchlighter than for a centralized Google-like server farm; thus more powerful techniques frominformation retrieval; statistical learning; computational linguistics; and ontological reasoningcan be employed on each peer's local search engine for boosting the quality of searchresults. In addition; peers can dynamically collaborate on advanced and particularly difficultqueries. Moroever; a peer-to-peer setting is ideally suited to capture local user behavior; likequery logs and click streams; and disseminate and aggregate this information in thenetwork; at the discretion of the corresponding user; in order to incorporate richer …,*,2005,*
Sizeof () in P2P Networks: Hash Sketches over DHTs,N Ntarmos; P Triantafillou,*,*,2004,*
SeAl: selfishness and altruism in peer-to-peer data sharing networks,N Ntarmos; P Triantafillou,*,*,2004,*
DICE and Co. In. S.: a data integration cache engine for a content integration system,Peter Triantafillou; Nikos Ntarmos; John Yannakopoulos,Content integration of Web data sources is becoming increasingly important for theformation of the next generation information systems. A common performance bottleneckfaced by all proposed solutions is the network overhead incurred when contacting theintegrated e-sites. With this paper we contribute ongoing work on DICE and Co. In. S.; adomain-independent content integration system and its data integration cache engine. DICEconstitutes a cache engine utilizing novel techniques for operating as a fully active semanticcache. We show how our contributions can be applied in the field of content integration inorder to improve the response time of content integration systems; representative of a largeclass of e-commerce applications. We have implemented the proposed architecture and arecurrently developing a number of applications.,Web Information Systems Engineering; 2003. WISE 2003. Proceedings of the Fourth International Conference on,2003,*
The HyperHotel application built over DICE and Co. In. S,Peter Triantafillou; Nikos Ntarmos; John Yannakopoulos,Content integration of Web data sources is becoming increasingly important for the nextgeneration information systems. However; all proposed solutions are faced with the sameperformance bottleneck: the network overhead incurred when contacting the integrated e-sites. With this demo paper; we shall demonstrate the functionality of HyperHotel.HyperHotel is used for finding appropriate hotel rooms when travelling. Its novetlies are thatit is designed and implemented as an Internet Web-hotel content integration application andthat it is built on top of DICE and CoInS; a novel content integration infrastructure consistingof a domain-independent content integration system and its data integration cache engine.We'll show how the infrastructure of DICE and CoInS can be applied and exploited inHyperHotel in order to improve the response time of complex user queries. This …,Web Information Systems Engineering; 2003. WISE 2003. Proceedings of the Fourth International Conference on,2003,*
ProxyTeller: A tool guiding web proxy placement decisions,P Triantafillou; I Aekaterinidis,*,*,2003,*
ProxyTeller: A Tool for Guiding Web Proxy Cache Placement Decisions.,Peter Triantafillou; Ioannis Aekaterinidis,ABSTRACT The efficient delivery of internet content has been identified as a key issue ofresearch for some time. Forward (or reverse) proxies; which are positioned along therequest route from the users' browsers to the origin content servers; maintain a cache withcopies of content from their origin servers. The strategic placement of proxies across thebackbone ISP network (or the Content Delivery Network) can drastically improve theperformance of the system (in terms of network bandwidth savings; origin server load; anduser-request latency). The ultimate goal of this work is to develop a tool that decides on theposition and the number of proxies required in order to achieve given performanceimprovements (expressed in terms of network bandwidth; origin server load; and user-latency). We believe such a tool will be very helpful to ISPs/CDNs; content providers; and …,WWW (Posters),2003,*
High performance data broadcasting: A comprehensive systems perspective (or The non-issue of broadcast scheduling),P Triantafillou; R Harpantidou; M Paterakis,*,*,2001,*
Optimal memory exploitation for continuous media: To cache or to prefetch,P Triantafillou; A Hondroulis; C Harizakis,*,*,2001,*
Concise Papers_,Peter Triantafillou; Stavros Christodoulakis; Costas Georgiadis,IN this paper; we consider a set of objects stored on a disk device. A disk drive partitions aplatter's surface into a set of concentric tracks containing a number of sectors. Sectors arethe minimum unit of disk data that can be accessed. Consecutive tracks from all the plattersform cylinders. Information is read and written onto the platter surfaces using a per-platter-surface read/write head. Each head is attached to a head-arm mechanical assembly whichpositions all the heads onto the desired cylinder. Finally; the disk pack is constantlyrevolving with the help of a spindle. When a request for a sector is issued; the head-armassembly must seek to the cylinder containing the target sector. After the rotational delayneeded for the target sector to is brought under the head; the sector can be transferred. 1.1Disk Technologies CAV disks consist of concentric tracks and have a constant angular …,IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,2000,*
Increased computer networking has sparked a resurgence of the on-line revolution of the 1970's; making ever larger amounts of data available on a world wide basi...,RR Muntz; L Golubchik; Daniel A Ford; Robert JT Morris; Alan E Bell,Disk array systems are rapidly becoming the secondary-storage media of choice for manyemerging applications with large storage and high bandwidth requirements. Striping dataacross the disks of a disk array introduces significant performance benefits mainly becausethe effective transfer rate of the secondary storage is increased by a factor equal to the stripewidth. However; the choice of the...,Parallel Computing,1998,*
Scheduling strategies for mixed workloads in multimedia information systems,G Nerjes; Y Robogianakis; P Muth; M Paterakis; P Triantafillou; G Weikum,*,*,1998,*
Efficient management of spatial data is becoming more and more important and for very large sets of 2-dimensional data; secondary memory data representations ar...,Seung-Jin Lim; Yiu-Kai Ng; Peter Triantafillou; Y Vassiliou; P Constantopoulos; J Mylopoulos; Moira C Norrie; Martin Wunderli,Although approaches for vertical fragmentation and data allocation have been proposed;algorithms for vertical fragmentation and allocation of data and rules in distributed deductivedatabase systems (DDDBSs) are lacking. In this paper; we present different approaches forvertical fragmentation of relations that are referenced by rules and an allocation strategy forrules and fragments in a DDDBS...,Information Systems,1997,*
Research and Development Issues for Large-Scale Multimedia Information Systems; Special Issue on Multimedia Information Systems,S Christodoulakis; P Triantafillou,Search all the public and authenticated articles in CiteULike. Include unauthenticated resultstoo (may include "spam") Enter a search phrase. You can also specify a CiteULike article id(123456);. a DOI (doi:10.1234/12345678). or a PubMed ID (pmid:12345678). Click Help foradvanced usage. CiteULike; Group: VT_DLRL; Search; Register; Log in …,ACM Computing Surveys,1995,*
Multi-class Replicated Data Management,Peter Triantafillou; David James Taylor,*,*,1990,*
VELOS: A Replication-control Protocol for Maintaining Availability in Partitioned Replicated Systems,Peter Triantafillou; David J Taylor,*,*,1990,*
Minimizing Replication Costs with Leaders; Location Information and Optimism,Peter Triantafillou; David James Taylor,*,*,1989,*
Nabil Adam; Rutgers University; USA Ishfaq Ahmad; Hong Kong University; Hong Kong Bharat Bhargava; Purdue University; USA Elisa B&no; University ofMilutto; Ir...,Andrew Campbell; Tiziana Catarci; Italy SF Chang; David Hung-Chuang Du; Edward Fox; Borko Furht; Wolfgang Klas; TD Little; E Neuhold; Raymong Ng; M Tamer Ozsu; Raghu Ramkrishnan; VS Subramanian; Don Tow&y; Peter Triantafillou; Grerce G Weikum,*,*,*,*
in Large-Scal~ e Distributed Systems,Peter Triantafillou,Abstract-In large systems; replication can become important means to improve data accesstimes and; availability. Existing 'recovery protocols; on the other hand; were proposed forsmall-scale distributed systems. Such protocols typically update stale; I newly-recoveredsites with replicated data and resolve the commit uncertainty of recovering sites. Thus; giventhat in large systems failures are more frequent and that data access times are costlier; suchprotocols can potentially introduce large overheads in large systems and must be avoided; ifpossible. We call these protocols dependent recovery protocols since they require arecovering site to consult with other sites. independent recovery has been studied in thecontext of one-copy systems and has been proven unattainable. This paper ottersindependent recovery protocols for large-scale systems with replicated data. it shows how …,*,*,*
Anne-Marie Kermarrec; INRIA; France Cecilia Mascolo; University College London; UK Martin May; ETH Zurich; Switzerland Alberto Montresor; University of Trento; I...,Gianluca Moro; Paolo Penna; Douglas Reeves; Matei Ripeanu; Christoph Schuba; Nahid Shahmehri; Sandeep Singhal; Orazio Tomarchio; Peter Triantafillou; Kurt Tutschku; Marcel Waldvogel; Klaus Wehrle; Nathalie Weiler; Adam Wierzbicki,Karl Aberer; EPFL; Switzerland Fabian Bustamante; Northwestern University; USA BrunoCrispo; University of Trento; Italy Christos Gkantsidis; Microsoft Research; UK David Hales; Universityof Bologna; Italy Seif Haridi; SICS; Sweden Manfred Hauswirth; EFPL; Switzerland YimingHu; University of Cincinnati; USA Gianluca Iannacone; Intel Labs; UK Mark Jelasity; Universityof Bologna; Italy Marco Mamei; University of Modena and Reggio-Emilia; Italy Anne-MarieKermarrec; INRIA; France Cecilia Mascolo; University College London; UK Martin May; ETHZurich; Switzerland Alberto Montresor; University of Trento; Italy Gianluca Moro; University ofBologna in Cesena; Italy Satoshi Nishiyama; KDDI; Japan Paolo Penna; University ofSalerno; Italy Douglas Reeves; North Carolina State University; USA Matei Ripeanu; Universityof British Columbia; Canada Christoph Schuba; Linköpings University; Sweden Patrick …,*,*,*
Consistency in Card-based Mobile Databases: Sharing Digital Money by Replicating Smart Cards.,Osmar R Zaïane; Peter Triantafillou; Jiawei Han,*,*,*,*
The Case for Volunteering-based Sharing Networks,Peter Triantafillou,ABSTRACT Our position is that a key to research efforts on ensuring high performance invery large scale sharing networks is the idea of volunteering; recognizing that such networksare comprised of largely heterogeneous nodes in terms of their capacity and behaviour; andthat; in many real-world manifestations; a few nodes carry the bulk of the request serviceload. In this paper we outline how we employ volunteering as the basic idea using which wedevelop altruism-endowed self-organizing sharing networks to help solve two openproblems in large-scale peer-to-peer networks:(i) to develop an overlay topology structurethat enjoys better performance than DHT-structured networks and; specifically; to offer O (loglog N) routing performance in a network of N nodes; instead of O (log N); and (ii) to efficientlyprocess complex queries and range queries; in particular.,*,*,*
P2P’08,Ernst Biersack; Sonja Buchegger; John Buford; Fabian Bustamante; Bruno Crispo; Zoran Despotovic; Joerg Eberspaecher; Lars Eggert; Dick Epema; Bernd Freisleben; Alain Gefflaut; Ali Ghodsi; Christos Gkantsidis; Andrei Gurtov; Gerhard Hasslinger; David Hausheer; Manfred Hauswirth; Pilar Herero; Yiming Hu; Matthias Jarke; Mark Jelasity; Jussi Kangasharju; Daniel Catrein; Manolis Koubarakis; Aleksandra Kovacevic; Fabrice Le Fessant; Christoph Lindemann; Andreas Mauthe; Martin May; Alberto Montresor; Gianluca Moro; Aaron J Quigley; Douglas Reeves; Matei Ripeanu; Kai-Uwe Sattler; Christoph Schuba; Henning Schulzrinne; Sherman Shen; Henk Sips; Phuoc Tran-Gia; Peter Triantafillou; Kurt Tutschku; Maarten van Steen; Yushun Wang; Michael Welzl; Roger Zimmermann,Karl Aberer; EPFL; Switzerland Wolf-Tilo Balke; L3S; Germany Marinho Barcellos; PUCRS; BrazilErnst Biersack; Institut Eurecom; France Sonja Buchegger; US Berkeley; USA John Buford;Avaya; USA Fabian Bustamante; Northwestern University; USA Bruno Crispo; University ofTrento; Italy Anwitaman Datta; NTU; Singapore Zoran Despotovic; NTT DoCoMo Euro-Labs;Germany Joerg Eberspaecher; TU Munich; Germany Lars Eggert; Nokia; Finland DickEpema; TU Delft; The Netherlands Bernd Freisleben; University of Marburg; Germany AlainGefflaut; Microsoft; Germany Ali Ghodsi; KTH/SICS; Sweden Christos Gkantsidis; MicrosoftResearch; UK Andrei Gurtov; University of Helsinki; Finland Gerhard Hasslinger; T-Systems;Germany David Hausheer; University of Zurich; Switzerland Manfred Hauswirth; National Universityof Ireland Pilar Herero; Madrid University of Technology; Spain Yiming Hu; University of …,*,*,*
Web Proxy Cache Placement; Replacement; and the ProxyTeller1,Peter Triantafillou; Ioannis Aekaterinides,Abstract With this paper our intention is to first study comprehensively some key issues forthe efficient delivery of internet content. In particular; we report the results of our study on theperformance of prominent cache replacement algorithms and their key components; as theyhave been proposed in the literature. We chart the problem solution space; identifying underwhich conditions each algorithm is preferable; for which performance metric; and the impactof components such as the size of cached objects; their communication cost; and theirpopularity. Subsequently; we focus on the performance of proxy-cache placementalgorithms. The results of the above two studies are finally used to meet our main goal withthis research; that is to develop a tool which helps decide on the number and placement ofproxy caches required to achieve certain performance goals with respect to network …,Submitted for Publication,*,*
Guiding Web Proxy and Server Placement for High Performance Internet Content Delivery1,Peter Triantafillou; Ioannis Aekaterinidis,Abstract The performance of a network designed for efficient web content delivery largelydepends on the strategic placement of (forward or reverse) web proxies and/or full-serverreplicas of web origin servers. This paper presents a tool; coined ProxyTeller; which wedeveloped specifically for guiding such placement decisions. ProxyTeller decides on theposition and the number of proxies and/or full server replicas required in order to achievegiven performance constraints; which are expressed as percentage improvements in termsof network bandwidth; origin server load; and user-latency. ProxyTeller would be veryhelpful to ISPs/CDNs; content providers; and end-users and is; thus; very much lacking. Weoverview the design and implementation of ProxyTeller and present scenarios of its use andrelated results. ProxyTeller is available through the internet and our hope is that it will …,*,*,*
Organization Committee,Nathalie Weiler; Nahid Shahmehri; Ross Lee Graham; Jeremy Bryans; Rajkumar Buyya; Manuel Hauswirth; Yiming Hu; Cecilia Mascolo; Alberto Montresor; Bernhard Plattner; Raphael Rom; ENSICA Patrick Senac; Orazio Tomarchio; Bernard Traversat; Peter Triantafillou; UCL Peter Van Roy; Belgium Marcel Waldvogel,*,*,*,*
DELIS-TR-0293,Sebastian Michel; Matthias Bender; Peter Triantafillou,*,*,*,*
