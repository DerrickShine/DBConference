Cyclone: A Safe Dialect of C.,Trevor Jim; J Gregory Morrisett; Dan Grossman; Michael W Hicks; James Cheney; Yanling Wang,Abstract: Cyclone is a safe dialect of C. It has been designed from the ground up to preventthe buffer overflows; format string attacks; and memory management errors that are commonin C programs; while retaining C's syntax and semantics. This paper examines safetyviolations enabled by C's design; and shows how Cyclone avoids them; without giving upC's hallmark control over low-level details such as data representation and memorymanagement.,USENIX Annual Technical Conference; General Track,2002,786
Provenance in databases: Why; how; and where,James Cheney; Laura Chiticariu; Wang-Chiew Tan,Abstract Different notions of provenance for database queries have been proposed andstudied in the past few years. In this article; we detail three main notions of databaseprovenance; some of their applications; and compare and contrast amongst them.Specifically; we review why; how; and where provenance; describe the relationships amongthese notions of provenance; and describe some of their applications in confidencecomputation; view maintenance and update; debugging; and annotation propagation.,Foundations and Trends in Databases,2009,448
Region-based memory management in Cyclone,Dan Grossman; Greg Morrisett; Trevor Jim; Michael Hicks; Yanling Wang; James Cheney,Abstract Cyclone is a type-safe programming language derived from C. The primary designgoal of Cyclone is to let programmers control data representation and memory managementwithout sacrificing type-safety. In this paper; we focus on the region-based memorymanagement of Cyclone and its static typing discipline. The design incorporates severaladvancements; including support for region subtyping and a coherent integration with stackallocation and a garbage collector. To support separate compilation; Cyclone requiresprogrammers to write some explicit region annotations; but a combination of defaultannotations; local type inference; and a novel treatment of region effects reduces thisburden. As a result; we integrate C idioms in a region-based framework. In our experience;porting legacy C to Cyclone has required altering about 8% of the code; of the changes …,ACM Sigplan Notices,2002,405
Provenance management in curated databases,Peter Buneman; Adriane Chapman; James Cheney,Abstract Curated databases in bioinformatics and other disciplines are the result of a greatdeal of manual annotation; correction and transfer of data from other sources. Provenanceinformation concerning the creation; attribution; or version history of such data is crucial forassessing its integrity and scientific value. General purpose database systems provide littlesupport for tracking provenance; especially when data moves among databases. This paperinvestigates general-purpose techniques for recording provenance for data that is copiedamong databases. We describe an approach in which we track the user's actions whilebrowsing source databases and copying data into a curated database; in order to record theuser's actions in a convenient; queryable form. We present an implementation of thistechnique and use it to evaluate the feasibility of database support for provenance …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,361
Compressing XML with multiplexed hierarchical PPM models,James Cheney,We established a working Extensible Markup Language (XML) compression benchmarkbased on text compression; and found that bzip2 compresses XML best; albeit more slowlythan gzip. Our experiments verified that T/sub XMILL/speeds up and improves compressionusing gzip and bounded-context PPM by up to 15%; but found that it worsens thecompression for bzip2 and PPM. We describe alternative approaches to XML compressionthat illustrate other tradeoffs between speed and effectiveness. We describe experimentsusing several text compressors and XMILL to compress a variety of XML documents. Usingthese as a benchmark; we describe our two main results: an online binary encoding for XMLcalled Encoded SAX (ESAX) that compresses better and faster than existing methods; andan online; adaptive; XML-conscious encoding based on prediction by partial match (PPM) …,Data Compression Conference; 2001. Proceedings. DCC 2001.,2001,285
Prov-o: The prov ontology,Timothy Lebo; Satya Sahoo; Deborah McGuinness; Khalid Belhajjame; James Cheney; David Corsar; Daniel Garijo; Stian Soiland-Reyes; Stephan Zednik; Jun Zhao,Abstract The PROV Ontology (PROV-O) expresses the PROV Data Model [PROV-DM] usingthe OWL2 Web Ontology Language (OWL2)[OWL2-OVERVIEW]. It provides a set of classes;properties; and restrictions that can be used to represent and interchange provenanceinformation generated in different systems and under different contexts. It can also bespecialized to create new classes and properties to model provenance information fordifferent applications and domains. The PROV Document Overview describes the overallstate of PROV; and should be read before other PROV documents.,W3C recommendation,2013,230
First-class phantom types,James Cheney; Ralf Hinze,Classical phantom types are datatypes in which type constraints are expressed using typevariables that do not appear in the datatype cases themselves. They can be used to embedtyped languages into Haskell or ML. However; while such encodings guarantee that onlywell-formed data can be constructed; they do not permit type-safe deconstruction withoutadditional tagging and run-time checks. We introduce first-class phantom types; which makesuch constraints explicit via type equations. Examples of first-class phantom types includetyped type representations and typed higher-order abstract syntax trees. These types can beused to support typed generic functions; dynamic typing; and staged compilation in higher-order; statically typed languages such as Haskell or Standard ML. In our system; typeconstraints can be equations between type constructors as well as type functions of …,*,2003,221
Prov-dm: The prov data model,Luc Moreau; Paolo Missier; Khalid Belhajjame; Reza B’Far; James Cheney; Sam Coppens; Stephen Cresswell; Yolanda Gil; Paul Groth; Graham Klyne; Timothy Lebo; Jim McCusker; Simon Miles; James Myers; Satya Sahoo; Curt Tilmes,*,Retrieved July,2013,205
Curated databases,Peter Buneman; James Cheney; Wang-Chiew Tan; Stijn Vansummeren,Abstract Curated databases are databases that are populated and updated with a great dealof human effort. Most reference works that one traditionally found on the reference shelves oflibraries--dictionaries; encyclopedias; gazetteers etc.--are now curated databases. Since it isnow easy to publish databases on the web; there has been an explosion in the number ofnew curated databases used in scientific research. The value of curated databases lies inthe organization and the quality of the data they contain. Like the paper reference works theyhave replaced; they usually represent the efforts of a dedicated group of people to produce adefinitive description of some subject area. Curated databases present a number ofchallenges for database research. The topics of annotation; provenance; and citation arecentral; because curated databases are heavily cross-referenced with; and include data …,Proceedings of the twenty-seventh ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2008,142
A lightweight implementation of generics and dynamics,James Cheney; Ralf Hinze,Abstract The recent years have seen a number of proposals for extending statically typedlanguages by dynamics or generics. Most proposals---if not all---require significantextensions to the underlying language. In this paper we show that this need not be the case.We propose a particularly lightweight extension that supports both dynamics and generics.Furthermore; the two features are smoothly integrated: dynamic values; for instance; can bepassed to generic functions. Our proposal makes do with a standard Hindley-Milner typesystem augmented by existential types. Building upon these ideas we have implemented asmall library that is readily usable both with Hugs and with the Glasgow Haskell compiler.,Proceedings of the 2002 ACM SIGPLAN workshop on Haskell,2002,137
On the expressiveness of implicit provenance in query and update languages,Peter Buneman; James Cheney; Stijn Vansummeren,Abstract Information describing the origin of data; generally referred to as provenance; isimportant in scientific and curated databases where it is the basis for the trust one puts intheir contents. Since such databases are constructed using operations of both query andupdate languages; it is of paramount importance to describe the effect of these languageson provenance. In this article we study provenance for query and update languages that areclosely related to SQL; and compare two ways in which they can manipulate provenance sothat elements of the input are rearranged to elements of the output: implicit provenance;where a query or update only provides the rearranged output; and provenance is providedimplicitly by a default provenance semantics; and explicit provenance; where a query orupdate provides both the output and the description of the provenance of each …,Database Theory–ICDT 2007,2007,133
Provenance: a future history,James Cheney; Stephen Chong; Nate Foster; Margo Seltzer; Stijn Vansummeren,Abstract Science; industry; and society are being revolutionized by radical new capabilitiesfor information sharing; distributed computation; and collaboration offered by the World WideWeb. This revolution promises dramatic benefits but also poses serious risks due to the fluidnature of digital information. One important cross-cutting issue is managing and recordingprovenance; or metadata about the origin; context; or history of data. We posit thatprovenance will play a central role in emerging advanced digital infrastructures. In thispaper; we outline the current state of provenance research and practice; identify hard openresearch problems involving provenance semantics; formal modeling; and security; andarticulate a vision for the future of provenance.,Proceedings of the 24th ACM SIGPLAN conference companion on Object oriented programming systems languages and applications,2009,97
alphaProlog: A Logic Programming Language with Names; Binding and-Equivalence,James Cheney; Christian Urban,*,ICLP,2004,85
The W3C PROV family of specifications for modelling provenance metadata,Paolo Missier; Khalid Belhajjame; James Cheney,Abstract Provenance; a form of structured metadata designed to record the origin or sourceof information; can be instrumental in deciding whether information is to be trusted; how itcan be integrated with other diverse information sources; and how to establish attribution ofinformation to authors throughout its history. The PROV set of specifications; produced bythe World Wide Web Consortium (W3C); is designed to promote the publication ofprovenance information on the Web; and offers a basis for interoperability across diverseprovenance management systems. The PROV provenance model is deliberately genericand domain-agnostic; but extension mechanisms are available and can be exploited formodelling specific domains. This tutorial provides an account of these specifications.Starting from intuitive and informal examples that present idiomatic provenance patterns …,Proceedings of the 16th International Conference on Extending Database Technology,2013,80
Provenance as dependency analysis,James Cheney; Amal Ahmed; Umut A Acar,Abstract Provenance is information recording the source; derivation; or history of someinformation. Provenance tracking has been studied in a variety of settings; however;although many design points have been explored; the mathematical or semanticfoundations of data provenance have received comparatively little attention. In this paper;we argue that dependency analysis techniques familiar from program analysis and programslicing provide a formal foundation for forms of provenance that are intended to show how(part of) the output of a query depends on (parts of) its input. We introduce a semanticcharacterization of such dependency provenance; show that this form of provenance is notcomputable; and provide dynamic and static approximation techniques.,International Symposium on Database Programming Languages,2007,74
Requirements for provenance on the web,Paul Groth; Yolanda Gil; James Cheney; Simon Miles,Abstract From where did this tweet originate? Was this quote from the New York Timesmodified? Daily; we rely on data from the Web; but often it is difficult or impossible todetermine where it came from or how it was produced. This lack of provenance is particularlyevident when people and systems deal with Web information or with any environment whereinformation comes from sources of varying quality. Provenance is not captured pervasivelyin information systems. There are major technical; social; and economic impediments thatstand in the way of using provenance effectively. This paper synthesizes requirements forprovenance on the Web for a number of dimensions; focusing on three key aspects ofprovenance: the content of provenance; the management of provenance records; and theuses of provenance information. To illustrate these requirements; we use three …,International Journal of Digital Curation,2012,66
Nominal logic programming,James Cheney; Christian Urban,Abstract Nominal logic is an extension of first-order logic which provides a simple foundationfor formalizing and reasoning about abstract syntax modulo consistent renaming of boundnames (that is; α-equivalence). This article investigates logic programming based onnominal logic. We describe some typical nominal logic programs; and develop the model-theoretic; proof-theoretic; and operational semantics of such programs. Besides being ofinterest for ensuring the correct behavior of implementations; these results provide arigorous foundation for techniques for analysis and reasoning about nominal logicprograms; as we illustrate via examples.,ACM Transactions on Programming Languages and Systems (TOPLAS),2008,66
A practical theory of language-integrated query,James Cheney; Sam Lindley; Philip Wadler,Abstract Language-integrated query is receiving renewed attention; in part because of itssupport through Microsoft's LINQ framework. We present a practical theory of language-integrated query based on quotation and normalisation of quoted terms. Our techniquesupports join queries; abstraction over values and predicates; composition of queries;dynamic generation of queries; and queries with nested intermediate data. Higher-orderfeatures prove useful even for constructing first-order queries. We prove a theoremcharacterising when a host query is guaranteed to generate a single SQL query. We presentexperimental results confirming our technique works; even in situations where Microsoft'sLINQ framework either fails to produce an SQL query or; in one case; produces anavalanche of SQL queries.,ACM SIGPLAN Notices,2013,64
A sequent calculus for nominal logic,Murdoch Gabbay; James Cheney,Nominal logic is a theory of names and binding based on the primitive concepts of freshnessand swapping; with a self-dual N-(or" new")-quantifier; originally presented as a Hilbert-styleaxiom system extending first-order logic. We present a sequent calculus for nominal logiccalled fresh logic; or FL; admitting cut-elimination. We use FL to provide a proof-theoreticfoundation for nominal logic programming and show how to interpret FO/spl lambda//splnabla/; another logic with a self-dual quantifier; within FL.,Logic in Computer Science; 2004. Proceedings of the 19th Annual IEEE Symposium on,2004,57
A provenance model for manually curated data,Peter Buneman; Adriane Chapman; James Cheney; Stijn Vansummeren,Abstract Many curated databases are constructed by scientists integrating various existingdata sources “by hand”; that is; by manually entering or copying data from other sources.Capturing provenance in such an environment is a challenging problem; requiring a goodmodel of the process of curation. Existing models of provenance focus on queries/views indatabases or computations on the Grid; not updates of databases or Web sites. In this paperwe motivate and present a simple model of provenance for manually curated databases anddiscuss ongoing and future work.,International Provenance and Annotation Workshop,2006,52
A formal framework for provenance security,James Cheney,Provenance; or information about the origin; derivation; or history of data; is becoming animportant topic especially for shared scientific or public data on the Web. It clearly hasimplications on security (and vice versa) yet these implications are not well-understood. Agreat deal of work has focused on mechanisms for recording; managing or using some kindof provenance information; but relatively little progress has been made on foundationalmodels that define provenance and relate it to security goals such as availability;confidentiality or privacy. We argue that such foundations are essential to makingmeaningful progress on these problems and should be developed. In this paper; we outlinea formal model of provenance; propose formalizations of security properties for provenancesuch as disclosure and obfuscation; and explore their implications in domains based on …,Computer Security Foundations Symposium (CSF); 2011 IEEE 24th,2011,48
YesWorkflow: a user-oriented; language-independent tool for recovering workflow information from scripts,Timothy McPhillips; Tianhong Song; Tyler Kolisnik; Steve Aulenbach; Khalid Belhajjame; Kyle Bocinsky; Yang Cao; Fernando Chirigati; Saumen Dey; Juliana Freire; Deborah Huntzinger; Christopher Jones; David Koop; Paolo Missier; Mark Schildhauer; Christopher Schwalm; Yaxing Wei; James Cheney; Mark Bieda; Bertram Ludaescher,Abstract: Scientific workflow management systems offer features for composing complexcomputational pipelines from modular building blocks; for executing the resulting automatedworkflows; and for recording the provenance of data products resulting from workflow runs.Despite the advantages such features provide; many automated workflows continue to beimplemented and executed outside of scientific workflow systems due to the convenienceand familiarity of scripting languages (such as Perl; Python; R; and MATLAB); and to thehigh productivity many scientists experience when using these languages. YesWorkflow is aset of software tools that aim to provide such users of scripting languages with many of thebenefits of scientific workflow systems. YesWorkflow requires neither the use of a workflowengine nor the overhead of adapting code to run effectively in such a system. Instead …,arXiv preprint arXiv:1502.02403,2015,45
A graph model of data and workflow provenance,Umut Acar; Peter Buneman; James Cheney; Jan Van Den Bussche; Natalia Kwasnikowska; Stijn Vansummeren,Abstract Provenance has been studied extensively in both database and workflowmanagement systems; so far with little convergence of definitions or models. Provenance indatabases has generally been defined for relational or complex object data; by propagatingfine-grained annotations or algebraic expressions from the input to the output. This kind ofprovenance has been found useful in other areas of computer science: annotationdatabases; probabilistic databases; schema and data integration; etc. In contrast; workflowprovenance aims to capture a complete description of evaluation–or enactment–of aworkflow; and this is crucial to verification in scientific computation. Workflows and theirprovenance are often presented using graphical notation; making them easy to visualize butcomplicating the formal semantics that relates their run-time behavior with their …,*,2010,42
Provenance XG final report,Yolanda Gil; James Cheney; Paul Groth; Olaf Hartig; Simon Miles; Luc Moreau; Paulo Pinheiro Da Silva,KNAW Narcis. Back to search results. Publication ProvenanceXG Final Report (2010). Pagina-navigatie: Main …,Final Incubator Group Report,2010,40
FLUX: functional updates for XML,James Cheney,Abstract XML database query languages have been studied extensively; but XML databaseupdates have received relatively little attention; and pose many challenges to languagedesign. We are developing an XML update language called FLUX; which stands forFunctionaL Updates for XML; drawing upon ideas from functional programming languages.In prior work; we have introduced a core language for FLUX with a clear operationalsemantics and a sound; decidable static type system based on regular expression types.Our initial proposal had several limitations. First; it lacked support for recursive types orupdate procedures. Second; although a high-level source language can easily be translatedto the core language; it is difficult to propagate meaningful type errors from the corelanguage back to the source. Third; certain updates are wellformed yet contain path …,ACM SIGPLAN Notices,2008,40
The complexity of equivariant unification,James Cheney,Abstract Nominal logic is a first-order theory of names and binding based on a primitiveoperation of swapping rather than substitution. Urban; Pitts; and Gabbay have developed anominal unification algorithm that unifies terms up to nominal equality. However; because ofnominal logic's equivariance principle; atomic formulas can be provably equivalent withoutbeing provably equal as terms; so resolution using nominal unification is sound butincomplete. For complete resolution; a more general form of unification called equivariantunification; or “unification up to a permutation” is required. Similarly; for rewrite rulesexpressed in nominal logic; a more general form of matching called equivariant matching isnecessary. In this paper; we study the complexity of the decision problem for equivariantunification and matching. We show that these problems are NP-complete in general …,International Colloquium on Automata; Languages; and Programming,2004,40
Functional programs that explain their work,Roly Perera; Umut A Acar; James Cheney; Paul Blain Levy,Abstract We present techniques that enable higher-order functional computations to"explain" their work by answering questions about how parts of their output were calculated.As explanations; we consider the traditional notion of program slices; which we show can beinadequate; and propose a new notion: trace slices. We present techniques for specifyingflexible and rich slicing criteria based on partial expressions; parts of which have beenreplaced by holes. We characterise program slices in an algorithm-independent fashion andshow that a least slice for a given criterion exists. We then present an algorithm; calledunevaluation; for computing least program slices from computations reified as traces.Observing a limitation of program slices; we develop a notion of trace slice as another formof explanation and present an algorithm for computing them. The unevaluation algorithm …,ACM SIGPLAN Notices,2012,39
Mechanizing the metatheory of LF,Christian Urban; James Cheney; Stefan Berghofer,Abstract LF is a dependent type theory in which many other formal systems can beconveniently embedded. However; correct use of LF relies on nontrivial metatheoreticdevelopments such as proofs of correctness of decision procedures for LF's judgments.Although detailed informal proofs of these properties have been published; they have notbeen formally verified in a theorem prover. We have formalized these properties withinIsabelle/HOL using the Nominal Datatype Package; closely following a recent article byHarper and Pfenning. In the process; we identified and resolved a gap in one of the proofsand a small number of minor lacunae in others. We also formally derive a version of the typechecking algorithm from which Isabelle/HOL can generate executable code. Besides itsintrinsic interest; our formalization provides a foundation for studying the adequacy of LF …,ACM Transactions on Computational Logic (TOCL),2011,37
Program slicing and data provenance.,James Cheney,Abstract Provenance is information that aids understanding and troubleshooting databasequeries by explaining the results in terms of the input. Slicing is a program analysistechnique for debugging and understanding programs that has been studied since the early1980s; in which program results are explained in terms of parts of the program thatcontributed to the results. This paper will briefly review ideas and techniques from programslicing and show how they might be useful for improving our understanding of provenance indatabases.,IEEE Data Eng. Bull.,2007,37
Scrap your nameplate:(functional pearl),James Cheney,Abstract Recent research has shown how boilerplate code; or repetitive code for traversingdatatypes; can be eliminated using generic programming techniques already availablewithin some implementations of Haskell. One particularly intractable kind of boilerplate isnameplate; or code having to do with names; name-binding; and fresh name generation.One reason for the difficulty is that operations on data structures involving names; as usuallyimplemented; are not regular instances of standard map; fold; or zip operations. However; innominal abstract syntax; an alternative treatment of names and binding based on swapping;operations such as α-equivalence; capture-avoiding substitution; and free variable setfunctions are much better-behaved. In this paper; we show how nominal abstract syntaxtechniques similar to those of FreshML can be provided as a Haskell library called …,ACM SIGPLAN Notices,2005,37
Semantics; types and effects for XML updates,Michael Benedikt; James Cheney,Abstract The W3C recently released the XQuery Update Facility 1.0; a CandidateRecommendation for an XML update language. It appears likely that this proposal willbecome standard. XQuery has been equip-ped with a formal semantics and sound typesystem; but there has been little work on static analysis or typechecking of XML updates; andthe typing rules in the current W3C proposal appear unsound for “transform” queries thatperform embedded updates. In this paper; we investigate the problem of schema alteration;or synthesizing an output schema describing the result of an update applied to a given inputschema. We review regular expression type systems for XQuery; present a core languageand semantics for W3C-style XML updates; and develop an effect analysis and schemaalteration; which can be used as the basis for sound typechecking for queries involving “ …,International Symposium on Database Programming Languages,2009,36
A simple sequent calculus for nominal logic,James Cheney,Abstract Nominal logic is a variant of first-order logic that provides support for reasoningabout bound names in abstract syntax. A key feature of nominal logic is the new-quantifier;which quantifies over fresh names (names not appearing in any values considered so far).Previous attempts have been made to develop convenient rules for reasoning with the new-quantifier; but we argue that none of these attempts is completely satisfactory. In this articlewe develop a new sequent calculus for nominal logic in which the rules for the new-quantifier are much simpler than in previous attempts. We also prove several structural andmetatheoretic properties; including cut-elimination; consistency and equivalence to Pitts'axiomatization of nominal logic.,Journal of Logic and Computation,2014,33
Completeness and Herbrand theorems for nominal logic,James Cheney,Abstract Nominal logic is a variant of first-order logic in which abstract syntax with namesand binding is formalized in terms of two basic operations: name-swapping and freshness. Itrelies on two important principles: equivariance (validity is preserved by name-swapping);and fresh name generation (“new” or fresh names can always be chosen). It is inspired by aparticular class of models for abstract syntax trees involving names and binding; drawing onideas from Fraenkel-Mostowski set theory: finite-support models in which each value candepend on only finitely many names.,The Journal of Symbolic Logic,2006,33
Relating nominal and higher-order pattern unification,James Cheney,Abstract. Higher-order pattern unification and nominal unification are two approaches tounifying modulo some form of α-equivalence (consistent renaming of bound names). Thehigher-order and nominal approaches seem superficially dissimilar. However; we show thata natural concretion (or name-application) operation for nominal terms can be used tosimulate the behavior of higherorder patterns. We describe a form of nominal terms callednominal patterns that includes concretion and for which unification is equivalent to a specialcase of higher-order pattern unification; and then show that full higher-order patternunification can be reduced to nominal unification via nominal patterns.,Proceedings of the 19th international workshop on Unification (UNIF 2005),2005,33
Causality and the semantics of provenance,James Cheney,Abstract: Provenance; or information about the sources; derivation; custody or history ofdata; has been studied recently in a number of contexts; including databases; scientificworkflows and the Semantic Web. Many provenance mechanisms have been developed;motivated by informal notions such as influence; dependence; explanation and causality.However; there has been little study of whether these mechanisms formally satisfyappropriate policies or even how to formalize relevant motivating concepts such ascausality. We contend that mathematical models of these concepts are needed to justify andcompare provenance techniques. In this paper we review a theory of causality based onstructural models that has been developed in artificial intelligence; and describe work inprogress on a causal semantics for provenance graphs.,arXiv preprint arXiv:1004.3241,2010,32
Row-based effect types for database integration,Sam Lindley; James Cheney,Abstract We present CoreLinks; a call-by-value variant of System F with row polymorphism;row-based effect types; and implicit subkinding; which forms the basis for the Links webprogramming language. We focus on extensions to CoreLinks for database programming.The effect types support abstraction over database queries; while ensuring that queries aretranslated predictably to idiomatic and efficient SQL at run-time. Subkinding staticallyenforces the constraint that queries must return a list of records of base type. Polymorphismover the presence of record labels supports abstraction over database queries; inserts;deletes and updates.,Proceedings of the 8th ACM SIGPLAN workshop on Types in language design and implementation,2012,31
Destabilizers and independence of XML updates,Michael Benedikt; James Cheney,Abstract Independence analysis is the problem of determining whether an update affects theresult of a query; eg a constraint or materialized view. We develop a new; modularframework for static independence analysis that decomposes the problem into twoorthogonal subproblems: approximating the destabilizer; that is; a finite representation of theset of updates that can change the result of the query; and testing whether the update anddestabilizer overlap via an intersection analysis. Focusing on XML queries as the viewlanguage and the XQuery Update Facility as the update language; we present a syntacticquery rewriting algorithm for translating queries to destabilizers; and show that intersectionchecking can be reduced to satisfiability problems for which efficient checkers already exist.We present an implementation based on an expressive tree satisfiability checker and a …,Proceedings of the VLDB Endowment,2010,30
Schema-based independence analysis for XML updates,Michael Benedikt; James Cheney,Abstract Query-update independence analysis is the problem of determining whether anupdate affects the results of a query. Query-update independence is useful for avoidingrecomputation of materialized views and may have applications to access control andconcurrency control. This paper develops static analysis techniques for query-updateindependence problems involving core XQuery queries and updates with a snapshotsemantics (based on the W3C XQuery Update Facility proposal). Our approach takesadvantage of schema information; in contrast to previous work on this problem. We formalizeour approach; sketch a proof of correctness; and report on the performance and accuracy ofour implementation.,Proceedings of the VLDB Endowment,2009,29
Towards a theory of information preservation,James Cheney; Carl Lagoze; Peter Botticelli,Abstract Digital preservation is a pressing challenge to the library community. In this paper;we describe the initial results of our efforts towards understanding digital (as well astraditional) preservation problems from first principles. Our approach is to use the languageof mathematics to formalize the concepts that are relevant to preservation. Our theory ofpreservation spaces draws upon ideas from logic and programming language semantics todescribe the relationship between concrete objects and their information contents. We alsodraw on game theory to show how objects change over time as a result of uncontrollableenvironment effects and directed preservation actions. In the second half of this paper; weshow how to use the mathematics of universal algebra as a language for objects whoseinformation content depends on many components. We use this language to describe …,International Conference on Theory and Practice of Digital Libraries,2001,29
Prov-dm: The prov data model,Khalid Belhajjame; Reza B'Far; James Cheney; Sam Coppens; Stephen Cresswell; Yolanda Gil; Paul Groth; Graham Klyne; Timothy Lebo; Jim McCusker; Simon Miles; James Myers; Satya Sahoo; Curt Tilmes,Provenance is information about entities; activities; and people involved in producing apiece of data or thing; which can be used to form assessments about its quality; reliability ortrustworthiness. PROV-DM is the conceptual data model that forms a basis for the W3Cprovenance (PROV) family of specifications. PROV-DM distinguishes core structures;forming the essence of provenance information; from extended structures catering for morespecific uses of provenance. PROV-DM is organized in six components; respectively dealingwith:(1) entities and activities; and the time at which they were created; used; or ended;(2)derivations of entities from entities;(3) agents bearing responsibility for entities that weregenerated and activities that happened;(4) a notion of bundle; a mechanism to supportprovenance of provenance;(5) properties to link entities that refer to the same thing; and …,*,2013,28
Provenance as dependency analysis,James Cheney; Amal Ahmed; Umut A Acar,Abstract Provenance is information recording the source; derivation or history of someinformation. Provenance tracking has been studied in a variety of settings; particularlydatabase management systems. However; although many candidate definitions ofprovenance have been proposed; the mathematical or semantic foundations of dataprovenance have received comparatively little attention. In this paper; we argue thatdependency analysis techniques familiar from program analysis and program slicingprovide a formal foundation for forms of provenance that are intended to show how (part of)the output of a query depends on (parts of) its input. We introduce a semanticcharacterisation of such dependency provenance for a core database query language; showthat minimal dependency provenance is not computable; and provide dynamic and static …,Mathematical Structures in Computer Science,2011,28
Recording provenance for sql queries and updates,Stijn Vansummeren; James Cheney,Knowing the origin of data (ie; where the data was copied or created from)---its provenance---is vital for assessing the trustworthiness of contemporary scientific databases such asUniProt and SWISS-PROT. Unfortunately; provenance information must currently berecorded manually; by added effort of the database maintainer. Since such maintenance istedious and error-prone; it is desirable to provide support for recording provenance in thedatabase system itself. We review a recent proposal for incorporating such support; as wellas its theoretical properties.,*,2007,28
Dynamic provenance for SPARQL updates,Harry Halpin; James Cheney,Abstract While the Semantic Web currently can exhibit provenance information by using theW3C PROV standards; there is a “missing link” in connecting PROV to storing and queryingfor dynamic changes to RDF graphs using SPARQL. Solving this problem would be requiredfor such clear use-cases as the creation of version control systems for RDF. While someprovenance models and annotation techniques for storing and querying provenance dataoriginally developed with databases or workflows in mind transfer readily to RDF andSPARQL; these techniques do not readily adapt to describing changes in dynamic RDFdatasets over time. In this paper we explore how to adapt the dynamic copy-pasteprovenance model of Buneman et al.[2] to RDF datasets that change over time in responseto SPARQL updates; how to represent the resulting provenance records themselves as …,International Semantic Web Conference,2014,27
Tradeoffs in XML database compression,James Cheney,Large XML data files; or XML databases; are now a common way to distribute scientific andbibliographic data; and storing such data efficiently is an important concern. A number ofapproaches to XML compression have been proposed in the last five years. The mostcompetitive approaches employ one or more statistical text compressors based on PPM orarithmetic coding in which some of the context is provided by the XML document structure.The purpose of this paper is to investigate the relationship between the extant proposals inmore detail. We review the two main statistical modeling approaches proposed so far; andevaluate their performance on two representative XML databases. Our main finding is thatwhile a recently-proposed multiple-model approach can provide better overall compressionfor large databases; it uses much more memory and converges more slowly than an older …,Data Compression Conference; 2006. DCC 2006. Proceedings,2006,27
Constraints of the PROV data model,James Cheney; Paolo Missier; Luc Moreau; Tom DeNies,*,W3C Recommendation. Available online: http://www. w3. org/TR/2013/REC-prov-constraints-20130430/(accessed on 30 April 2013),2013,26
A core calculus for provenance,Umut A Acar; Amal Ahmed; James Cheney; Roly Perera,Abstract Provenance is an increasing concern due to the ongoing revolution in sharing andprocessing scientific data on the Web and in other computer systems. It is proposed thatmany computer systems will need to become provenance-aware in order to providesatisfactory accountability; reproducibility; and trust for scientific or other high-value data. Todate; there is not a consensus concerning appropriate formal models or security propertiesfor provenance. In previous work; we introduced a formal framework for provenance securityand proposed formal definitions of properties called disclosure and obfuscation.,Journal of Computer Security,2013,26
Equivariant unification,James Cheney,Abstract Nominal logic is a variant of first-order logic with special facilities for reasoningabout names and binding based on the underlying concepts of swapping and freshness. Itserves as the basis of logic programming; term rewriting; and automated theorem provingtechniques that support reasoning about languages with name-binding. These applicationsoften require nominal unification; or equational reasoning and constraint solving in nominallogic. Urban; Pitts and Gabbay developed an algorithm for a broadly applicable class ofnominal unification problems. However; because of nominal logic's equivariance property;these applications also require a different form of unification; which we call equivariantunification. In this article; we first study the complexity of the decision problem for equivariantunification and equivariant matching. We show that these problems are NP-hard in …,Journal of Automated Reasoning,2010,26
A simple nominal type theory,James Cheney,Abstract Nominal logic is an extension of first-order logic with features useful for reasoningabout abstract syntax with bound names. For computational applications such asprogramming and formal reasoning; it is desirable to develop constructive type theories fornominal logic that extend standard type theories for propositional; first-or higher-order logic.This has proven difficult; largely because of complex interactions between nominal logic'sname-abstraction operation and ordinary functional abstraction. This difficulty already arisesin the case of propositional logic and simple type theory. In this paper we show how thisdifficulty can be overcome; and present a simple nominal type theory that enjoys propertiessuch as type soundness and strong normalization; and that can be soundly interpreted usingexisting nominal set models of nominal logic. We also sketch how recursion combinators …,Electronic Notes in Theoretical Computer Science,2009,25
PROV-N: the provenance notation,Luc Moreau; Paolo Missier; James Cheney; Stian Soiland-Reyes,*,W3C Recommendation. Available online: http://www. w3. org/TR/2013/REC-prov-n-20130430/(accessed on 30 April 2013),2012,24
Provenance traces,James Cheney; Umut Acar; Amal Ahmed,Abstract: Provenance is information about the origin; derivation; ownership; or history of anobject. It has recently been studied extensively in scientific databases and other settings dueto its importance in helping scientists judge data validity; quality and integrity. However; mostmodels of provenance have been stated as ad hoc definitions motivated by informalconcepts such as" comes from";" influences";" produces"; or" depends on". These modelslack clear formalizations describing in what sense the definitions capture these intuitiveconcepts. This makes it difficult to compare approaches; evaluate their effectiveness; orargue about their validity. We introduce provenance traces; a general form of provenance forthe nested relational calculus (NRC); a core database query language. Provenance tracescan be thought of as concrete data structures representing the operational semantics …,arXiv preprint arXiv:0812.0564,2008,23
An Empirical Evaluation of Simple DTD-Conscious Compression Techniques.,James Cheney,The term “XML compression” has been used to describe techniques addressing severaldifferent (though related) problems; all relevant to Web data management: 1. minimum-length coding for efficient XML document storage and transmission [13; 5; 10; 1]; 2. compactbinary formats for efficient (streaming) XML message processing and transmission [8; 9]; and3. storage techniques for efficient XML database query processing [11; 17; 3; 2].,WebDB,2005,23
Avoiding equivariance in alpha-prolog,Christian Urban; James Cheney,Abstract α Prolog is a logic programming language which is well-suited for rapid prototypingof type systems and operational semantics of typed λ-calculi and many other languagesinvolving bound names. In α Prolog; the nominal unification algorithm of Urban; Pitts andGabbay is used instead of first-order unification. However; although α Prolog can be viewedas Horn-clause logic programming in Pitts' nominal logic; proof search using nominalunification is incomplete in nominal logic. Because of nominal logic's equivariance principle;complete proof search would require solving NP-hard equivariant unification problems.Nevertheless; the α Prolog programs we studied run correctly without equivariant unification.In this paper; we give several examples of α Prolog programs that do not require equivariantunification; develop a test for identifying such programs; and prove the correctness of this …,Typed Lambda Calculi and Applications,2005,23
The rationale of PROV,Luc Moreau; Paul Groth; James Cheney; Timothy Lebo; Simon Miles,Abstract The prov family of documents are the final output of the World Wide WebConsortium Provenance Working Group; chartered to specify a representation ofprovenance to facilitate its exchange over the Web. This article reflects upon the keyrequirements; guiding principles; and design decisions that influenced the prov family ofdocuments. A broad range of requirements were found; relating to the key conceptsnecessary for describing provenance; such as resources; activities; agents and events; andto balancing prov's ease of use with the facility to check its validity. By this retrospectiverequirement analysis; the article aims to provide some insights into how prov turned out as itdid and why. Benefits of this insight include better inter-operability; a roadmap for alternateinvestigations and improvements; and solid foundations for future standardization …,Web Semantics: Science; Services and Agents on the World Wide Web,2015,22
Towards a repository of bx examples,James Cheney; James McKinna; Perdita Stevens; Jeremy Gibbons,Abstract: We argue for the creation of a curated repository of examples of bidirectionaltransformations (BX). In particular; such a resource may support research on BX; especiallycross-fertilisation between the different communities involved. We have initiated a BXrepository; which is introduced in this paper. We discuss our design decisions and theirrationale; and illustrate them using the now classic\" Composers\" example. We discuss thedifficulties that this undertaking may face; and comment on how they may be overcome.,BX Workshop,2014,21
Mechanized metatheory model-checking,James Cheney; Alberto Momigliano,Abstract The problem of mechanically formalizing and proving metatheoretic properties ofprogramming language calculi; type systems; operational semantics; and related formalsystems has received considerable attention recently. However; the dual problem ofsearching for errors in such formalizations has received comparatively little attention. In thispaper; we consider the problem of bounded model-checking for metatheoretic properties offormal systems specified using nominal logic. In contrast to the current state of the art formetatheory verification; our approach is fully automatic; does not require expertise intheorem proving on the part of the user; and produces counterexamples in the case that aflaw is detected. We present two implementations of this technique; one based on negation-as-failure and one based on negation elimination; along with experimental results …,Proceedings of the 9th ACM SIGPLAN international conference on Principles and practice of declarative programming,2007,21
Equivariant unification,James Cheney,Abstract Nominal logic is a variant of first-order logic with special facilities for reasoningabout names and binding based on the underlying concepts of swapping and freshness. Itserves as the basis of logic programming and term rewriting techniques that provide similaradvantages to; but remain simpler than; higher-order logic programming or term rewritingsystems. Previous work on nominal rewriting and logic programming has relied on nominalunification; that is; unification up to equality in nominal logic. However; because of nominallogic's equivariance property; these applications require a stronger form of unification; whichwe call equivariant unification. Unfortunately; equivariant unification and matching are NP-hard decision problems. This paper presents an algorithm for equivariant unification thatproduces a complete set of finitely many solutions; as well as NP decision procedure and …,International Conference on Rewriting Techniques and Applications,2005,21
System description: Alpha-Prolog; a fresh approach to logic programming modulo alpha-equivalence,James Cheney; Christian Urban,Abstract. αProlog is a prototype logic programming language with a built-in notion of bindersand unification modulo α-equivalence. It is based on a mild extension of first-order Hornformulae: instead of the usual first-order terms and first-order unification; αProlog usesnominal terms and nominal unification introduced in [3]. In this paper; we give threeexamples that demonstrate the advantages of αProlog and describe our currentimplementation.,Proc. 17th Int. Workshop on Unification; UNIF,2003,21
A dependent nominal type theory,James Cheney,Abstract: Nominal abstract syntax is an approach to representing names and bindingpioneered by Gabbay and Pitts. So far nominal techniques have mostly been studied usingclassical logic or model theory; not type theory. Nominal extensions to simple; dependentand ML-like polymorphic languages have been studied; but decidability and normalizationresults have only been established for simple nominal type theories. We present a LF-styledependent type theory extended with name-abstraction types; prove soundness anddecidability of beta-eta-equivalence checking; discuss adequacy and canonical forms via anexample; and discuss extensions such as dependently-typed recursion and inductionprinciples. Subjects: Logic in Computer Science (cs. LO); Programming Languages (cs. PL)ACM classes: F. 4.1 Journal reference: Logical Methods in Computer Science; Volume 8 …,Logical Methods in Computer Science,2012,20
Toward a general theory of names: binding and scope,James Cheney,Abstract High-level formalisms for reasoning about names and binding such as de Bruijnindices; various flavors of higher-order abstract syntax; the Theory of Contexts; and nominalabstract syntax address only one relatively restrictive form of scoping: namely; unary lexicalscoping; in which the scope of a (single) bound name is a subtree of the abstract syntax tree(possibly with other subtrees removed due to shadowing). Many languages exhibit bindingor renaming structure that does not fit this mold. Examples include binding transitions in theπ-calculus; unique identifiers in contexts; memory heaps; and XML documents; declarationscoping in modules and namespaces; anonymous identifiers in automata; type schemes;and Horn clauses; and pattern matching and mutual recursion constructs in functionallanguages. In these cases; it appears necessary to either rearrange the abstract syntax so …,Proceedings of the 3rd ACM SIGPLAN workshop on Mechanized reasoning about languages with variable binding,2005,18
ACCOn: checking consistency of XML write-access control policies,Loreto Bravo; James Cheney; Irini Fundulaki,Abstract XML access control policies involving updates may contain security flaws; herecalled inconsistencies; in which a forbidden operation may be simulated by performing asequence of allowed operations. ACCOn implements i) consistency checking algorithms thatexamine whether a write-access control policy defined over a DTD is inconsistent and ii)repair algorithms that propose repairs to an inconsistent policy to obtain a consistent one.,Proceedings of the 11th international conference on Extending database technology: Advances in database technology,2008,17
A linearly typed assembly language,James Cheney; Greg Morrisett,Today's type-safe low-level languages rely on garbage collection to recycle heap-allocatedobjects safely. We present LTAL; a safe; low-level; yet simple language that``stands on itsown'': it guarantees safe execution within a fixed memory space; without relying on externalrun-time support. We demonstrate the expressiveness of LTAL by giving a type-preservingcompiler for the functional core of ML. But this independence comes at a steep price: LTAL'stype system imposes a draconian discipline of linearity that ensures that memory can bereused safely; but prohibits any useful kind of sharing. We present the results of experimentswith a prototype LTAL system that show just how high the price of linearity can be.,*,2003,17
Query shredding: efficient relational evaluation of queries over nested multisets,James Cheney; Sam Lindley; Philip Wadler,Abstract Nested relational query languages have been explored extensively; and underlieindustrial language-integrated query systems such as Microsoft's LINQ. However; relationaldatabases do not natively support nested collections in query results. This can lead to majorperformance problems: if programmers write queries that yield nested results; then suchsystems typically either fail or generate a large number of queries. We present a newapproach to query shredding; which converts a query returning nested data to a fixednumber of SQL queries. Our approach; in contrast to prior work; handles multiset semantics;and generates an idiomatic SQL: 1999 query directly from a normal form for nested queries.We provide a detailed description of our translation and present experiments showing that itoffers comparable or better performance than a recent alternative approach on a range of …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,15
An analytical survey of provenance sanitization,James Cheney; Roly Perera,Abstract Security is likely to be a critical factor in the future adoption of provenancetechnology; because of the risk of inadvertent disclosure of sensitive information. In thissurvey paper we review the state of the art in secure provenance; considering mechanismsfor controlling access; and the extent to which these mechanisms preserve provenanceintegrity. We examine seven systems or approaches; comparing features and identifyingareas for future work.,International Provenance and Annotation Workshop,2014,14
Principles of Provenance (Dagstuhl Seminar 12091),James Cheney; Anthony Finkelstein; Bertram Ludaescher; Stijn Vansummeren,Abstract This report documents the program and the outcomes of Dagstuhl Seminar12091``Principles of Provenance''. The term``provenance''refers to information about theorigin; context; derivation; ownership or history of some artifact. In both art and science;provenance information is crucial for establishing the value of a real-world artifact;guaranteeing for example that the artifact is an original work produced by an important artist;or that a stated scientific conclusion is reproducible. Since it is much easier to copy or alterdigital information than it is to copy or alter real-world artifacts; the need for tracking andmanagement of provenance information to testify the value and correctness of digitalinformation has been firmly established in the last few years. As a result; provenancetracking and management has been studied in many settings; ranging from databases …,Dagstuhl Reports,2012,14
The Database Wiki Project: A General-Purpose Platform for Data Curation and Collaboration,Peter Buneman; James Cheney; Sam Lindley; Heiko Mueller,Abstract Databases and wikis have complementary strengths and weaknesses for use incollaborative data management and data curation. Relational databases; for example; offeradvantages such as scalability; query optimization and concurrency control; but are not easyto use and lack other features needed for collaboration. Wikis have proved enormouslysuccessful as a means to collaborate because they are easy to use; encourage sharing; andprovide built-in support for archiving; history-tracking and annotation. However; wikis lacksupport for structured data; efficiently querying data at scale; and localized provenance andannotation. To achieve the best of both worlds; we are developing a general-purposeplatform for collaborative data management; called DBWIKI. Our system not only facilitatesthe collaborative creation of structured data; it also provides features not usually provided …,SIGMOD Record,2011,14
Logic column 14: Nominal logic and abstract syntax,James Cheney,Abstract: Formalizing syntactic proofs of properties of logics; programming languages;security protocols; and other formal systems is a significant challenge; in large part becauseof the obligation to handle name-binding correctly. We present an approach called nominalabstract syntax that has attracted considerable interest since its introduction approximatelysix years ago. After an overview of other approaches; we describe nominal abstract syntaxand nominal logic; a logic for reasoning about nominal abstract syntax. We also discussapplications of nominal techniques to programming; automated reasoning; and identifysome future directions.,arXiv preprint cs/0511025,2005,14
DBWiki: a structured wiki for curated data and collaborative data management,Peter Buneman; James Cheney; Sam Lindley; Heiko Müller,Abstract Wikis have proved enormously successful as a means to collaborate in the creationand publication of textual information. At the same time; a large number of curateddatabases have been developed through collaboration for the dissemination of structureddata in specific domains; particularly bioinformatics. We demonstrate a general-purposeplatform for collaborative data management; DBWiki; designed to achieve the best of bothworlds. Our system not only facilitates the collaborative creation of a database; it alsoprovides features not usually provided by database technology such as versioning;provenance tracking; citability; and annotation. In our demonstration we will show howDBWiki makes it easy to create; correct; discuss and query structured data; placing morepower in the hands of users while managing tedious details of data curation automatically.,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,13
Report on the principles of provenance workshop,James Cheney; Peter Buneman; Bertram Ludäscher,Abstract Provenance; or records of the origin; context; custody; derivation or other historicalinformation about a (digital) object; has recently become an important research topic in anumber of areas; particularly databases. However; there has been little interaction betweenresearchers across subdisciplines of computer science working on related problems. Thisarticle reports on a workshop on Principles of Provenance held in Edinburgh; Scotland inNovember 2007; which facilitated interaction among researchers working on provenance indatabases; security; information retrieval; Semantic Web; and software engineering settings;as well as developers and database administrators who are currently working withprovenance in practice; or foresee the need to do so in the near future.,ACM SIGMOD Record,2008,12
Towards a Principle of Least Surprise for Bidirectional Transformations.,James Cheney; Jeremy Gibbons; James McKinna; Perdita Stevens,Abstract In software engineering and elsewhere; it is common for different people to workintensively with different; but related; artefacts; eg models; documents; or code. They mayuse bidirectional transformations (bx) to maintain consistency between them. Naturally; theydo not want their deliberate decisions disrupted; or their comprehension of their artefactinterfered with; by a bx that makes changes to their artefact beyond the strictly necessary.This gives rise to a desire for a principle of Least Change; which has been often alluded to inthe field; but seldom addressed head on. In this paper we present examples; briefly surveywhat has been said about least change in the context of bx; and identify relevant notionsfrom elsewhere that may be applicable. We identify that what is actually needed is aPrinciple of Least Surprise; to limit a bx to reasonable behaviour. We present candidate …,Bx@ STAF,2015,11
Semantics of the PROV data model,James Cheney,Abstract Provenance is information about entities; activities; and people involved inproducing a piece of data or thing; which can be used to form assessments about its quality;reliability or trustworthiness. PROV-DM is the conceptual data model that forms a basis forthe W3C provenance (PROV) family of specifications. This document presents a model-theoretic semantics for the PROV data model; viewing PROV-DM statements as atomicformulas in the sense of first-order logic; and viewing the constraints and inferencesspecified in PROV-CONSTRAINTS as a first-order theory. It is shown that valid PROVinstances (in the sense of PROV-CONSTRAINTS) correspond to satisfiable theories. Thisinformation may be useful to researchers or users of PROV to understand the intendedmeaning and use of PROV for modeling information about the actual history; derivation or …,*,2013,11
Consistency and repair for XML write-access control policies,Loreto Bravo; James Cheney; Irini Fundulaki; Ricardo Segovia,Abstract XML access control policies involving updates may contain security flaws; herecalled inconsistencies; in which a forbidden operation may be simulated by performing asequence of allowed operations. This article investigates the problem of deciding whether apolicy is consistent; and if not; how its inconsistencies can be repaired. We consider totaland partial policies expressed in terms of annotated schemas defining which operations areallowed or denied for the XML trees that are instances of the schema. We show thatconsistency is decidable in PTIME for such policies and that consistent partial policies canbe extended to unique least-privilege consistent total policies. We also consider repairproblems based on deleting privileges to restore consistency; show that finding minimalrepairs is NP-complete; and give heuristics for finding repairs. Finally; we experimentally …,The VLDB Journal,2012,11
Provenance; XML and the scientific web,James Cheney,Abstract Science is now being revolutionized by the capabilities of distributing computationand human effort over the World Wide Web. This revolution offers dramatic benefits but alsoposes serious risks due to the fluid nature of digital information. The Web today does notprovide adequate repeatability; reliability; accountability and trust guarantees for scientificapplications. One important part of this problem is tracking and managing provenanceinformation; or metadata about sources; authorship; derivation; or other historical aspects ofdata. In this paper; we discuss motivating examples for provenance in science and computersecurity; introduce four models of provenance for XML queries; and discuss their propertiesand prospects for further research.,ACM SIGPLAN Workshop on Programming Language Technology and XML (PLAN-X 2009),2009,11
Repairing inconsistent XML write-access control policies,Loreto Bravo; James Cheney; Irini Fundulaki,Abstract XML access control policies involving updates may contain security flaws; herecalled inconsistencies; in which a forbidden operation may be simulated by performing asequence of allowed operations. This paper investigates the problem of deciding whether apolicy is consistent; and if not; how its inconsistencies can be repaired. We consider policiesexpressed in terms of annotated DTDs defining which operations are allowed or denied forthe XML trees that are instances of the DTD. We show that consistency is decidable in ptimefor such policies and that consistent partial policies can be extended to unique “least-privilege” consistent total policies. We also consider repair problems based on deletingprivileges to restore consistency; show that finding minimal repairs is np-complete; and giveheuristics for finding repairs.,International Symposium on Database Programming Languages,2007,11
Lux: A Lightweight; Statically Typed XML Update Language.,James Cheney,ABSTRACT Several proposals for updating XML have been introduced. Many of them havea rather complicated semantics due to the interaction of side-effects and updates; and someproposals also complicate the semantics of XQuery because arbitrary side-effecting updatestatements are allowed inside queries. Moreover; static typechecking has not been studiedfor any proposed XML update language. In this paper; we survey prior work on XML updatelanguages and motivate an alternative approach to updating XML. We introduce an updatelanguage called Lux; which stands for Lightweight Updates for XML. Lux can performrelational database-style updates; has a simple; deterministic operational semantics; andhas a sound static type system based on regular expression types and structural subtyping.,PLAN-X,2007,11
Statistical Models for Term Compression.,James Cheney,Abstract Symbolic tree data structures; or terms; are used in many computing systems.Although terms can be compressed by hand; using specialized algorithms; or usinguniversal compression utilities; all of these approaches have drawbacks. We propose anapproach which avoids these problems by using knowledge of term structure to obtain moreaccurate predictive models for term compression. We describe two models that predict childsymbols based on their parents and locations. Our experiments compared these modelswith first-order Markov sequence models using Huffman coding and found that one modelcan obtain 20% better compression in similar time; and the other; simpler model can obtainsimilar compression 40% faster. These compression models also approach; but do notexceed; the performance of gzip.,Data Compression Conference,2000,11
Notions of bidirectional computation and entangled state monads,Faris Abou-Saleh; James Cheney; Jeremy Gibbons; James McKinna; Perdita Stevens,Abstract Bidirectional transformations (bx) support principled consistency maintenancebetween data sources. Each data source corresponds to one perspective on a compositesystem; manifested by operations to 'get'and 'set'a view of the whole from that particularperspective. Bx are important in a wide range of settings; including databases; interactiveapplications; and model-driven development. We show that bx are naturally modelled interms of mutable state; in particular; the 'set'operations are stateful functions. This leadsnaturally to considering bx that exploit other computational effects too; such as I/O;nondeterminism; and failure; all largely ignored in the bx literature to date. We present asemantic foundation for symmetric bidirectional transformations with effects. We build on themature theory of monadic encapsulation of effects in functional programming; develop the …,International Conference on Mathematics of Program Construction,2015,10
Lenses for web data,Raghu Rajkumar; Nate Foster; Sam Lindley; James Cheney,Abstract Putting data on the web typically involves implementing two transformations: one toconvert the data into HTML; and another to parse modifications out of interactions withclients. Unfortunately; in current systems; these transformations are usually implementedusing two separate functions—an approach that replicates functionality across multiplepieces of code; and makes programs difficult to write; reason about; and maintain. Thispaper presents a different approach: an abstraction based on formlets that makes it easy tobridge the gap between data stored on a server and values embedded in HTML forms. Weintroduce formlenses; which combine the advantages of formlets with those of lenses toprovide compositional; bidirectional form-based views of Web data. We show thatformlenses can be viewed as monoidal functors over lenses; analogously to formlets …,Electronic Communications of the EASST,2014,9
Database queries that explain their work,James Cheney; Amal Ahmed; Umut A Acar,Abstract Provenance for database queries or scientific workflows is often motivated asproviding explanation; increasing understanding of the underlying data sources andprocesses used to compute the query; and reproducibility; the capability to recompute theresults on different inputs; possibly specialized to a part of the output. Many provenancesystems claim to provide such capabilities; however; most lack formal definitions orguarantees of these properties; while others provide formal guarantees only for relativelylimited classes of changes. Building on recent work on provenance traces and slicing forfunctional programming languages; we introduce a detailed tracing model of provenance formultiset-valued Nested Relational Calculus; define trace slicing algorithms that extractsubtraces needed to explain or recompute specific parts of the output; and define query …,Proceedings of the 16th International Symposium on Principles and Practice of Declarative Programming,2014,8
Provenance for seismological processing pipelines in a distributed streaming workflow,Alessandro Spinuso; James Cheney; Malcolm Atkinson,Abstract Harvesting provenance for streaming workflows presents challenges related to thehigh rate of the updates and a large distribution of the execution; which can be spreadacross several institutional infrastructures. Moreover; the typically large volume of dataproduced by each transformation step can not be always stored and preserved efficiently.This can represent an obstacle for the evaluation of the results; for instance; in real-time;suggesting the importance of customisable metadata extraction procedures. In this paper wepresent our approach to the aforementioned provenance challenges within a use-casedriven scenario in the field of seismology; which requires the execution of processingpipelines over a large datastream. In particular; we will discuss the current implementationand the upcoming challenges for an in-worfklow programmatic approach to provenance …,Proceedings of the Joint EDBT/ICDT 2013 Workshops,2013,8
Hierarchical Models of Provenance.,Peter Buneman; James Cheney; Egor V Kostylev,Abstract There is general agreement that we need to understand provenance at variouslevels of granularity; however; there appears; as yet; to be no general agreement on whatgranularity means. It can refer both to the detail with which we can view a process or thedetail with which we view the data. We describe a simple and straightforward method forimposing a hierarchical structure on a provenance graph and show how it can; if we want;be derived from the program whose execution created that graph.,TaPP,2012,8
Regular expression subtyping for XML query and update languages,James Cheney,Abstract XML database query languages such as XQuery employ regular expression typeswith structural subtyping. Subtyping systems typically have two presentations; which shouldbe equivalent: a declarative version in which the subsumption rule may be used anywhere;and an algorithmic version in which the use of subsumption is limited in order to maketypechecking syntax-directed and decidable. However; the XQuery standard type systemcircumvents this issue by using imprecise typing rules for iteration constructs and definingonly algorithmic typechecking; and another extant proposal provides more precise types foriteration constructs but ignores subtyping. In this paper; we consider a core XQuery-likelanguage with a subsumption rule and prove the completeness of algorithmic typechecking;this is straightforward for XQuery proper but requires some care in the presence of more …,European Symposium on Programming,2008,8
Formal type soundness for Cyclone's region system,Dan Grossman; Greg Morrisett; Trevor Jim; Mike Hicks; Yanling Wang; James Cheney,Cyclone is a polymorphic; type-safe programming language derived from C\@. The primarydesign goals of Cyclone are to let programmers control data representations and memorymanagement without sacrificing type-safety. In this paper; we focus on the region-basedmemory management of Cyclone and its static typing discipline. The design incorporatesseveral advancements; including support for region subtyping and a coherent integrationwith stack allocation and a garbage collector. To support separate compilation; Cyclonerequires programmers to write some explicit region annotations; but uses a combination ofdefault annotations; local type inference; and a novel treatment of region effects to reducethis burden. As a result; we integrate C idioms in a region-based framework. In ourexperience; porting legacy C to Cyclone has required altering about 8\% of the code; of …,*,2001,8
Toward Provenance-Based Security for Configuration Languages.,Paul Anderson; James Cheney,Abstract Large system installations are increasingly configured using high-level; mostly-declarative languages. Often; different users contribute data that is compiled centrally anddistributed to individual systems. Although the systems themselves have been developedwith reliability and availability in mind; the configuration compilation process can lead tounforeseen vulnerabilities because of the lack of access control on the different componentscombined to build the final configuration. Even if simple change-based access controls areapplied to validate changes to the final version; changes can be lost or incorrectly attributed.Based on the growing literature on provenance for database queries and other models ofcomputation; we identify a potential application area for provenance to securingconfiguration languages.,TaPP,2012,7
Mechanizing the Metatheory of mini-XQuery,James Cheney; Christian Urban,Abstract We present a Nominal Isabelle formalization of an expressive core fragment ofXQuery; a W3C standard functional language for querying XML documents. Ourformalization focuses on results presented in the literature concerning XQuery's operationalsemantics; typechecking; and optimizations. Our core language; called mini-XQuery; omitsmany complications of XQuery such as ancestor and sibling axes; recursive types andfunctions; node identity; and unordered processing modes; but does handle distinctivefeatures of XQuery including monadic comprehensions; downward XPath steps and regularexpression types. To our knowledge no language with similar features has beenmechanically formalized previously. Our formalization is a first step towards a completeformalization of full XQuery.,Certified Programs and Proofs,2011,7
Workshop on theory and practice of provenance event report,James Cheney,Abstract Provenance; or metadata about the creation; influences upon; or other history ofobjects or data; has attracted attention in a wide variety of contexts in computer science overthe last few years. This event report describes a recent workshop on" Theory and Practice ofProvenance"; intended as a forum for presenting novel ideas about provenance andencouraging interaction among provenance researchers.,ACM SIGMOD Record,2009,7
Estimating the distribution and propagation of genetic programming building blocks through tree compression,Robert I McKay; Xuan Hoai Nguyen; James R Cheney; MinHyeok Kim; Naoki Mori; Tuan Hao Hoang,Abstract Shin et al [19] and McKay et al [15] previously applied tree compression andsemantics-based simplification to study the distribution of building blocks in evolving GeneticProgramming populations. However their method could only give static estimates of thedegree of repetition of building blocks in one generation at a time; supplying no informationabout the flow of building blocks between generations. Here; we use a state-of-the-art treecompression algorithm; xmlppm; to estimate the extent to which frequent building blocksfrom one generation are still in use in a later generation. While they compared the behaviourof different GP algorithms on one specific problem--a simple symbolic regression problem--we extend the analysis to a more complex problem; a symbolic regression problem to find aFourier approximation to a sawtooth wave; and to a Boolean domain; odd parity.,Proceedings of the 11th Annual conference on Genetic and evolutionary computation,2009,6
First-order term compression: techniques and applications,JAMES R Cheney,Abstract. Lossless sequential data compression techniques are well-known. Suchtechniques work best on data with repetition in its sequential representation. One might wishto compress data that can possess non-sequential repetitive structure; for example; first-order terms. Sequential techniques often do not compress such data well. This paperpresents improved compression techniques that can take advantage of repetitive termstructure. The first part presents a formal background for the problem of compressing terms.The second part outlines several approaches to compressing terms by pattern substitution;using a modified form of substitution tree indexing. The third part shows how termcompression can be used to compress proofs in a system for mobile code safety andpresents experimental results establishing the effectiveness of term compression.,*,1998,6
Advances in Property-Based Testing for ɑProlog,James Cheney; Alberto Momigliano; Matteo Pessina,*,International Conference on Tests and Proofs,2016,5
Entangled state monads,James Cheney; James McKinna; Perdita Stevens; Jeremy Gibbons; Faris Abou,Abstract: We present a monadic treatment of symmetric state-based bidirectionaltransformations; and show how it arises naturally from the well-known asymmetric lens-based account. We introduce two presentations of a concept we dub the\" entangled\" statemonad; and prove their equivalence. As a step towards a unifying account of bidirectionalityin general; we exhibit existing classes of state-based approaches from the literature asinstances of our new constructions. This extended abstract reports on work in progress.,BX Workshop,2014,5
Project report–theorem prover usability,James Cheney,*,*,2001,5
µPuppet: A Declarative Subset of the Puppet Configuration Language,Weili Fu; Roly Perera; Paul Anderson; James Cheney,Abstract Puppet is a popular declarative framework for specifying and managing complexsystem configurations. The Puppet framework includes a domain-specific language withseveral advanced features inspired by object-oriented programming; including user-definedresource types;'classes' with a form of inheritance; and dependency management. Like mostreal-world languages; the language has evolved in an ad hoc fashion; resulting in a designwith numerous features; some of which are complex; hard to understand; and difficult to usecorrectly. We present an operational semantics for muPuppet; a representative subset of thePuppet language that covers the distinctive features of Puppet; while excluding features thatare either deprecated or work-in-progress. Formalising the semantics sheds light on difficultparts of the language; identifies opportunities for future improvements; and provides a …,ECOOP,2017,4
Reflections on monadic lenses,Faris Abou-Saleh; James Cheney; Jeremy Gibbons; James McKinna; Perdita Stevens,Abstract Bidirectional transformations (bx) have primarily been modeled as pure functions;and do not account for the possibility of the side-effects that are available in mostprogramming languages. Recently several formulations of bx that use monads to account foreffects have been proposed; both among practitioners and in academic research. Thecombination of bx with effects turns out to be surprisingly subtle; leading to problems withsome of these proposals and increasing the complexity of others. This paper reviews theproposals for monadic lenses to date; and offers some improved definitions; payingparticular attention to the obstacles to naively adding monadic effects to existing definitionsof pure bx such as lenses and symmetric lenses; and the subtleties of equivalence ofsymmetric bidirectional transformations in the presence of effects.,*,2016,4
Effective quotation: relating approaches to language-integrated query,James Cheney; Sam Lindley; Gabriel Radanne; Philip Wadler,Abstract Language-integrated query techniques have been explored in a number of differentlanguage designs. We consider two different; type-safe approaches employed by Links andF#. Both approaches provide rich dynamic query generation capabilities; and thus amount toa form of heterogeneous staged computation; but to date there has been no formalinvestigation of their relative expressiveness. We present two core calculi Eff and Quot;respectively capturing the essential aspects of language-integrated querying using effects inLinks and quotation in LINQ. We show via translations from Eff to Quot and back that the twoapproaches are equivalent in expressiveness. Based on the translation from Eff to Quot; weextend a simple Links compiler to handle queries.,Proceedings of the ACM SIGPLAN 2014 Workshop on Partial Evaluation and Program Manipulation,2014,4
Is provenance logical?,James Cheney,Abstract Research on provenance in databases (or other settings) sometimes has anarbitrary flavor. Once we abandon the classical semantics of queries there is a large designspace for alternative semantics that could provide some useful provenance information; butthere is little guidance for how to explore this space or justify or compare different proposals.Topics from mathematical or philosophical logic could be used as a way of inspiring;justifying or comparing different approaches to provenance in databases. This paper andinvited talk will present several topics in logic that may be less familiar to databaseresearchers and that could bear upon provenance techniques. These areas includenonclassical logics (eg relevance logic); algebraic logic (cylindric algebras); substructurallogic (eg linear logic) and logics of knowledge; belief or causality.,Proceedings of the 4th International Workshop on Logic in Databases,2011,4
Language-integrated provenance,Stefan Fehrenbach; James Cheney,Abstract Provenance; or information about the origin or derivation of data; is important forassessing the trustworthiness of data and identifying and correcting mistakes. Most priorimplementations of data provenance have involved heavyweight modifications to databasesystems and little attention has been paid to how the provenance data can be used outsidesuch a system. We present extensions to the Links programming language that build on itssupport for language-integrated query to support provenance queries by rewriting andnormalizing monadic comprehensions and extending the type system to distinguishprovenance metadata from normal data. The main contribution of this article is to show thatthe two most common forms of provenance can be implemented efficiently and used safelyas a programming language feature with no changes to the database system.,Science of Computer Programming,2017,3
αCheck: A mechanized metatheory model-checker,James Cheney; Alberto Momigliano,Abstract The problem of mechanically formalizing and proving metatheoretic properties ofprogramming language calculi; type systems; operational semantics; and related formalsystems has received considerable attention recently. However; the dual problem ofsearching for errors in such formalizations has attracted comparatively little attention. In thisarticle; we present αCheck; a bounded model checker for metatheoretic properties of formalsystems specified using nominal logic. In contrast to the current state of the art formetatheory verification; our approach is fully automatic; does not require expertise intheorem proving on the part of the user; and produces counterexamples in the case that aflaw is detected. We present two implementations of this technique; one based on negation-as-failure and one based on negation elimination; along with experimental results …,arXiv preprint arXiv:1704.00617,2017,3
On principles of Least Change and Least Surprise for bidirectional transformations,James Cheney; Jeremy Gibbons; James McKinna; Perdita Stevens,Abstract In software engineering and elsewhere; different people may work intensively withdifferent; but related; artefacts; eg models; documents; or code. They may use bidirectionaltransformations (bx) to maintain consistency between them. Naturally; they do not want theirdeliberate decisions disrupted; or their comprehension of their artefact interfered with; by abx that makes changes to their artefact beyond the strictly necessary. This gives rise to adesire for a principle of Least Change; which has been often alluded to in the field; butseldom addressed head on. In this paper we present examples; briefly survey what hasbeen said about least change in the context of bx; and identify relevant notions fromelsewhere that may be applicable. We conclude that we cannot expect a Principle of LeastChange to determine the optimal behaviour of a bx based on the consistency relation it …,Journal of Object Technology,2017,3
Prov-n: The provenance notation,James Cheney; Stian Soiland-Reyes,Provenance is information about entities; activities; and people involved in producing apiece of data or thing; which can be used to form assessments about its quality; reliability ortrustworthiness. PROV-DM is the conceptual data model that forms a basis for the W3Cprovenance (PROV) family of specifications. PROV-DM distinguishes core structures;forming the essence of provenance information; from extended structures catering for morespecific uses of provenance. PROV-DM is organized in six components; respectively dealingwith:(1) entities and activities; and the time at which they were created; used; or ended;(2)derivations of entities from entities;(3) agents bearing responsibility for entities that weregenerated and activities that happened;(4) a notion of bundle; a mechanism to supportprovenance of provenance; and;(5) properties to link entities that refer to the same thing; …,*,2013,3
Toward a theory of self-explaining computation,James Cheney; Umut A Acar; Roly Perera,Abstract Provenance techniques aim to increase the reliability of human judgments aboutdata by making its origin and derivation process explicit. Originally motivated by the needs ofscientific databases and scientific computation; provenance has also become a major issuefor business and government data on the Web. However; so far provenance has beenstudied only in relatively restrictive settings: typically; for data stored in databases orscientific workflow systems; and processed by query or workflow languages of limitedexpressiveness. Long-term provenance solutions require an understanding of provenancein other settings; particularly the general-purpose programming or scripting languages thatare used to glue different components such as databases; Web services and workflowstogether. Moreover; what is required is not only an account of mechanisms for recording …,*,2013,3
Satisfiability algorithms for conjunctive queries over trees,James Cheney,Abstract We investigate the satisfiability problem for conjunctions of constraints overordered; unranked trees; including child; descendant; following-sibling; root; leaf; andfirst/last child constraints. We introduce new; symbolic approaches based on graphtransformations; which simplify and check the consistency of a problem first; and delay blindsearch as long as possible. We prove correctness and termination for these algorithms. Wealso analyze the complexity of important special cases: binary and κ-ary intersection ofcertain classes of XPath expressions. Our main complexity result is that binary intersection(for positive; simple navigational XPath over all axes) is tractable for expressions with abounded number of changes in direction in the path; which is typically small.,Proceedings of the 14th International Conference on Database Theory,2011,3
A copy-and-paste model for provenance in curated databases,Peter Buneman; James Cheney,Abstract Provenance is information describing the origin; construction; location; ownership;or other aspects of the history of an object. Previous work on provenance has concentratedon an understanding of how provenance is described when the data of interest has beenderived by queries from other data sources; as is the case in data warehouses. In this paperwe focus on another important class of databases; curated databases. These are databasesthat are constructed and maintained with a great deal of human effort by manuallytransferring data from other sources such as other databases; web pages and files; or byinserting comments or transcriptions of (paper) literature. We exploit a data model for datastored in deterministic trees that provides a simple notion of location of data. From this whichwe can characterize dataflow provenance (information about where data comes from) …,Notes,2005,3
Category theory for dummies (I),James Cheney,Page 1. Category Theory for Dummies (I) James Cheney Programming Languages DiscussionGroup March 12; 2004 1 Page 2. Not quite everything you've ever wanted to know... • Youkeep hearing about category theory. • Cool-sounding papers by brilliant researchers (egWadler's “Theorems for free!”) • But it's scary and incomprehensible. • And Category Theoryis not even taught here. • Goal of this series: Familarity with basic ideas; not expertise 2 Page3. Outline • Categories: Why are they interesting? • Categories: What are they? Examples. •Some familiar properties expressed categorically • Some basic categorical constructions 3Page 4. Category theory • An abstract theory of “structured things” and “structure preservingfunction-like things”. • Independent of the concrete representation of the things and functions. •An alternative foundation for mathematics? (Lawvere) …,Programming Languages Discussion Group,2004,3
Toward a theory of information preservation,James Cheney; Carl Lagoze; Peter Botticelli,Digital preservation is a pressing challenge to the library community. In this paper; wedescribe the initial results of our efforts towards understanding digital (as well as traditional)preservation problems from first principles. Our approach is to use the language ofmathematics to formalize the concepts that are relevant to preservation. Our theory of_preservation spaces_ draws upon ideas from logic and programming language semanticsto describe the relationship between concrete objects and their information contents. Wealso draw on game theory to show how objects change over time as a result ofuncontrollable environment effects and directed preservation actions. In the second half ofthis paper; we show how to use the mathematics of universal algebra as a language forobjects whose information content depends on many components. We use this language …,*,2001,3
Imperative functional programs that explain their work,Wilmer Ricciotti; Jan Stolarek; Roly Perera; James Cheney,Abstract Program slicing provides explanations that illustrate how program outputs wereproduced from inputs. We build on an approach introduced in prior work; where dynamicslicing was defined for pure higher-order functional programs as a Galois connectionbetween lattices of partial inputs and partial outputs. We extend this approach to imperativefunctional programs that combine higher-order programming with references andexceptions. We present proofs of correctness and optimality of our approach and a proof-of-concept implementation and experimental evaluation.,Proceedings of the ACM on Programming Languages,2017,2
Strongly Normalizing Audited Computation,Wilmer Ricciotti; James Cheney,Abstract: Auditing is an increasingly important operation for computer programming; forexample in security (eg to enable history-based access control) and to enable reproducibilityand accountability (eg provenance in scientific programming). Most proposed auditingtechniques are ad hoc or treat auditing as a second-class; extralinguistic operation; logicalor semantic foundations for auditing are not yet well-established. Justification Logic (JL)offers one such foundation; Bavera and Bonelli introduced a computational interpretation ofJL called $\lambda^ h $ that supports auditing. However; $\lambda^ h $ is technicallycomplex and strong normalization was only established for special cases. In addition; weshow that the equational theory of $\lambda^ h $ is inconsistent. We introduce a newcalculus $\lambda^{hc} $ that is simpler than $\lambda^ h $; consistent; and strongly …,arXiv preprint arXiv:1706.03711,2017,2
Causally consistent dynamic slicing,Roly Perera; Deepak Garg; James Cheney,Abstract: We offer a lattice-theoretic account of dynamic slicing for {\pi}-calculus; building onprior work in the sequential setting. For any run of a concurrent program; we exhibit a Galoisconnection relating forward slices of the start configuration to backward slices of the endconfiguration. We prove that; up to lattice isomorphism; the same Galois connection arisesfor any causally equivalent execution; allowing an efficient concurrent implementation ofslicing via a standard interleaving semantics. Our approach has been formalised in thedependently-typed language Agda.,arXiv preprint arXiv:1610.02327,2016,2
PROV-O: The PROV ontology: W3C recommendation 30 April 2013,Khalid Belhajjame; James Cheney; David Corsar; Daniel Garijo; Stian Soiland-Reyes; Stephan Zednik; Jun Zhao,*,*,2013,2
Formalizing adequacy: a case study for higher-order abstract syntax,James Cheney; Michael Norrish; René Vestergaard,Abstract Adequacy is an important criterion for judging whether a formalization is suitable forreasoning about the actual object of study. The issue is particularly subtle in the expansivecase of approaches to languages with name-binding. In prior work; adequacy has beenformalized only with respect to specific representation techniques. In this article; we give ageneral formal definition based on model-theoretic isomorphisms or interpretations. Weinvestigate and formalize an adequate interpretation of untyped lambda-calculus within ahigher-order metalanguage in Isabelle/HOL using the Nominal Datatype Package.Formalization elucidates some subtle issues that have been neglected in informalarguments concerning adequacy.,Journal of Automated Reasoning,2012,2
Resource bound analysis for database queries,James Cheney; Morten Dahl,Abstract Many scientific disciplines; such as biology and astronomy; are now collecting largeamounts of raw data systematically and storing it centrally in relational databases for shareduse by their communities. In many cases such systems accept arbitrary SQL queriessubmitted using a Web form. Typical database management systems do not provide supportfor enforcing resource access control policies to guard against expensive or pointlessqueries submitted accidentally by legitimate users or intentionally by attackers. Instead; suchsystems typically employ timeouts to halt queries that do not terminate within a reasonableperiod of time. This approach can limit misuse but cannot prevent it; moreover; it does notprovide useful feedback for legitimate users whose queries exceed the time limit. In thispaper; we study a language-based technique for bounding the time and space resource …,Proceedings of the third ACM SIGPLAN workshop on Programming languages and analysis for security,2008,2
The semantics of nominal logic programs,James Cheney,Abstract Nominal logic programming is a form of logic programming with “concrete” namesand binding; based on nominal logic; a theory of α-equivalence founded on swapping andfreshness constraints. Previous papers have employed diverse characterizations of thesemantics of nominal logic programs; including operational; denotational; and proof-theoretic characterizations; however; the formal properties and relationships among themhave not been fully investigated. In this paper we give a uniform and improved presentationof these characterizations and prove appropriate soundness and completeness results. Wealso give some applications of these results.,International Conference on Logic Programming,2006,2
Poor man's generics and dynamics,James Cheney; Ralf Hinze,*,Haskell Workshop 2002,2002,2
Guest editorial: The provenance of online data,Adriane Chapman; James Cheney; Simon Miles,Across many domains; there is a need to trace how data has been created; manipulated;and disseminated. This has led to strong recent interest in technology for modelling andreasoning about provenance. Provenance is information about the entities; activities; andpeople involved in producing a piece of data or thing; which can be used to formassessments about its quality; reliability; or trustworthiness. It is itself data; commonlyrepresented as a directed acyclic graph linking these elements (entities; activities; andagents) to the earlier elements that influenced them. Provenance is becoming a key Internettechnology; and the World Wide Web Consortium has standardised PROV as arepresentation for exchanging provenance on the (Semantic) Web. It is also important in anumber of other settings to address the problems that arise in a distributed …,ACM Transactions on Internet Technology (TOIT),2017,1
Expressiveness Benchmarking for System-Level Provenance,Sheung Chi Chan; Ashish Gehani; James Cheney; Ripduman Sohan; Hassaan Irshad,Abstract Provenance is increasingly being used as a foundation for security analysis andforensics. System-level provenance can help us trace activities at the level of libraries orsystem calls; which offers great potential for detecting subtle malicious activities that canotherwise go undetected. However; analysing the raw provenance trace is challenging; dueto scale and to differences in data representation among system-level provenancerecorders: for example; common queries to identify malicious patterns need to be formulatedin different ways on different systems. As a first step toward understanding the similaritiesand differences among approaches; this paper proposes an expressiveness benchmarkconsisting of tests intended to capture the provenance of individual system calls. We presentwork in progress on the benchmark examples for Linux and discuss how they are …,TaPP 2017,2017,1
Provenance segmentation,Rui Abreu; Dave Archer; Erin Chapman; James Cheney; Hoda Eldardiry; Adria Gascón,Abstract Using pervasive provenance to secure mainstream systems has recently attractedinterest from industry and government. Recording; storing and managing all of theprovenance associated with a system is a considerable challenge. Analyzing the resultingnoisy; heterogeneous; continuously-growing provenance graph adds to this challenge; andapparently necessitates segmentation; that is; approximating; compressing or summarizingpart or all of the graph in order to identify patterns or features. In this paper; we describe thisnew problem space for provenance data management; contrast it with related problemspaces addressed by prior work on provenance abstraction and sanitization; and highlightchallenges and future directions toward solutions to the provenance segmentation problem.,Proceedings of the 8th USENIX Conference on Theory and Practice of Provenance,2016,1
Proof-relevant pi-calculus,Roly Perera; James Cheney,Abstract: We present a formalisation in Agda of the theory of concurrent transitions;residuation; and causal equivalence of traces for the pi-calculus. Our formalisation employsde Bruijn indices and dependently-typed syntax; and aligns the" proved transitions"proposed by Boudol and Castellani in the context of CCS with the proof terms naturallypresent in Agda's representation of the labelled transition relation. Our main contributionsare proofs of the" diamond lemma" for the residuals of concurrent transitions and a formaldefinition of equivalence of traces up to permutation of transitions. In the pi-calculustransitions represent propagating binders whenever their actions involve bound names. Toaccommodate these cases; we require a more general diamond lemma where the targetstates of equivalent traces are no longer identical; but are related by a braiding that …,arXiv preprint arXiv:1604.04575,2016,1
Language-integrated provenance in Links,Stefan Fehrenbach; James Cheney,Abstract Today's programming languages provide no support for data provenance. In aworld that increasingly relies on data; we need provenance to judge the reliability of dataand therefore should aim for making it easily accessible to programmers. We report our workin progress on an extension to the Links programming language that builds on its support forlanguage-integrated query to support where-provenance queries through query rewritingand a type system extension that distinguishes provenance metadata from other data. Ourapproach aims to work solely within the language implementation and thus require nochanges to the database system. The type system together with automatic propagation ofprovenance metadata will prevent programmers from accidentally changing provenance;losing it; or misattributing it to other data.,TaPP Workshop,2015,1
Bidirectionality; traceability and provenance,James Cheney,Search. The University of British Columbia. UBC - A Place of Mind. The University of BritishColumbia. UBC Search UBC Search. Library. Library Home; Search Collections: Search;General (Summon); Books & Media (Catalogue); Indexes; Databases & Articles; Journals;Research Guides; UBC Research; UBC Open Collections. Hours & Locations: UBC VancouverCampus; Asian Library; Chapman Learning Commons Help Desk; David Lam Library; EducationLibrary; Irving K. Barber Learning Centre; Koerner Library; Law Library; Music; Art andArchitecture Library; Rare Books and Special Collections; University Archives; WoodwardLibrary; Xwi7xwa Library. UBC Vancouver Off-Campus; Biomedical Branch Library; UBCOkanagan Campus; Innovation Library; Okanagan Library. Use The Library: Borrowing Services;My Library Account; How to Get a Library Card; See More …,*,2013,1
Semantics and provenance for processing element composition in dispel workflows,Eric Griffis; Paul Martin; James Cheney,Abstract Dispel is a scripting language for constructing workflow graph which can then beexecuted by some other computational infrastructure. It facilitates construction of abstractcomponents (called Processing Elements; or PEs) that can be instantiated in different waysto produce a concrete; executable workflow. In this paper; we present a formal semantics forDispel that explains its key features; particularly definition and use of composite PEs. Wealso develop an alternative semantics of Dispel programs that constructs a workflowenriched with PEs that can record provenance for the original workflow. The semantics iswork in progress that will inform future development of Dispel and of provenancemanagement techniques for Dispel.,Proceedings of the 8th Workshop on Workflows in Support of Large-Scale Science,2013,1
Static Enforceability of XPath-Based Access Control Policies,James Cheney,Abstract: We consider the problem of extending XML databases with fine-grained; high-levelaccess control policies specified using XPath expressions. Most prior work checks individualupdates dynamically; which is expensive (requiring worst-case execution time proportionalto the size of the database). On the other hand; static enforcement can be performed withoutaccessing the database but may be incomplete; in the sense that it may forbid accesses thatdynamic enforcement would allow. We introduce topological characterizations of XPathfragments in order to study the problem of determining when an access control policy can beenforced statically without loss of precision. We introduce the notion of fair policies that arestatically enforceable; and study the complexity of determining fairness and of staticenforcement itself.,arXiv preprint arXiv:1308.0502,2013,1
Prov-dictionary: Modeling provenance for dictionary data structures,Paolo Missier; Luc Moreau; James Cheney; Timothy Lebo; Stian Soiland-Reyes,Provenance is information about entities; activities; and people involved in producing apiece of data or thing; which can be used to form assessments about its quality; reliability ortrustworthiness. This document describes extensions to PROV to facilitate the modeling ofprovenance for dictionary data structures.[PROV-DM] specifies a Collection as an entity thatprovides a structure to some constituents; which are themselves entities. However; someapplications may need a mechanism to specify more structure to a Collection; in order toaccurately describe its provenance. Therefore; in this document; we introduce Dictionary; aspecific type of Collection with a logical structure consisting of key-entity pairs.,*,2013,1
Using Links to prototype a Database Wiki.,James Cheney; Sam Lindley; Heiko Müller,ABSTRACT Both relational databases and wikis have strengths that make them attractive foruse in collaborative applications. In the last decade; database-backed Web applicationshave been used extensively to develop valuable shared biological references called curateddatabases. Databases offer many advantages such as scalability; query optimization andconcurrency control; but are not easy to use and lack other features needed forcollaboration. Wikis have become very popular for early-stage biocuration projects becausethey are easy to use; encourage sharing and collaboration; and provide built-in support forarchiving; history-tracking and annotation. However; curation projects often outgrow thelimited capabilities of wikis for structuring and efficiently querying data at scale; necessitatinga painful phase transition to a database-backed Web application. We perceive a need for …,DBPL,2011,1
A metaprogramming approach to data provenance,James Cheney,Abstract Provenance is information about the history; origin; or derivation of a piece of datain a database. It is useful for debugging; error correction; and data integrity; especially inscientific databases. For automatic provenance tracking techniques to behave consistentlyand reliably; it is important to prove their correctness. While semantic correctness criteria forprovenance have been proposed for simple query languages; these criteria have beendifficult to adapt to realistic database query languages including features such as grouping;negation; and aggregation. We take the view that correct provenance must show how thedata depends on the input and how the output data was computed from the input. Moreover;since provenance should be userreadable; we believe that it is expedient to use theexpressions of the query language itself as provenance. Thus; in our view; provenance …,*,2000,1
Dr. Formlens; Or: How I Learned to Stop Worrying and Love Monoidal Functors,Raghu Rajkumar; Nate Foster; Sam Lindley; James Cheney,Abstract To make data available on the Web; one must typically implement two components:one to convert the data into HTML; and another to parse updates out of client responses. Incurrent systems; these components are usually implemented using separate functions—anapproach that replicates functionality across multiple pieces of code; making programsdifficult to write; reason about; and maintain. This paper presents formlenses; a newabstraction based on formlets that makes it easy to bridge the gap between data stored on aserver and values embedded into an HTML form. We present a new foundation for formletsbased on monoidal functors (replacing the classic definition based on applicative functors);and show how to endow the resulting structures with a bidirectional semantics. Weinvestigate the connection between linearity and bidirectional transformations and …,*,*,1
Proof-relevant π-calculus: a constructive account of concurrency and causality,Roly Perera; James Cheney,Abstract We present a formalisation in Agda of the theory of concurrent transitions;residuation and causal equivalence of traces for the π-calculus. Our formalisation employsde Bruijn indices and dependently typed syntax; and aligns the 'proved transitions' proposedby Boudol and Castellani in the context of CCS with the proof terms naturally present inAgda's representation of the labelled transition relation. Our main contributions are proofs ofthe 'diamond lemma'for the residuals of concurrent transitions and a formal definition ofequivalence of traces up to permutation of transitions.,Mathematical Structures in Computer Science,2017,*
Causally Consistent Dynamic Slicing,Deepak Garg; James Cheney; Roly Perera,*,*,2016,*
Programming Languages for Big Data (PlanBig)(Dagstuhl Seminar 14511),James Cheney; Torsten Grust; Dimitrios Vytiniotis,Abstract This report documents the program and the outcomes of Dagstuhl Seminar 14511"Programming Languages for Big Data (PlanBig)". The seminar was motivated by recentdevelopments in programming languages; databases; machine learning; and cloudcomputing; and particularly by the opportunities offered by research drawing on more thanone of these areas. Participants included researchers working in each of these areas andseveral who have previously been involved in research in the intersection of databases andprogramming languages. The seminar included talks; demos and free time for discussion orcollaboration. This report collects the abstracts of talks and other activities; a summary of thegroup discussions at the seminar; and a list of outcomes.,Dagstuhl Reports,2015,*
Revisiting “forward node-selecting queries over trees”,James Cheney,Abstract In “Forward Node-Selecting Queries over Trees;” Olteanu [2007] gives threerewriting systems for eliminating reverse XPath axis steps from node-selecting queries overtrees; together with arguments for their correctness and termination for a large class of inputgraphs; including cyclic ones. These proofs are valid for tree or acyclic formulas; but two ofthe rewrite systems (TRS 2 and TRS 3) do not terminate on cyclic graphs; that is; there areinfinite rewrite sequences that never yield a normal form. We investigate the reasons whythe termination arguments do not work for general cyclic formulas; and develop alternativealgorithms that can be used instead. We prove that TRS 2 is weakly normalizing; while TRS3 is not weakly normalizing; but it can be extended to a weakly normalizing system TRS 3&cir;. The algorithms and proof techniques illustrate unforeseen subtleties in the handling …,ACM Transactions on Database Systems (TODS),2013,*
Editorial: Special issue dedicated to icfp 2010,Umut Acar; James Cheney; Stephanie Weirich,*,Journal of Functional Programming,2012,*
Higher-Order Unification for the λαν calculus,Ben Kavanagh; James Cheney,Abstract. In this paper we propose an equational theory of a lambda calculus with names;name abstraction; and restriction; and a derived unification algorithm for this theory. Ourcalculus is very closely related to the calculus of Pitts 2011 [3]. We restrict the calculuspresented there to obtain soundness of additional equalities; allowing us to define aunification algorithm very similar to the one given for the lambda calculus. We discuss theproperties of this algorithm and give examples of its utility in meta-programming.,UNIF 2011,2011,*
Interactive Debugging with Traces and Trace Slices,Roly Perera; Umat A Acar; James Cheney,*,*,2011,*
Formalizing adequacy,James Cheney; Rene Vestergaard; Michael Norrish,Abstract Adequacy is an important criterion for judging the correctness of formal reasoning.The issue is particularly subtle in the expansive case of approaches to languages with name-binding. We posit that adequacy of a novel representation technique is best addressed byformalizing an isomorphism or; more generally; an interpretation explicating the newapproach in terms of a more conventional one. We present an example formalization of anisomorphism relating nominal and higher-order abstract syntax techniques. We also outlinesteps towards a systematic framework that could be used for proving adequacy resultsautomatically; which we believe would help make representation techniques moretransparent to end-users of mechanized metatheory verification systems; and provide insightinto the relative merits of different approaches.,TAASN,2009,*
TOIS reviewers January 2006 through May 2007,Gary Marchionini; Ahmed Abbasi; Eugene Agichtein; Khurshid Ahmad; Azzah Al-Maskari; Gianni Amati; Sihem Amer Yahia; Shlomo Argamon; Daniel Ashbrook; Paolo Atzeni; Michela Bacchin; Godmar Back; Antonio Badia; Andras Banczur; Bettina Berendt; Elisa Bertino; B Bhagyavati; Suresh Bhavnani; Devdutta Bhosale; David Bodoff; Paolo Boldi; Johan Bollen; Angela Bonifati; Pia Borlund; Jit Bose; Athman Bouguettaya; Michael Brinkmeier; Peter Brown; Peter Brusilovsky; Peter Bruza; Christopher Burges; Robin Burke; Ben Carterette; Arthur Cater; Kuiyu Chang; Hsin Hsi Chen; Zheng Chen; James Cheney; Pu Jen Cheng; Roger Chiang; Byron Choi; Tat Seng Chua; Charlie Clarke; Paul Clough; Mariano Consens; Gordon Cormack; Nick Craswell; Fabio Crestani; Carolyn Crouch; Silviu Petru Cucerzan; Hang Cui; Sally Jo Cunningham; Edward Cutrell; Pablo De La Fuente; Arjen De Vries; Anne Diekema; Sandor Dominich; Shyamala Doraisamy; Mark Dunlop; Georges Dupret; Miles Efron; Jeremy Ellman; Peter Enser; Gunes Erkan; Laura Fochtmann; Anders Fongen; Nigel Ford; Martin Franz; Xin Fu; Paolo Garza; Susan Gauch; Pierre Geneves; Henry Gladney; Melanie Gnasa; Andrew Goldberg; Marcos Goncalves; Cyril Goutte; David Grossman; Dennis Groth; Jacek Gwizdka; Stephanie Haas; Sanda Harabagiu; Donna Harman; Andreas Henrich; Djoerd Hiemstra; Lee Hollaar; Chun Nan Hsu; Fei Huang; Zan Huang; Mike Huhns; Carlos Hurtado; Keisuke Innoue; Panagiotis Ipeirotis; Bernard Jansen; Wang Jianqiang; Rong Jin; Marko Junkkari; Patrick Juola; Vinay Kakade; Jaap Kamps; In Ho Kang; Damianos Karakos; Vangelis Karkaletsis; Martin Kaszkiel; Siddharth Kaza; Jaana Kekäläinen; Diane Kelly; Benny Kimelfeld; Alek Kolcz; Joseph Konstan; Kui Lam Kwok; Abhimanyu Lad; Alberto Laender; Mounia Lalmas; Leah Larkey; Ray Larson; Nabil Layaida; Zhang Le; Dik Lun Lee; Dongwon Lee; Jochen Leidner; Gina Levow; Hang Li; Xin Li; Chin Yew Lin; Jimmy Lin; Tie Yan Liu; Zehua Liu; David Losada; Jie Lu; Yiming Ma; Inderjeet Mani; Murali Mani; Ioana Manolescu; Catherine Marshall; Mercedes Martinez; Yosi Mass; Paul McNamee; Sean McNee; Brahim Medjahed; Lokman Meho; Donald Metzler; Rada Mihalcea; Ruslan Mitkov; Bamshad Mobasher; Marina Mongiello; Ani Nenkova; Frank Neven; Dorbin Ng; Wilfred Ng,Marchionini; Gary; Abbasi; Ahmed; Agichtein; Eugene; Ahmad; Khurshid; Al-Maskari; Azzah;Amati; Gianni; Yahia; Sihem Amer; Argamon; Shlomo; Ashbrook; Daniel; Atzeni; Paolo;Bacchin; Michela; Back; Godmar; Badia; Antonio; Banczur; Andras; Berendt; Bettina; Bertino;Elisa; Bhagyavati; B.; Bhavnani; Suresh; Bhosale; Devdutta; Bodoff; David; Boldi; Paolo;Bollen; Johan; Bonifati; Angela; Borlund; Pia; Bose; Jit; Bouguettaya; Athman; Brinkmeier;Michael; Brown; Peter; Brusilovsky; Peter; Bruza; Peter; Burges; Christopher; Burke; Robin;Carterette; Ben; Cater; Arthur; Chang; Kuiyu; Chen; Hsin Hsi; Chen; Zheng; Cheney; James;Cheng; Pu Jen; Chiang; Roger; Choi; Byron; Chua; Tat Seng; Clarke; Charlie; Clough; Paul;Consens; Mariano; Cormack; Gordon; Craswell; Nick; Crestani; Fabio; Crouch … In: ACMTransactions on Information Systems; Vol. 25; No. 4; 15; 01.10.2007.,ACM Transactions on Information Systems,2007,*
A Process Algebra Approach to Provenance,James Cheney,The history expresses the most detailed form of provenance information we are interested in.This can be used to evaluate other approaches wrt accuracy and correctness. Key question:Given a provenance tracking system; what questions about the history can be answered withcertainty (given only the final state and provenance information)?,*,2006,*
A Nominal Logical Framework,James Cheney,Page 1. A Nominal Logical Framework Logic and Semantics Club January 20; 2006 JamesCheney University of Edinburgh A Nominal Logical Framework – p.1/28 Page 2. IntroductionA logical framework is a formal system for defining (and reasoning about) other formal systems.Typically; employs a dependent type theory to represent syntax and judgments of a logic orlanguage Idea goes back to at least de Bruijn's AUTOMATH project (Edinburgh) LF; Calculusof Constructions; and variants have proven very expressive and powerful. Fertile area forinteresting type theories (Linear LF; Ordered LF; Concurrent LF; Inductive Constructions) ANominal Logical Framework – p.2/28 Page 3. Do we really need another LF? Existing LF familyvery expressive; but: 1. Names are “second-class”: difficult to encode judgments based onname-inequality 2. Inductive (meta-)reasoning apparently must be …,*,2006,*
αProlog User’s Guide & Language Reference Version 0.3 DRAFT,James Cheney,Names; binding; and scope are perennial problems in many programming tasks; includingimplementing compilers and interpreters as well as symbolic mathematical tools andtheorem proving systems. Few languages provide any assistance for programming withnames; so programmers must reinvent the wheel every time a new system which makes useof names is built. This is often a tedious and error-prone process; also; the resultingprograms tend to be more difficult to read and analyze. Programming language support forkey data structures is crucial to writing clear; optimizable code in traditional domains such asmatrix computations (via arrays); databases (via records); large-scale interactive systems(via objects); and algebraic symbolic computation (via ML-style algebraic datatypes orProlog-style terms): these features have been standard in high-level programming …,*,2003,*
System Description: Prolog; a Fresh Approach to Logic Programming Modulo-Equivalence,James Cheney; Christian Urban,*,UNIF,2003,*
Cornell University Ithaca; NY 14853,James Cheney,ABSTRACT This paper presents a domain-specific language; XParse; that attempts tocombine the power of tools like lex and yacc; which generate efficient parsers fromdeclarative specifications; with the convenience; safety; and usability of textor XML-processing languages such as Perl or XSLT. XParse is a standalone language whichprovides lex-style regular expression matching and yacc-style LALR (1) parsing. Existingparsing tools such as lex and yacc can be difficult to learn; are usually highly language-dependent; and often vary across languages and platforms. Parsers are therefore difficult tocreate and reuse; so are usually developed on a per-application basis rather than a per-language basis. Unlike traditional; language-dependent parser tools; the semantic actions inXParse denote XML fragments rather than uninterpreted source code. This design …,*,2002,*
Cyclone User's Manual; Version 0.1. 3,Dan Grossman; Greg Morrisett; Trevor Jim; Michael Hicks; Yanling Wang; James Cheney,The current version of this manual should be available at http://www. cs. cornell.edu/projects/cyclone/and http://www. research. att. com/projects/cyclone/. The version heredescribes Cyclone Version 0.1. 3; although minor changes may have occurred before therelease.,*,2001,*
CS 686 Project Report Quantum PDL,James Cheney,Abstract this report; I will rst briey overview the mathematical foundations of quantummechanics and quantum computation. Then I will describe quantum logic; a form of logicdeveloped to describe quantum-mechanical situations for which classical propositional logicis inadequate. It was not clear to me how to make use of quantum logic directly inunderstanding quantum computation but there may be some way to tie work in quantumlogic and quantum computation together. In the remainder of the paper; I will show how tomodify the semantics of Probabilistic PDL (PPDL)[5] to model pure quantum computations.Measurements introduce impure probabilistic behavior; and so we show how to embed purequantum PDL (PQPDL) into probabilistic PDL to get generalized quantum PDL (QPDL).Finally; we argue that various instantiations of QPDL are expressive enough to describe …,*,2001,*
Information theoretic problems in digital library research,James Cheney,*,*,2000,*
Reasoning Constructively about Probability and Programming with Chance,James Cheney,Probability and statistics are the deductive and inductive theories; respectively; of chanceand uncertainty. However; they typically do not have a role in traditional logic and theoremproving. This is unfortunate; because chance is an important concept in computer science.The interaction between computer science and uncertainty typically takes three forms. First;algorithms may make random decisions and find solutions to difficult problems quickly withhigh probability. For example; GSAT is a random SAT solver which typically beatsdeterministic solvers such as the Davis-Putnam procedure. AI techniques such as geneticalgorithms and simulated annealing are also randomized. Some randomized algorithmssuggest efficient exact or approximate deterministic algorithms. Randomized algorithms relyon probability theory to reason about the consequences of generating data which is …,*,1999,*
Shape Analysis and Cache Locality for Recursive Data Structures in Java,James Cheney; Kevin Hamlen,There is already a substantial amount of practical research into optimizing programs withstatic; sequential; array-based data structures. Numerical programs in particular tend tomake heavy use of these data structures. However; not all numerical programs make heavyuse of such structures; nor do many other programs we would like to be able to optimize.Another class of data structures which is widely used but currently difficult to optimize is thatof dynamic; recursive; pointer-based data structures (including lists; trees; graphs; etc.) Inthis project we looked at various approaches to optimizing such data structures; includingshape analysis; cache locality optimization; and containeroriented optimization. Shapeanalysis algorithms attempt to statically determine the invariant properties on the structure or“shape” of a data structure. In cache locality optimization; data structures are organized in …,*,1999,*
Learning Programming Language Parsing Rules,James Cheney,Modern programming languages have some natural language properties; such asscoping/precedence ambiguities regarding arithmetic operators (eg+ vs.*) and word(symbol) sense ambiguities (eg unary vs. binary+). Typically these ambiguities are resolvedaccording to well-known conventions so that programs do not need to contain too manyparentheses. These ambiguities are not necessary in programming languages; but they arecommon because people seem to find languages which allow such ambiguities morereadable. Efficient parsers for such languages are difficult to construct by hand. Usually thisproblem is solved by using parser generation tools (such as yacc [9; 7]) which transform acontext-free grammar in some restricted form (such as LR or LALR; see [1]) into an efficient(linear time) parser. However; these tools are also difficult to use. It is not too difficult to …,*,1999,*
Explicit Auditing,Wilmer Ricciotti; James Cheney,Abstract. The Calculus of Audited Units (CAU) is a typed lambda calculus resulting from acomputational interpretation of Artemov's Justification Logic under the Curry-Howardisomorphism; it extends the simply typed lambda calculus by providing audited types;inhabited by expressions carrying a trail of their past computation history. Unlike most otherauditing techniques; CAU allows the inspection of trails at runtime as a first-class operation;with applications in security; debugging; and transparency of scientific computation. Anefficient implementation of CAU is challenging: not only do the sizes of trails grow rapidly;but they also need to be normalized after every beta reduction. In this paper; we study howto reduce terms more efficiently in an untyped variant of CAU by means of explicitsubstitutions and explicit auditing operations.,*,*,*
Additional services for Journal of Functional Programming,UMUT A ACAR; JAMES CHENEY; STEPHANIE WEIRICH,Citation for published version: Acar; UA; Cheney; J & Weirich; S 2012; 'Editorial: Special issuededicated to ICFP 2010' Journal of Functional Programming; vol 22; pp. 379-381. DOI:10.1017/S0956796812000287 … Link: Link to publication record in Edinburgh Research Explorer… Document Version: Publisher's PDF; also known as Version of record … Publisher RightsStatement: Cambridge Journal Open Access … General rights Copyright for the publicationsmade accessible via the Edinburgh Research Explorer is retained by the author(s) and / or othercopyright owners and it is a condition of accessing these publications that users recognise andabide by the legal requirements associated with these rights … Take down policy The Universityof Edinburgh has made every reasonable effort to ensure that Edinburgh Research Explorercontent complies with UK legislation. If you believe that the public display of this file …,*,*,*
ŦProlog; Programming Modulo Ŧ-Equivalence,James Cheney; Christian Urban,Abstract. ŦProlog is a prototype logic programming language with a built-in notion of bindersand unification modulo Ŧ-equivalence. It is based on a mild extension of firstorder Hornformulae: instead of the usual first-order terms and first-order unification; ŦProlog usesnominal terms and nominal unification introduced in [3]. In this paper; we give threeexamples that demonstrate the advantages of ŦProlog and describe our currentimplementation.,Jordi Levy Michael Kohlhase Joachim Niehren,*,*
Schema-based independence analysis for XML updates. http://web. comlab. ox. ac. uk/people/Michael. Benedikt/papers/tr. pdf,Michael Benedikt; James Cheney,Abstract Query-update independence analysis is the problem of determining whether anupdate affects the results of a query. Query-update independence is useful for avoidingrecomputation of materialized views and may have applications to access control andconcurrency control. This paper develops static analysis techniques for queryupdateindependence problems involving core XQuery queries and updates with a snapshotsemantics (based on the W3C XQuery Update Facility proposal). Our approach takesadvantage of schema information; in contrast to previous work on this problem. We formalizeour approach; sketch a proof of correctness; and report on the performance and accuracy ofour implementation. 1.,*,*,*
Comments from the Digital Curation Centre on Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century; a draft report of the Natio...,Rajendra Bose; Peter Buneman; Peter Burnhill; James Cheney; Floris Geerts; Anastasios Kementsietsidis; Liz Lyon; Mags McGinley; Robin Rice; Chris Rusbridge; Charlotte Waelde; Stratis Viglas,The development of the Internet has caused revolution in the way in which all forms ofscholarship are conducted. The Scientific Method itself is changing as a result of thedependence of all sciences on their accumulations of digital data. The National ScienceBoard report on long-lived digital data collections is a strong confirmation for the need tohave policies; technology and methodologies for caring for–curating–these digitalresources. The Digital Curation Centre (DCC) is an organisation supported by UK researchand university councils; which is dedicated to these issues. The following comments2 on thereport are a summary of discussions among members of the DCC. The DCC welcomes thedraft report. In particular it endorses the view that curation is much more than preservation;: itinvolves all aspects of data acquisition and selection; it is about adding value to and …,*,*,*
Motivation and Vision,James Cheney,Sophisticated systems such as databases and distributed (“grid” or cyberinfrastructure)computing are becoming essential in most scientific disciplines; particularly bioinformatics.As a result; computational artifacts such as databases are now becoming a part of thescientific record; scientific judgments now depend on the correctness; integrity; and securityof such systems. However; as these systems grow more complex; the effort needed to makethem comprehensible; usable; reliable or predictable also increases dramatically. Forexample; while many bioinformatics researchers have expended a great deal of effort onbuilding large curated databases and using them to guide research in molecular biologyand other areas; it is not well-understood how to combine data from different sources; how tocorrect errors in noisy data; how to record the “complete” history of the data; or how to …,*,*,*
found at the ENTCS Macro Home Page. Formalizing adequacy,James Cheney; Rene Vestergaard; Michael Norrish,CiteSeerX - Document Details (Isaac Councill; Lee Giles; PradeepTeregowda): this file with prentcsmacro.sty for your meeting;.,*,*,*
EU Project No: 601043 (Integrated Project (IP)) DIACHRON,James Cheney; Slawek Staworko; Panagiotis Hasapis; Marios Meimaris,Work package (s): 2 Status & version: Draft Number of pages: 44 WP contributing to thedeliverable: 2 WP/Task responsible: WP2; Tasks 2.1; 2.2; 2.3 Coordinator (name/contact):Stratis D. Viglas (sviglas@ inf. ed. ac. uk) Other Contributors: Peter Buneman (opb@ inf. ed.ac. uk); James Cheney (jcheney@ inf. ed. ac. uk); Slawek Staworko (sstawork@ inf. ed. ac.uk) Panagiotis Hasapis (INTRASOFT) Marios Meimaris (ATHENA) EC Project Officer:Federico Milani,*,*,*
Run your Research; Mind the Binders,James Cheney; Alberto Momigliano; Matteo Pessina,Universita degli Studi di Milano matteo. pessina3@ studenti. di. unimi. it αCheck is a light-weight property-based testing tool built on top of αProlog. Being based on nominal logicprogramming; it is particularly suited to the validation of the meta-theory of formal systems.To substantiate this claim we compare its performances wrt its main competitors in thelogical framework niche; namely the QuickCheck/Nitpick combination offered byIsabelle/HOL and the random testing facility in PLT-Redex. We briefly sketch the architectureof αCheck and mention some future directions for the tool and for benchmarkingmechanized meta-theory model-checking systems more generally.,*,*,*
Bidirectional Transformations with Effects,Faris Abou-Saleh; James Cheney; Jeremy Gibbons; James McKinna; Perdita Stevens,*,*,*,*
JOURNAL OF FUNCTIONAL PR,UMUT A ACAR; JAMES CHENEY; STEPHANIE WEIRICH; LARS BERGSTROM; MATTHEW FLUET; MIKE RAINEY; JOHN REPPY; ADAM SHAW; RYAN CULPEPPER; DEREK DREYER; GEORG NEIS; LARS BIRKEDAL; ANDREW J KENNEDY; DIMITRIOS VYTINIOTIS; MATTHEW NAYLOR; COLIN RUNCIMAN; NICOLAS POUILLARD; FRANÇOIS POTTIER; DAVID VAN HORN; MATTHEW MIGHT,INSTRUCTIONS TO AUTHORS Scope Papers may describe original technical work; survey anarea; or present a tutorial ; and may be either short or long. Anything related to functional programmingis of interest; including: foundations (semantics; abstract interpretation; lambda calculi;rewriting; logic; type theory; category theory); implementation (compilation; architectures;parallelism; garbage collection; I/O; debugging; profiling); linguistics (pure and impure languagefeatures; non-determinism; side effects; logical variables; relation to other programmingparadigms; proofs about programs; program transformation; program synthesis; partialevaluation); applications (applications programs; practical experience; programmingtechniques; prototyping). Book Reviews Books for review; or suggestions for reviews; shouldbe sent to the reviews editor; Simon Thompson (address on inside front cover) …,*,*,*
Language-integrated query using comprehension syntax: state of the art; open problems; and work in progress,James Cheney; Sam Lindley; Philip Wadler,Abstract Comprehension syntax has proved to be a powerful tool for embedding querylanguage features into strongly-typed functional languages. This work may also beapplicable to other programming models (data-parallel; GPU; MapReduce) and deserves tobe betterknown to the data-centric programming community. This talk will give a technicaloverview of the highlights in the development of monadic comprehensions; particularlyfocusing on language designs; theory; and systems; identify current open problems; anddiscuss some current work.,*,*,*
Provenance for configuration language security,James Cheney; Paul Anderson; Dimitrios Vytiniotis,1. Abstract Declarative; high-level configuration languages (eg LCFG; Puppet; Chef) arewidely used in industry to configure large system installations. Configurations are oftencomposed from distributed source files managed by many different users within differentsystem and organisational boundaries. Users may make changes whose consequences arenot easy to understand; and such systems also currently lack mature security accesscontrols; the few currently available techniques have idiosyncratic behaviour and offer noformal guarantees. In the worst case; misconfiguration can lead to costly system failures;because of the complexity of the configuration build; it is difficult to recover from failures;trace the source of the error or identify the responsible party. In this project; we will explorethe application of provenance techniques (originally developed in the context of …,*,*,*
3.13 Database Wiki and provenance for SPARQL updates,James Cheney,Databases and wikis have complementary strengths and weaknesses for use incollaborative data management and data curation. Relational databases; for example; offeradvantages such as scalability; query optimization and concurrency control; but are not easyto use and lack other features needed for collaboration. Wikis have proved enormouslysuccessful as a means to collaborate because they are easy to use; encourage sharing; andprovide built-in support for archiving; history-tracking and annotation. However; wikis lacksupport for structured data; efficiently querying data at scale; and localized provenance andannotation. To achieve the best of both worlds; we are developing a general-purposeplatform for collaborative data management; called DBWIKI. Our system not only facilitatesthe collaborative creation of structured data; it also provides features not usually provided …,Semantic Data Management,*,*
Shredding higher-order nested queries,Sam Lindley; James Cheney; Philip Wadler,ABSTRACT We present a modular account of query shredding; the simulation of a singlenested relational query by a number of flat relational queries; applicable to both set andmultiset semantics. Our key insight is that shredding can be greatly simplified by firstrewriting the input query into a canonical normal form. Normalisation allows us to defineshredding translations on types and terms independently of one another; unlike previouswork. An added benefit of normalisation is that we support higher-order terms for free;provided that the result type is a plain nested relation type (without higher ordercomponents). In order to generate SQL we consider several alternatives for generatingindexes; focusing on a lightweight use of SQL OLAP features.,*,*,*
SIGACT News Logic Column 14,James Cheney,A great deal of research in programming languages; type theory; and security is based onproving properties such as strong normalization; type soundness or noninterference byinduction or coinduction on the structure of typing derivations; operational semantics rules;or other syntactic constructs. Such proofs are essentially combinatorial in nature; usuallyinvolving O (np) cases; where n is the number of syntactic constructs; typing rules;operational transitions; etc.; and p is small. Usually; only a small number of cases are“interesting”; and published proofs often give only a few illustrative cases. This provides littleassurance that a proof is correct. It is widely felt that machine assistance for constructingsuch proofs is desirable [60; 3]. Providing such assistance is severely complicated by theproblem of dealing with names and binding in abstract syntax. Logicians since Frege …,arXiv preprint cs/0511025,*,*
3.3 Tutorial: Software Engineering; Programming Languages and Security Perspectives,Perdita Stevens; Steve Chong; James Cheney,This tutorial touched upon three distinct themes: provenance in software engineering(presented by Perdita Stevens); provenance in programming languages (presented byJames Cheney); and provenance and security (presented by Steve Chong). From theearliest days of software engineering; practitioners have been concerned to trace theconnections between the requirements that a software system must satisfy and the tests thatestablish that requirements have been met. This is termed traceability; and the same term isthen used much more broadly in software engineering could be called provenance.Traceability is typically recorded as a so-called requirements traceability matrix; which isformally a binary relation on Requirements and Tests. Even with the best availablecommercial tool support; maintaining traceability information is a time-consuming partly …,Principles of Provenance,*,*
Shredding higher-order nested queries,James Cheney; Sam Lindley; Philip Wadler,Abstract Reconciling high-level functional programming abstractions with the capabilities ofdatabases is a major challenge. We present a modular account of shredding; the simulationof a single nested relational query by a number of flat relational queries. Our key insight isthat shredding can be greatly simplified by first rewriting the input query into a canonicalnormal form. Normalisation allows us to define shredding translations on types and termsindependently of one another. An added benefit of normalisation is that we get higher-orderterms for free; provided that the result type is a plain nested relation type (without higherorder components). In order to generate SQL we consider several alternatives for generatingindexes; focusing on a lightweight use of SQL OLAP features. We prove correctness of ourtranslations; focusing on the central shredding step: shredding a nested query; running …,*,*,*
Provenance in Manually Curated Databases,Peter Buneman; Adriane Chapman; James Cheney; Stijn Vansummeren,Abstract. Many curated databases are constructed by scientists integrating various existingdata sources. Most current approaches to provenance in databases are based on views andfail to take account of the added value of the work done by scientists in manually creatingand modifying data. Capturing provenance in such an environment is a challengingproblem; requiring changes in practice; changes to existing software; and crucially; a goodmodel of the process of curation.,*,*,*
Principles of Provenance Theme Proposal,Peter Buneman; James Cheney; Bertram Ludaescher,Theme topic and brief description Recent research in a variety of settings (databases anddata warehouses [10; 7; 20; 8]; file systems [17]; geographic information systems [6];scientific workflows and grid computation [19; 13]; archiving and digital curation [1]; and theSemantic Web [14]) has considered the problem of keeping track of metadata about creationand modification history; influences; ownership; and other provenance or lineageinformation. Such metadata is essential for making informed judgements about data quality;integrity; and authenticity. In addition; ideas about provenance are now being used inseveral other areas of computer science such as probabilistic databases [12]; adaptivecomputation [4]; security [15; 3] file synchronisation [11]; and annotation propagation [5].Other topics; such as version control and archiving; may also benefit from better …,*,*,*
Work in Progress: Logic Programming with Names and Binding,James Cheney; Christian Urban,Abstract. In this paper we describe work in progress on αProlog; a logic programminglanguage with a built-in notion of names; binding and unification modulo α-equivalence.αProlog is based on a mild extension of first-order Horn formulae: instead of the usual first-order terms and first-order unification; it uses nominal terms and nominal unificationintroduced in [3]. Note: This paper is a revised version of [1].,*,*,*
Real DTD-Conscious Compression,James Cheney,*,*,*,*
Mechanized metatheory: ready for prime time?,James Cheney,The POPLMark challenge has inspired; and workshops such as WMM and LFMTP havedocumented; great progress in mechanizing proofs of properties of programming languages.There is growing evidence that several current techniques are adequate for ensuring thecorrectness of syntactic proofs of results in papers in top conferences and journals; orverifying compilers for existing languages. These are excellent goals for mechanizedmetatheory research. Nevertheless; we should aim higher. Is mechanized metatheory readyfor prime time? Can it be used to inform the design of new languages; rather than forpostmortem analysis of existing ones? I think the answer is still no. I suggest that thecommunity of researchers interested in mechanized metatheory consider targeting somenew languages that are interesting to people outside this community; and trying to use …,*,*,*
Vision and career plans,James Cheney,*,*,*,*
dbWiki–A Wiki for Structured Data,Peter Buneman; James Cheney; Heiko Müller,Page 1. Background dbWiki – A Wiki for Structured Data Peter Buneman; James Cheney; HeikoMüller; University of Edinburgh Sam Lindley; Contemplate Ltd. Citation Versioning ProvenanceModification and Views Annotation Q Q' S KEY /COUNTRY BY VALUES (NAME) KEY/COUNTRY/NAME BY EXISTENCE EXAMPLE <CIAWFB> <COUNTRY> <CATEGORY> <NAME><PROPERTY> <NAME> <NAME> People 11;394;043 Population Cuba <TEXT> 11;423;952<CIAWFB> <COUNTRY> <CATEGORY> <NAME> <PROPERTY> <NAME> <NAME> People11;394;043 Population Cuba <TEXT> 2007 <CIAWFB> <COUNTRY> <CATEGORY> <NAME><PROPERTY> <NAME> <NAME> People 11;423;952 Population Cuba <TEXT> 2008 2007-20082007 2008 Sharing and publishing data is one of the most important activities of modern science.Well-known examples include Wikipedia; a vast store of mostly …,*,*,*
A safe data analysis environment for astronomical data centres,James Cheney; Bob Mann,The volume of data available to astronomers via the Internet is doubling every twelvemonths or so. This is due both to the increase in the number of data sources (astronomy isan active field; while archives of old observations retain scientific value) and in the size ofindividual data sources (multi-TB datasets are the norm in the current generation of skysurveys). The new sky surveys are generally motivated scientifically by large-scale statisticalanalyses; such as searches for rare objects (eg high-redshift quasars or brown dwarfs);measurements of the clustering strength of galaxies or the production of summaries of theproperties of a particular source population. In many cases; these analyses may require theintegration of data from several sources (which is the motivation for the creation of the VirtualObservatory1); but; even when an analysis uses data from only one source; it will often …,*,*,*
Building Core Ontologies,Carl Lagoze; ILRT Dan Brickley; UK Donatella Castelli; James Cheney; CNR Nicola Guarino; Italy Stephan Koernig; CNR Carlo Meghini; Italy Wolfgang Meier; ILRT Libby Miller; UK Nikolay A Skvortsov,*,*,*,*
